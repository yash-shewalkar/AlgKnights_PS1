






     Contributing to Spark | Apache Spark
    
  























Download



          Libraries
        

SQL and DataFrames
Spark Connect
Spark Streaming
pandas on Spark
MLlib (machine learning)
GraphX (graph)



Third-Party Projects




          Documentation
        

Latest Release
Older Versions and Other Resources
Frequently Asked Questions



Examples



          Community
        

Mailing Lists & Resources
Contributing to Spark
Improvement Proposals (SPIP)

Issue Tracker

Powered By
Project Committers
Project History




          Developers
        

Useful Developer Tools
Versioning Policy
Release Process
Security




          GitHub
        

spark
spark-connect-go
spark-connect-swift
spark-docker
spark-kubernetes-operator
spark-website






          Apache Software Foundation
        

Apache Homepage
License
Sponsorship
Thanks
Security
Event








This guide documents the best way to make various types of contribution to Apache Spark, 
including what is required before submitting a code change.
Contributing to Spark doesn’t just mean writing code. Helping new users on the mailing list, 
testing releases, and improving documentation are also welcome. In fact, proposing significant 
code changes usually requires first gaining experience and credibility within the community by 
helping in other ways. This is also a guide to becoming an effective contributor.
So, this guide organizes contributions in order that they should probably be considered by new 
contributors who intend to get involved long-term. Build some track record of helping others, 
rather than just open pull requests.
Contributing by helping other users
A great way to contribute to Spark is to help answer user questions on the user@spark.apache.org 
mailing list or on StackOverflow. There are always many new Spark users; taking a few minutes to 
help answer a question is a very valuable community service.
Contributors should subscribe to this list and follow it in order to keep up to date on what’s 
happening in Spark. Answering questions is an excellent and visible way to help the community, 
which also demonstrates your expertise.
See the Mailing Lists guide for guidelines 
about how to effectively participate in discussions on the mailing list, as well as forums 
like StackOverflow.
Contributing by testing releases
Spark’s release process is community-oriented, and members of the community can vote on new 
releases on the dev@spark.apache.org mailing list. Spark users are invited to subscribe to 
this list to receive announcements, and test their workloads on newer release and provide 
feedback on any performance or correctness issues found in the newer release.
Contributing by reviewing changes
Changes to Spark source code are proposed, reviewed and committed via 
GitHub pull requests (described later). 
Anyone can view and comment on active changes here. 
Reviewing others’ changes is a good way to learn how the change process works and gain exposure 
to activity in various parts of the code. You can help by reviewing the changes and asking 
questions or pointing out issues – as simple as typos or small issues of style.
See also https://spark-prs.appspot.com/ for a
convenient way to view and filter open PRs.
Contributing documentation changes
To propose a change to release documentation (that is, docs that appear under 
https://spark.apache.org/docs/), 
edit the Markdown source files in Spark’s 
docs/ directory, 
whose README file shows how to build the documentation locally to test your changes.
The process to propose a doc change is otherwise the same as the process for proposing code 
changes below.
To propose a change to the rest of the documentation (that is, docs that do not appear under 
https://spark.apache.org/docs/), similarly, edit the Markdown in the 
spark-website repository and open a pull request.
Contributing user libraries to Spark
Just as Java and Scala applications can access a huge selection of libraries and utilities, 
none of which are part of Java or Scala themselves, Spark aims to support a rich ecosystem of 
libraries. Many new useful utilities or features belong outside of Spark rather than in the core. 
For example: language support probably has to be a part of core Spark, but, useful machine 
learning algorithms can happily exist outside of MLlib.
To that end, large and independent new functionality is often rejected for inclusion in Spark 
itself, but, can and should be hosted as a separate project and repository, and included in 
the spark-packages.org collection.
Contributing bug reports
Ideally, bug reports are accompanied by a proposed code change to fix the bug. This isn’t 
always possible, as those who discover a bug may not have the experience to fix it. A bug 
may be reported by creating a JIRA but without creating a pull request (see below).
Bug reports are only useful however if they include enough information to understand, isolate 
and ideally reproduce the bug. Simply encountering an error does not mean a bug should be 
reported; as below, search JIRA and search and inquire on the Spark user / dev mailing lists 
first. Unreproducible bugs, or simple error reports, may be closed.
It’s very helpful if the bug report has a description about how the bug was introduced, by 
which commit, so that reviewers can easily understand the bug. It also helps committers to 
decide how far the bug fix should be backported, when the pull request is merged. The pull 
request to fix the bug should narrow down the problem to the root cause.
Performance regression is also one kind of bug. The pull request to fix a performance regression 
must provide a benchmark to prove the problem is indeed fixed.
Note that, data correctness/data loss bugs are very serious. Make sure the corresponding bug 
report JIRA ticket is labeled as correctness or data-loss. If the bug report doesn’t get 
enough attention, please send an email to dev@spark.apache.org, to draw more attentions.
It is possible to propose new features as well. These are generally not helpful unless 
accompanied by detail, such as a design document and/or code change. Large new contributions 
should consider spark-packages.org first (see above), 
or be discussed on the mailing 
list first. Feature requests may be rejected, or closed after a long period of inactivity.
Contributing to JIRA maintenance
Given the sheer volume of issues raised in the Apache Spark JIRA, inevitably some issues are 
duplicates, or become obsolete and eventually fixed otherwise, or can’t be reproduced, or could 
benefit from more detail, and so on. It’s useful to help identify these issues and resolve them, 
either by advancing the discussion or even resolving the JIRA. Most contributors are able to 
directly resolve JIRAs. Use judgment in determining whether you are quite confident the issue 
should be resolved, although changes can be easily undone. If in doubt, just leave a comment 
on the JIRA.
When resolving JIRAs, observe a few useful conventions:

Resolve as Fixed if there’s a change you can point to that resolved the issue
    
Set Fix Version(s), if and only if the resolution is Fixed
Set Assignee to the person who most contributed to the resolution, which is usually the person 
who opened the PR that resolved the issue.
In case several people contributed, prefer to assign to the more ‘junior’, non-committer contributor


For issues that can’t be reproduced against master as reported, resolve as Cannot Reproduce

Fixed is reasonable too, if it’s clear what other previous pull request resolved it. Link to it.


If the issue is the same as or a subset of another issue, resolved as Duplicate

Make sure to link to the JIRA it duplicates
Prefer to resolve the issue that has less activity or discussion as the duplicate


If the issue seems clearly obsolete and applies to issues or components that have changed 
radically since it was opened, resolve as Not a Problem
If the issue doesn’t make sense – not actionable, for example, a non-Spark issue, resolve 
as Invalid
If it’s a coherent issue, but there is a clear indication that there is not support or interest 
in acting on it, then resolve as Won’t Fix
Umbrellas are frequently marked Done if they are just container issues that don’t correspond 
to an actionable change of their own

Preparing to contribute code changes
Choosing what to contribute
Spark is an exceptionally busy project, with a new JIRA or pull request every few hours on average. 
Review can take hours or days of committer time. Everyone benefits if contributors focus on 
changes that are useful, clear, easy to evaluate, and already pass basic checks.
Sometimes, a contributor will already have a particular new change or bug in mind. If seeking 
ideas, consult the list of starter tasks in JIRA, or ask the user@spark.apache.org mailing list.
Before proceeding, contributors should evaluate if the proposed change is likely to be relevant, 
new and actionable:

Is it clear that code must change? Proposing a JIRA and pull request is appropriate only when a 
clear problem or change has been identified. If simply having trouble using Spark, use the mailing 
lists first, rather than consider filing a JIRA or proposing a change. When in doubt, email 
user@spark.apache.org first about the possible change
Search the user@spark.apache.org and dev@spark.apache.org mailing list 
archives for 
related discussions.
Often, the problem has been discussed before, with a resolution that doesn’t require a code 
change, or recording what kinds of changes will not be accepted as a resolution.
Search JIRA for existing issues: 
https://issues.apache.org/jira/browse/SPARK
Type spark [search terms] at the top right search box. If a logically similar issue already 
exists, then contribute to the discussion on the existing JIRA and pull request first, instead of 
creating a new one.
Is the scope of the change matched to the contributor’s level of experience? Anyone is qualified 
to suggest a typo fix, but refactoring core scheduling logic requires much more understanding of 
Spark. Some changes require building up experience first (see above).

It’s worth reemphasizing that changes to the core of Spark, or to highly complex and important modules
like SQL and Catalyst, are more difficult to make correctly. They will be subjected to more scrutiny,
and held to a higher standard of review than changes to less critical code.
MLlib-specific contribution guidelines
While a rich set of algorithms is an important goal for MLLib, scaling the project requires 
that maintainability, consistency, and code quality come first. New algorithms should:

Be widely known
Be used and accepted (academic citations and concrete use cases can help justify this)
Be highly scalable
Be well documented
Have APIs consistent with other algorithms in MLLib that accomplish the same thing
Come with a reasonable expectation of developer support.
Have @Since annotation on public classes, methods, and variables.

Error message guidelines
Exceptions thrown in Spark should be associated with standardized and actionable
error messages.
Error messages should answer the following questions:

What was the problem?
Why did the problem happen?
How can the problem be solved?

When writing error messages, you should:

Use active voice
Avoid time-based statements, such as promises of future support
Use the present tense to describe the error and provide suggestions
Provide concrete examples if the resolution is unclear
Avoid sounding accusatory, judgmental, or insulting
Be direct
Do not use programming jargon in user-facing errors

See the error message guidelines for more details.
Behavior changes
Behavior changes are user-visible functional changes in a new release via public APIs. The term ‘user’ here refers
not only to those who write queries and/or develop Spark plugins, but also to those who deploy and/or manage Spark
clusters. New features and bug fixes, such as correcting query results or schemas and failing unsupported queries
that previously returned incorrect results, are considered behavior changes. However, performance improvements,
code refactoring, and changes to unreleased APIs/features are not.
Everyone makes mistakes, including Spark developers. We will continue to fix defects in Spark as they arise.
However, it is important to communicate these behavior changes so that Spark users can be prepared for version
upgrades. If a PR introduces behavior changes, it should be explicitly mentioned in the PR description. If the
behavior change may require additional user actions, this should be highlighted in the migration guide
(docs/sql-migration-guide.md for the SQL component and similar files for other components). Where possible,
provide options to restore the previous behavior and mention these options in the error message. Some examples include:

Bug fixes that change query results. Users may need to backfill to correct existing data and must be informed about
these correctness fixes.
Bug fixes that change the query schema. Users may need to update the schema of tables in their data pipelines and must
be informed about these changes.
Removing or renaming Spark configurations.
Renaming error classes or conditions.
Any non-additive changes to the public Python/SQL/Scala/Java/R APIs (including developer APIs), such as renaming
functions, removing parameters, adding parameters, renaming parameters, or changing parameter default values. These
changes should generally be avoided, or if necessary, done in a binary-compatible manner by deprecating the old function
and introducing a new one instead.
Any non-additive changes to the way Spark should be deployed and managed: renaming argument names in deployment scripts,
updates to the REST API, changes to the method of loading configuration files, etc.

This list is not meant to be comprehensive. Anyone reviewing a PR can ask the PR author to add to the migration guide
if they believe the change is risky and may disrupt users during an upgrade.
Code review criteria
Before considering how to contribute code, it’s useful to understand how code is reviewed, 
and why changes may be rejected. See the 
detailed guide for code reviewers 
from Google’s Engineering Practices documentation. 
Simply put, changes that have many or large 
positives, and few negative effects or risks, are much more likely to be merged, and merged quickly. 
Risky and less valuable changes are very unlikely to be merged, and may be rejected outright 
rather than receive iterations of review.
Positives

Fixes the root cause of a bug in existing functionality
Adds functionality or fixes a problem needed by a large number of users
Simple, targeted
Maintains or improves consistency across Python, Java, Scala
Easily tested; has tests
Reduces complexity and lines of code
Change has already been discussed and is known to committers

Negatives, risks

Band-aids a symptom of a bug only
Introduces complex new functionality, especially an API that needs to be supported
Adds complexity that only helps a niche use case
Adds user-space functionality that does not need to be maintained in Spark, but could be hosted 
externally and indexed by spark-packages.org
Changes a public API or semantics (rarely allowed)
Adds large dependencies
Changes versions of existing dependencies
Adds a large amount of code
Makes lots of modifications in one “big bang” change

Contributing code changes
Please review the preceding section before proposing a code change. This section documents how to do so.
When you contribute code, you affirm that the contribution is your original work and that you 
license the work to the project under the project’s open source license. Whether or not you state 
this explicitly, by submitting any copyrighted material via pull request, email, or other means 
you agree to license the material under the project’s open source license and warrant that you 
have the legal authority to do so.
Cloning the Apache Spark™ source code
If you are interested in working with the newest under-development code or contributing to Apache Spark development, you can check out the master branch from Git:
# Master development branch
git clone git://github.com/apache/spark.git

Once you’ve downloaded Spark, you can find instructions for installing and building it on the documentation page.
JIRA
Generally, Spark uses JIRA to track logical issues, including bugs and improvements, and uses 
GitHub pull requests to manage the review and merge of specific code changes. That is, JIRAs are 
used to describe what should be fixed or changed, and high-level approaches, and pull requests 
describe how to implement that change in the project’s source code. For example, major design 
decisions are discussed in JIRA.

Find the existing Spark JIRA that the change pertains to.
    
Do not create a new JIRA if creating a change to address an existing issue in JIRA; add to 
 the existing discussion and work instead
Look for existing pull requests that are linked from the JIRA, to understand if someone is 
 already working on the JIRA


If the change is new, then it usually needs a new JIRA. However, trivial changes, where the
what should change is virtually the same as the how it should change do not require a JIRA. 
Example: Fix typos in Foo scaladoc
If required, create a new JIRA:
    
Provide a descriptive Title. “Update web UI” or “Problem in scheduler” is not sufficient.
 “Kafka Streaming support fails to handle empty queue in YARN cluster mode” is good.
Write a detailed Description. For bug reports, this should ideally include a short 
 reproduction of the problem. For new features, it may include a design document.
Set required fields:
        
Issue Type. Generally, Bug, Improvement and New Feature are the only types used in Spark.
Priority. Set to Major or below; higher priorities are generally reserved for 
 committers to set. The main exception is correctness or data-loss issues, which can be flagged as
 Blockers. JIRA tends to unfortunately conflate “size” and “importance” in its 
 Priority field values. Their meaning is roughly:
            
Blocker: pointless to release without this change as the release would be unusable 
  to a large minority of users. Correctness and data loss issues should be considered Blockers for their target versions.
Critical: a large minority of users are missing important functionality without 
  this, and/or a workaround is difficult
Major: a small minority of users are missing important functionality without this, 
  and there is a workaround
Minor: a niche use case is missing some support, but it does not affect usage or 
  is easily worked around
Trivial: a nice-to-have change but unlikely to be any problem in practice otherwise


Component
Affects Version. For Bugs, assign at least one version that is known to exhibit the 
 problem or need the change
Label. Not widely used, except for the following:
            
correctness: a correctness issue
data-loss: a data loss issue
release-notes: the change’s effects need mention in release notes. The JIRA or pull request
  should include detail suitable for inclusion in release notes – see “Docs Text” below.
starter: small, simple change suitable for new contributors


Docs Text: For issues that require an entry in the release notes, this should contain the
 information that the release manager should include in Release Notes. This should include a short summary
 of what behavior is impacted, and detail on what behavior changed. It can be provisionally filled out
 when the JIRA is opened, but will likely need to be updated with final details when the issue is
 resolved.


Do not set the following fields:
        
Fix Version. This is assigned by committers only when resolved.
Target Version. This is assigned by committers to indicate a PR has been accepted for 
 possible fix by the target version.


Do not include a patch file; pull requests are used to propose the actual change.


If the change is a large change, consider inviting discussion on the issue at 
dev@spark.apache.org first before proceeding to implement the change.

Pull request
Before creating a pull request in Apache Spark, it is important to check if tests can pass on your branch because 
our GitHub Actions workflows automatically run tests for your pull request/following commits 
and every run burdens the limited resources of GitHub Actions in Apache Spark repository.
Below steps will take your through the process.

Fork the GitHub repository at 
https://github.com/apache/spark if you haven’t already
Go to “Actions” tab on your forked repository and enable “Build and test” and “Report test results” workflows
Clone your fork and create a new branch
Consider whether documentation or tests need to be added or updated as part of the change, 
and add them as needed.
    
When you add tests, make sure the tests are self-descriptive.
Also, you should consider writing a JIRA ID in the tests when your pull request targets to fix
   a specific issue. In practice, usually it is added when a JIRA type is a bug or a PR adds
   a couple of tests to an existing test class. See the examples below:
        
Scala
            test("SPARK-12345: a short description of the test") {
  ...
 

Java
            @Test
public void testCase() {
  // SPARK-12345: a short description of the test
  ...
 

Python
            def test_case(self):
    # SPARK-12345: a short description of the test
    ...
 

R
            test_that("SPARK-12345: a short description of the test", {
  ...
 





Consider whether benchmark results should be added or updated as part of the change, and add them as needed by
Running benchmarks in your forked repository
to generate benchmark results.
Run all tests with ./dev/run-tests to verify that the code still compiles, passes tests, and 
passes style checks. 
If style checks fail, review the Code Style Guide below.
Push commits to your branch. This will trigger “Build and test” and “Report test results” workflows 
on your forked repository and start testing and validating your changes.
Open a pull request against 
the master branch of apache/spark. (Only in special cases would the PR be opened against other branches). This 
will trigger workflows “On pull request*” (on Spark repo) that will look/watch for successful workflow runs on “your” forked repository (it will wait if one is running).
    
The PR title should be of the form [SPARK-xxxx][COMPONENT] Title, where SPARK-xxxx is 
  the relevant JIRA number, COMPONENT is one of the PR categories shown at 
  spark-prs.appspot.com and 
  Title may be the JIRA’s title or a more specific title describing the PR itself.
If the pull request is still a work in progress, and so is not ready to be merged, 
  but needs to be pushed to GitHub to facilitate review, then add [WIP] after the component.
Consider identifying committers or other contributors who have worked on the code being 
  changed. Find the file(s) in GitHub and click “Blame” to see a line-by-line annotation of 
  who changed the code last. You can add @username in the PR description to ping them 
  immediately.
Please state that the contribution is your original work and that you license the work 
  to the project under the project’s open source license.


The related JIRA, if any, will be marked as “In Progress” and your pull request will 
automatically be linked to it. There is no need to be the Assignee of the JIRA to work on it, 
though you are welcome to comment that you have begun work.
If there is a change related to SparkR in your pull request, AppVeyor will be triggered
automatically to test SparkR on Windows, which takes roughly an hour. Similarly to the steps
above, fix failures and push new commits which will request the re-test in AppVeyor.

The review process

Other reviewers, including committers, may comment on the changes and suggest modifications. 
Changes can be added by simply pushing more commits to the same branch.
Lively, polite, rapid technical debate is encouraged from everyone in the community. The outcome 
may be a rejection of the entire change.
Keep in mind that changes to more critical parts of Spark, like its core and SQL components, will
be subjected to more review, and may require more testing and proof of its correctness than
other changes.
Reviewers can indicate that a change looks suitable for merging with a comment such as: “I think 
this patch looks good”. Spark uses the LGTM convention for indicating the strongest level of 
technical sign-off on a patch: simply comment with the word “LGTM”. It specifically means: “I’ve 
looked at this thoroughly and take as much ownership as if I wrote the patch myself”. If you 
comment LGTM you will be expected to help with bugs or follow-up issues on the patch. Consistent, 
judicious use of LGTMs is a great way to gain credibility as a reviewer with the broader community.
Sometimes, other changes will be merged which conflict with your pull request’s changes. The 
PR can’t be merged until the conflict is resolved. This can be resolved by, for example, adding a remote
to keep up with upstream changes by git remote add upstream https://github.com/apache/spark.git,
running git fetch upstream followed by git rebase upstream/master and resolving the conflicts by hand,
then pushing the result to your branch.
Try to be responsive to the discussion rather than let days pass between replies

Closing your pull request / JIRA

If a change is accepted, it will be merged and the pull request will automatically be closed, 
along with the associated JIRA if any
    
Note that in the rare case you are asked to open a pull request against a branch besides 
master, that you will actually have to close the pull request manually
The JIRA will be Assigned to the primary contributor to the change as a way of giving credit. 
If the JIRA isn’t closed and/or Assigned promptly, comment on the JIRA.


If your pull request is ultimately rejected, please close it promptly
    
… because committers can’t close PRs directly
Pull requests will be automatically closed by an automated process at Apache after about a 
week if a committer has made a comment like “mind closing this PR?” This means that the 
committer is specifically requesting that it be closed.


If a pull request has gotten little or no attention, consider improving the description or 
the change itself and ping likely reviewers again after a few days. Consider proposing a 
change that’s easier to include, like a smaller and/or less invasive change.
If it has been reviewed but not taken up after weeks, after soliciting review from the 
most relevant reviewers, or, has met with neutral reactions, the outcome may be considered a 
“soft no”. It is helpful to withdraw and close the PR in this case.
If a pull request is closed because it is deemed not the right approach to resolve a JIRA, 
then leave the JIRA open. However if the review makes it clear that the issue identified in 
the JIRA is not going to be resolved by any pull request (not a problem, won’t fix) then also 
resolve the JIRA.


Code style guide
Please follow the style of the existing codebase.

For Python code, Apache Spark follows 
PEP 8 with one exception: 
lines can be up to 100 characters in length, not 79.
For R code, Apache Spark follows
Google’s R Style Guide with three exceptions:
lines can be up to 100 characters in length, not 80, there is no limit on function name but it has a initial
lower case latter and S4 objects/methods are allowed.
For Java code, Apache Spark follows
Oracle’s Java code conventions and
Scala guidelines below. The latter is preferred.
For Scala code, Apache Spark follows the official 
Scala style guide and
Databricks Scala guide. The latter is preferred. To format Scala code, run ./dev/scalafmt prior to submitting a PR.

If in doubt
If you’re not sure about the right style for something, try to follow the style of the existing 
codebase. Look at whether there are other examples in the code that use your feature. Feel free 
to ask on the dev@spark.apache.org list as well and/or ask committers.
Code of conduct
The Apache Spark project follows the Apache Software Foundation Code of Conduct.  The code of conduct applies to all spaces managed by the Apache Software Foundation, including IRC, all public and private mailing lists, issue trackers, wikis, blogs, Twitter, and any other communication channel used by our communities. A code of conduct which is specific to in-person events (ie., conferences) is codified in the published ASF anti-harassment policy.
We expect this code of conduct to be honored by everyone who participates in the Apache community formally or informally, or claims any affiliation with the Foundation, in any Foundation-related activities and especially when representing the ASF, in any role.
This code is not exhaustive or complete. It serves to distill our common understanding of a collaborative, shared environment and goals. We expect it to be followed in spirit as much as in the letter, so that it can enrich all of us and the technical communities in which we participate.
For more information and specific guidelines, refer to the Apache Software Foundation Code of Conduct.



Latest News

Spark 3.5.5 released
(Feb 27, 2025)
Spark 3.5.4 released
(Dec 20, 2024)
Spark 3.4.4 released
(Oct 27, 2024)
Preview release of Spark 4.0
(Sep 26, 2024)

Archive








          Download Spark
        

          Built-in Libraries:
        

SQL and DataFrames
Spark Streaming
MLlib (machine learning)
GraphX (graph)

Third-Party Projects





    Apache Spark, Spark, Apache, the Apache feather logo, and the Apache Spark project logo are either registered
    trademarks or trademarks of The Apache Software Foundation in the United States and other countries.
    See guidance on use of Apache Spark trademarks.
    All other marks mentioned may be trademarks or registered trademarks of their respective owners.
    Copyright © 2018 The Apache Software Foundation, Licensed under the
    Apache License, Version 2.0.
  

















R Front End for Apache Spark • SparkR

















Skip to contents



SparkR
3.5.5





Reference

Articles

SparkR - Practical Guide





















R on Spark

SparkR is an R package that provides a light-weight frontend to use Spark from R.

Installing sparkR

Libraries of sparkR need to be created in $SPARK_HOME/R/lib. This can be done by running the script $SPARK_HOME/R/install-dev.sh. By default the above script uses the system wide installation of R. However, this can be changed to any user installed location of R by setting the environment variable R_HOME the full path of the base directory where R is installed, before running install-dev.sh script. Example:
# where /home/username/R is where R is installed and /home/username/R/bin contains the files R and RScript
export R_HOME=/home/username/R
./install-dev.sh


SparkR development


Build Spark

Build Spark with Maven or SBT, and include the -Psparkr profile to build the R package. For example to use the default Hadoop versions you can run
# Maven
./build/mvn -DskipTests -Psparkr package

# SBT
./build/sbt -Psparkr package


Running sparkR

You can start using SparkR by launching the SparkR shell with
./bin/sparkR
The sparkR script automatically creates a SparkContext with Spark by default in local mode. To specify the Spark master of a cluster for the automatically created SparkContext, you can run
./bin/sparkR --master "local[2]"
To set other options like driver memory, executor memory etc. you can pass in the spark-submit arguments to ./bin/sparkR


Using SparkR from RStudio

If you wish to use SparkR from RStudio, please refer SparkR documentation.


Making changes to SparkR

The instructions for making contributions to Spark also apply to SparkR. If you only make R file changes (i.e. no Scala changes) then you can just re-install the R package using R/install-dev.sh and test your changes. Once you have made your changes, please include unit tests for them and run existing unit tests using the R/run-tests.sh script as described below.


Generating documentation

The SparkR documentation (Rd files and HTML files) are not a part of the source repository. To generate them you can run the script R/create-docs.sh. This script uses devtools and knitr to generate the docs and these packages need to be installed on the machine before using the script. Also, you may need to install these prerequisites. See also, R/DOCUMENTATION.md



Examples, Unit tests

SparkR comes with several sample programs in the examples/src/main/r directory. To run one of them, use ./bin/spark-submit <filename> <args>. For example:
./bin/spark-submit examples/src/main/r/dataframe.R
You can run R unit tests by following the instructions under Running R Tests.


Running on YARN

The ./bin/spark-submit can also be used to submit jobs to YARN clusters. You will need to set YARN conf dir before doing so. For example on CDH you can run
export YARN_CONF_DIR=/etc/hadoop/conf
./bin/spark-submit --master yarn examples/src/main/r/dataframe.R



Links

View on CRAN
Report a bug



License

Apache License (== 2.0)



Citation

Citing SparkR



Developers


 The Apache Software Foundation  Author, maintainer, copyright holder  






Developed by  The Apache Software Foundation.



Site built with pkgdown 2.1.1.
Using preferably template.











Spark 3.5.5 JavaDoc










JavaScript is disabled on your browser.

Frame Alert
This document is designed to be viewed using the frames feature. If you see this message, you are using a non-frame-capable web client. Link to Non-frame version.









Getting Started — PySpark 3.5.5 documentation













































  Overview
 



  Getting Started
 



  User Guides
 



  API Reference
 



  Development
 



  Migration Guides
 









        3.5.5
        























   Installation
  



   Quickstart: DataFrame
  



   Quickstart: Spark Connect
  



   Quickstart: Pandas API on Spark
  



   Testing PySpark
  



















Getting Started¶
This page summarizes the basic steps required to setup and get started with PySpark.
There are more guides shared with other languages such as
Quick Start in Programming Guides
at the Spark documentation.
There are live notebooks where you can try PySpark out without any other step:

Live Notebook: DataFrame
Live Notebook: Spark Connect
Live Notebook: pandas API on Spark

The list below is the contents of this quickstart page:


Installation
Python Versions Supported
Using PyPI
Using Conda
Manually Downloading
Installing from Source
Dependencies


Quickstart: DataFrame
DataFrame Creation
Viewing Data
Selecting and Accessing Data
Applying a Function
Grouping Data
Getting Data In/Out
Working with SQL


Quickstart: Spark Connect
Launch Spark server with Spark Connect
Connect to Spark Connect server
Create DataFrame


Quickstart: Pandas API on Spark
Object Creation
Missing Data
Operations
Grouping
Plotting
Getting data in/out


Testing PySpark
Build a PySpark Application
Testing your PySpark Application
Putting It All Together!











previous
PySpark Overview




next
Installation












    © Copyright .




Created using Sphinx 3.0.4.











PySpark Overview — PySpark 3.5.5 documentation












































  Overview
 



  Getting Started
 



  User Guides
 



  API Reference
 



  Development
 



  Migration Guides
 









        3.5.5
        





































PySpark Overview¶
Date: Feb 23, 2025 Version: 3.5.5
Useful links:
Live Notebook | GitHub | Issues | Examples | Community
PySpark is the Python API for Apache Spark. It enables you to perform real-time,
large-scale data processing in a distributed environment using Python. It also provides a PySpark
shell for interactively analyzing your data.
PySpark combines Python’s learnability and ease of use with the power of Apache Spark
to enable processing and analysis of data at any size for everyone familiar with Python.
PySpark supports all of Spark’s features such as Spark SQL,
DataFrames, Structured Streaming, Machine Learning (MLlib) and Spark Core.





































Spark SQL and DataFrames
Spark SQL is Apache Spark’s module for working with structured data.
It allows you to seamlessly mix SQL queries with Spark programs.
With PySpark DataFrames you can efficiently read, write, transform,
and analyze data using Python and SQL.
Whether you use Python or SQL, the same underlying execution
engine is used so you will always leverage the full power of Spark.

Quickstart: DataFrame
Live Notebook: DataFrame
Spark SQL API Reference

Pandas API on Spark
Pandas API on Spark allows you to scale your pandas workload to any size
by running it distributed across multiple nodes. If you are already familiar
with pandas and want to leverage Spark for big data, pandas API on Spark makes
you immediately productive and lets you migrate your applications without modifying the code.
You can have a single codebase that works both with pandas (tests, smaller datasets)
and with Spark (production, distributed datasets) and you can switch between the
pandas API and the Pandas API on Spark easily and without overhead.
Pandas API on Spark aims to make the transition from pandas to Spark easy but
if you are new to Spark or deciding which API to use, we recommend using PySpark
(see Spark SQL and DataFrames).

Quickstart: Pandas API on Spark
Live Notebook: pandas API on Spark
Pandas API on Spark Reference

Structured Streaming
Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.
You can express your streaming computation the same way you would express a batch computation on static data.
The Spark SQL engine will take care of running it incrementally and continuously and updating the final result
as streaming data continues to arrive.

Structured Streaming Programming Guide
Structured Streaming API Reference

Machine Learning (MLlib)
Built on top of Spark, MLlib is a scalable machine learning library that provides
a uniform set of high-level APIs that help users create and tune practical machine
learning pipelines.

Machine Learning Library (MLlib) Programming Guide
Machine Learning (MLlib) API Reference

Spark Core and RDDs
Spark Core is the underlying general execution engine for the Spark platform that all
other functionality is built on top of. It provides RDDs (Resilient Distributed Datasets)
and in-memory computing capabilities.
Note that the RDD API is a low-level API which can be difficult to use and you do not get
the benefit of Spark’s automatic query optimization capabilities.
We recommend using DataFrames (see Spark SQL and DataFrames above)
instead of RDDs as it allows you to express what you want more easily and lets Spark automatically
construct the most efficient query for you.

Spark Core API Reference

Spark Streaming (Legacy)
Spark Streaming is an extension of the core Spark API that enables scalable,
high-throughput, fault-tolerant stream processing of live data streams.
Note that Spark Streaming is the previous generation of Spark’s streaming engine.
It is a legacy project and it is no longer being updated.
There is a newer and easier to use streaming engine in Spark called
Structured Streaming which you
should use for your streaming applications and pipelines.

Spark Streaming Programming Guide (Legacy)
Spark Streaming API Reference (Legacy)









next
Getting Started












    © Copyright .




Created using Sphinx 3.0.4.











Spark 3.5.5 ScalaDoc  - org.apache.spark

















Spark 3.5.5 ScalaDoc
< Back





















Packages










package


root

 Definition Classesroot









package


org

 Definition Classesroot









package


apache

 Definition Classesorg









package


parquet

 Definition Classesapache









package


spark

Core Spark functionality.Core Spark functionality. org.apache.spark.SparkContext serves as the main entry point to
Spark, while org.apache.spark.rdd.RDD is the data type representing a distributed collection,
and provides most parallel operations.In addition, org.apache.spark.rdd.PairRDDFunctions contains operations available only on RDDs
of key-value pairs, such as groupByKey and join; org.apache.spark.rdd.DoubleRDDFunctions
contains operations available only on RDDs of Doubles; and
org.apache.spark.rdd.SequenceFileRDDFunctions contains operations available on RDDs that can
be saved as SequenceFiles. These operations are automatically available on any RDD of the right
type (e.g. RDD[(Int, Int)] through implicit conversions.Java programmers should reference the org.apache.spark.api.java package
for Spark programming APIs in Java.Classes and methods marked with 
Experimental are user-facing features which have not been officially adopted by the
Spark project. These are subject to change or removal in minor releases.Classes and methods marked with 
Developer API are intended for advanced users want to extend Spark through lower
level interfaces. These are subject to changes or removal in minor releases.
 Definition Classesapache









package


api










package


broadcast

Spark's broadcast variables, used to broadcast immutable datasets to all nodes.









package


graphx

ALPHA COMPONENT
GraphX is a graph processing framework built on top of Spark.









package


input










package


io

IO codecs used for compression.IO codecs used for compression. See org.apache.spark.io.CompressionCodec.










package


launcher










package


mapred










package


metrics










package


ml

DataFrame-based machine learning APIs to let users quickly assemble and configure practical
machine learning pipelines.









package


mllib

RDD-based machine learning APIs (in maintenance mode).RDD-based machine learning APIs (in maintenance mode).The spark.mllib package is in maintenance mode as of the Spark 2.0.0 release to encourage
migration to the DataFrame-based APIs under the org.apache.spark.ml package.
While in maintenance mode,no new features in the RDD-based spark.mllib package will be accepted, unless they block
   implementing new features in the DataFrame-based spark.ml package;bug fixes in the RDD-based APIs will still be accepted.The developers will continue adding more features to the DataFrame-based APIs in the 2.x series
to reach feature parity with the RDD-based APIs.
And once we reach feature parity, this package will be deprecated.
 See alsoSPARK-4591 to track
the progress of feature parity









package


partial

Support for approximate results.Support for approximate results. This provides convenient api and also implementation for
approximate calculation.
 See alsoorg.apache.spark.rdd.RDD.countApprox









package


paths










package


rdd

Provides several RDD implementations.Provides several RDD implementations. See org.apache.spark.rdd.RDD.










package


resource










package


scheduler

Spark's scheduling components.Spark's scheduling components. This includes the org.apache.spark.scheduler.DAGScheduler and
lower level org.apache.spark.scheduler.TaskScheduler.










package


security










package


serializer

Pluggable serializers for RDD and shuffle data.Pluggable serializers for RDD and shuffle data.
 See alsoorg.apache.spark.serializer.Serializer









package


shuffle










package


sql

Allows the execution of relational queries, including those expressed in SQL using Spark.









package


status










package


storage










package


streaming

Spark Streaming functionality.Spark Streaming functionality. org.apache.spark.streaming.StreamingContext serves as the main
entry point to Spark Streaming, while org.apache.spark.streaming.dstream.DStream is the data
type representing a continuous sequence of RDDs, representing a continuous stream of data.In addition, org.apache.spark.streaming.dstream.PairDStreamFunctions contains operations
available only on DStreams
of key-value pairs, such as groupByKey and reduceByKey. These operations are automatically
available on any DStream of the right type (e.g. DStream[(Int, Int)] through implicit
conversions.For the Java API of Spark Streaming, take a look at the
org.apache.spark.streaming.api.java.JavaStreamingContext which serves as the entry point, and
the org.apache.spark.streaming.api.java.JavaDStream and the
org.apache.spark.streaming.api.java.JavaPairDStream which have the DStream functionality.










package


unsafe










package


util

Spark utilities.



Aggregator



BarrierTaskContext



BarrierTaskInfo



ComplexFutureAction



ContextAwareIterator



Dependency



ErrorClassesJsonReader



ExceptionFailure



ExecutorLostFailure



FetchFailed



FutureAction



HashPartitioner



InterruptibleIterator



JobExecutionStatus



JobSubmitter



NarrowDependency



OneToOneDependency



Partition



PartitionEvaluator



PartitionEvaluatorFactory



Partitioner



QueryContext



RangeDependency



RangePartitioner



Resubmitted



SerializableWritable



ShuffleDependency



SimpleFutureAction



SparkConf



SparkContext



SparkEnv



SparkException



SparkExecutorInfo



SparkFiles



SparkFirehoseListener



SparkJobInfo



SparkStageInfo



SparkStatusTracker



SparkThrowable



Success



TaskCommitDenied



TaskContext



TaskEndReason



TaskFailedReason



TaskKilled



TaskKilledException



TaskResultLost



UnknownReason



WritableConverter



WritableFactory







p
org.apache
spark








package


spark


Core Spark functionality. org.apache.spark.SparkContext serves as the main entry point to
Spark, while org.apache.spark.rdd.RDD is the data type representing a distributed collection,
and provides most parallel operations.In addition, org.apache.spark.rdd.PairRDDFunctions contains operations available only on RDDs
of key-value pairs, such as groupByKey and join; org.apache.spark.rdd.DoubleRDDFunctions
contains operations available only on RDDs of Doubles; and
org.apache.spark.rdd.SequenceFileRDDFunctions contains operations available on RDDs that can
be saved as SequenceFiles. These operations are automatically available on any RDD of the right
type (e.g. RDD[(Int, Int)] through implicit conversions.Java programmers should reference the org.apache.spark.api.java package
for Spark programming APIs in Java.Classes and methods marked with 
Experimental are user-facing features which have not been officially adopted by the
Spark project. These are subject to change or removal in minor releases.Classes and methods marked with 
Developer API are intended for advanced users want to extend Spark through lower
level interfaces. These are subject to changes or removal in minor releases.
 Sourcepackage.scala

            Linear Supertypes
          
AnyRef, Any












Ordering

Alphabetic
By Inheritance



Inherited


sparkAnyRefAny




Hide All
Show All



Visibility
PublicAll






Type Members









case class


Aggregator[K, V, C](createCombiner: (V) ⇒ C, mergeValue: (C, V) ⇒ C, mergeCombiners: (C, C) ⇒ C) extends Product with Serializable

:: DeveloperApi ::
A set of functions used to aggregate data.:: DeveloperApi ::
A set of functions used to aggregate data.
createCombinerfunction to create the initial value of the aggregation.mergeValuefunction to merge a new value into the aggregation result.mergeCombinersfunction to merge outputs from multiple mergeValue function. Annotations
@DeveloperApi()










class


BarrierTaskContext extends TaskContext with Logging

:: Experimental ::
A TaskContext with extra contextual info and tooling for tasks in a barrier stage.:: Experimental ::
A TaskContext with extra contextual info and tooling for tasks in a barrier stage.
Use BarrierTaskContext#get to obtain the barrier context for a running barrier task.
 Annotations
@Experimental()
@Since(
"2.4.0"
)










class


BarrierTaskInfo extends AnyRef

:: Experimental ::
Carries all task infos of a barrier task.:: Experimental ::
Carries all task infos of a barrier task.
 Annotations
@Experimental()
@Since(
"2.4.0"
)










class


ComplexFutureAction[T] extends FutureAction[T]

A FutureAction for actions that could trigger multiple Spark jobs.A FutureAction for actions that could trigger multiple Spark jobs. Examples include take,
takeSample. Cancellation works by setting the cancelled flag to true and cancelling any pending
jobs.
 Annotations
@DeveloperApi()










class


ContextAwareIterator[+T] extends Iterator[T]

:: DeveloperApi ::
A TaskContext aware iterator.:: DeveloperApi ::
A TaskContext aware iterator.As the Python evaluation consumes the parent iterator in a separate thread,
it could consume more data from the parent even after the task ends and the parent is closed.
If an off-heap access exists in the parent iterator, it could cause segmentation fault
which crashes the executor.
Thus, we should use ContextAwareIterator to stop consuming after the task ends.
 Annotations
@DeveloperApi()
Since3.1.0








abstract 
class


Dependency[T] extends Serializable

:: DeveloperApi ::
Base class for dependencies.:: DeveloperApi ::
Base class for dependencies.
 Annotations
@DeveloperApi()










class


ErrorClassesJsonReader extends AnyRef

A reader to load error information from one or more JSON files.A reader to load error information from one or more JSON files. Note that, if one error appears
in more than one JSON files, the latter wins. Please read core/src/main/resources/error/README.md
for more details.
 Annotations
@DeveloperApi()










case class


ExceptionFailure(className: String, description: String, stackTrace: Array[StackTraceElement], fullStackTrace: String, exceptionWrapper: Option[ThrowableSerializationWrapper], accumUpdates: Seq[AccumulableInfo] = Seq.empty, accums: Seq[AccumulatorV2[_, _]] = Nil, metricPeaks: Seq[Long] = Seq.empty) extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
Task failed due to a runtime exception.:: DeveloperApi ::
Task failed due to a runtime exception. This is the most common failure case and also captures
user program exceptions.stackTrace contains the stack trace of the exception itself. It still exists for backward
compatibility. It's better to use this(e: Throwable, metrics: Option[TaskMetrics]) to
create ExceptionFailure as it will handle the backward compatibility properly.fullStackTrace is a better representation of the stack trace because it contains the whole
stack trace including the exception and its causesexception is the actual exception that caused the task to fail. It may be None in
the case that the exception is not in fact serializable. If a task fails more than
once (due to retries), exception is that one that caused the last failure.
 Annotations
@DeveloperApi()










case class


ExecutorLostFailure(execId: String, exitCausedByApp: Boolean = true, reason: Option[String]) extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
The task failed because the executor that it was running on was lost.:: DeveloperApi ::
The task failed because the executor that it was running on was lost. This may happen because
the task crashed the JVM.
 Annotations
@DeveloperApi()










case class


FetchFailed(bmAddress: BlockManagerId, shuffleId: Int, mapId: Long, mapIndex: Int, reduceId: Int, message: String) extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
Task failed to fetch shuffle data from a remote node.:: DeveloperApi ::
Task failed to fetch shuffle data from a remote node. Probably means we have lost the remote
executors the task is trying to fetch from, and thus need to rerun the previous stage.
 Annotations
@DeveloperApi()










trait


FutureAction[T] extends Future[T]

A future for the result of an action to support cancellation.A future for the result of an action to support cancellation. This is an extension of the
Scala Future interface to support cancellation.










class


HashPartitioner extends Partitioner

A org.apache.spark.Partitioner that implements hash-based partitioning using
Java's Object.hashCode.A org.apache.spark.Partitioner that implements hash-based partitioning using
Java's Object.hashCode.Java arrays have hashCodes that are based on the arrays' identities rather than their contents,
so attempting to partition an RDD[Array[_]] or RDD[(Array[_], _)] using a HashPartitioner will
produce an unexpected or incorrect result.










class


InterruptibleIterator[+T] extends Iterator[T]

:: DeveloperApi ::
An iterator that wraps around an existing iterator to provide task killing functionality.:: DeveloperApi ::
An iterator that wraps around an existing iterator to provide task killing functionality.
It works by checking the interrupted flag in TaskContext.
 Annotations
@DeveloperApi()









sealed abstract final 
class


JobExecutionStatus extends Enum[JobExecutionStatus]










trait


JobSubmitter extends AnyRef

Handle via which a "run" function passed to a ComplexFutureAction
can submit jobs for execution.Handle via which a "run" function passed to a ComplexFutureAction
can submit jobs for execution.
 Annotations
@DeveloperApi()









abstract 
class


NarrowDependency[T] extends Dependency[T]

:: DeveloperApi ::
Base class for dependencies where each partition of the child RDD depends on a small number
of partitions of the parent RDD.:: DeveloperApi ::
Base class for dependencies where each partition of the child RDD depends on a small number
of partitions of the parent RDD. Narrow dependencies allow for pipelined execution.
 Annotations
@DeveloperApi()










class


OneToOneDependency[T] extends NarrowDependency[T]

:: DeveloperApi ::
Represents a one-to-one dependency between partitions of the parent and child RDDs.:: DeveloperApi ::
Represents a one-to-one dependency between partitions of the parent and child RDDs.
 Annotations
@DeveloperApi()










trait


Partition extends Serializable

An identifier for a partition in an RDD.









trait


PartitionEvaluator[T, U] extends AnyRef

An evaluator for computing RDD partitions.An evaluator for computing RDD partitions. Spark serializes and sends
PartitionEvaluatorFactory to executors, and then creates PartitionEvaluator via the
factory at the executor side.
 Annotations
@DeveloperApi()
@Since(
"3.5.0"
)










trait


PartitionEvaluatorFactory[T, U] extends Serializable

A factory to create PartitionEvaluator.A factory to create PartitionEvaluator. Spark serializes and sends
PartitionEvaluatorFactory to executors, and then creates PartitionEvaluator via the
factory at the executor side.
 Annotations
@DeveloperApi()
@Since(
"3.5.0"
)









abstract 
class


Partitioner extends Serializable

An object that defines how the elements in a key-value pair RDD are partitioned by key.An object that defines how the elements in a key-value pair RDD are partitioned by key.
Maps each key to a partition ID, from 0 to numPartitions - 1.Note that, partitioner must be deterministic, i.e. it must return the same partition id given
the same partition key.










trait


QueryContext extends AnyRef

 Annotations
@Evolving()










class


RangeDependency[T] extends NarrowDependency[T]

:: DeveloperApi ::
Represents a one-to-one dependency between ranges of partitions in the parent and child RDDs.:: DeveloperApi ::
Represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. Annotations
@DeveloperApi()










class


RangePartitioner[K, V] extends Partitioner

A org.apache.spark.Partitioner that partitions sortable records by range into roughly
equal ranges.A org.apache.spark.Partitioner that partitions sortable records by range into roughly
equal ranges. The ranges are determined by sampling the content of the RDD passed in.
 NoteThe actual number of partitions created by the RangePartitioner might not be the same
as the partitions parameter, in the case where the number of sampled records is less than
the value of partitions.









class


SerializableWritable[T <: Writable] extends Serializable

 Annotations
@DeveloperApi()










class


ShuffleDependency[K, V, C] extends Dependency[Product2[K, V]] with Logging

:: DeveloperApi ::
Represents a dependency on the output of a shuffle stage.:: DeveloperApi ::
Represents a dependency on the output of a shuffle stage. Note that in the case of shuffle,
the RDD is transient since we don't need it on the executor side.
 Annotations
@DeveloperApi()










class


SimpleFutureAction[T] extends FutureAction[T]

A FutureAction holding the result of an action that triggers a single job.A FutureAction holding the result of an action that triggers a single job. Examples include
count, collect, reduce.
 Annotations
@DeveloperApi()










class


SparkConf extends Cloneable with Logging with Serializable

Configuration for a Spark application.Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.Most of the time, you would create a SparkConf object with new SparkConf(), which will load
values from any spark.* Java system properties set in your application as well. In this case,
parameters you set directly on the SparkConf object take priority over system properties.For unit tests, you can also call new SparkConf(false) to skip loading external settings and
get the same configuration no matter what the system properties are.All setter methods in this class support chaining. For example, you can write
new SparkConf().setMaster("local").setAppName("My app").
 NoteOnce a SparkConf object is passed to Spark, it is cloned and can no longer be modified
by the user. Spark does not support modifying the configuration at runtime.









class


SparkContext extends Logging

Main entry point for Spark functionality.Main entry point for Spark functionality. A SparkContext represents the connection to a Spark
cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.
 NoteOnly one SparkContext should be active per JVM. You must stop() the
  active SparkContext before creating a new one.









class


SparkEnv extends Logging

:: DeveloperApi ::
Holds all the runtime environment objects for a running Spark instance (either master or worker),
including the serializer, RpcEnv, block manager, map output tracker, etc.:: DeveloperApi ::
Holds all the runtime environment objects for a running Spark instance (either master or worker),
including the serializer, RpcEnv, block manager, map output tracker, etc. Currently
Spark code finds the SparkEnv through a global variable, so all the threads can access the same
SparkEnv. It can be accessed by SparkEnv.get (e.g. after creating a SparkContext).
 Annotations
@DeveloperApi()










class


SparkException extends Exception with SparkThrowable










trait


SparkExecutorInfo extends Serializable










class


SparkFirehoseListener extends SparkListenerInterface

 Annotations
@DeveloperApi()










trait


SparkJobInfo extends Serializable










trait


SparkStageInfo extends Serializable










class


SparkStatusTracker extends AnyRef

Low-level status reporting APIs for monitoring job and stage progress.Low-level status reporting APIs for monitoring job and stage progress.These APIs intentionally provide very weak consistency semantics; consumers of these APIs should
be prepared to handle empty / missing information.  For example, a job's stage ids may be known
but the status API may not have any information about the details of those stages, so
getStageInfo could potentially return None for a valid stage id.To limit memory usage, these APIs only provide information on recent jobs / stages.  These APIs
will provide information for the last spark.ui.retainedStages stages and
spark.ui.retainedJobs jobs.NOTE: this class's constructor should be considered private and may be subject to change.










trait


SparkThrowable extends AnyRef

 Annotations
@Evolving()










case class


TaskCommitDenied(jobID: Int, partitionID: Int, attemptNumber: Int) extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
Task requested the driver to commit, but was denied.:: DeveloperApi ::
Task requested the driver to commit, but was denied.
 Annotations
@DeveloperApi()









abstract 
class


TaskContext extends Serializable

Contextual information about a task which can be read or mutated during
execution.Contextual information about a task which can be read or mutated during
execution. To access the TaskContext for a running task, use:org.apache.spark.TaskContext.get()








sealed 
trait


TaskEndReason extends AnyRef

:: DeveloperApi ::
Various possible reasons why a task ended.:: DeveloperApi ::
Various possible reasons why a task ended. The low-level TaskScheduler is supposed to retry
tasks several times for "ephemeral" failures, and only report back failures that require some
old stages to be resubmitted, such as shuffle map fetch failures.
 Annotations
@DeveloperApi()









sealed 
trait


TaskFailedReason extends TaskEndReason

:: DeveloperApi ::
Various possible reasons why a task failed.:: DeveloperApi ::
Various possible reasons why a task failed.
 Annotations
@DeveloperApi()










case class


TaskKilled(reason: String, accumUpdates: Seq[AccumulableInfo] = Seq.empty, accums: Seq[AccumulatorV2[_, _]] = Nil, metricPeaks: Seq[Long] = Seq.empty) extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
Task was killed intentionally and needs to be rescheduled.:: DeveloperApi ::
Task was killed intentionally and needs to be rescheduled.
 Annotations
@DeveloperApi()










class


TaskKilledException extends RuntimeException

:: DeveloperApi ::
Exception thrown when a task is explicitly killed (i.e., task failure is expected).:: DeveloperApi ::
Exception thrown when a task is explicitly killed (i.e., task failure is expected).
 Annotations
@DeveloperApi()




Value Members










val


SPARK_BRANCH: String










val


SPARK_BUILD_DATE: String










val


SPARK_BUILD_USER: String










val


SPARK_DOC_ROOT: String










val


SPARK_REPO_URL: String










val


SPARK_REVISION: String










val


SPARK_VERSION: String










val


SPARK_VERSION_SHORT: String










object


BarrierTaskContext extends Serializable

 Annotations
@Experimental()
@Since(
"2.4.0"
)










object


Partitioner extends Serializable










object


Resubmitted extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
A org.apache.spark.scheduler.ShuffleMapTask that completed successfully earlier, but we
lost the executor before the stage completed.:: DeveloperApi ::
A org.apache.spark.scheduler.ShuffleMapTask that completed successfully earlier, but we
lost the executor before the stage completed. This means Spark needs to reschedule the task
to be re-executed on a different executor.
 Annotations
@DeveloperApi()










object


SparkContext extends Logging

The SparkContext object contains a number of implicit conversions and parameters for use with
various Spark features.









object


SparkEnv extends Logging










object


SparkException extends Serializable










object


SparkFiles

Resolves paths to files added through SparkContext.addFile().









object


Success extends TaskEndReason with Product with Serializable

:: DeveloperApi ::
Task succeeded.:: DeveloperApi ::
Task succeeded.
 Annotations
@DeveloperApi()










object


TaskContext extends Serializable










object


TaskResultLost extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
The task finished successfully, but the result was lost from the executor's block manager before
it was fetched.:: DeveloperApi ::
The task finished successfully, but the result was lost from the executor's block manager before
it was fetched.
 Annotations
@DeveloperApi()










object


UnknownReason extends TaskFailedReason with Product with Serializable

:: DeveloperApi ::
We don't know why the task ended -- for example, because of a ClassNotFound exception when
deserializing the task result.:: DeveloperApi ::
We don't know why the task ended -- for example, because of a ClassNotFound exception when
deserializing the task result.
 Annotations
@DeveloperApi()










object


WritableConverter extends Serializable










object


WritableFactory extends Serializable







Inherited from AnyRef

Inherited from Any




Ungrouped




 









  






Spark SQL, Built-in Functions















 Spark SQL, Built-in Functions








Functions

!

!=

%

&

*

+

-

/

<

<=

<=>

<>

=

==

>

>=

^

abs

acos

acosh

add_months

aes_decrypt

aes_encrypt

aggregate

and

any

any_value

approx_count_distinct

approx_percentile

array

array_agg

array_append

array_compact

array_contains

array_distinct

array_except

array_insert

array_intersect

array_join

array_max

array_min

array_position

array_prepend

array_remove

array_repeat

array_size

array_sort

array_union

arrays_overlap

arrays_zip

ascii

asin

asinh

assert_true

atan

atan2

atanh

avg

base64

between

bigint

bin

binary

bit_and

bit_count

bit_get

bit_length

bit_or

bit_xor

bitmap_bit_position

bitmap_bucket_number

bitmap_construct_agg

bitmap_count

bitmap_or_agg

bool_and

bool_or

boolean

bround

btrim

cardinality

case

cast

cbrt

ceil

ceiling

char

char_length

character_length

chr

coalesce

collect_list

collect_set

concat

concat_ws

contains

conv

convert_timezone

corr

cos

cosh

cot

count

count_if

count_min_sketch

covar_pop

covar_samp

crc32

csc

cume_dist

curdate

current_catalog

current_database

current_date

current_schema

current_timestamp

current_timezone

current_user

date

date_add

date_diff

date_format

date_from_unix_date

date_part

date_sub

date_trunc

dateadd

datediff

datepart

day

dayofmonth

dayofweek

dayofyear

decimal

decode

degrees

dense_rank

div

double

e

element_at

elt

encode

endswith

equal_null

every

exists

exp

explode

explode_outer

expm1

extract

factorial

filter

find_in_set

first

first_value

flatten

float

floor

forall

format_number

format_string

from_csv

from_json

from_unixtime

from_utc_timestamp

get

get_json_object

getbit

greatest

grouping

grouping_id

hash

hex

histogram_numeric

hll_sketch_agg

hll_sketch_estimate

hll_union

hll_union_agg

hour

hypot

if

ifnull

ilike

in

initcap

inline

inline_outer

input_file_block_length

input_file_block_start

input_file_name

instr

int

isnan

isnotnull

isnull

java_method

json_array_length

json_object_keys

json_tuple

kurtosis

lag

last

last_day

last_value

lcase

lead

least

left

len

length

levenshtein

like

ln

localtimestamp

locate

log

log10

log1p

log2

lower

lpad

ltrim

luhn_check

make_date

make_dt_interval

make_interval

make_timestamp

make_timestamp_ltz

make_timestamp_ntz

make_ym_interval

map

map_concat

map_contains_key

map_entries

map_filter

map_from_arrays

map_from_entries

map_keys

map_values

map_zip_with

mask

max

max_by

md5

mean

median

min

min_by

minute

mod

mode

monotonically_increasing_id

month

months_between

named_struct

nanvl

negative

next_day

not

now

nth_value

ntile

nullif

nvl

nvl2

octet_length

or

overlay

parse_url

percent_rank

percentile

percentile_approx

pi

pmod

posexplode

posexplode_outer

position

positive

pow

power

printf

quarter

radians

raise_error

rand

randn

random

rank

reduce

reflect

regexp

regexp_count

regexp_extract

regexp_extract_all

regexp_instr

regexp_like

regexp_replace

regexp_substr

regr_avgx

regr_avgy

regr_count

regr_intercept

regr_r2

regr_slope

regr_sxx

regr_sxy

regr_syy

repeat

replace

reverse

right

rint

rlike

round

row_number

rpad

rtrim

schema_of_csv

schema_of_json

sec

second

sentences

sequence

session_window

sha

sha1

sha2

shiftleft

shiftright

shiftrightunsigned

shuffle

sign

signum

sin

sinh

size

skewness

slice

smallint

some

sort_array

soundex

space

spark_partition_id

split

split_part

sqrt

stack

startswith

std

stddev

stddev_pop

stddev_samp

str_to_map

string

struct

substr

substring

substring_index

sum

tan

tanh

timestamp

timestamp_micros

timestamp_millis

timestamp_seconds

tinyint

to_binary

to_char

to_csv

to_date

to_json

to_number

to_timestamp

to_timestamp_ltz

to_timestamp_ntz

to_unix_timestamp

to_utc_timestamp

to_varchar

transform

transform_keys

transform_values

translate

trim

trunc

try_add

try_aes_decrypt

try_avg

try_divide

try_element_at

try_multiply

try_subtract

try_sum

try_to_binary

try_to_number

try_to_timestamp

typeof

ucase

unbase64

unhex

unix_date

unix_micros

unix_millis

unix_seconds

unix_timestamp

upper

url_decode

url_encode

user

uuid

var_pop

var_samp

variance

version

weekday

weekofyear

when

width_bucket

window

window_time

xpath

xpath_boolean

xpath_double

xpath_float

xpath_int

xpath_long

xpath_number

xpath_short

xpath_string

xxhash64

year

zip_with

|

||

~










Spark SQL, Built-in Functions





Docs »
Functions







Built-in Functions
!
! expr - Logical not.
Examples:
> SELECT ! true;
 false
> SELECT ! false;
 true
> SELECT ! NULL;
 NULL

Since: 1.0.0

!=
expr1 != expr2 - Returns true if expr1 is not equal to expr2, or false otherwise.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to
                 a common type, and must be a type that can be used in equality comparison.
                 Map type is not supported. For complex types such array/struct,
                 the data types of fields must be orderable.

Examples:
> SELECT 1 != 2;
 true
> SELECT 1 != '2';
 true
> SELECT true != NULL;
 NULL
> SELECT NULL != NULL;
 NULL

Since: 1.0.0

%
expr1 % expr2 - Returns the remainder after expr1/expr2.
Examples:
> SELECT 2 % 1.8;
 0.2
> SELECT MOD(2, 1.8);
 0.2

Since: 1.0.0

&
expr1 & expr2 - Returns the result of bitwise AND of expr1 and expr2.
Examples:
> SELECT 3 & 5;
 1

Since: 1.4.0

*
expr1 * expr2 - Returns expr1*expr2.
Examples:
> SELECT 2 * 3;
 6

Since: 1.0.0

+
expr1 + expr2 - Returns expr1+expr2.
Examples:
> SELECT 1 + 2;
 3

Since: 1.0.0

-
expr1 - expr2 - Returns expr1-expr2.
Examples:
> SELECT 2 - 1;
 1

Since: 1.0.0

/
expr1 / expr2 - Returns expr1/expr2. It always performs floating point division.
Examples:
> SELECT 3 / 2;
 1.5
> SELECT 2L / 2L;
 1.0

Since: 1.0.0

<
expr1 < expr2 - Returns true if expr1 is less than expr2.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be ordered. For example, map type is not orderable, so it
    is not supported. For complex types such array/struct, the data types of fields must
    be orderable.

Examples:
> SELECT 1 < 2;
 true
> SELECT 1.1 < '1';
 false
> SELECT to_date('2009-07-30 04:17:52') < to_date('2009-07-30 04:17:52');
 false
> SELECT to_date('2009-07-30 04:17:52') < to_date('2009-08-01 04:17:52');
 true
> SELECT 1 < NULL;
 NULL

Since: 1.0.0

<=
expr1 <= expr2 - Returns true if expr1 is less than or equal to expr2.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be ordered. For example, map type is not orderable, so it
    is not supported. For complex types such array/struct, the data types of fields must
    be orderable.

Examples:
> SELECT 2 <= 2;
 true
> SELECT 1.0 <= '1';
 true
> SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-07-30 04:17:52');
 true
> SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-08-01 04:17:52');
 true
> SELECT 1 <= NULL;
 NULL

Since: 1.0.0

<=>
expr1 <=> expr2 - Returns same result as the EQUAL(=) operator for non-null operands,
but returns true if both are null, false if one of the them is null.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be used in equality comparison. Map type is not supported.
    For complex types such array/struct, the data types of fields must be orderable.

Examples:
> SELECT 2 <=> 2;
 true
> SELECT 1 <=> '1';
 true
> SELECT true <=> NULL;
 false
> SELECT NULL <=> NULL;
 true

Since: 1.1.0

<>
expr1 != expr2 - Returns true if expr1 is not equal to expr2, or false otherwise.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to
                 a common type, and must be a type that can be used in equality comparison.
                 Map type is not supported. For complex types such array/struct,
                 the data types of fields must be orderable.

Examples:
> SELECT 1 != 2;
 true
> SELECT 1 != '2';
 true
> SELECT true != NULL;
 NULL
> SELECT NULL != NULL;
 NULL

Since: 1.0.0

=
expr1 = expr2 - Returns true if expr1 equals expr2, or false otherwise.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be used in equality comparison. Map type is not supported.
    For complex types such array/struct, the data types of fields must be orderable.

Examples:
> SELECT 2 = 2;
 true
> SELECT 1 = '1';
 true
> SELECT true = NULL;
 NULL
> SELECT NULL = NULL;
 NULL

Since: 1.0.0

==
expr1 == expr2 - Returns true if expr1 equals expr2, or false otherwise.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be used in equality comparison. Map type is not supported.
    For complex types such array/struct, the data types of fields must be orderable.

Examples:
> SELECT 2 == 2;
 true
> SELECT 1 == '1';
 true
> SELECT true == NULL;
 NULL
> SELECT NULL == NULL;
 NULL

Since: 1.0.0

>
expr1 > expr2 - Returns true if expr1 is greater than expr2.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be ordered. For example, map type is not orderable, so it
    is not supported. For complex types such array/struct, the data types of fields must
    be orderable.

Examples:
> SELECT 2 > 1;
 true
> SELECT 2 > 1.1;
 true
> SELECT to_date('2009-07-30 04:17:52') > to_date('2009-07-30 04:17:52');
 false
> SELECT to_date('2009-07-30 04:17:52') > to_date('2009-08-01 04:17:52');
 false
> SELECT 1 > NULL;
 NULL

Since: 1.0.0

>=
expr1 >= expr2 - Returns true if expr1 is greater than or equal to expr2.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be ordered. For example, map type is not orderable, so it
    is not supported. For complex types such array/struct, the data types of fields must
    be orderable.

Examples:
> SELECT 2 >= 1;
 true
> SELECT 2.0 >= '2.1';
 false
> SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-07-30 04:17:52');
 true
> SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-08-01 04:17:52');
 false
> SELECT 1 >= NULL;
 NULL

Since: 1.0.0

^
expr1 ^ expr2 - Returns the result of bitwise exclusive OR of expr1 and expr2.
Examples:
> SELECT 3 ^ 5;
 6

Since: 1.4.0

abs
abs(expr) - Returns the absolute value of the numeric or interval value.
Examples:
> SELECT abs(-1);
 1
> SELECT abs(INTERVAL -'1-1' YEAR TO MONTH);
 1-1

Since: 1.2.0

acos
acos(expr) - Returns the inverse cosine (a.k.a. arc cosine) of expr, as if computed by
java.lang.Math.acos.
Examples:
> SELECT acos(1);
 0.0
> SELECT acos(2);
 NaN

Since: 1.4.0

acosh
acosh(expr) - Returns inverse hyperbolic cosine of expr.
Examples:
> SELECT acosh(1);
 0.0
> SELECT acosh(0);
 NaN

Since: 3.0.0

add_months
add_months(start_date, num_months) - Returns the date that is num_months after start_date.
Examples:
> SELECT add_months('2016-08-31', 1);
 2016-09-30

Since: 1.5.0

aes_decrypt
aes_decrypt(expr, key[, mode[, padding[, aad]]]) - Returns a decrypted value of expr using AES in mode with padding.
Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS').
Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption.
The default mode is GCM.
Arguments:

expr - The binary value to decrypt.
key - The passphrase to use to decrypt the data.
mode - Specifies which block cipher mode should be used to decrypt messages.
         Valid modes: ECB, GCM, CBC.
padding - Specifies how to pad messages whose length is not a multiple of the block size.
            Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.
aad - Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and
        must be provided for both encryption and decryption.

Examples:
> SELECT aes_decrypt(unhex('83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94'), '0000111122223333');
 Spark
> SELECT aes_decrypt(unhex('6E7CA17BBB468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210'), '0000111122223333', 'GCM');
 Spark SQL
> SELECT aes_decrypt(unbase64('3lmwu+Mw0H3fi5NDvcu9lg=='), '1234567890abcdef', 'ECB', 'PKCS');
 Spark SQL
> SELECT aes_decrypt(unbase64('2NYmDCjgXTbbxGA3/SnJEfFC/JQ7olk2VQWReIAAFKo='), '1234567890abcdef', 'CBC');
 Apache Spark
> SELECT aes_decrypt(unbase64('AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg='), 'abcdefghijklmnop12345678ABCDEFGH', 'CBC', 'DEFAULT');
 Spark
> SELECT aes_decrypt(unbase64('AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4'), 'abcdefghijklmnop12345678ABCDEFGH', 'GCM', 'DEFAULT', 'This is an AAD mixed into the input');
 Spark

Since: 3.3.0

aes_encrypt
aes_encrypt(expr, key[, mode[, padding[, iv[, aad]]]]) - Returns an encrypted value of expr using AES in given mode with the specified padding.
Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (mode, padding) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS').
Optional initialization vectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12 bytes for GCM. If not provided, a random vector will be generated and prepended to the output.
Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption.
The default mode is GCM.
Arguments:

expr - The binary value to encrypt.
key - The passphrase to use to encrypt the data.
mode - Specifies which block cipher mode should be used to encrypt messages.
         Valid modes: ECB, GCM, CBC.
padding - Specifies how to pad messages whose length is not a multiple of the block size.
            Valid values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS for CBC.
iv - Optional initialization vector. Only supported for CBC and GCM modes.
       Valid values: None or ''. 16-byte array for CBC mode. 12-byte array for GCM mode.
aad - Optional additional authenticated data. Only supported for GCM mode. This can be any free-form input and
        must be provided for both encryption and decryption.

Examples:
> SELECT hex(aes_encrypt('Spark', '0000111122223333'));
 83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94
> SELECT hex(aes_encrypt('Spark SQL', '0000111122223333', 'GCM'));
 6E7CA17BBB468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210
> SELECT base64(aes_encrypt('Spark SQL', '1234567890abcdef', 'ECB', 'PKCS'));
 3lmwu+Mw0H3fi5NDvcu9lg==
> SELECT base64(aes_encrypt('Apache Spark', '1234567890abcdef', 'CBC', 'DEFAULT'));
 2NYmDCjgXTbbxGA3/SnJEfFC/JQ7olk2VQWReIAAFKo=
> SELECT base64(aes_encrypt('Spark', 'abcdefghijklmnop12345678ABCDEFGH', 'CBC', 'DEFAULT', unhex('00000000000000000000000000000000')));
 AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=
> SELECT base64(aes_encrypt('Spark', 'abcdefghijklmnop12345678ABCDEFGH', 'GCM', 'DEFAULT', unhex('000000000000000000000000'), 'This is an AAD mixed into the input'));
 AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4

Since: 3.3.0

aggregate
aggregate(expr, start, merge, finish) - Applies a binary operator to an initial state and all
elements in the array, and reduces this to a single state. The final state is converted
into the final result by applying a finish function.
Examples:
> SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x);
 6
> SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x, acc -> acc * 10);
 60

Since: 2.4.0

and
expr1 and expr2 - Logical AND.
Examples:
> SELECT true and true;
 true
> SELECT true and false;
 false
> SELECT true and NULL;
 NULL
> SELECT false and NULL;
 false

Since: 1.0.0

any
any(expr) - Returns true if at least one value of expr is true.
Examples:
> SELECT any(col) FROM VALUES (true), (false), (false) AS tab(col);
 true
> SELECT any(col) FROM VALUES (NULL), (true), (false) AS tab(col);
 true
> SELECT any(col) FROM VALUES (false), (false), (NULL) AS tab(col);
 false

Since: 3.0.0

any_value
any_value(expr[, isIgnoreNull]) - Returns some value of expr for a group of rows.
If isIgnoreNull is true, returns only non-null values.
Examples:
> SELECT any_value(col) FROM VALUES (10), (5), (20) AS tab(col);
 10
> SELECT any_value(col) FROM VALUES (NULL), (5), (20) AS tab(col);
 NULL
> SELECT any_value(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
 5

Note:
The function is non-deterministic.
Since: 3.4.0

approx_count_distinct
approx_count_distinct(expr[, relativeSD]) - Returns the estimated cardinality by HyperLogLog++.
relativeSD defines the maximum relative standard deviation allowed.
Examples:
> SELECT approx_count_distinct(col1) FROM VALUES (1), (1), (2), (2), (3) tab(col1);
 3

Since: 1.6.0

approx_percentile
approx_percentile(col, percentage [, accuracy]) - Returns the approximate percentile of the numeric or
ansi interval column col which is the smallest value in the ordered col values (sorted
from least to greatest) such that no more than percentage of col values is less than
the value or equal to that value. The value of percentage must be between 0.0 and 1.0.
The accuracy parameter (default: 10000) is a positive numeric literal which controls
approximation accuracy at the cost of memory. Higher value of accuracy yields better
accuracy, 1.0/accuracy is the relative error of the approximation.
When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.
In this case, returns the approximate percentile array of column col at the given
percentage array.
Examples:
> SELECT approx_percentile(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);
 [1,1,0]
> SELECT approx_percentile(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);
 7
> SELECT approx_percentile(col, 0.5, 100) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '1' MONTH), (INTERVAL '2' MONTH), (INTERVAL '10' MONTH) AS tab(col);
 0-1
> SELECT approx_percentile(col, array(0.5, 0.7), 100) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '1' SECOND), (INTERVAL '2' SECOND), (INTERVAL '10' SECOND) AS tab(col);
 [0 00:00:01.000000000,0 00:00:02.000000000]

Since: 2.1.0

array
array(expr, ...) - Returns an array with the given elements.
Examples:
> SELECT array(1, 2, 3);
 [1,2,3]

Since: 1.1.0

array_agg
array_agg(expr) - Collects and returns a list of non-unique elements.
Examples:
> SELECT array_agg(col) FROM VALUES (1), (2), (1) AS tab(col);
 [1,2,1]

Note:
The function is non-deterministic because the order of collected results depends
on the order of the rows which may be non-deterministic after a shuffle.
Since: 3.3.0

array_append
array_append(array, element) - Add the element at the end of the array passed as first
argument. Type of element should be similar to type of the elements of the array.
Null element is also appended into the array. But if the array passed, is NULL
output is NULL
Examples:
> SELECT array_append(array('b', 'd', 'c', 'a'), 'd');
 ["b","d","c","a","d"]
> SELECT array_append(array(1, 2, 3, null), null);
 [1,2,3,null,null]
> SELECT array_append(CAST(null as Array<Int>), 2);
 NULL

Since: 3.4.0

array_compact
array_compact(array) - Removes null values from the array.
Examples:
> SELECT array_compact(array(1, 2, 3, null));
 [1,2,3]
> SELECT array_compact(array("a", "b", "c"));
 ["a","b","c"]

Since: 3.4.0

array_contains
array_contains(array, value) - Returns true if the array contains the value.
Examples:
> SELECT array_contains(array(1, 2, 3), 2);
 true

Since: 1.5.0

array_distinct
array_distinct(array) - Removes duplicate values from the array.
Examples:
> SELECT array_distinct(array(1, 2, 3, null, 3));
 [1,2,3,null]

Since: 2.4.0

array_except
array_except(array1, array2) - Returns an array of the elements in array1 but not in array2,
without duplicates.
Examples:
> SELECT array_except(array(1, 2, 3), array(1, 3, 5));
 [2]

Since: 2.4.0

array_insert
array_insert(x, pos, val) - Places val into index pos of array x.
Array indices start at 1. The maximum negative index is -1 for which the function inserts
new element after the current last element.
Index above array size appends the array, or prepends the array if index is negative,
with 'null' elements.
Examples:
> SELECT array_insert(array(1, 2, 3, 4), 5, 5);
 [1,2,3,4,5]
> SELECT array_insert(array(5, 4, 3, 2), -1, 1);
 [5,4,3,2,1]
> SELECT array_insert(array(5, 3, 2, 1), -4, 4);
 [5,4,3,2,1]

Since: 3.4.0

array_intersect
array_intersect(array1, array2) - Returns an array of the elements in the intersection of array1 and
array2, without duplicates.
Examples:
> SELECT array_intersect(array(1, 2, 3), array(1, 3, 5));
 [1,3]

Since: 2.4.0

array_join
array_join(array, delimiter[, nullReplacement]) - Concatenates the elements of the given array
using the delimiter and an optional string to replace nulls. If no value is set for
nullReplacement, any null value is filtered.
Examples:
> SELECT array_join(array('hello', 'world'), ' ');
 hello world
> SELECT array_join(array('hello', null ,'world'), ' ');
 hello world
> SELECT array_join(array('hello', null ,'world'), ' ', ',');
 hello , world

Since: 2.4.0

array_max
array_max(array) - Returns the maximum value in the array. NaN is greater than
any non-NaN elements for double/float type. NULL elements are skipped.
Examples:
> SELECT array_max(array(1, 20, null, 3));
 20

Since: 2.4.0

array_min
array_min(array) - Returns the minimum value in the array. NaN is greater than
any non-NaN elements for double/float type. NULL elements are skipped.
Examples:
> SELECT array_min(array(1, 20, null, 3));
 1

Since: 2.4.0

array_position
array_position(array, element) - Returns the (1-based) index of the first matching element of
the array as long, or 0 if no match is found.
Examples:
> SELECT array_position(array(312, 773, 708, 708), 708);
 3
> SELECT array_position(array(312, 773, 708, 708), 414);
 0

Since: 2.4.0

array_prepend
array_prepend(array, element) - Add the element at the beginning of the array passed as first
argument. Type of element should be the same as the type of the elements of the array.
Null element is also prepended to the array. But if the array passed is NULL
output is NULL
Examples:
> SELECT array_prepend(array('b', 'd', 'c', 'a'), 'd');
 ["d","b","d","c","a"]
> SELECT array_prepend(array(1, 2, 3, null), null);
 [null,1,2,3,null]
> SELECT array_prepend(CAST(null as Array<Int>), 2);
 NULL

Since: 3.5.0

array_remove
array_remove(array, element) - Remove all elements that equal to element from array.
Examples:
> SELECT array_remove(array(1, 2, 3, null, 3), 3);
 [1,2,null]

Since: 2.4.0

array_repeat
array_repeat(element, count) - Returns the array containing element count times.
Examples:
> SELECT array_repeat('123', 2);
 ["123","123"]

Since: 2.4.0

array_size
array_size(expr) - Returns the size of an array. The function returns null for null input.
Examples:
> SELECT array_size(array('b', 'd', 'c', 'a'));
 4

Since: 3.3.0

array_sort
array_sort(expr, func) - Sorts the input array. If func is omitted, sort
in ascending order. The elements of the input array must be orderable.
NaN is greater than any non-NaN elements for double/float type.
Null elements will be placed at the end of the returned array.
Since 3.0.0 this function also sorts and returns the array based on the
given comparator function. The comparator will take two arguments representing
two elements of the array.
It returns a negative integer, 0, or a positive integer as the first element is less than,
equal to, or greater than the second element. If the comparator function returns null,
the function will fail and raise an error.
Examples:
> SELECT array_sort(array(5, 6, 1), (left, right) -> case when left < right then -1 when left > right then 1 else 0 end);
 [1,5,6]
> SELECT array_sort(array('bc', 'ab', 'dc'), (left, right) -> case when left is null and right is null then 0 when left is null then -1 when right is null then 1 when left < right then 1 when left > right then -1 else 0 end);
 ["dc","bc","ab"]
> SELECT array_sort(array('b', 'd', null, 'c', 'a'));
 ["a","b","c","d",null]

Since: 2.4.0

array_union
array_union(array1, array2) - Returns an array of the elements in the union of array1 and array2,
without duplicates.
Examples:
> SELECT array_union(array(1, 2, 3), array(1, 3, 5));
 [1,2,3,5]

Since: 2.4.0

arrays_overlap
arrays_overlap(a1, a2) - Returns true if a1 contains at least a non-null element present also in a2. If the arrays have no common element and they are both non-empty and either of them contains a null element null is returned, false otherwise.
Examples:
> SELECT arrays_overlap(array(1, 2, 3), array(3, 4, 5));
 true

Since: 2.4.0

arrays_zip
arrays_zip(a1, a2, ...) - Returns a merged array of structs in which the N-th struct contains all
N-th values of input arrays.
Examples:
> SELECT arrays_zip(array(1, 2, 3), array(2, 3, 4));
 [{"0":1,"1":2},{"0":2,"1":3},{"0":3,"1":4}]
> SELECT arrays_zip(array(1, 2), array(2, 3), array(3, 4));
 [{"0":1,"1":2,"2":3},{"0":2,"1":3,"2":4}]

Since: 2.4.0

ascii
ascii(str) - Returns the numeric value of the first character of str.
Examples:
> SELECT ascii('222');
 50
> SELECT ascii(2);
 50

Since: 1.5.0

asin
asin(expr) - Returns the inverse sine (a.k.a. arc sine) the arc sin of expr,
as if computed by java.lang.Math.asin.
Examples:
> SELECT asin(0);
 0.0
> SELECT asin(2);
 NaN

Since: 1.4.0

asinh
asinh(expr) - Returns inverse hyperbolic sine of expr.
Examples:
> SELECT asinh(0);
 0.0

Since: 3.0.0

assert_true
assert_true(expr) - Throws an exception if expr is not true.
Examples:
> SELECT assert_true(0 < 1);
 NULL

Since: 2.0.0

atan
atan(expr) - Returns the inverse tangent (a.k.a. arc tangent) of expr, as if computed by
java.lang.Math.atan
Examples:
> SELECT atan(0);
 0.0

Since: 1.4.0

atan2
atan2(exprY, exprX) - Returns the angle in radians between the positive x-axis of a plane
and the point given by the coordinates (exprX, exprY), as if computed by
java.lang.Math.atan2.
Arguments:

exprY - coordinate on y-axis
exprX - coordinate on x-axis

Examples:
> SELECT atan2(0, 0);
 0.0

Since: 1.4.0

atanh
atanh(expr) - Returns inverse hyperbolic tangent of expr.
Examples:
> SELECT atanh(0);
 0.0
> SELECT atanh(2);
 NaN

Since: 3.0.0

avg
avg(expr) - Returns the mean calculated from values of a group.
Examples:
> SELECT avg(col) FROM VALUES (1), (2), (3) AS tab(col);
 2.0
> SELECT avg(col) FROM VALUES (1), (2), (NULL) AS tab(col);
 1.5

Since: 1.0.0

base64
base64(bin) - Converts the argument from a binary bin to a base 64 string.
Examples:
> SELECT base64('Spark SQL');
 U3BhcmsgU1FM
> SELECT base64(x'537061726b2053514c');
 U3BhcmsgU1FM

Since: 1.5.0

between
expr1 [NOT] BETWEEN expr2 AND expr3 - evaluate if expr1 is [not] in between expr2 and expr3.
Examples:
> SELECT col1 FROM VALUES 1, 3, 5, 7 WHERE col1 BETWEEN 2 AND 5;
 3
 5

Since: 1.0.0

bigint
bigint(expr) - Casts the value expr to the target data type bigint.
Since: 2.0.1

bin
bin(expr) - Returns the string representation of the long value expr represented in binary.
Examples:
> SELECT bin(13);
 1101
> SELECT bin(-13);
 1111111111111111111111111111111111111111111111111111111111110011
> SELECT bin(13.3);
 1101

Since: 1.5.0

binary
binary(expr) - Casts the value expr to the target data type binary.
Since: 2.0.1

bit_and
bit_and(expr) - Returns the bitwise AND of all non-null input values, or null if none.
Examples:
> SELECT bit_and(col) FROM VALUES (3), (5) AS tab(col);
 1

Since: 3.0.0

bit_count
bit_count(expr) - Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer, or NULL if the argument is NULL.
Examples:
> SELECT bit_count(0);
 0

Since: 3.0.0

bit_get
bit_get(expr, pos) - Returns the value of the bit (0 or 1) at the specified position.
The positions are numbered from right to left, starting at zero.
The position argument cannot be negative.
Examples:
> SELECT bit_get(11, 0);
 1
> SELECT bit_get(11, 2);
 0

Since: 3.2.0

bit_length
bit_length(expr) - Returns the bit length of string data or number of bits of binary data.
Examples:
> SELECT bit_length('Spark SQL');
 72
> SELECT bit_length(x'537061726b2053514c');
 72

Since: 2.3.0

bit_or
bit_or(expr) - Returns the bitwise OR of all non-null input values, or null if none.
Examples:
> SELECT bit_or(col) FROM VALUES (3), (5) AS tab(col);
 7

Since: 3.0.0

bit_xor
bit_xor(expr) - Returns the bitwise XOR of all non-null input values, or null if none.
Examples:
> SELECT bit_xor(col) FROM VALUES (3), (5) AS tab(col);
 6

Since: 3.0.0

bitmap_bit_position
bitmap_bit_position(child) - Returns the bit position for the given input child expression.
Examples:
> SELECT bitmap_bit_position(1);
 0
> SELECT bitmap_bit_position(123);
 122

Since: 3.5.0

bitmap_bucket_number
bitmap_bucket_number(child) - Returns the bucket number for the given input child expression.
Examples:
> SELECT bitmap_bucket_number(123);
 1
> SELECT bitmap_bucket_number(0);
 0

Since: 3.5.0

bitmap_construct_agg
bitmap_construct_agg(child) - Returns a bitmap with the positions of the bits set from all the values from
the child expression. The child expression will most likely be bitmap_bit_position().
Examples:
> SELECT substring(hex(bitmap_construct_agg(bitmap_bit_position(col))), 0, 6) FROM VALUES (1), (2), (3) AS tab(col);
 070000
> SELECT substring(hex(bitmap_construct_agg(bitmap_bit_position(col))), 0, 6) FROM VALUES (1), (1), (1) AS tab(col);
 010000

Since: 3.5.0

bitmap_count
bitmap_count(child) - Returns the number of set bits in the child bitmap.
Examples:
> SELECT bitmap_count(X '1010');
 2
> SELECT bitmap_count(X 'FFFF');
 16
> SELECT bitmap_count(X '0');
 0

Since: 3.5.0

bitmap_or_agg
bitmap_or_agg(child) - Returns a bitmap that is the bitwise OR of all of the bitmaps from the child
expression. The input should be bitmaps created from bitmap_construct_agg().
Examples:
> SELECT substring(hex(bitmap_or_agg(col)), 0, 6) FROM VALUES (X '10'), (X '20'), (X '40') AS tab(col);
 700000
> SELECT substring(hex(bitmap_or_agg(col)), 0, 6) FROM VALUES (X '10'), (X '10'), (X '10') AS tab(col);
 100000

Since: 3.5.0

bool_and
bool_and(expr) - Returns true if all values of expr are true.
Examples:
> SELECT bool_and(col) FROM VALUES (true), (true), (true) AS tab(col);
 true
> SELECT bool_and(col) FROM VALUES (NULL), (true), (true) AS tab(col);
 true
> SELECT bool_and(col) FROM VALUES (true), (false), (true) AS tab(col);
 false

Since: 3.0.0

bool_or
bool_or(expr) - Returns true if at least one value of expr is true.
Examples:
> SELECT bool_or(col) FROM VALUES (true), (false), (false) AS tab(col);
 true
> SELECT bool_or(col) FROM VALUES (NULL), (true), (false) AS tab(col);
 true
> SELECT bool_or(col) FROM VALUES (false), (false), (NULL) AS tab(col);
 false

Since: 3.0.0

boolean
boolean(expr) - Casts the value expr to the target data type boolean.
Since: 2.0.1

bround
bround(expr, d) - Returns expr rounded to d decimal places using HALF_EVEN rounding mode.
Examples:
> SELECT bround(2.5, 0);
 2
> SELECT bround(25, -1);
 20

Since: 2.0.0

btrim
btrim(str) - Removes the leading and trailing space characters from str.
btrim(str, trimStr) - Remove the leading and trailing trimStr characters from str.
Arguments:

str - a string expression
trimStr - the trim string characters to trim, the default value is a single space

Examples:
> SELECT btrim('    SparkSQL   ');
 SparkSQL
> SELECT btrim(encode('    SparkSQL   ', 'utf-8'));
 SparkSQL
> SELECT btrim('SSparkSQLS', 'SL');
 parkSQ
> SELECT btrim(encode('SSparkSQLS', 'utf-8'), encode('SL', 'utf-8'));
 parkSQ

Since: 3.2.0

cardinality
cardinality(expr) - Returns the size of an array or a map.
The function returns null for null input if spark.sql.legacy.sizeOfNull is set to false or
spark.sql.ansi.enabled is set to true. Otherwise, the function returns -1 for null input.
With the default settings, the function returns -1 for null input.
Examples:
> SELECT cardinality(array('b', 'd', 'c', 'a'));
 4
> SELECT cardinality(map('a', 1, 'b', 2));
 2

Since: 1.5.0

case
CASE expr1 WHEN expr2 THEN expr3 [WHEN expr4 THEN expr5]* [ELSE expr6] END - When expr1 = expr2, returns expr3; when expr1 = expr4, return expr5; else return expr6.
Arguments:

expr1 - the expression which is one operand of comparison.
expr2, expr4 - the expressions each of which is the other   operand of comparison.
expr3, expr5, expr6 - the branch value expressions and else value expression  should all be same type or coercible to a common type.

Examples:
> SELECT CASE col1 WHEN 1 THEN 'one' WHEN 2 THEN 'two' ELSE '?' END FROM VALUES 1, 2, 3;
 one
 two
 ?
> SELECT CASE col1 WHEN 1 THEN 'one' WHEN 2 THEN 'two' END FROM VALUES 1, 2, 3;
 one
 two
 NULL

Since: 1.0.1

cast
cast(expr AS type) - Casts the value expr to the target data type type.
Examples:
> SELECT cast('10' as int);
 10

Since: 1.0.0

cbrt
cbrt(expr) - Returns the cube root of expr.
Examples:
> SELECT cbrt(27.0);
 3.0

Since: 1.4.0

ceil
ceil(expr[, scale]) - Returns the smallest number after rounding up that is not smaller than expr. An optional scale parameter can be specified to control the rounding behavior.
Examples:
> SELECT ceil(-0.1);
 0
> SELECT ceil(5);
 5
> SELECT ceil(3.1411, 3);
 3.142
> SELECT ceil(3.1411, -3);
 1000

Since: 3.3.0

ceiling
ceiling(expr[, scale]) - Returns the smallest number after rounding up that is not smaller than expr. An optional scale parameter can be specified to control the rounding behavior.
Examples:
> SELECT ceiling(-0.1);
 0
> SELECT ceiling(5);
 5
> SELECT ceiling(3.1411, 3);
 3.142
> SELECT ceiling(3.1411, -3);
 1000

Since: 3.3.0

char
char(expr) - Returns the ASCII character having the binary equivalent to expr. If n is larger than 256 the result is equivalent to chr(n % 256)
Examples:
> SELECT char(65);
 A

Since: 2.3.0

char_length
char_length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.
Examples:
> SELECT char_length('Spark SQL ');
 10
> SELECT char_length(x'537061726b2053514c');
 9
> SELECT CHAR_LENGTH('Spark SQL ');
 10
> SELECT CHARACTER_LENGTH('Spark SQL ');
 10

Since: 1.5.0

character_length
character_length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.
Examples:
> SELECT character_length('Spark SQL ');
 10
> SELECT character_length(x'537061726b2053514c');
 9
> SELECT CHAR_LENGTH('Spark SQL ');
 10
> SELECT CHARACTER_LENGTH('Spark SQL ');
 10

Since: 1.5.0

chr
chr(expr) - Returns the ASCII character having the binary equivalent to expr. If n is larger than 256 the result is equivalent to chr(n % 256)
Examples:
> SELECT chr(65);
 A

Since: 2.3.0

coalesce
coalesce(expr1, expr2, ...) - Returns the first non-null argument if exists. Otherwise, null.
Examples:
> SELECT coalesce(NULL, 1, NULL);
 1

Since: 1.0.0

collect_list
collect_list(expr) - Collects and returns a list of non-unique elements.
Examples:
> SELECT collect_list(col) FROM VALUES (1), (2), (1) AS tab(col);
 [1,2,1]

Note:
The function is non-deterministic because the order of collected results depends
on the order of the rows which may be non-deterministic after a shuffle.
Since: 2.0.0

collect_set
collect_set(expr) - Collects and returns a set of unique elements.
Examples:
> SELECT collect_set(col) FROM VALUES (1), (2), (1) AS tab(col);
 [1,2]

Note:
The function is non-deterministic because the order of collected results depends
on the order of the rows which may be non-deterministic after a shuffle.
Since: 2.0.0

concat
concat(col1, col2, ..., colN) - Returns the concatenation of col1, col2, ..., colN.
Examples:
> SELECT concat('Spark', 'SQL');
 SparkSQL
> SELECT concat(array(1, 2, 3), array(4, 5), array(6));
 [1,2,3,4,5,6]

Note:
Concat logic for arrays is available since 2.4.0.
Since: 1.5.0

concat_ws
concat_ws(sep[, str | array(str)]+) - Returns the concatenation of the strings separated by sep, skipping null values.
Examples:
> SELECT concat_ws(' ', 'Spark', 'SQL');
  Spark SQL
> SELECT concat_ws('s');

> SELECT concat_ws('/', 'foo', null, 'bar');
  foo/bar
> SELECT concat_ws(null, 'Spark', 'SQL');
  NULL

Since: 1.5.0

contains
contains(left, right) - Returns a boolean. The value is True if right is found inside left.
Returns NULL if either input expression is NULL. Otherwise, returns False.
Both left or right must be of STRING or BINARY type.
Examples:
> SELECT contains('Spark SQL', 'Spark');
 true
> SELECT contains('Spark SQL', 'SPARK');
 false
> SELECT contains('Spark SQL', null);
 NULL
> SELECT contains(x'537061726b2053514c', x'537061726b');
 true

Since: 3.3.0

conv
conv(num, from_base, to_base) - Convert num from from_base to to_base.
Examples:
> SELECT conv('100', 2, 10);
 4
> SELECT conv(-10, 16, -10);
 -16

Since: 1.5.0

convert_timezone
convert_timezone([sourceTz, ]targetTz, sourceTs) - Converts the timestamp without time zone sourceTs from the sourceTz time zone to targetTz.
Arguments:

sourceTz - the time zone for the input timestamp.
             If it is missed, the current session time zone is used as the source time zone.
targetTz - the time zone to which the input timestamp should be converted
sourceTs - a timestamp without time zone

Examples:
> SELECT convert_timezone('Europe/Brussels', 'America/Los_Angeles', timestamp_ntz'2021-12-06 00:00:00');
 2021-12-05 15:00:00
> SELECT convert_timezone('Europe/Brussels', timestamp_ntz'2021-12-05 15:00:00');
 2021-12-06 00:00:00

Since: 3.4.0

corr
corr(expr1, expr2) - Returns Pearson coefficient of correlation between a set of number pairs.
Examples:
> SELECT corr(c1, c2) FROM VALUES (3, 2), (3, 3), (6, 4) as tab(c1, c2);
 0.8660254037844387

Since: 1.6.0

cos
cos(expr) - Returns the cosine of expr, as if computed by
java.lang.Math.cos.
Arguments:

expr - angle in radians

Examples:
> SELECT cos(0);
 1.0

Since: 1.4.0

cosh
cosh(expr) - Returns the hyperbolic cosine of expr, as if computed by
java.lang.Math.cosh.
Arguments:

expr - hyperbolic angle

Examples:
> SELECT cosh(0);
 1.0

Since: 1.4.0

cot
cot(expr) - Returns the cotangent of expr, as if computed by 1/java.lang.Math.tan.
Arguments:

expr - angle in radians

Examples:
> SELECT cot(1);
 0.6420926159343306

Since: 2.3.0

count
count(*) - Returns the total number of retrieved rows, including rows containing null.
count(expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are all non-null.
count(DISTINCT expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are unique and non-null.
Examples:
> SELECT count(*) FROM VALUES (NULL), (5), (5), (20) AS tab(col);
 4
> SELECT count(col) FROM VALUES (NULL), (5), (5), (20) AS tab(col);
 3
> SELECT count(DISTINCT col) FROM VALUES (NULL), (5), (5), (10) AS tab(col);
 2

Since: 1.0.0

count_if
count_if(expr) - Returns the number of TRUE values for the expression.
Examples:
> SELECT count_if(col % 2 = 0) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);
 2
> SELECT count_if(col IS NULL) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);
 1

Since: 3.0.0

count_min_sketch
count_min_sketch(col, eps, confidence, seed) - Returns a count-min sketch of a column with the given esp,
confidence and seed. The result is an array of bytes, which can be deserialized to a
CountMinSketch before usage. Count-min sketch is a probabilistic data structure used for
cardinality estimation using sub-linear space.
Examples:
> SELECT hex(count_min_sketch(col, 0.5d, 0.5d, 1)) FROM VALUES (1), (2), (1) AS tab(col);
 0000000100000000000000030000000100000004000000005D8D6AB90000000000000000000000000000000200000000000000010000000000000000

Since: 2.2.0

covar_pop
covar_pop(expr1, expr2) - Returns the population covariance of a set of number pairs.
Examples:
> SELECT covar_pop(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);
 0.6666666666666666

Since: 2.0.0

covar_samp
covar_samp(expr1, expr2) - Returns the sample covariance of a set of number pairs.
Examples:
> SELECT covar_samp(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);
 1.0

Since: 2.0.0

crc32
crc32(expr) - Returns a cyclic redundancy check value of the expr as a bigint.
Examples:
> SELECT crc32('Spark');
 1557323817

Since: 1.5.0

csc
csc(expr) - Returns the cosecant of expr, as if computed by 1/java.lang.Math.sin.
Arguments:

expr - angle in radians

Examples:
> SELECT csc(1);
 1.1883951057781212

Since: 3.3.0

cume_dist
cume_dist() - Computes the position of a value relative to all values in the partition.
Examples:
> SELECT a, b, cume_dist() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   0.6666666666666666
 A1 1   0.6666666666666666
 A1 2   1.0
 A2 3   1.0

Since: 2.0.0

curdate
curdate() - Returns the current date at the start of query evaluation. All calls of curdate within the same query return the same value.
Examples:
> SELECT curdate();
 2022-09-06

Since: 3.4.0

current_catalog
current_catalog() - Returns the current catalog.
Examples:
> SELECT current_catalog();
 spark_catalog

Since: 3.1.0

current_database
current_database() - Returns the current database.
Examples:
> SELECT current_database();
 default

Since: 1.6.0

current_date
current_date() - Returns the current date at the start of query evaluation. All calls of current_date within the same query return the same value.
current_date - Returns the current date at the start of query evaluation.
Examples:
> SELECT current_date();
 2020-04-25
> SELECT current_date;
 2020-04-25

Note:
The syntax without braces has been supported since 2.0.1.
Since: 1.5.0

current_schema
current_schema() - Returns the current database.
Examples:
> SELECT current_schema();
 default

Since: 1.6.0

current_timestamp
current_timestamp() - Returns the current timestamp at the start of query evaluation. All calls of current_timestamp within the same query return the same value.
current_timestamp - Returns the current timestamp at the start of query evaluation.
Examples:
> SELECT current_timestamp();
 2020-04-25 15:49:11.914
> SELECT current_timestamp;
 2020-04-25 15:49:11.914

Note:
The syntax without braces has been supported since 2.0.1.
Since: 1.5.0

current_timezone
current_timezone() - Returns the current session local timezone.
Examples:
> SELECT current_timezone();
 Asia/Shanghai

Since: 3.1.0

current_user
current_user() - user name of current execution context.
Examples:
> SELECT current_user();
 mockingjay

Since: 3.2.0

date
date(expr) - Casts the value expr to the target data type date.
Since: 2.0.1

date_add
date_add(start_date, num_days) - Returns the date that is num_days after start_date.
Examples:
> SELECT date_add('2016-07-30', 1);
 2016-07-31

Since: 1.5.0

date_diff
date_diff(endDate, startDate) - Returns the number of days from startDate to endDate.
Examples:
> SELECT date_diff('2009-07-31', '2009-07-30');
 1

> SELECT date_diff('2009-07-30', '2009-07-31');
 -1

Since: 3.4.0

date_format
date_format(timestamp, fmt) - Converts timestamp to a value of string in the format specified by the date format fmt.
Arguments:

timestamp - A date/timestamp or string to be converted to the given format.
fmt - Date/time format pattern to follow. See Datetime Patterns for valid date
        and time format patterns.

Examples:
> SELECT date_format('2016-04-08', 'y');
 2016

Since: 1.5.0

date_from_unix_date
date_from_unix_date(days) - Create date from the number of days since 1970-01-01.
Examples:
> SELECT date_from_unix_date(1);
 1970-01-02

Since: 3.1.0

date_part
date_part(field, source) - Extracts a part of the date/timestamp or interval source.
Arguments:

field - selects which part of the source should be extracted, and supported string values are as same as the fields of the equivalent function EXTRACT.
source - a date/timestamp or interval column from where field should be extracted

Examples:
> SELECT date_part('YEAR', TIMESTAMP '2019-08-12 01:00:00.123456');
 2019
> SELECT date_part('week', timestamp'2019-08-12 01:00:00.123456');
 33
> SELECT date_part('doy', DATE'2019-08-12');
 224
> SELECT date_part('SECONDS', timestamp'2019-10-01 00:00:01.000001');
 1.000001
> SELECT date_part('days', interval 5 days 3 hours 7 minutes);
 5
> SELECT date_part('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
 30.001001
> SELECT date_part('MONTH', INTERVAL '2021-11' YEAR TO MONTH);
 11
> SELECT date_part('MINUTE', INTERVAL '123 23:55:59.002001' DAY TO SECOND);
 55

Note:
The date_part function is equivalent to the SQL-standard function EXTRACT(field FROM source)
Since: 3.0.0

date_sub
date_sub(start_date, num_days) - Returns the date that is num_days before start_date.
Examples:
> SELECT date_sub('2016-07-30', 1);
 2016-07-29

Since: 1.5.0

date_trunc
date_trunc(fmt, ts) - Returns timestamp ts truncated to the unit specified by the format model fmt.
Arguments:

fmt - the format representing the unit to be truncated to
"YEAR", "YYYY", "YY" - truncate to the first date of the year that the ts falls in, the time part will be zero out
"QUARTER" - truncate to the first date of the quarter that the ts falls in, the time part will be zero out
"MONTH", "MM", "MON" - truncate to the first date of the month that the ts falls in, the time part will be zero out
"WEEK" - truncate to the Monday of the week that the ts falls in, the time part will be zero out
"DAY", "DD" - zero out the time part
"HOUR" - zero out the minute and second with fraction part
"MINUTE"- zero out the second with fraction part
"SECOND" -  zero out the second fraction part
"MILLISECOND" - zero out the microseconds
"MICROSECOND" - everything remains


ts - datetime value or valid timestamp string

Examples:
> SELECT date_trunc('YEAR', '2015-03-05T09:32:05.359');
 2015-01-01 00:00:00
> SELECT date_trunc('MM', '2015-03-05T09:32:05.359');
 2015-03-01 00:00:00
> SELECT date_trunc('DD', '2015-03-05T09:32:05.359');
 2015-03-05 00:00:00
> SELECT date_trunc('HOUR', '2015-03-05T09:32:05.359');
 2015-03-05 09:00:00
> SELECT date_trunc('MILLISECOND', '2015-03-05T09:32:05.123456');
 2015-03-05 09:32:05.123

Since: 2.3.0

dateadd
dateadd(start_date, num_days) - Returns the date that is num_days after start_date.
Examples:
> SELECT dateadd('2016-07-30', 1);
 2016-07-31

Since: 3.4.0

datediff
datediff(endDate, startDate) - Returns the number of days from startDate to endDate.
Examples:
> SELECT datediff('2009-07-31', '2009-07-30');
 1

> SELECT datediff('2009-07-30', '2009-07-31');
 -1

Since: 1.5.0

datepart
datepart(field, source) - Extracts a part of the date/timestamp or interval source.
Arguments:

field - selects which part of the source should be extracted, and supported string values are as same as the fields of the equivalent function EXTRACT.
source - a date/timestamp or interval column from where field should be extracted

Examples:
> SELECT datepart('YEAR', TIMESTAMP '2019-08-12 01:00:00.123456');
 2019
> SELECT datepart('week', timestamp'2019-08-12 01:00:00.123456');
 33
> SELECT datepart('doy', DATE'2019-08-12');
 224
> SELECT datepart('SECONDS', timestamp'2019-10-01 00:00:01.000001');
 1.000001
> SELECT datepart('days', interval 5 days 3 hours 7 minutes);
 5
> SELECT datepart('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
 30.001001
> SELECT datepart('MONTH', INTERVAL '2021-11' YEAR TO MONTH);
 11
> SELECT datepart('MINUTE', INTERVAL '123 23:55:59.002001' DAY TO SECOND);
 55

Note:
The datepart function is equivalent to the SQL-standard function EXTRACT(field FROM source)
Since: 3.4.0

day
day(date) - Returns the day of month of the date/timestamp.
Examples:
> SELECT day('2009-07-30');
 30

Since: 1.5.0

dayofmonth
dayofmonth(date) - Returns the day of month of the date/timestamp.
Examples:
> SELECT dayofmonth('2009-07-30');
 30

Since: 1.5.0

dayofweek
dayofweek(date) - Returns the day of the week for date/timestamp (1 = Sunday, 2 = Monday, ..., 7 = Saturday).
Examples:
> SELECT dayofweek('2009-07-30');
 5

Since: 2.3.0

dayofyear
dayofyear(date) - Returns the day of year of the date/timestamp.
Examples:
> SELECT dayofyear('2016-04-09');
 100

Since: 1.5.0

decimal
decimal(expr) - Casts the value expr to the target data type decimal.
Since: 2.0.1

decode
decode(bin, charset) - Decodes the first argument using the second argument character set.
decode(expr, search, result [, search, result ] ... [, default]) - Compares expr
to each search value in order. If expr is equal to a search value, decode returns
the corresponding result. If no match is found, then it returns default. If default
is omitted, it returns null.
Examples:
> SELECT decode(encode('abc', 'utf-8'), 'utf-8');
 abc
> SELECT decode(2, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');
 San Francisco
> SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');
 Non domestic
> SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle');
 NULL
> SELECT decode(null, 6, 'Spark', NULL, 'SQL', 4, 'rocks');
 SQL

Since: 3.2.0

degrees
degrees(expr) - Converts radians to degrees.
Arguments:

expr - angle in radians

Examples:
> SELECT degrees(3.141592653589793);
 180.0

Since: 1.4.0

dense_rank
dense_rank() - Computes the rank of a value in a group of values. The result is one plus the
previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps
in the ranking sequence.
Arguments:

children - this is to base the rank on; a change in the value of one the children will
    trigger a change in rank. This is an internal parameter and will be assigned by the
    Analyser.

Examples:
> SELECT a, b, dense_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   1
 A1 1   1
 A1 2   2
 A2 3   1

Since: 2.0.0

div
expr1 div expr2 - Divide expr1 by expr2. It returns NULL if an operand is NULL or expr2 is 0. The result is casted to long.
Examples:
> SELECT 3 div 2;
 1
> SELECT INTERVAL '1-1' YEAR TO MONTH div INTERVAL '-1' MONTH;
 -13

Since: 3.0.0

double
double(expr) - Casts the value expr to the target data type double.
Since: 2.0.1

e
e() - Returns Euler's number, e.
Examples:
> SELECT e();
 2.718281828459045

Since: 1.5.0

element_at
element_at(array, index) - Returns element of array at given (1-based) index. If Index is 0,
Spark will throw an error. If index < 0, accesses elements from the last to the first.
The function returns NULL if the index exceeds the length of the array and
spark.sql.ansi.enabled is set to false.
If spark.sql.ansi.enabled is set to true, it throws ArrayIndexOutOfBoundsException
for invalid indices.
element_at(map, key) - Returns value for given key. The function returns NULL if the key is not
contained in the map.
Examples:
> SELECT element_at(array(1, 2, 3), 2);
 2
> SELECT element_at(map(1, 'a', 2, 'b'), 2);
 b

Since: 2.4.0

elt
elt(n, input1, input2, ...) - Returns the n-th input, e.g., returns input2 when n is 2.
The function returns NULL if the index exceeds the length of the array
and spark.sql.ansi.enabled is set to false. If spark.sql.ansi.enabled is set to true,
it throws ArrayIndexOutOfBoundsException for invalid indices.
Examples:
> SELECT elt(1, 'scala', 'java');
 scala
> SELECT elt(2, 'a', 1);
 1

Since: 2.0.0

encode
encode(str, charset) - Encodes the first argument using the second argument character set.
Examples:
> SELECT encode('abc', 'utf-8');
 abc

Since: 1.5.0

endswith
endswith(left, right) - Returns a boolean. The value is True if left ends with right.
Returns NULL if either input expression is NULL. Otherwise, returns False.
Both left or right must be of STRING or BINARY type.
Examples:
> SELECT endswith('Spark SQL', 'SQL');
 true
> SELECT endswith('Spark SQL', 'Spark');
 false
> SELECT endswith('Spark SQL', null);
 NULL
> SELECT endswith(x'537061726b2053514c', x'537061726b');
 false
> SELECT endswith(x'537061726b2053514c', x'53514c');
 true

Since: 3.3.0

equal_null
equal_null(expr1, expr2) - Returns same result as the EQUAL(=) operator for non-null operands,
but returns true if both are null, false if one of the them is null.
Arguments:

expr1, expr2 - the two expressions must be same type or can be casted to a common type,
    and must be a type that can be used in equality comparison. Map type is not supported.
    For complex types such array/struct, the data types of fields must be orderable.

Examples:
> SELECT equal_null(3, 3);
 true
> SELECT equal_null(1, '11');
 false
> SELECT equal_null(true, NULL);
 false
> SELECT equal_null(NULL, 'abc');
 false
> SELECT equal_null(NULL, NULL);
 true

Since: 3.4.0

every
every(expr) - Returns true if all values of expr are true.
Examples:
> SELECT every(col) FROM VALUES (true), (true), (true) AS tab(col);
 true
> SELECT every(col) FROM VALUES (NULL), (true), (true) AS tab(col);
 true
> SELECT every(col) FROM VALUES (true), (false), (true) AS tab(col);
 false

Since: 3.0.0

exists
exists(expr, pred) - Tests whether a predicate holds for one or more elements in the array.
Examples:
> SELECT exists(array(1, 2, 3), x -> x % 2 == 0);
 true
> SELECT exists(array(1, 2, 3), x -> x % 2 == 10);
 false
> SELECT exists(array(1, null, 3), x -> x % 2 == 0);
 NULL
> SELECT exists(array(0, null, 2, 3, null), x -> x IS NULL);
 true
> SELECT exists(array(1, 2, 3), x -> x IS NULL);
 false

Since: 2.4.0

exp
exp(expr) - Returns e to the power of expr.
Examples:
> SELECT exp(0);
 1.0

Since: 1.4.0

explode
explode(expr) - Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Unless specified otherwise, uses the default column name col for elements of the array or key and value for the elements of the map.
Examples:
> SELECT explode(array(10, 20));
 10
 20
> SELECT explode(collection => array(10, 20));
 10
 20
> SELECT * FROM explode(collection => array(10, 20));
 10
 20

Since: 1.0.0

explode_outer
explode_outer(expr) - Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Unless specified otherwise, uses the default column name col for elements of the array or key and value for the elements of the map.
Examples:
> SELECT explode_outer(array(10, 20));
 10
 20
> SELECT explode_outer(collection => array(10, 20));
 10
 20
> SELECT * FROM explode_outer(collection => array(10, 20));
 10
 20

Since: 1.0.0

expm1
expm1(expr) - Returns exp(expr) - 1.
Examples:
> SELECT expm1(0);
 0.0

Since: 1.4.0

extract
extract(field FROM source) - Extracts a part of the date/timestamp or interval source.
Arguments:

field - selects which part of the source should be extracted
Supported string values of field for dates and timestamps are(case insensitive):
"YEAR", ("Y", "YEARS", "YR", "YRS") - the year field
"YEAROFWEEK" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004
"QUARTER", ("QTR") - the quarter (1 - 4) of the year that the datetime falls in
"MONTH", ("MON", "MONS", "MONTHS") - the month field (1 - 12)
"WEEK", ("W", "WEEKS") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013
"DAY", ("D", "DAYS") - the day of the month field (1 - 31)
"DAYOFWEEK",("DOW") - the day of the week for datetime as Sunday(1) to Saturday(7)
"DAYOFWEEK_ISO",("DOW_ISO") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)
"DOY" - the day of the year (1 - 365/366)
"HOUR", ("H", "HOURS", "HR", "HRS") - The hour field (0 - 23)
"MINUTE", ("M", "MIN", "MINS", "MINUTES") - the minutes field (0 - 59)
"SECOND", ("S", "SEC", "SECONDS", "SECS") - the seconds field, including fractional parts


Supported string values of field for interval(which consists of months, days, microseconds) are(case insensitive):
"YEAR", ("Y", "YEARS", "YR", "YRS") - the total months / 12
"MONTH", ("MON", "MONS", "MONTHS") - the total months % 12
"DAY", ("D", "DAYS") - the days part of interval
"HOUR", ("H", "HOURS", "HR", "HRS") - how many hours the microseconds contains
"MINUTE", ("M", "MIN", "MINS", "MINUTES") - how many minutes left after taking hours from microseconds
"SECOND", ("S", "SEC", "SECONDS", "SECS") - how many second with fractions left after taking hours and minutes from microseconds




source - a date/timestamp or interval column from where field should be extracted

Examples:
> SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');
 2019
> SELECT extract(week FROM timestamp'2019-08-12 01:00:00.123456');
 33
> SELECT extract(doy FROM DATE'2019-08-12');
 224
> SELECT extract(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');
 1.000001
> SELECT extract(days FROM interval 5 days 3 hours 7 minutes);
 5
> SELECT extract(seconds FROM interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
 30.001001
> SELECT extract(MONTH FROM INTERVAL '2021-11' YEAR TO MONTH);
 11
> SELECT extract(MINUTE FROM INTERVAL '123 23:55:59.002001' DAY TO SECOND);
 55

Note:
The extract function is equivalent to date_part(field, source).
Since: 3.0.0

factorial
factorial(expr) - Returns the factorial of expr. expr is [0..20]. Otherwise, null.
Examples:
> SELECT factorial(5);
 120

Since: 1.5.0

filter
filter(expr, func) - Filters the input array using the given predicate.
Examples:
> SELECT filter(array(1, 2, 3), x -> x % 2 == 1);
 [1,3]
> SELECT filter(array(0, 2, 3), (x, i) -> x > i);
 [2,3]
> SELECT filter(array(0, null, 2, 3, null), x -> x IS NOT NULL);
 [0,2,3]

Note:
The inner function may use the index argument since 3.0.0.
Since: 2.4.0

find_in_set
find_in_set(str, str_array) - Returns the index (1-based) of the given string (str) in the comma-delimited list (str_array).
Returns 0, if the string was not found or if the given string (str) contains a comma.
Examples:
> SELECT find_in_set('ab','abc,b,ab,c,def');
 3

Since: 1.5.0

first
first(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows.
If isIgnoreNull is true, returns only non-null values.
Examples:
> SELECT first(col) FROM VALUES (10), (5), (20) AS tab(col);
 10
> SELECT first(col) FROM VALUES (NULL), (5), (20) AS tab(col);
 NULL
> SELECT first(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
 5

Note:
The function is non-deterministic because its results depends on the order of the rows
which may be non-deterministic after a shuffle.
Since: 2.0.0

first_value
first_value(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows.
If isIgnoreNull is true, returns only non-null values.
Examples:
> SELECT first_value(col) FROM VALUES (10), (5), (20) AS tab(col);
 10
> SELECT first_value(col) FROM VALUES (NULL), (5), (20) AS tab(col);
 NULL
> SELECT first_value(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
 5

Note:
The function is non-deterministic because its results depends on the order of the rows
which may be non-deterministic after a shuffle.
Since: 2.0.0

flatten
flatten(arrayOfArrays) - Transforms an array of arrays into a single array.
Examples:
> SELECT flatten(array(array(1, 2), array(3, 4)));
 [1,2,3,4]

Since: 2.4.0

float
float(expr) - Casts the value expr to the target data type float.
Since: 2.0.1

floor
floor(expr[, scale]) - Returns the largest number after rounding down that is not greater than expr. An optional scale parameter can be specified to control the rounding behavior.
Examples:
> SELECT floor(-0.1);
 -1
> SELECT floor(5);
 5
> SELECT floor(3.1411, 3);
 3.141
> SELECT floor(3.1411, -3);
 0

Since: 3.3.0

forall
forall(expr, pred) - Tests whether a predicate holds for all elements in the array.
Examples:
> SELECT forall(array(1, 2, 3), x -> x % 2 == 0);
 false
> SELECT forall(array(2, 4, 8), x -> x % 2 == 0);
 true
> SELECT forall(array(1, null, 3), x -> x % 2 == 0);
 false
> SELECT forall(array(2, null, 8), x -> x % 2 == 0);
 NULL

Since: 3.0.0

format_number
format_number(expr1, expr2) - Formats the number expr1 like '#,###,###.##', rounded to expr2
decimal places. If expr2 is 0, the result has no decimal point or fractional part.
expr2 also accept a user specified format.
This is supposed to function like MySQL's FORMAT.
Examples:
> SELECT format_number(12332.123456, 4);
 12,332.1235
> SELECT format_number(12332.123456, '##################.###');
 12332.123

Since: 1.5.0

format_string
format_string(strfmt, obj, ...) - Returns a formatted string from printf-style format strings.
Examples:
> SELECT format_string("Hello World %d %s", 100, "days");
 Hello World 100 days

Since: 1.5.0

from_csv
from_csv(csvStr, schema[, options]) - Returns a struct value with the given csvStr and schema.
Examples:
> SELECT from_csv('1, 0.8', 'a INT, b DOUBLE');
 {"a":1,"b":0.8}
> SELECT from_csv('26/08/2015', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));
 {"time":2015-08-26 00:00:00}

Since: 3.0.0

from_json
from_json(jsonStr, schema[, options]) - Returns a struct value with the given jsonStr and schema.
Examples:
> SELECT from_json('{"a":1, "b":0.8}', 'a INT, b DOUBLE');
 {"a":1,"b":0.8}
> SELECT from_json('{"time":"26/08/2015"}', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));
 {"time":2015-08-26 00:00:00}
> SELECT from_json('{"teacher": "Alice", "student": [{"name": "Bob", "rank": 1}, {"name": "Charlie", "rank": 2}]}', 'STRUCT<teacher: STRING, student: ARRAY<STRUCT<name: STRING, rank: INT>>>');
 {"teacher":"Alice","student":[{"name":"Bob","rank":1},{"name":"Charlie","rank":2}]}

Since: 2.2.0

from_unixtime
from_unixtime(unix_time[, fmt]) - Returns unix_time in the specified fmt.
Arguments:

unix_time - UNIX Timestamp to be converted to the provided format.
fmt - Date/time format pattern to follow. See Datetime Patterns
        for valid date and time format patterns. The 'yyyy-MM-dd HH:mm:ss' pattern is used if omitted.

Examples:
> SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss');
 1969-12-31 16:00:00

> SELECT from_unixtime(0);
 1969-12-31 16:00:00

Since: 1.5.0

from_utc_timestamp
from_utc_timestamp(timestamp, timezone) - Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders that time as a timestamp in the given time zone. For example, 'GMT+1' would yield '2017-07-14 03:40:00.0'.
Examples:
> SELECT from_utc_timestamp('2016-08-31', 'Asia/Seoul');
 2016-08-31 09:00:00

Since: 1.5.0

get
get(array, index) - Returns element of array at given (0-based) index. If the index points
outside of the array boundaries, then this function returns NULL.
Examples:
> SELECT get(array(1, 2, 3), 0);
 1
> SELECT get(array(1, 2, 3), 3);
 NULL
> SELECT get(array(1, 2, 3), -1);
 NULL

Since: 3.4.0

get_json_object
get_json_object(json_txt, path) - Extracts a json object from path.
Examples:
> SELECT get_json_object('{"a":"b"}', '$.a');
 b

Since: 1.5.0

getbit
getbit(expr, pos) - Returns the value of the bit (0 or 1) at the specified position.
The positions are numbered from right to left, starting at zero.
The position argument cannot be negative.
Examples:
> SELECT getbit(11, 0);
 1
> SELECT getbit(11, 2);
 0

Since: 3.2.0

greatest
greatest(expr, ...) - Returns the greatest value of all parameters, skipping null values.
Examples:
> SELECT greatest(10, 9, 2, 4, 3);
 10

Since: 1.5.0

grouping
grouping(col) - indicates whether a specified column in a GROUP BY is aggregated or
not, returns 1 for aggregated or 0 for not aggregated in the result set.",
Examples:
> SELECT name, grouping(name), sum(age) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY cube(name);
  Alice 0   2
  Bob   0   5
  NULL  1   7

Since: 2.0.0

grouping_id
grouping_id([col1[, col2 ..]]) - returns the level of grouping, equals to
(grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)
Examples:
> SELECT name, grouping_id(), sum(age), avg(height) FROM VALUES (2, 'Alice', 165), (5, 'Bob', 180) people(age, name, height) GROUP BY cube(name, height);
  Alice 0   2   165.0
  Alice 1   2   165.0
  NULL  3   7   172.5
  Bob   0   5   180.0
  Bob   1   5   180.0
  NULL  2   2   165.0
  NULL  2   5   180.0

Note:
Input columns should match with grouping columns exactly, or empty (means all the grouping
columns).
Since: 2.0.0

hash
hash(expr1, expr2, ...) - Returns a hash value of the arguments.
Examples:
> SELECT hash('Spark', array(123), 2);
 -1321691492

Since: 2.0.0

hex
hex(expr) - Converts expr to hexadecimal.
Examples:
> SELECT hex(17);
 11
> SELECT hex('Spark SQL');
 537061726B2053514C

Since: 1.5.0

histogram_numeric
histogram_numeric(expr, nb) - Computes a histogram on numeric 'expr' using nb bins.
The return value is an array of (x,y) pairs representing the centers of the
histogram's bins. As the value of 'nb' is increased, the histogram approximation
gets finer-grained, but may yield artifacts around outliers. In practice, 20-40
histogram bins appear to work well, with more bins being required for skewed or
smaller datasets. Note that this function creates a histogram with non-uniform
bin widths. It offers no guarantees in terms of the mean-squared-error of the
histogram, but in practice is comparable to the histograms produced by the R/S-Plus
statistical computing packages. Note: the output type of the 'x' field in the return value is
propagated from the input value consumed in the aggregate function.
Examples:
> SELECT histogram_numeric(col, 5) FROM VALUES (0), (1), (2), (10) AS tab(col);
 [{"x":0,"y":1.0},{"x":1,"y":1.0},{"x":2,"y":1.0},{"x":10,"y":1.0}]

Since: 3.3.0

hll_sketch_agg
hll_sketch_agg(expr, lgConfigK) - Returns the HllSketch's updatable binary representation.
lgConfigK (optional) the log-base-2 of K, with K is the number of buckets or
slots for the HllSketch.
Examples:
> SELECT hll_sketch_estimate(hll_sketch_agg(col, 12)) FROM VALUES (1), (1), (2), (2), (3) tab(col);
 3

Since: 3.5.0

hll_sketch_estimate
hll_sketch_estimate(expr) - Returns the estimated number of unique values given the binary representation
of a Datasketches HllSketch.
Examples:
> SELECT hll_sketch_estimate(hll_sketch_agg(col)) FROM VALUES (1), (1), (2), (2), (3) tab(col);
 3

Since: 3.5.0

hll_union
hll_union(first, second, allowDifferentLgConfigK) - Merges two binary representations of
Datasketches HllSketch objects, using a Datasketches Union object. Set
allowDifferentLgConfigK to true to allow unions of sketches with different
lgConfigK values (defaults to false).
Examples:
> SELECT hll_sketch_estimate(hll_union(hll_sketch_agg(col1), hll_sketch_agg(col2))) FROM VALUES (1, 4), (1, 4), (2, 5), (2, 5), (3, 6) tab(col1, col2);
 6

Since: 3.5.0

hll_union_agg
hll_union_agg(expr, allowDifferentLgConfigK) - Returns the estimated number of unique values.
allowDifferentLgConfigK (optional) Allow sketches with different lgConfigK values
to be unioned (defaults to false).
Examples:
> SELECT hll_sketch_estimate(hll_union_agg(sketch, true)) FROM (SELECT hll_sketch_agg(col) as sketch FROM VALUES (1) tab(col) UNION ALL SELECT hll_sketch_agg(col, 20) as sketch FROM VALUES (1) tab(col));
 1

Since: 3.5.0

hour
hour(timestamp) - Returns the hour component of the string/timestamp.
Examples:
> SELECT hour('2009-07-30 12:58:59');
 12

Since: 1.5.0

hypot
hypot(expr1, expr2) - Returns sqrt(expr12 + expr22).
Examples:
> SELECT hypot(3, 4);
 5.0

Since: 1.4.0

if
if(expr1, expr2, expr3) - If expr1 evaluates to true, then returns expr2; otherwise returns expr3.
Examples:
> SELECT if(1 < 2, 'a', 'b');
 a

Since: 1.0.0

ifnull
ifnull(expr1, expr2) - Returns expr2 if expr1 is null, or expr1 otherwise.
Examples:
> SELECT ifnull(NULL, array('2'));
 ["2"]

Since: 2.0.0

ilike
str ilike pattern[ ESCAPE escape] - Returns true if str matches pattern with escape case-insensitively, null if any arguments are null, false otherwise.
Arguments:

str - a string expression
pattern - a string expression. The pattern is a string which is matched literally and
    case-insensitively, with exception to the following special symbols:
    _ matches any one character in the input (similar to . in posix regular expressions)
    % matches zero or more characters in the input (similar to .* in posix regular
    expressions)
    Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order
    to match "\abc", the pattern should be "\abc".
    When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back
    to Spark 1.6 behavior regarding string literal parsing. For example, if the config is
    enabled, the pattern to match "\abc" should be "\abc".
escape - an character added since Spark 3.0. The default escape character is the '\'.
    If an escape character precedes a special symbol or another escape character, the
    following character is matched literally. It is invalid to escape any other character.

Examples:
> SELECT ilike('Spark', '_Park');
true
> SET spark.sql.parser.escapedStringLiterals=true;
spark.sql.parser.escapedStringLiterals  true
> SELECT '%SystemDrive%\Users\John' ilike '\%SystemDrive\%\\users%';
true
> SET spark.sql.parser.escapedStringLiterals=false;
spark.sql.parser.escapedStringLiterals  false
> SELECT '%SystemDrive%\\USERS\\John' ilike '\%SystemDrive\%\\\\Users%';
true
> SELECT '%SystemDrive%/Users/John' ilike '/%SYSTEMDrive/%//Users%' ESCAPE '/';
true

Note:
Use RLIKE to match with standard regular expressions.
Since: 3.3.0

in
expr1 in(expr2, expr3, ...) - Returns true if expr equals to any valN.
Arguments:

expr1, expr2, expr3, ... - the arguments must be same type.

Examples:
> SELECT 1 in(1, 2, 3);
 true
> SELECT 1 in(2, 3, 4);
 false
> SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 1), named_struct('a', 1, 'b', 3));
 false
> SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 2), named_struct('a', 1, 'b', 3));
 true

Since: 1.0.0

initcap
initcap(str) - Returns str with the first letter of each word in uppercase.
All other letters are in lowercase. Words are delimited by white space.
Examples:
> SELECT initcap('sPark sql');
 Spark Sql

Since: 1.5.0

inline
inline(expr) - Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.
Examples:
> SELECT inline(array(struct(1, 'a'), struct(2, 'b')));
 1  a
 2  b

Since: 2.0.0

inline_outer
inline_outer(expr) - Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.
Examples:
> SELECT inline_outer(array(struct(1, 'a'), struct(2, 'b')));
 1  a
 2  b

Since: 2.0.0

input_file_block_length
input_file_block_length() - Returns the length of the block being read, or -1 if not available.
Examples:
> SELECT input_file_block_length();
 -1

Since: 2.2.0

input_file_block_start
input_file_block_start() - Returns the start offset of the block being read, or -1 if not available.
Examples:
> SELECT input_file_block_start();
 -1

Since: 2.2.0

input_file_name
input_file_name() - Returns the name of the file being read, or empty string if not available.
Examples:
> SELECT input_file_name();

Since: 1.5.0

instr
instr(str, substr) - Returns the (1-based) index of the first occurrence of substr in str.
Examples:
> SELECT instr('SparkSQL', 'SQL');
 6

Since: 1.5.0

int
int(expr) - Casts the value expr to the target data type int.
Since: 2.0.1

isnan
isnan(expr) - Returns true if expr is NaN, or false otherwise.
Examples:
> SELECT isnan(cast('NaN' as double));
 true

Since: 1.5.0

isnotnull
isnotnull(expr) - Returns true if expr is not null, or false otherwise.
Examples:
> SELECT isnotnull(1);
 true

Since: 1.0.0

isnull
isnull(expr) - Returns true if expr is null, or false otherwise.
Examples:
> SELECT isnull(1);
 false

Since: 1.0.0

java_method
java_method(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.
Examples:
> SELECT java_method('java.util.UUID', 'randomUUID');
 c33fb387-8500-4bfa-81d2-6e0e3e930df2
> SELECT java_method('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');
 a5cf6c42-0c85-418f-af6c-3e4e5b1328f2

Since: 2.0.0

json_array_length
json_array_length(jsonArray) - Returns the number of elements in the outermost JSON array.
Arguments:

jsonArray - A JSON array. NULL is returned in case of any other valid JSON string,
    NULL or an invalid JSON.

Examples:
> SELECT json_array_length('[1,2,3,4]');
  4
> SELECT json_array_length('[1,2,3,{"f1":1,"f2":[5,6]},4]');
  5
> SELECT json_array_length('[1,2');
  NULL

Since: 3.1.0

json_object_keys
json_object_keys(json_object) - Returns all the keys of the outermost JSON object as an array.
Arguments:

json_object - A JSON object. If a valid JSON object is given, all the keys of the outermost
    object will be returned as an array. If it is any other valid JSON string, an invalid JSON
    string or an empty string, the function returns null.

Examples:
> SELECT json_object_keys('{}');
  []
> SELECT json_object_keys('{"key": "value"}');
  ["key"]
> SELECT json_object_keys('{"f1":"abc","f2":{"f3":"a", "f4":"b"}}');
  ["f1","f2"]

Since: 3.1.0

json_tuple
json_tuple(jsonStr, p1, p2, ..., pn) - Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.
Examples:
> SELECT json_tuple('{"a":1, "b":2}', 'a', 'b');
 1  2

Since: 1.6.0

kurtosis
kurtosis(expr) - Returns the kurtosis value calculated from values of a group.
Examples:
> SELECT kurtosis(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);
 -0.7014368047529627
> SELECT kurtosis(col) FROM VALUES (1), (10), (100), (10), (1) as tab(col);
 0.19432323191699075

Since: 1.6.0

lag
lag(input[, offset[, default]]) - Returns the value of input at the offsetth row
before the current row in the window. The default value of offset is 1 and the default
value of default is null. If the value of input at the offsetth row is null,
null is returned. If there is no such offset row (e.g., when the offset is 1, the first
row of the window does not have any previous row), default is returned.
Arguments:

input - a string expression to evaluate offset rows before the current row.
offset - an int expression which is rows to jump back in the partition.
default - a string expression which is to use when the offset row does not exist.

Examples:
> SELECT a, b, lag(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   NULL
 A1 1   1
 A1 2   1
 A2 3   NULL

Since: 2.0.0

last
last(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows.
If isIgnoreNull is true, returns only non-null values
Examples:
> SELECT last(col) FROM VALUES (10), (5), (20) AS tab(col);
 20
> SELECT last(col) FROM VALUES (10), (5), (NULL) AS tab(col);
 NULL
> SELECT last(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);
 5

Note:
The function is non-deterministic because its results depends on the order of the rows
which may be non-deterministic after a shuffle.
Since: 2.0.0

last_day
last_day(date) - Returns the last day of the month which the date belongs to.
Examples:
> SELECT last_day('2009-01-12');
 2009-01-31

Since: 1.5.0

last_value
last_value(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows.
If isIgnoreNull is true, returns only non-null values
Examples:
> SELECT last_value(col) FROM VALUES (10), (5), (20) AS tab(col);
 20
> SELECT last_value(col) FROM VALUES (10), (5), (NULL) AS tab(col);
 NULL
> SELECT last_value(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);
 5

Note:
The function is non-deterministic because its results depends on the order of the rows
which may be non-deterministic after a shuffle.
Since: 2.0.0

lcase
lcase(str) - Returns str with all characters changed to lowercase.
Examples:
> SELECT lcase('SparkSql');
 sparksql

Since: 1.0.1

lead
lead(input[, offset[, default]]) - Returns the value of input at the offsetth row
after the current row in the window. The default value of offset is 1 and the default
value of default is null. If the value of input at the offsetth row is null,
null is returned. If there is no such an offset row (e.g., when the offset is 1, the last
row of the window does not have any subsequent row), default is returned.
Arguments:

input - a string expression to evaluate offset rows after the current row.
offset - an int expression which is rows to jump ahead in the partition.
default - a string expression which is to use when the offset is larger than the window.
    The default value is null.

Examples:
> SELECT a, b, lead(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   1
 A1 1   2
 A1 2   NULL
 A2 3   NULL

Since: 2.0.0

least
least(expr, ...) - Returns the least value of all parameters, skipping null values.
Examples:
> SELECT least(10, 9, 2, 4, 3);
 2

Since: 1.5.0

left
left(str, len) - Returns the leftmost len(len can be string type) characters from the string str,if len is less or equal than 0 the result is an empty string.
Examples:
> SELECT left('Spark SQL', 3);
 Spa
> SELECT left(encode('Spark SQL', 'utf-8'), 3);
 Spa

Since: 2.3.0

len
len(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.
Examples:
> SELECT len('Spark SQL ');
 10
> SELECT len(x'537061726b2053514c');
 9
> SELECT CHAR_LENGTH('Spark SQL ');
 10
> SELECT CHARACTER_LENGTH('Spark SQL ');
 10

Since: 3.4.0

length
length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.
Examples:
> SELECT length('Spark SQL ');
 10
> SELECT length(x'537061726b2053514c');
 9
> SELECT CHAR_LENGTH('Spark SQL ');
 10
> SELECT CHARACTER_LENGTH('Spark SQL ');
 10

Since: 1.5.0

levenshtein
levenshtein(str1, str2[, threshold]) - Returns the Levenshtein distance between the two given strings. If threshold is set and distance more than it, return -1.
Examples:
> SELECT levenshtein('kitten', 'sitting');
 3
> SELECT levenshtein('kitten', 'sitting', 2);
 -1

Since: 1.5.0

like
str like pattern[ ESCAPE escape] - Returns true if str matches pattern with escape, null if any arguments are null, false otherwise.
Arguments:

str - a string expression
pattern - a string expression. The pattern is a string which is matched literally, with
    exception to the following special symbols:
    _ matches any one character in the input (similar to . in posix regular expressions)\
    % matches zero or more characters in the input (similar to .* in posix regular
    expressions)
    Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order
    to match "\abc", the pattern should be "\abc".
    When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back
    to Spark 1.6 behavior regarding string literal parsing. For example, if the config is
    enabled, the pattern to match "\abc" should be "\abc".
escape - an character added since Spark 3.0. The default escape character is the '\'.
    If an escape character precedes a special symbol or another escape character, the
    following character is matched literally. It is invalid to escape any other character.

Examples:
> SELECT like('Spark', '_park');
true
> SET spark.sql.parser.escapedStringLiterals=true;
spark.sql.parser.escapedStringLiterals  true
> SELECT '%SystemDrive%\Users\John' like '\%SystemDrive\%\\Users%';
true
> SET spark.sql.parser.escapedStringLiterals=false;
spark.sql.parser.escapedStringLiterals  false
> SELECT '%SystemDrive%\\Users\\John' like '\%SystemDrive\%\\\\Users%';
true
> SELECT '%SystemDrive%/Users/John' like '/%SystemDrive/%//Users%' ESCAPE '/';
true

Note:
Use RLIKE to match with standard regular expressions.
Since: 1.0.0

ln
ln(expr) - Returns the natural logarithm (base e) of expr.
Examples:
> SELECT ln(1);
 0.0

Since: 1.4.0

localtimestamp
localtimestamp() - Returns the current timestamp without time zone at the start of query evaluation. All calls of localtimestamp within the same query return the same value.
localtimestamp - Returns the current local date-time at the session time zone at the start of query evaluation.
Examples:
> SELECT localtimestamp();
 2020-04-25 15:49:11.914

Since: 3.4.0

locate
locate(substr, str[, pos]) - Returns the position of the first occurrence of substr in str after position pos.
The given pos and return value are 1-based.
Examples:
> SELECT locate('bar', 'foobarbar');
 4
> SELECT locate('bar', 'foobarbar', 5);
 7
> SELECT POSITION('bar' IN 'foobarbar');
 4

Since: 1.5.0

log
log(base, expr) - Returns the logarithm of expr with base.
Examples:
> SELECT log(10, 100);
 2.0

Since: 1.5.0

log10
log10(expr) - Returns the logarithm of expr with base 10.
Examples:
> SELECT log10(10);
 1.0

Since: 1.4.0

log1p
log1p(expr) - Returns log(1 + expr).
Examples:
> SELECT log1p(0);
 0.0

Since: 1.4.0

log2
log2(expr) - Returns the logarithm of expr with base 2.
Examples:
> SELECT log2(2);
 1.0

Since: 1.4.0

lower
lower(str) - Returns str with all characters changed to lowercase.
Examples:
> SELECT lower('SparkSql');
 sparksql

Since: 1.0.1

lpad
lpad(str, len[, pad]) - Returns str, left-padded with pad to a length of len.
If str is longer than len, the return value is shortened to len characters or bytes.
If pad is not specified, str will be padded to the left with space characters if it is
a character string, and with zeros if it is a byte sequence.
Examples:
> SELECT lpad('hi', 5, '??');
 ???hi
> SELECT lpad('hi', 1, '??');
 h
> SELECT lpad('hi', 5);
    hi
> SELECT hex(lpad(unhex('aabb'), 5));
 000000AABB
> SELECT hex(lpad(unhex('aabb'), 5, unhex('1122')));
 112211AABB

Since: 1.5.0

ltrim
ltrim(str) - Removes the leading space characters from str.
Arguments:

str - a string expression
trimStr - the trim string characters to trim, the default value is a single space

Examples:
> SELECT ltrim('    SparkSQL   ');
 SparkSQL

Since: 1.5.0

luhn_check
luhn_check(str ) - Checks that a string of digits is valid according to the Luhn algorithm.
This checksum function is widely applied on credit card numbers and government identification
numbers to distinguish valid numbers from mistyped, incorrect numbers.
Examples:
> SELECT luhn_check('8112189876');
 true
> SELECT luhn_check('79927398713');
 true
> SELECT luhn_check('79927398714');
 false

Since: 3.5.0

make_date
make_date(year, month, day) - Create date from year, month and day fields. If the configuration spark.sql.ansi.enabled is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.
Arguments:

year - the year to represent, from 1 to 9999
month - the month-of-year to represent, from 1 (January) to 12 (December)
day - the day-of-month to represent, from 1 to 31

Examples:
> SELECT make_date(2013, 7, 15);
 2013-07-15
> SELECT make_date(2019, 7, NULL);
 NULL

Since: 3.0.0

make_dt_interval
make_dt_interval([days[, hours[, mins[, secs]]]]) - Make DayTimeIntervalType duration from days, hours, mins and secs.
Arguments:

days - the number of days, positive or negative
hours - the number of hours, positive or negative
mins - the number of minutes, positive or negative
secs - the number of seconds with the fractional part in microsecond precision.

Examples:
> SELECT make_dt_interval(1, 12, 30, 01.001001);
 1 12:30:01.001001000
> SELECT make_dt_interval(2);
 2 00:00:00.000000000
> SELECT make_dt_interval(100, null, 3);
 NULL

Since: 3.2.0

make_interval
make_interval([years[, months[, weeks[, days[, hours[, mins[, secs]]]]]]]) - Make interval from years, months, weeks, days, hours, mins and secs.
Arguments:

years - the number of years, positive or negative
months - the number of months, positive or negative
weeks - the number of weeks, positive or negative
days - the number of days, positive or negative
hours - the number of hours, positive or negative
mins - the number of minutes, positive or negative
secs - the number of seconds with the fractional part in microsecond precision.

Examples:
> SELECT make_interval(100, 11, 1, 1, 12, 30, 01.001001);
 100 years 11 months 8 days 12 hours 30 minutes 1.001001 seconds
> SELECT make_interval(100, null, 3);
 NULL
> SELECT make_interval(0, 1, 0, 1, 0, 0, 100.000001);
 1 months 1 days 1 minutes 40.000001 seconds

Since: 3.0.0

make_timestamp
make_timestamp(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. The result data type is consistent with the value of configuration spark.sql.timestampType. If the configuration spark.sql.ansi.enabled is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.
Arguments:

year - the year to represent, from 1 to 9999
month - the month-of-year to represent, from 1 (January) to 12 (December)
day - the day-of-month to represent, from 1 to 31
hour - the hour-of-day to represent, from 0 to 23
min - the minute-of-hour to represent, from 0 to 59
sec - the second-of-minute and its micro-fraction to represent, from 0 to 60.
        The value can be either an integer like 13 , or a fraction like 13.123.
        If the sec argument equals to 60, the seconds field is set
        to 0 and 1 minute is added to the final timestamp.
timezone - the time zone identifier. For example, CET, UTC and etc.

Examples:
> SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887);
 2014-12-28 06:30:45.887
> SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887, 'CET');
 2014-12-27 21:30:45.887
> SELECT make_timestamp(2019, 6, 30, 23, 59, 60);
 2019-07-01 00:00:00
> SELECT make_timestamp(2019, 6, 30, 23, 59, 1);
 2019-06-30 23:59:01
> SELECT make_timestamp(null, 7, 22, 15, 30, 0);
 NULL

Since: 3.0.0

make_timestamp_ltz
make_timestamp_ltz(year, month, day, hour, min, sec[, timezone]) - Create the current timestamp with local time zone from year, month, day, hour, min, sec and timezone fields. If the configuration spark.sql.ansi.enabled is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.
Arguments:

year - the year to represent, from 1 to 9999
month - the month-of-year to represent, from 1 (January) to 12 (December)
day - the day-of-month to represent, from 1 to 31
hour - the hour-of-day to represent, from 0 to 23
min - the minute-of-hour to represent, from 0 to 59
sec - the second-of-minute and its micro-fraction to represent, from
        0 to 60. If the sec argument equals to 60, the seconds field is set
        to 0 and 1 minute is added to the final timestamp.
timezone - the time zone identifier. For example, CET, UTC and etc.

Examples:
> SELECT make_timestamp_ltz(2014, 12, 28, 6, 30, 45.887);
 2014-12-28 06:30:45.887
> SELECT make_timestamp_ltz(2014, 12, 28, 6, 30, 45.887, 'CET');
 2014-12-27 21:30:45.887
> SELECT make_timestamp_ltz(2019, 6, 30, 23, 59, 60);
 2019-07-01 00:00:00
> SELECT make_timestamp_ltz(null, 7, 22, 15, 30, 0);
 NULL

Since: 3.4.0

make_timestamp_ntz
make_timestamp_ntz(year, month, day, hour, min, sec) - Create local date-time from year, month, day, hour, min, sec fields. If the configuration spark.sql.ansi.enabled is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.
Arguments:

year - the year to represent, from 1 to 9999
month - the month-of-year to represent, from 1 (January) to 12 (December)
day - the day-of-month to represent, from 1 to 31
hour - the hour-of-day to represent, from 0 to 23
min - the minute-of-hour to represent, from 0 to 59
sec - the second-of-minute and its micro-fraction to represent, from
        0 to 60. If the sec argument equals to 60, the seconds field is set
        to 0 and 1 minute is added to the final timestamp.

Examples:
> SELECT make_timestamp_ntz(2014, 12, 28, 6, 30, 45.887);
 2014-12-28 06:30:45.887
> SELECT make_timestamp_ntz(2019, 6, 30, 23, 59, 60);
 2019-07-01 00:00:00
> SELECT make_timestamp_ntz(null, 7, 22, 15, 30, 0);
 NULL

Since: 3.4.0

make_ym_interval
make_ym_interval([years[, months]]) - Make year-month interval from years, months.
Arguments:

years - the number of years, positive or negative
months - the number of months, positive or negative

Examples:
> SELECT make_ym_interval(1, 2);
 1-2
> SELECT make_ym_interval(1, 0);
 1-0
> SELECT make_ym_interval(-1, 1);
 -0-11
> SELECT make_ym_interval(2);
 2-0

Since: 3.2.0

map
map(key0, value0, key1, value1, ...) - Creates a map with the given key/value pairs.
Examples:
> SELECT map(1.0, '2', 3.0, '4');
 {1.0:"2",3.0:"4"}

Since: 2.0.0

map_concat
map_concat(map, ...) - Returns the union of all the given maps
Examples:
> SELECT map_concat(map(1, 'a', 2, 'b'), map(3, 'c'));
 {1:"a",2:"b",3:"c"}

Since: 2.4.0

map_contains_key
map_contains_key(map, key) - Returns true if the map contains the key.
Examples:
> SELECT map_contains_key(map(1, 'a', 2, 'b'), 1);
 true
> SELECT map_contains_key(map(1, 'a', 2, 'b'), 3);
 false

Since: 3.3.0

map_entries
map_entries(map) - Returns an unordered array of all entries in the given map.
Examples:
> SELECT map_entries(map(1, 'a', 2, 'b'));
 [{"key":1,"value":"a"},{"key":2,"value":"b"}]

Since: 3.0.0

map_filter
map_filter(expr, func) - Filters entries in a map using the function.
Examples:
> SELECT map_filter(map(1, 0, 2, 2, 3, -1), (k, v) -> k > v);
 {1:0,3:-1}

Since: 3.0.0

map_from_arrays
map_from_arrays(keys, values) - Creates a map with a pair of the given key/value arrays. All elements
in keys should not be null
Examples:
> SELECT map_from_arrays(array(1.0, 3.0), array('2', '4'));
 {1.0:"2",3.0:"4"}

Since: 2.4.0

map_from_entries
map_from_entries(arrayOfEntries) - Returns a map created from the given array of entries.
Examples:
> SELECT map_from_entries(array(struct(1, 'a'), struct(2, 'b')));
 {1:"a",2:"b"}

Since: 2.4.0

map_keys
map_keys(map) - Returns an unordered array containing the keys of the map.
Examples:
> SELECT map_keys(map(1, 'a', 2, 'b'));
 [1,2]

Since: 2.0.0

map_values
map_values(map) - Returns an unordered array containing the values of the map.
Examples:
> SELECT map_values(map(1, 'a', 2, 'b'));
 ["a","b"]

Since: 2.0.0

map_zip_with
map_zip_with(map1, map2, function) - Merges two given maps into a single map by applying
function to the pair of values with the same key. For keys only presented in one map,
NULL will be passed as the value for the missing key. If an input map contains duplicated
keys, only the first entry of the duplicated key is passed into the lambda function.
Examples:
> SELECT map_zip_with(map(1, 'a', 2, 'b'), map(1, 'x', 2, 'y'), (k, v1, v2) -> concat(v1, v2));
 {1:"ax",2:"by"}
> SELECT map_zip_with(map('a', 1, 'b', 2), map('b', 3, 'c', 4), (k, v1, v2) -> coalesce(v1, 0) + coalesce(v2, 0));
 {"a":1,"b":5,"c":4}

Since: 3.0.0

mask
mask(input[, upperChar, lowerChar, digitChar, otherChar]) - masks the given string value.
The function replaces characters with 'X' or 'x', and numbers with 'n'.
This can be useful for creating copies of tables with sensitive information removed.
Arguments:

input      - string value to mask. Supported types: STRING, VARCHAR, CHAR
upperChar  - character to replace upper-case characters with. Specify NULL to retain original character. Default value: 'X'
lowerChar  - character to replace lower-case characters with. Specify NULL to retain original character. Default value: 'x'
digitChar  - character to replace digit characters with. Specify NULL to retain original character. Default value: 'n'
otherChar  - character to replace all other characters with. Specify NULL to retain original character. Default value: NULL

Examples:
> SELECT mask('abcd-EFGH-8765-4321');
  xxxx-XXXX-nnnn-nnnn
> SELECT mask('abcd-EFGH-8765-4321', 'Q');
  xxxx-QQQQ-nnnn-nnnn
> SELECT mask('AbCD123-@$#', 'Q', 'q');
  QqQQnnn-@$#
> SELECT mask('AbCD123-@$#');
  XxXXnnn-@$#
> SELECT mask('AbCD123-@$#', 'Q');
  QxQQnnn-@$#
> SELECT mask('AbCD123-@$#', 'Q', 'q');
  QqQQnnn-@$#
> SELECT mask('AbCD123-@$#', 'Q', 'q', 'd');
  QqQQddd-@$#
> SELECT mask('AbCD123-@$#', 'Q', 'q', 'd', 'o');
  QqQQdddoooo
> SELECT mask('AbCD123-@$#', NULL, 'q', 'd', 'o');
  AqCDdddoooo
> SELECT mask('AbCD123-@$#', NULL, NULL, 'd', 'o');
  AbCDdddoooo
> SELECT mask('AbCD123-@$#', NULL, NULL, NULL, 'o');
  AbCD123oooo
> SELECT mask(NULL, NULL, NULL, NULL, 'o');
  NULL
> SELECT mask(NULL);
  NULL
> SELECT mask('AbCD123-@$#', NULL, NULL, NULL, NULL);
  AbCD123-@$#

Since: 3.4.0

max
max(expr) - Returns the maximum value of expr.
Examples:
> SELECT max(col) FROM VALUES (10), (50), (20) AS tab(col);
 50

Since: 1.0.0

max_by
max_by(x, y) - Returns the value of x associated with the maximum value of y.
Examples:
> SELECT max_by(x, y) FROM VALUES ('a', 10), ('b', 50), ('c', 20) AS tab(x, y);
 b

Since: 3.0.0

md5
md5(expr) - Returns an MD5 128-bit checksum as a hex string of expr.
Examples:
> SELECT md5('Spark');
 8cde774d6f7333752ed72cacddb05126

Since: 1.5.0

mean
mean(expr) - Returns the mean calculated from values of a group.
Examples:
> SELECT mean(col) FROM VALUES (1), (2), (3) AS tab(col);
 2.0
> SELECT mean(col) FROM VALUES (1), (2), (NULL) AS tab(col);
 1.5

Since: 1.0.0

median
median(col) - Returns the median of numeric or ANSI interval column col.
Examples:
> SELECT median(col) FROM VALUES (0), (10) AS tab(col);
 5.0
> SELECT median(col) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);
 0-5

Since: 3.4.0

min
min(expr) - Returns the minimum value of expr.
Examples:
> SELECT min(col) FROM VALUES (10), (-1), (20) AS tab(col);
 -1

Since: 1.0.0

min_by
min_by(x, y) - Returns the value of x associated with the minimum value of y.
Examples:
> SELECT min_by(x, y) FROM VALUES ('a', 10), ('b', 50), ('c', 20) AS tab(x, y);
 a

Since: 3.0.0

minute
minute(timestamp) - Returns the minute component of the string/timestamp.
Examples:
> SELECT minute('2009-07-30 12:58:59');
 58

Since: 1.5.0

mod
expr1 mod expr2 - Returns the remainder after expr1/expr2.
Examples:
> SELECT 2 % 1.8;
 0.2
> SELECT MOD(2, 1.8);
 0.2

Since: 1.0.0

mode
mode(col) - Returns the most frequent value for the values within col. NULL values are ignored. If all the values are NULL, or there are 0 rows, returns NULL.
Examples:
> SELECT mode(col) FROM VALUES (0), (10), (10) AS tab(col);
 10
> SELECT mode(col) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH), (INTERVAL '10' MONTH) AS tab(col);
 0-10
> SELECT mode(col) FROM VALUES (0), (10), (10), (null), (null), (null) AS tab(col);
 10

Since: 3.4.0

monotonically_increasing_id
monotonically_increasing_id() - Returns monotonically increasing 64-bit integers. The generated ID is guaranteed
to be monotonically increasing and unique, but not consecutive. The current implementation
puts the partition ID in the upper 31 bits, and the lower 33 bits represent the record number
within each partition. The assumption is that the data frame has less than 1 billion
partitions, and each partition has less than 8 billion records.
The function is non-deterministic because its result depends on partition IDs.
Examples:
> SELECT monotonically_increasing_id();
 0

Since: 1.4.0

month
month(date) - Returns the month component of the date/timestamp.
Examples:
> SELECT month('2016-07-30');
 7

Since: 1.5.0

months_between
months_between(timestamp1, timestamp2[, roundOff]) - If timestamp1 is later than timestamp2, then the result
is positive. If timestamp1 and timestamp2 are on the same day of month, or both
are the last day of month, time of day will be ignored. Otherwise, the difference is
calculated based on 31 days per month, and rounded to 8 digits unless roundOff=false.
Examples:
> SELECT months_between('1997-02-28 10:30:00', '1996-10-30');
 3.94959677
> SELECT months_between('1997-02-28 10:30:00', '1996-10-30', false);
 3.9495967741935485

Since: 1.5.0

named_struct
named_struct(name1, val1, name2, val2, ...) - Creates a struct with the given field names and values.
Examples:
> SELECT named_struct("a", 1, "b", 2, "c", 3);
 {"a":1,"b":2,"c":3}

Since: 1.5.0

nanvl
nanvl(expr1, expr2) - Returns expr1 if it's not NaN, or expr2 otherwise.
Examples:
> SELECT nanvl(cast('NaN' as double), 123);
 123.0

Since: 1.5.0

negative
negative(expr) - Returns the negated value of expr.
Examples:
> SELECT negative(1);
 -1

Since: 1.0.0

next_day
next_day(start_date, day_of_week) - Returns the first date which is later than start_date and named as indicated.
The function returns NULL if at least one of the input parameters is NULL.
When both of the input parameters are not NULL and day_of_week is an invalid input,
the function throws IllegalArgumentException if spark.sql.ansi.enabled is set to true, otherwise NULL.
Examples:
> SELECT next_day('2015-01-14', 'TU');
 2015-01-20

Since: 1.5.0

not
not expr - Logical not.
Examples:
> SELECT not true;
 false
> SELECT not false;
 true
> SELECT not NULL;
 NULL

Since: 1.0.0

now
now() - Returns the current timestamp at the start of query evaluation.
Examples:
> SELECT now();
 2020-04-25 15:49:11.914

Since: 1.6.0

nth_value
nth_value(input[, offset]) - Returns the value of input at the row that is the offsetth row
from beginning of the window frame. Offset starts at 1. If ignoreNulls=true, we will skip
nulls when finding the offsetth row. Otherwise, every row counts for the offset. If
there is no such an offsetth row (e.g., when the offset is 10, size of the window frame
is less than 10), null is returned.
Arguments:

input - the target column or expression that the function operates on.
offset - a positive int literal to indicate the offset in the window frame. It starts
    with 1.
ignoreNulls - an optional specification that indicates the NthValue should skip null
    values in the determination of which row to use.

Examples:
> SELECT a, b, nth_value(b, 2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   1
 A1 1   1
 A1 2   1
 A2 3   NULL

Since: 3.1.0

ntile
ntile(n) - Divides the rows for each window partition into n buckets ranging
from 1 to at most n.
Arguments:

buckets - an int expression which is number of buckets to divide the rows in.
    Default value is 1.

Examples:
> SELECT a, b, ntile(2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   1
 A1 1   1
 A1 2   2
 A2 3   1

Since: 2.0.0

nullif
nullif(expr1, expr2) - Returns null if expr1 equals to expr2, or expr1 otherwise.
Examples:
> SELECT nullif(2, 2);
 NULL

Since: 2.0.0

nvl
nvl(expr1, expr2) - Returns expr2 if expr1 is null, or expr1 otherwise.
Examples:
> SELECT nvl(NULL, array('2'));
 ["2"]

Since: 2.0.0

nvl2
nvl2(expr1, expr2, expr3) - Returns expr2 if expr1 is not null, or expr3 otherwise.
Examples:
> SELECT nvl2(NULL, 2, 1);
 1

Since: 2.0.0

octet_length
octet_length(expr) - Returns the byte length of string data or number of bytes of binary data.
Examples:
> SELECT octet_length('Spark SQL');
 9
> SELECT octet_length(x'537061726b2053514c');
 9

Since: 2.3.0

or
expr1 or expr2 - Logical OR.
Examples:
> SELECT true or false;
 true
> SELECT false or false;
 false
> SELECT true or NULL;
 true
> SELECT false or NULL;
 NULL

Since: 1.0.0

overlay
overlay(input, replace, pos[, len]) - Replace input with replace that starts at pos and is of length len.
Examples:
> SELECT overlay('Spark SQL' PLACING '_' FROM 6);
 Spark_SQL
> SELECT overlay('Spark SQL' PLACING 'CORE' FROM 7);
 Spark CORE
> SELECT overlay('Spark SQL' PLACING 'ANSI ' FROM 7 FOR 0);
 Spark ANSI SQL
> SELECT overlay('Spark SQL' PLACING 'tructured' FROM 2 FOR 4);
 Structured SQL
> SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('_', 'utf-8') FROM 6);
 Spark_SQL
> SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('CORE', 'utf-8') FROM 7);
 Spark CORE
> SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('ANSI ', 'utf-8') FROM 7 FOR 0);
 Spark ANSI SQL
> SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('tructured', 'utf-8') FROM 2 FOR 4);
 Structured SQL

Since: 3.0.0

parse_url
parse_url(url, partToExtract[, key]) - Extracts a part from a URL.
Examples:
> SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST');
 spark.apache.org
> SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY');
 query=1
> SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query');
 1

Since: 2.0.0

percent_rank
percent_rank() - Computes the percentage ranking of a value in a group of values.
Arguments:

children - this is to base the rank on; a change in the value of one the children will
    trigger a change in rank. This is an internal parameter and will be assigned by the
    Analyser.

Examples:
> SELECT a, b, percent_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   0.0
 A1 1   0.0
 A1 2   1.0
 A2 3   0.0

Since: 2.0.0

percentile
percentile(col, percentage [, frequency]) - Returns the exact percentile value of numeric
or ANSI interval column col at the given percentage. The value of percentage must be
between 0.0 and 1.0. The value of frequency should be positive integral
percentile(col, array(percentage1 [, percentage2]...) [, frequency]) - Returns the exact
percentile value array of numeric column col at the given percentage(s). Each value
of the percentage array must be between 0.0 and 1.0. The value of frequency should be
positive integral
Examples:
> SELECT percentile(col, 0.3) FROM VALUES (0), (10) AS tab(col);
 3.0
> SELECT percentile(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col);
 [2.5,7.5]
> SELECT percentile(col, 0.5) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);
 0-5
> SELECT percentile(col, array(0.2, 0.5)) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '10' SECOND) AS tab(col);
 [0 00:00:02.000000000,0 00:00:05.000000000]

Since: 2.1.0

percentile_approx
percentile_approx(col, percentage [, accuracy]) - Returns the approximate percentile of the numeric or
ansi interval column col which is the smallest value in the ordered col values (sorted
from least to greatest) such that no more than percentage of col values is less than
the value or equal to that value. The value of percentage must be between 0.0 and 1.0.
The accuracy parameter (default: 10000) is a positive numeric literal which controls
approximation accuracy at the cost of memory. Higher value of accuracy yields better
accuracy, 1.0/accuracy is the relative error of the approximation.
When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.
In this case, returns the approximate percentile array of column col at the given
percentage array.
Examples:
> SELECT percentile_approx(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);
 [1,1,0]
> SELECT percentile_approx(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);
 7
> SELECT percentile_approx(col, 0.5, 100) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '1' MONTH), (INTERVAL '2' MONTH), (INTERVAL '10' MONTH) AS tab(col);
 0-1
> SELECT percentile_approx(col, array(0.5, 0.7), 100) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '1' SECOND), (INTERVAL '2' SECOND), (INTERVAL '10' SECOND) AS tab(col);
 [0 00:00:01.000000000,0 00:00:02.000000000]

Since: 2.1.0

pi
pi() - Returns pi.
Examples:
> SELECT pi();
 3.141592653589793

Since: 1.5.0

pmod
pmod(expr1, expr2) - Returns the positive value of expr1 mod expr2.
Examples:
> SELECT pmod(10, 3);
 1
> SELECT pmod(-10, 3);
 2

Since: 1.5.0

posexplode
posexplode(expr) - Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Unless specified otherwise, uses the column name pos for position, col for elements of the array or key and value for elements of the map.
Examples:
> SELECT posexplode(array(10,20));
 0  10
 1  20
> SELECT * FROM posexplode(array(10,20));
 0  10
 1  20

Since: 2.0.0

posexplode_outer
posexplode_outer(expr) - Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Unless specified otherwise, uses the column name pos for position, col for elements of the array or key and value for elements of the map.
Examples:
> SELECT posexplode_outer(array(10,20));
 0  10
 1  20
> SELECT * FROM posexplode_outer(array(10,20));
 0  10
 1  20

Since: 2.0.0

position
position(substr, str[, pos]) - Returns the position of the first occurrence of substr in str after position pos.
The given pos and return value are 1-based.
Examples:
> SELECT position('bar', 'foobarbar');
 4
> SELECT position('bar', 'foobarbar', 5);
 7
> SELECT POSITION('bar' IN 'foobarbar');
 4

Since: 1.5.0

positive
positive(expr) - Returns the value of expr.
Examples:
> SELECT positive(1);
 1

Since: 1.5.0

pow
pow(expr1, expr2) - Raises expr1 to the power of expr2.
Examples:
> SELECT pow(2, 3);
 8.0

Since: 1.4.0

power
power(expr1, expr2) - Raises expr1 to the power of expr2.
Examples:
> SELECT power(2, 3);
 8.0

Since: 1.4.0

printf
printf(strfmt, obj, ...) - Returns a formatted string from printf-style format strings.
Examples:
> SELECT printf("Hello World %d %s", 100, "days");
 Hello World 100 days

Since: 1.5.0

quarter
quarter(date) - Returns the quarter of the year for date, in the range 1 to 4.
Examples:
> SELECT quarter('2016-08-31');
 3

Since: 1.5.0

radians
radians(expr) - Converts degrees to radians.
Arguments:

expr - angle in degrees

Examples:
> SELECT radians(180);
 3.141592653589793

Since: 1.4.0

raise_error
raise_error(expr) - Throws an exception with expr.
Examples:
> SELECT raise_error('custom error message');
 java.lang.RuntimeException
 custom error message

Since: 3.1.0

rand
rand([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).
Examples:
> SELECT rand();
 0.9629742951434543
> SELECT rand(0);
 0.7604953758285915
> SELECT rand(null);
 0.7604953758285915

Note:
The function is non-deterministic in general case.
Since: 1.5.0

randn
randn([seed]) - Returns a random value with independent and identically distributed (i.i.d.) values drawn from the standard normal distribution.
Examples:
> SELECT randn();
 -0.3254147983080288
> SELECT randn(0);
 1.6034991609278433
> SELECT randn(null);
 1.6034991609278433

Note:
The function is non-deterministic in general case.
Since: 1.5.0

random
random([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).
Examples:
> SELECT random();
 0.9629742951434543
> SELECT random(0);
 0.7604953758285915
> SELECT random(null);
 0.7604953758285915

Note:
The function is non-deterministic in general case.
Since: 1.5.0

rank
rank() - Computes the rank of a value in a group of values. The result is one plus the number
of rows preceding or equal to the current row in the ordering of the partition. The values
will produce gaps in the sequence.
Arguments:

children - this is to base the rank on; a change in the value of one the children will
    trigger a change in rank. This is an internal parameter and will be assigned by the
    Analyser.

Examples:
> SELECT a, b, rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   1
 A1 1   1
 A1 2   3
 A2 3   1

Since: 2.0.0

reduce
reduce(expr, start, merge, finish) - Applies a binary operator to an initial state and all
elements in the array, and reduces this to a single state. The final state is converted
into the final result by applying a finish function.
Examples:
> SELECT reduce(array(1, 2, 3), 0, (acc, x) -> acc + x);
 6
> SELECT reduce(array(1, 2, 3), 0, (acc, x) -> acc + x, acc -> acc * 10);
 60

Since: 3.4.0

reflect
reflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.
Examples:
> SELECT reflect('java.util.UUID', 'randomUUID');
 c33fb387-8500-4bfa-81d2-6e0e3e930df2
> SELECT reflect('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');
 a5cf6c42-0c85-418f-af6c-3e4e5b1328f2

Since: 2.0.0

regexp
regexp(str, regexp) - Returns true if str matches regexp, or false otherwise.
Arguments:

str - a string expression

regexp - a string expression. The regex string should be a Java regular expression.
Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL
parser. For example, to match "\abc", a regular expression for regexp can be
"^\abc$".
There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to
fallback to the Spark 1.6 behavior regarding string literal parsing. For example,
if the config is enabled, the regexp that can match "\abc" is "^\abc$".


Examples:
> SET spark.sql.parser.escapedStringLiterals=true;
spark.sql.parser.escapedStringLiterals  true
> SELECT regexp('%SystemDrive%\Users\John', '%SystemDrive%\\Users.*');
true
> SET spark.sql.parser.escapedStringLiterals=false;
spark.sql.parser.escapedStringLiterals  false
> SELECT regexp('%SystemDrive%\\Users\\John', '%SystemDrive%\\\\Users.*');
true

Note:
Use LIKE to match with simple string pattern.
Since: 3.2.0

regexp_count
regexp_count(str, regexp) - Returns a count of the number of times that the regular expression pattern regexp is matched in the string str.
Arguments:

str - a string expression.
regexp - a string representing a regular expression. The regex string should be a
    Java regular expression.

Examples:
> SELECT regexp_count('Steven Jones and Stephen Smith are the best players', 'Ste(v|ph)en');
 2
> SELECT regexp_count('abcdefghijklmnopqrstuvwxyz', '[a-z]{3}');
 8

Since: 3.4.0

regexp_extract
regexp_extract(str, regexp[, idx]) - Extract the first string in the str that match the regexp
expression and corresponding to the regex group index.
Arguments:

str - a string expression.
regexp - a string representing a regular expression. The regex string should be a
    Java regular expression.
    Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL
    parser. For example, to match "\abc", a regular expression for regexp can be
    "^\abc$".
    There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to
    fallback to the Spark 1.6 behavior regarding string literal parsing. For example,
    if the config is enabled, the regexp that can match "\abc" is "^\abc$".
idx - an integer expression that representing the group index. The regex maybe contains
    multiple groups. idx indicates which regex group to extract. The group index should
    be non-negative. The minimum value of idx is 0, which means matching the entire
    regular expression. If idx is not specified, the default group index value is 1. The
    idx parameter is the Java regex Matcher group() method index.

Examples:
> SELECT regexp_extract('100-200', '(\\d+)-(\\d+)', 1);
 100

Since: 1.5.0

regexp_extract_all
regexp_extract_all(str, regexp[, idx]) - Extract all strings in the str that match the regexp
expression and corresponding to the regex group index.
Arguments:

str - a string expression.
regexp - a string representing a regular expression. The regex string should be a
    Java regular expression.
    Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL
    parser. For example, to match "\abc", a regular expression for regexp can be
    "^\abc$".
    There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to
    fallback to the Spark 1.6 behavior regarding string literal parsing. For example,
    if the config is enabled, the regexp that can match "\abc" is "^\abc$".
idx - an integer expression that representing the group index. The regex may contains
    multiple groups. idx indicates which regex group to extract. The group index should
    be non-negative. The minimum value of idx is 0, which means matching the entire
    regular expression. If idx is not specified, the default group index value is 1. The
    idx parameter is the Java regex Matcher group() method index.

Examples:
> SELECT regexp_extract_all('100-200, 300-400', '(\\d+)-(\\d+)', 1);
 ["100","300"]

Since: 3.1.0

regexp_instr
regexp_instr(str, regexp) - Searches a string for a regular expression and returns an integer that indicates the beginning position of the matched substring. Positions are 1-based, not 0-based. If no match is found, returns 0.
Arguments:

str - a string expression.
regexp - a string representing a regular expression. The regex string should be a
    Java regular expression.
    Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL
    parser. For example, to match "\abc", a regular expression for regexp can be
    "^\abc$".
    There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to
    fallback to the Spark 1.6 behavior regarding string literal parsing. For example,
    if the config is enabled, the regexp that can match "\abc" is "^\abc$".

Examples:
> SELECT regexp_instr('user@spark.apache.org', '@[^.]*');
 5

Since: 3.4.0

regexp_like
regexp_like(str, regexp) - Returns true if str matches regexp, or false otherwise.
Arguments:

str - a string expression

regexp - a string expression. The regex string should be a Java regular expression.
Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL
parser. For example, to match "\abc", a regular expression for regexp can be
"^\abc$".
There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to
fallback to the Spark 1.6 behavior regarding string literal parsing. For example,
if the config is enabled, the regexp that can match "\abc" is "^\abc$".


Examples:
> SET spark.sql.parser.escapedStringLiterals=true;
spark.sql.parser.escapedStringLiterals  true
> SELECT regexp_like('%SystemDrive%\Users\John', '%SystemDrive%\\Users.*');
true
> SET spark.sql.parser.escapedStringLiterals=false;
spark.sql.parser.escapedStringLiterals  false
> SELECT regexp_like('%SystemDrive%\\Users\\John', '%SystemDrive%\\\\Users.*');
true

Note:
Use LIKE to match with simple string pattern.
Since: 3.2.0

regexp_replace
regexp_replace(str, regexp, rep[, position]) - Replaces all substrings of str that match regexp with rep.
Arguments:

str - a string expression to search for a regular expression pattern match.
regexp - a string representing a regular expression. The regex string should be a
    Java regular expression.
    Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL
    parser. For example, to match "\abc", a regular expression for regexp can be
    "^\abc$".
    There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to
    fallback to the Spark 1.6 behavior regarding string literal parsing. For example,
    if the config is enabled, the regexp that can match "\abc" is "^\abc$".
rep - a string expression to replace matched substrings.
position - a positive integer literal that indicates the position within str to begin searching.
    The default is 1. If position is greater than the number of characters in str, the result is str.

Examples:
> SELECT regexp_replace('100-200', '(\\d+)', 'num');
 num-num

Since: 1.5.0

regexp_substr
regexp_substr(str, regexp) - Returns the substring that matches the regular expression regexp within the string str. If the regular expression is not found, the result is null.
Arguments:

str - a string expression.
regexp - a string representing a regular expression. The regex string should be a Java regular expression.

Examples:
> SELECT regexp_substr('Steven Jones and Stephen Smith are the best players', 'Ste(v|ph)en');
 Steven
> SELECT regexp_substr('Steven Jones and Stephen Smith are the best players', 'Jeck');
 NULL

Since: 3.4.0

regr_avgx
regr_avgx(y, x) - Returns the average of the independent variable for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_avgx(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
 2.75
> SELECT regr_avgx(y, x) FROM VALUES (1, null) AS tab(y, x);
 NULL
> SELECT regr_avgx(y, x) FROM VALUES (null, 1) AS tab(y, x);
 NULL
> SELECT regr_avgx(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
 3.0
> SELECT regr_avgx(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
 3.0

Since: 3.3.0

regr_avgy
regr_avgy(y, x) - Returns the average of the dependent variable for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_avgy(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
 1.75
> SELECT regr_avgy(y, x) FROM VALUES (1, null) AS tab(y, x);
 NULL
> SELECT regr_avgy(y, x) FROM VALUES (null, 1) AS tab(y, x);
 NULL
> SELECT regr_avgy(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
 1.6666666666666667
> SELECT regr_avgy(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
 1.5

Since: 3.3.0

regr_count
regr_count(y, x) - Returns the number of non-null number pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_count(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
 4
> SELECT regr_count(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
 3
> SELECT regr_count(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
 2

Since: 3.3.0

regr_intercept
regr_intercept(y, x) - Returns the intercept of the univariate linear regression line for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_intercept(y, x) FROM VALUES (1,1), (2,2), (3,3) AS tab(y, x);
 0.0
> SELECT regr_intercept(y, x) FROM VALUES (1, null) AS tab(y, x);
 NULL
> SELECT regr_intercept(y, x) FROM VALUES (null, 1) AS tab(y, x);
 NULL

Since: 3.4.0

regr_r2
regr_r2(y, x) - Returns the coefficient of determination for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_r2(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
 0.2727272727272727
> SELECT regr_r2(y, x) FROM VALUES (1, null) AS tab(y, x);
 NULL
> SELECT regr_r2(y, x) FROM VALUES (null, 1) AS tab(y, x);
 NULL
> SELECT regr_r2(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
 0.7500000000000001
> SELECT regr_r2(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
 1.0

Since: 3.3.0

regr_slope
regr_slope(y, x) - Returns the slope of the linear regression line for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_slope(y, x) FROM VALUES (1,1), (2,2), (3,3) AS tab(y, x);
 1.0
> SELECT regr_slope(y, x) FROM VALUES (1, null) AS tab(y, x);
 NULL
> SELECT regr_slope(y, x) FROM VALUES (null, 1) AS tab(y, x);
 NULL

Since: 3.4.0

regr_sxx
regr_sxx(y, x) - Returns REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_sxx(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
 2.75
> SELECT regr_sxx(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
 2.0
> SELECT regr_sxx(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
 2.0

Since: 3.4.0

regr_sxy
regr_sxy(y, x) - Returns REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_sxy(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
 0.75
> SELECT regr_sxy(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
 1.0
> SELECT regr_sxy(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
 1.0

Since: 3.4.0

regr_syy
regr_syy(y, x) - Returns REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs in a group, where y is the dependent variable and x is the independent variable.
Examples:
> SELECT regr_syy(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
 0.75
> SELECT regr_syy(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
 0.6666666666666666
> SELECT regr_syy(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
 0.5

Since: 3.4.0

repeat
repeat(str, n) - Returns the string which repeats the given string value n times.
Examples:
> SELECT repeat('123', 2);
 123123

Since: 1.5.0

replace
replace(str, search[, replace]) - Replaces all occurrences of search with replace.
Arguments:

str - a string expression
search - a string expression. If search is not found in str, str is returned unchanged.
replace - a string expression. If replace is not specified or is an empty string, nothing replaces
    the string that is removed from str.

Examples:
> SELECT replace('ABCabc', 'abc', 'DEF');
 ABCDEF

Since: 2.3.0

reverse
reverse(array) - Returns a reversed string or an array with reverse order of elements.
Examples:
> SELECT reverse('Spark SQL');
 LQS krapS
> SELECT reverse(array(2, 1, 4, 3));
 [3,4,1,2]

Note:
Reverse logic for arrays is available since 2.4.0.
Since: 1.5.0

right
right(str, len) - Returns the rightmost len(len can be string type) characters from the string str,if len is less or equal than 0 the result is an empty string.
Examples:
> SELECT right('Spark SQL', 3);
 SQL

Since: 2.3.0

rint
rint(expr) - Returns the double value that is closest in value to the argument and is equal to a mathematical integer.
Examples:
> SELECT rint(12.3456);
 12.0

Since: 1.4.0

rlike
rlike(str, regexp) - Returns true if str matches regexp, or false otherwise.
Arguments:

str - a string expression

regexp - a string expression. The regex string should be a Java regular expression.
Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL
parser. For example, to match "\abc", a regular expression for regexp can be
"^\abc$".
There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to
fallback to the Spark 1.6 behavior regarding string literal parsing. For example,
if the config is enabled, the regexp that can match "\abc" is "^\abc$".


Examples:
> SET spark.sql.parser.escapedStringLiterals=true;
spark.sql.parser.escapedStringLiterals  true
> SELECT rlike('%SystemDrive%\Users\John', '%SystemDrive%\\Users.*');
true
> SET spark.sql.parser.escapedStringLiterals=false;
spark.sql.parser.escapedStringLiterals  false
> SELECT rlike('%SystemDrive%\\Users\\John', '%SystemDrive%\\\\Users.*');
true

Note:
Use LIKE to match with simple string pattern.
Since: 1.0.0

round
round(expr, d) - Returns expr rounded to d decimal places using HALF_UP rounding mode.
Examples:
> SELECT round(2.5, 0);
 3

Since: 1.5.0

row_number
row_number() - Assigns a unique, sequential number to each row, starting with one,
according to the ordering of rows within the window partition.
Examples:
> SELECT a, b, row_number() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
 A1 1   1
 A1 1   2
 A1 2   3
 A2 3   1

Since: 2.0.0

rpad
rpad(str, len[, pad]) - Returns str, right-padded with pad to a length of len.
If str is longer than len, the return value is shortened to len characters.
If pad is not specified, str will be padded to the right with space characters if it is
a character string, and with zeros if it is a binary string.
Examples:
> SELECT rpad('hi', 5, '??');
 hi???
> SELECT rpad('hi', 1, '??');
 h
> SELECT rpad('hi', 5);
 hi
> SELECT hex(rpad(unhex('aabb'), 5));
 AABB000000
> SELECT hex(rpad(unhex('aabb'), 5, unhex('1122')));
 AABB112211

Since: 1.5.0

rtrim
rtrim(str) - Removes the trailing space characters from str.
Arguments:

str - a string expression
trimStr - the trim string characters to trim, the default value is a single space

Examples:
> SELECT rtrim('    SparkSQL   ');
 SparkSQL

Since: 1.5.0

schema_of_csv
schema_of_csv(csv[, options]) - Returns schema in the DDL format of CSV string.
Examples:
> SELECT schema_of_csv('1,abc');
 STRUCT<_c0: INT, _c1: STRING>

Since: 3.0.0

schema_of_json
schema_of_json(json[, options]) - Returns schema in the DDL format of JSON string.
Examples:
> SELECT schema_of_json('[{"col":0}]');
 ARRAY<STRUCT<col: BIGINT>>
> SELECT schema_of_json('[{"col":01}]', map('allowNumericLeadingZeros', 'true'));
 ARRAY<STRUCT<col: BIGINT>>

Since: 2.4.0

sec
sec(expr) - Returns the secant of expr, as if computed by 1/java.lang.Math.cos.
Arguments:

expr - angle in radians

Examples:
> SELECT sec(0);
 1.0

Since: 3.3.0

second
second(timestamp) - Returns the second component of the string/timestamp.
Examples:
> SELECT second('2009-07-30 12:58:59');
 59

Since: 1.5.0

sentences
sentences(str[, lang, country]) - Splits str into an array of array of words.
Examples:
> SELECT sentences('Hi there! Good morning.');
 [["Hi","there"],["Good","morning"]]

Since: 2.0.0

sequence
sequence(start, stop, step) - Generates an array of elements from start to stop (inclusive),
incrementing by step. The type of the returned elements is the same as the type of argument
expressions.
Supported types are: byte, short, integer, long, date, timestamp.
The start and stop expressions must resolve to the same type.
If start and stop expressions resolve to the 'date' or 'timestamp' type
then the step expression must resolve to the 'interval' or 'year-month interval' or
'day-time interval' type, otherwise to the same type as the start and stop expressions.
Arguments:

start - an expression. The start of the range.
stop - an expression. The end the range (inclusive).
step - an optional expression. The step of the range.
    By default step is 1 if start is less than or equal to stop, otherwise -1.
    For the temporal sequences it's 1 day and -1 day respectively.
    If start is greater than stop then the step must be negative, and vice versa.

Examples:
> SELECT sequence(1, 5);
 [1,2,3,4,5]
> SELECT sequence(5, 1);
 [5,4,3,2,1]
> SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);
 [2018-01-01,2018-02-01,2018-03-01]
> SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval '0-1' year to month);
 [2018-01-01,2018-02-01,2018-03-01]

Since: 2.4.0

session_window
session_window(time_column, gap_duration) - Generates session window given a timestamp specifying column and gap duration.
See 'Types of time windows' in Structured Streaming guide doc for detailed explanation and examples.
Arguments:

time_column - The column or the expression to use as the timestamp for windowing by time. The time column must be of TimestampType.
gap_duration - A string specifying the timeout of the session represented as "interval value"
  (See Interval Literal for more details.) for the fixed gap duration, or
  an expression which is applied for each input and evaluated to the "interval value" for the dynamic gap duration.

Examples:
> SELECT a, session_window.start, session_window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:10:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, session_window(b, '5 minutes') ORDER BY a, start;
  A1    2021-01-01 00:00:00 2021-01-01 00:09:30 2
  A1    2021-01-01 00:10:00 2021-01-01 00:15:00 1
  A2    2021-01-01 00:01:00 2021-01-01 00:06:00 1
> SELECT a, session_window.start, session_window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:10:00'), ('A2', '2021-01-01 00:01:00'), ('A2', '2021-01-01 00:04:30') AS tab(a, b) GROUP by a, session_window(b, CASE WHEN a = 'A1' THEN '5 minutes' WHEN a = 'A2' THEN '1 minute' ELSE '10 minutes' END) ORDER BY a, start;
  A1    2021-01-01 00:00:00 2021-01-01 00:09:30 2
  A1    2021-01-01 00:10:00 2021-01-01 00:15:00 1
  A2    2021-01-01 00:01:00 2021-01-01 00:02:00 1
  A2    2021-01-01 00:04:30 2021-01-01 00:05:30 1

Since: 3.2.0

sha
sha(expr) - Returns a sha1 hash value as a hex string of the expr.
Examples:
> SELECT sha('Spark');
 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c

Since: 1.5.0

sha1
sha1(expr) - Returns a sha1 hash value as a hex string of the expr.
Examples:
> SELECT sha1('Spark');
 85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c

Since: 1.5.0

sha2
sha2(expr, bitLength) - Returns a checksum of SHA-2 family as a hex string of expr.
SHA-224, SHA-256, SHA-384, and SHA-512 are supported. Bit length of 0 is equivalent to 256.
Examples:
> SELECT sha2('Spark', 256);
 529bc3b07127ecb7e53a4dcf1991d9152c24537d919178022b2c42657f79a26b

Since: 1.5.0

shiftleft
shiftleft(base, expr) - Bitwise left shift.
Examples:
> SELECT shiftleft(2, 1);
 4

Since: 1.5.0

shiftright
shiftright(base, expr) - Bitwise (signed) right shift.
Examples:
> SELECT shiftright(4, 1);
 2

Since: 1.5.0

shiftrightunsigned
shiftrightunsigned(base, expr) - Bitwise unsigned right shift.
Examples:
> SELECT shiftrightunsigned(4, 1);
 2

Since: 1.5.0

shuffle
shuffle(array) - Returns a random permutation of the given array.
Examples:
> SELECT shuffle(array(1, 20, 3, 5));
 [3,1,5,20]
> SELECT shuffle(array(1, 20, null, 3));
 [20,null,3,1]

Note:
The function is non-deterministic.
Since: 2.4.0

sign
sign(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive.
Examples:
> SELECT sign(40);
 1.0
> SELECT sign(INTERVAL -'100' YEAR);
 -1.0

Since: 1.4.0

signum
signum(expr) - Returns -1.0, 0.0 or 1.0 as expr is negative, 0 or positive.
Examples:
> SELECT signum(40);
 1.0
> SELECT signum(INTERVAL -'100' YEAR);
 -1.0

Since: 1.4.0

sin
sin(expr) - Returns the sine of expr, as if computed by java.lang.Math.sin.
Arguments:

expr - angle in radians

Examples:
> SELECT sin(0);
 0.0

Since: 1.4.0

sinh
sinh(expr) - Returns hyperbolic sine of expr, as if computed by java.lang.Math.sinh.
Arguments:

expr - hyperbolic angle

Examples:
> SELECT sinh(0);
 0.0

Since: 1.4.0

size
size(expr) - Returns the size of an array or a map.
The function returns null for null input if spark.sql.legacy.sizeOfNull is set to false or
spark.sql.ansi.enabled is set to true. Otherwise, the function returns -1 for null input.
With the default settings, the function returns -1 for null input.
Examples:
> SELECT size(array('b', 'd', 'c', 'a'));
 4
> SELECT size(map('a', 1, 'b', 2));
 2

Since: 1.5.0

skewness
skewness(expr) - Returns the skewness value calculated from values of a group.
Examples:
> SELECT skewness(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);
 1.1135657469022011
> SELECT skewness(col) FROM VALUES (-1000), (-100), (10), (20) AS tab(col);
 -1.1135657469022011

Since: 1.6.0

slice
slice(x, start, length) - Subsets array x starting from index start (array indices start at 1, or starting from the end if start is negative) with the specified length.
Examples:
> SELECT slice(array(1, 2, 3, 4), 2, 2);
 [2,3]
> SELECT slice(array(1, 2, 3, 4), -2, 2);
 [3,4]

Since: 2.4.0

smallint
smallint(expr) - Casts the value expr to the target data type smallint.
Since: 2.0.1

some
some(expr) - Returns true if at least one value of expr is true.
Examples:
> SELECT some(col) FROM VALUES (true), (false), (false) AS tab(col);
 true
> SELECT some(col) FROM VALUES (NULL), (true), (false) AS tab(col);
 true
> SELECT some(col) FROM VALUES (false), (false), (NULL) AS tab(col);
 false

Since: 3.0.0

sort_array
sort_array(array[, ascendingOrder]) - Sorts the input array in ascending or descending order
according to the natural ordering of the array elements. NaN is greater than any non-NaN
elements for double/float type. Null elements will be placed at the beginning of the returned
array in ascending order or at the end of the returned array in descending order.
Examples:
> SELECT sort_array(array('b', 'd', null, 'c', 'a'), true);
 [null,"a","b","c","d"]

Since: 1.5.0

soundex
soundex(str) - Returns Soundex code of the string.
Examples:
> SELECT soundex('Miller');
 M460

Since: 1.5.0

space
space(n) - Returns a string consisting of n spaces.
Examples:
> SELECT concat(space(2), '1');
   1

Since: 1.5.0

spark_partition_id
spark_partition_id() - Returns the current partition id.
Examples:
> SELECT spark_partition_id();
 0

Since: 1.4.0

split
split(str, regex, limit) - Splits str around occurrences that match regex and returns an array with a length of at most limit
Arguments:

str - a string expression to split.
regex - a string representing a regular expression. The regex string should be a
  Java regular expression.
limit - an integer expression which controls the number of times the regex is applied.
limit > 0: The resulting array's length will not be more than limit,
  and the resulting array's last entry will contain all input
  beyond the last matched regex.
limit <= 0: regex will be applied as many times as possible, and
  the resulting array can be of any size.



Examples:
> SELECT split('oneAtwoBthreeC', '[ABC]');
 ["one","two","three",""]
> SELECT split('oneAtwoBthreeC', '[ABC]', -1);
 ["one","two","three",""]
> SELECT split('oneAtwoBthreeC', '[ABC]', 2);
 ["one","twoBthreeC"]

Since: 1.5.0

split_part
split_part(str, delimiter, partNum) - Splits str by delimiter and return
requested part of the split (1-based). If any input is null, returns null.
if partNum is out of range of split parts, returns empty string. If partNum is 0,
throws an error. If partNum is negative, the parts are counted backward from the
end of the string. If the delimiter is an empty string, the str is not split.
Examples:
> SELECT split_part('11.12.13', '.', 3);
 13

Since: 3.3.0

sqrt
sqrt(expr) - Returns the square root of expr.
Examples:
> SELECT sqrt(4);
 2.0

Since: 1.1.1

stack
stack(n, expr1, ..., exprk) - Separates expr1, ..., exprk into n rows. Uses column names col0, col1, etc. by default unless specified otherwise.
Examples:
> SELECT stack(2, 1, 2, 3);
 1  2
 3  NULL

Since: 2.0.0

startswith
startswith(left, right) - Returns a boolean. The value is True if left starts with right.
Returns NULL if either input expression is NULL. Otherwise, returns False.
Both left or right must be of STRING or BINARY type.
Examples:
> SELECT startswith('Spark SQL', 'Spark');
 true
> SELECT startswith('Spark SQL', 'SQL');
 false
> SELECT startswith('Spark SQL', null);
 NULL
> SELECT startswith(x'537061726b2053514c', x'537061726b');
 true
> SELECT startswith(x'537061726b2053514c', x'53514c');
 false

Since: 3.3.0

std
std(expr) - Returns the sample standard deviation calculated from values of a group.
Examples:
> SELECT std(col) FROM VALUES (1), (2), (3) AS tab(col);
 1.0

Since: 1.6.0

stddev
stddev(expr) - Returns the sample standard deviation calculated from values of a group.
Examples:
> SELECT stddev(col) FROM VALUES (1), (2), (3) AS tab(col);
 1.0

Since: 1.6.0

stddev_pop
stddev_pop(expr) - Returns the population standard deviation calculated from values of a group.
Examples:
> SELECT stddev_pop(col) FROM VALUES (1), (2), (3) AS tab(col);
 0.816496580927726

Since: 1.6.0

stddev_samp
stddev_samp(expr) - Returns the sample standard deviation calculated from values of a group.
Examples:
> SELECT stddev_samp(col) FROM VALUES (1), (2), (3) AS tab(col);
 1.0

Since: 1.6.0

str_to_map
str_to_map(text[, pairDelim[, keyValueDelim]]) - Creates a map after splitting the text into key/value pairs using delimiters. Default delimiters are ',' for pairDelim and ':' for keyValueDelim. Both pairDelim and keyValueDelim are treated as regular expressions.
Examples:
> SELECT str_to_map('a:1,b:2,c:3', ',', ':');
 {"a":"1","b":"2","c":"3"}
> SELECT str_to_map('a');
 {"a":null}

Since: 2.0.1

string
string(expr) - Casts the value expr to the target data type string.
Since: 2.0.1

struct
struct(col1, col2, col3, ...) - Creates a struct with the given field values.
Examples:
> SELECT struct(1, 2, 3);
 {"col1":1,"col2":2,"col3":3}

Since: 1.4.0

substr
substr(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len.
substr(str FROM pos[ FOR len]]) - Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len.
Examples:
> SELECT substr('Spark SQL', 5);
 k SQL
> SELECT substr('Spark SQL', -3);
 SQL
> SELECT substr('Spark SQL', 5, 1);
 k
> SELECT substr('Spark SQL' FROM 5);
 k SQL
> SELECT substr('Spark SQL' FROM -3);
 SQL
> SELECT substr('Spark SQL' FROM 5 FOR 1);
 k
> SELECT substr(encode('Spark SQL', 'utf-8'), 5);
 k SQL

Since: 1.5.0

substring
substring(str, pos[, len]) - Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len.
substring(str FROM pos[ FOR len]]) - Returns the substring of str that starts at pos and is of length len, or the slice of byte array that starts at pos and is of length len.
Examples:
> SELECT substring('Spark SQL', 5);
 k SQL
> SELECT substring('Spark SQL', -3);
 SQL
> SELECT substring('Spark SQL', 5, 1);
 k
> SELECT substring('Spark SQL' FROM 5);
 k SQL
> SELECT substring('Spark SQL' FROM -3);
 SQL
> SELECT substring('Spark SQL' FROM 5 FOR 1);
 k
> SELECT substring(encode('Spark SQL', 'utf-8'), 5);
 k SQL

Since: 1.5.0

substring_index
substring_index(str, delim, count) - Returns the substring from str before count occurrences of the delimiter delim.
If count is positive, everything to the left of the final delimiter (counting from the
left) is returned. If count is negative, everything to the right of the final delimiter
(counting from the right) is returned. The function substring_index performs a case-sensitive match
when searching for delim.
Examples:
> SELECT substring_index('www.apache.org', '.', 2);
 www.apache

Since: 1.5.0

sum
sum(expr) - Returns the sum calculated from values of a group.
Examples:
> SELECT sum(col) FROM VALUES (5), (10), (15) AS tab(col);
 30
> SELECT sum(col) FROM VALUES (NULL), (10), (15) AS tab(col);
 25
> SELECT sum(col) FROM VALUES (NULL), (NULL) AS tab(col);
 NULL

Since: 1.0.0

tan
tan(expr) - Returns the tangent of expr, as if computed by java.lang.Math.tan.
Arguments:

expr - angle in radians

Examples:
> SELECT tan(0);
 0.0

Since: 1.4.0

tanh
tanh(expr) - Returns the hyperbolic tangent of expr, as if computed by
java.lang.Math.tanh.
Arguments:

expr - hyperbolic angle

Examples:
> SELECT tanh(0);
 0.0

Since: 1.4.0

timestamp
timestamp(expr) - Casts the value expr to the target data type timestamp.
Since: 2.0.1

timestamp_micros
timestamp_micros(microseconds) - Creates timestamp from the number of microseconds since UTC epoch.
Examples:
> SELECT timestamp_micros(1230219000123123);
 2008-12-25 07:30:00.123123

Since: 3.1.0

timestamp_millis
timestamp_millis(milliseconds) - Creates timestamp from the number of milliseconds since UTC epoch.
Examples:
> SELECT timestamp_millis(1230219000123);
 2008-12-25 07:30:00.123

Since: 3.1.0

timestamp_seconds
timestamp_seconds(seconds) - Creates timestamp from the number of seconds (can be fractional) since UTC epoch.
Examples:
> SELECT timestamp_seconds(1230219000);
 2008-12-25 07:30:00
> SELECT timestamp_seconds(1230219000.123);
 2008-12-25 07:30:00.123

Since: 3.1.0

tinyint
tinyint(expr) - Casts the value expr to the target data type tinyint.
Since: 2.0.1

to_binary
to_binary(str[, fmt]) - Converts the input str to a binary value based on the supplied fmt.
fmt can be a case-insensitive string literal of "hex", "utf-8", "utf8", or "base64".
By default, the binary format for conversion is "hex" if fmt is omitted.
The function returns NULL if at least one of the input parameters is NULL.
Examples:
> SELECT to_binary('abc', 'utf-8');
 abc

Since: 3.3.0

to_char
to_char(numberExpr, formatExpr) - Convert numberExpr to a string based on the formatExpr.
Throws an exception if the conversion fails. The format can consist of the following
characters, case insensitive:
'0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format
string matches a sequence of digits in the input value, generating a result string of the
same length as the corresponding sequence in the format string. The result string is
left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of
the decimal value, starts with 0, and is before the decimal point. Otherwise, it is
padded with spaces.
'.' or 'D': Specifies the position of the decimal point (optional, only allowed once).
',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be
a 0 or 9 to the left and right of each grouping separator.
'$': Specifies the location of the $ currency sign. This character may only be specified
once.
'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at
the beginning or end of the format string). Note that 'S' prints '+' for positive values
but 'MI' prints a space.
'PR': Only allowed at the end of the format string; specifies that the result string will be
wrapped by angle brackets if the input value is negative.
('<1>').
Examples:
> SELECT to_char(454, '999');
 454
> SELECT to_char(454.00, '000D00');
 454.00
> SELECT to_char(12454, '99G999');
 12,454
> SELECT to_char(78.12, '$99.99');
 $78.12
> SELECT to_char(-12454.8, '99G999D9S');
 12,454.8-

Since: 3.4.0

to_csv
to_csv(expr[, options]) - Returns a CSV string with a given struct value
Examples:
> SELECT to_csv(named_struct('a', 1, 'b', 2));
 1,2
> SELECT to_csv(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));
 26/08/2015

Since: 3.0.0

to_date
to_date(date_str[, fmt]) - Parses the date_str expression with the fmt expression to
a date. Returns null with invalid input. By default, it follows casting rules to a date if
the fmt is omitted.
Arguments:

date_str - A string to be parsed to date.
fmt - Date format pattern to follow. See Datetime Patterns for valid
        date and time format patterns.

Examples:
> SELECT to_date('2009-07-30 04:17:52');
 2009-07-30
> SELECT to_date('2016-12-31', 'yyyy-MM-dd');
 2016-12-31

Since: 1.5.0

to_json
to_json(expr[, options]) - Returns a JSON string with a given struct value
Examples:
> SELECT to_json(named_struct('a', 1, 'b', 2));
 {"a":1,"b":2}
> SELECT to_json(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));
 {"time":"26/08/2015"}
> SELECT to_json(array(named_struct('a', 1, 'b', 2)));
 [{"a":1,"b":2}]
> SELECT to_json(map('a', named_struct('b', 1)));
 {"a":{"b":1}}
> SELECT to_json(map(named_struct('a', 1),named_struct('b', 2)));
 {"[1]":{"b":2}}
> SELECT to_json(map('a', 1));
 {"a":1}
> SELECT to_json(array(map('a', 1)));
 [{"a":1}]

Since: 2.2.0

to_number
to_number(expr, fmt) - Convert string 'expr' to a number based on the string format 'fmt'.
Throws an exception if the conversion fails. The format can consist of the following
characters, case insensitive:
'0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format
string matches a sequence of digits in the input string. If the 0/9 sequence starts with
0 and is before the decimal point, it can only match a digit sequence of the same size.
Otherwise, if the sequence starts with 9 or is after the decimal point, it can match a
digit sequence that has the same or smaller size.
'.' or 'D': Specifies the position of the decimal point (optional, only allowed once).
',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be
a 0 or 9 to the left and right of each grouping separator. 'expr' must match the
grouping separator relevant for the size of the number.
'$': Specifies the location of the $ currency sign. This character may only be specified
once.
'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at
the beginning or end of the format string). Note that 'S' allows '-' but 'MI' does not.
'PR': Only allowed at the end of the format string; specifies that 'expr' indicates a
negative number with wrapping angled brackets.
('<1>').
Examples:
> SELECT to_number('454', '999');
 454
> SELECT to_number('454.00', '000.00');
 454.00
> SELECT to_number('12,454', '99,999');
 12454
> SELECT to_number('$78.12', '$99.99');
 78.12
> SELECT to_number('12,454.8-', '99,999.9S');
 -12454.8

Since: 3.3.0

to_timestamp
to_timestamp(timestamp_str[, fmt]) - Parses the timestamp_str expression with the fmt expression
to a timestamp. Returns null with invalid input. By default, it follows casting rules to
a timestamp if the fmt is omitted. The result data type is consistent with the value of
configuration spark.sql.timestampType.
Arguments:

timestamp_str - A string to be parsed to timestamp.
fmt - Timestamp format pattern to follow. See Datetime Patterns for valid
        date and time format patterns.

Examples:
> SELECT to_timestamp('2016-12-31 00:12:00');
 2016-12-31 00:12:00
> SELECT to_timestamp('2016-12-31', 'yyyy-MM-dd');
 2016-12-31 00:00:00

Since: 2.2.0

to_timestamp_ltz
to_timestamp_ltz(timestamp_str[, fmt]) - Parses the timestamp_str expression with the fmt expression
to a timestamp with local time zone. Returns null with invalid input. By default, it follows casting rules to
a timestamp if the fmt is omitted.
Arguments:

timestamp_str - A string to be parsed to timestamp with local time zone.
fmt - Timestamp format pattern to follow. See Datetime Patterns for valid
        date and time format patterns.

Examples:
> SELECT to_timestamp_ltz('2016-12-31 00:12:00');
 2016-12-31 00:12:00
> SELECT to_timestamp_ltz('2016-12-31', 'yyyy-MM-dd');
 2016-12-31 00:00:00

Since: 3.4.0

to_timestamp_ntz
to_timestamp_ntz(timestamp_str[, fmt]) - Parses the timestamp_str expression with the fmt expression
to a timestamp without time zone. Returns null with invalid input. By default, it follows casting rules to
a timestamp if the fmt is omitted.
Arguments:

timestamp_str - A string to be parsed to timestamp without time zone.
fmt - Timestamp format pattern to follow. See Datetime Patterns for valid
        date and time format patterns.

Examples:
> SELECT to_timestamp_ntz('2016-12-31 00:12:00');
 2016-12-31 00:12:00
> SELECT to_timestamp_ntz('2016-12-31', 'yyyy-MM-dd');
 2016-12-31 00:00:00

Since: 3.4.0

to_unix_timestamp
to_unix_timestamp(timeExp[, fmt]) - Returns the UNIX timestamp of the given time.
Arguments:

timeExp - A date/timestamp or string which is returned as a UNIX timestamp.
fmt - Date/time format pattern to follow. Ignored if timeExp is not a string.
        Default value is "yyyy-MM-dd HH:mm:ss". See Datetime Patterns
        for valid date and time format patterns.

Examples:
> SELECT to_unix_timestamp('2016-04-08', 'yyyy-MM-dd');
 1460098800

Since: 1.6.0

to_utc_timestamp
to_utc_timestamp(timestamp, timezone) - Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield '2017-07-14 01:40:00.0'.
Examples:
> SELECT to_utc_timestamp('2016-08-31', 'Asia/Seoul');
 2016-08-30 15:00:00

Since: 1.5.0

to_varchar
to_varchar(numberExpr, formatExpr) - Convert numberExpr to a string based on the formatExpr.
Throws an exception if the conversion fails. The format can consist of the following
characters, case insensitive:
'0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format
string matches a sequence of digits in the input value, generating a result string of the
same length as the corresponding sequence in the format string. The result string is
left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of
the decimal value, starts with 0, and is before the decimal point. Otherwise, it is
padded with spaces.
'.' or 'D': Specifies the position of the decimal point (optional, only allowed once).
',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be
a 0 or 9 to the left and right of each grouping separator.
'$': Specifies the location of the $ currency sign. This character may only be specified
once.
'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at
the beginning or end of the format string). Note that 'S' prints '+' for positive values
but 'MI' prints a space.
'PR': Only allowed at the end of the format string; specifies that the result string will be
wrapped by angle brackets if the input value is negative.
('<1>').
Examples:
> SELECT to_varchar(454, '999');
 454
> SELECT to_varchar(454.00, '000D00');
 454.00
> SELECT to_varchar(12454, '99G999');
 12,454
> SELECT to_varchar(78.12, '$99.99');
 $78.12
> SELECT to_varchar(-12454.8, '99G999D9S');
 12,454.8-

Since: 3.5.0

transform
transform(expr, func) - Transforms elements in an array using the function.
Examples:
> SELECT transform(array(1, 2, 3), x -> x + 1);
 [2,3,4]
> SELECT transform(array(1, 2, 3), (x, i) -> x + i);
 [1,3,5]

Since: 2.4.0

transform_keys
transform_keys(expr, func) - Transforms elements in a map using the function.
Examples:
> SELECT transform_keys(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + 1);
 {2:1,3:2,4:3}
> SELECT transform_keys(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + v);
 {2:1,4:2,6:3}

Since: 3.0.0

transform_values
transform_values(expr, func) - Transforms values in the map using the function.
Examples:
> SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> v + 1);
 {1:2,2:3,3:4}
> SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + v);
 {1:2,2:4,3:6}

Since: 3.0.0

translate
translate(input, from, to) - Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string.
Examples:
> SELECT translate('AaBbCc', 'abc', '123');
 A1B2C3

Since: 1.5.0

trim
trim(str) - Removes the leading and trailing space characters from str.
trim(BOTH FROM str) - Removes the leading and trailing space characters from str.
trim(LEADING FROM str) - Removes the leading space characters from str.
trim(TRAILING FROM str) - Removes the trailing space characters from str.
trim(trimStr FROM str) - Remove the leading and trailing trimStr characters from str.
trim(BOTH trimStr FROM str) - Remove the leading and trailing trimStr characters from str.
trim(LEADING trimStr FROM str) - Remove the leading trimStr characters from str.
trim(TRAILING trimStr FROM str) - Remove the trailing trimStr characters from str.
Arguments:

str - a string expression
trimStr - the trim string characters to trim, the default value is a single space
BOTH, FROM - these are keywords to specify trimming string characters from both ends of
    the string
LEADING, FROM - these are keywords to specify trimming string characters from the left
    end of the string
TRAILING, FROM - these are keywords to specify trimming string characters from the right
    end of the string

Examples:
> SELECT trim('    SparkSQL   ');
 SparkSQL
> SELECT trim(BOTH FROM '    SparkSQL   ');
 SparkSQL
> SELECT trim(LEADING FROM '    SparkSQL   ');
 SparkSQL
> SELECT trim(TRAILING FROM '    SparkSQL   ');
     SparkSQL
> SELECT trim('SL' FROM 'SSparkSQLS');
 parkSQ
> SELECT trim(BOTH 'SL' FROM 'SSparkSQLS');
 parkSQ
> SELECT trim(LEADING 'SL' FROM 'SSparkSQLS');
 parkSQLS
> SELECT trim(TRAILING 'SL' FROM 'SSparkSQLS');
 SSparkSQ

Since: 1.5.0

trunc
trunc(date, fmt) - Returns date with the time portion of the day truncated to the unit specified by the format model fmt.
Arguments:

date - date value or valid date string
fmt - the format representing the unit to be truncated to
"YEAR", "YYYY", "YY" - truncate to the first date of the year that the date falls in
"QUARTER" - truncate to the first date of the quarter that the date falls in
"MONTH", "MM", "MON" - truncate to the first date of the month that the date falls in
"WEEK" - truncate to the Monday of the week that the date falls in



Examples:
> SELECT trunc('2019-08-04', 'week');
 2019-07-29
> SELECT trunc('2019-08-04', 'quarter');
 2019-07-01
> SELECT trunc('2009-02-12', 'MM');
 2009-02-01
> SELECT trunc('2015-10-27', 'YEAR');
 2015-01-01

Since: 1.5.0

try_add
try_add(expr1, expr2) - Returns the sum of expr1and expr2 and the result is null on overflow. The acceptable input types are the same with the + operator.
Examples:
> SELECT try_add(1, 2);
 3
> SELECT try_add(2147483647, 1);
 NULL
> SELECT try_add(date'2021-01-01', 1);
 2021-01-02
> SELECT try_add(date'2021-01-01', interval 1 year);
 2022-01-01
> SELECT try_add(timestamp'2021-01-01 00:00:00', interval 1 day);
 2021-01-02 00:00:00
> SELECT try_add(interval 1 year, interval 2 year);
 3-0

Since: 3.2.0

try_aes_decrypt
try_aes_decrypt(expr, key[, mode[, padding[, aad]]]) - This is a special version of aes_decrypt that performs the same operation, but returns a NULL value instead of raising an error if the decryption cannot be performed.
Examples:
> SELECT try_aes_decrypt(unhex('6E7CA17BBB468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210'), '0000111122223333', 'GCM');
 Spark SQL
> SELECT try_aes_decrypt(unhex('----------468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210'), '0000111122223333', 'GCM');
 NULL

Since: 3.5.0

try_avg
try_avg(expr) - Returns the mean calculated from values of a group and the result is null on overflow.
Examples:
> SELECT try_avg(col) FROM VALUES (1), (2), (3) AS tab(col);
 2.0
> SELECT try_avg(col) FROM VALUES (1), (2), (NULL) AS tab(col);
 1.5
> SELECT try_avg(col) FROM VALUES (interval '2147483647 months'), (interval '1 months') AS tab(col);
 NULL

Since: 3.3.0

try_divide
try_divide(dividend, divisor) - Returns dividend/divisor. It always performs floating point division. Its result is always null if expr2 is 0. dividend must be a numeric or an interval. divisor must be a numeric.
Examples:
> SELECT try_divide(3, 2);
 1.5
> SELECT try_divide(2L, 2L);
 1.0
> SELECT try_divide(1, 0);
 NULL
> SELECT try_divide(interval 2 month, 2);
 0-1
> SELECT try_divide(interval 2 month, 0);
 NULL

Since: 3.2.0

try_element_at
try_element_at(array, index) - Returns element of array at given (1-based) index. If Index is 0,
Spark will throw an error. If index < 0, accesses elements from the last to the first.
The function always returns NULL if the index exceeds the length of the array.
try_element_at(map, key) - Returns value for given key. The function always returns NULL
if the key is not contained in the map.
Examples:
> SELECT try_element_at(array(1, 2, 3), 2);
 2
> SELECT try_element_at(map(1, 'a', 2, 'b'), 2);
 b

Since: 3.3.0

try_multiply
try_multiply(expr1, expr2) - Returns expr1*expr2 and the result is null on overflow. The acceptable input types are the same with the * operator.
Examples:
> SELECT try_multiply(2, 3);
 6
> SELECT try_multiply(-2147483648, 10);
 NULL
> SELECT try_multiply(interval 2 year, 3);
 6-0

Since: 3.3.0

try_subtract
try_subtract(expr1, expr2) - Returns expr1-expr2 and the result is null on overflow. The acceptable input types are the same with the - operator.
Examples:
> SELECT try_subtract(2, 1);
 1
> SELECT try_subtract(-2147483648, 1);
 NULL
> SELECT try_subtract(date'2021-01-02', 1);
 2021-01-01
> SELECT try_subtract(date'2021-01-01', interval 1 year);
 2020-01-01
> SELECT try_subtract(timestamp'2021-01-02 00:00:00', interval 1 day);
 2021-01-01 00:00:00
> SELECT try_subtract(interval 2 year, interval 1 year);
 1-0

Since: 3.3.0

try_sum
try_sum(expr) - Returns the sum calculated from values of a group and the result is null on overflow.
Examples:
> SELECT try_sum(col) FROM VALUES (5), (10), (15) AS tab(col);
 30
> SELECT try_sum(col) FROM VALUES (NULL), (10), (15) AS tab(col);
 25
> SELECT try_sum(col) FROM VALUES (NULL), (NULL) AS tab(col);
 NULL
> SELECT try_sum(col) FROM VALUES (9223372036854775807L), (1L) AS tab(col);
 NULL

Since: 3.3.0

try_to_binary
try_to_binary(str[, fmt]) - This is a special version of to_binary that performs the same operation, but returns a NULL value instead of raising an error if the conversion cannot be performed.
Examples:
> SELECT try_to_binary('abc', 'utf-8');
 abc
> select try_to_binary('a!', 'base64');
 NULL
> select try_to_binary('abc', 'invalidFormat');
 NULL

Since: 3.3.0

try_to_number
try_to_number(expr, fmt) - Convert string 'expr' to a number based on the string format fmt.
Returns NULL if the string 'expr' does not match the expected format. The format follows the
same semantics as the to_number function.
Examples:
> SELECT try_to_number('454', '999');
 454
> SELECT try_to_number('454.00', '000.00');
 454.00
> SELECT try_to_number('12,454', '99,999');
 12454
> SELECT try_to_number('$78.12', '$99.99');
 78.12
> SELECT try_to_number('12,454.8-', '99,999.9S');
 -12454.8

Since: 3.3.0

try_to_timestamp
try_to_timestamp(timestamp_str[, fmt]) - Parses the timestamp_str expression with the fmt expression
to a timestamp. The function always returns null on an invalid input with/without ANSI SQL
mode enabled. By default, it follows casting rules to a timestamp if the fmt is omitted.
The result data type is consistent with the value of configuration spark.sql.timestampType.
Arguments:

timestamp_str - A string to be parsed to timestamp.
fmt - Timestamp format pattern to follow. See Datetime Patterns for valid
        date and time format patterns.

Examples:
> SELECT try_to_timestamp('2016-12-31 00:12:00');
 2016-12-31 00:12:00
> SELECT try_to_timestamp('2016-12-31', 'yyyy-MM-dd');
 2016-12-31 00:00:00
> SELECT try_to_timestamp('foo', 'yyyy-MM-dd');
 NULL

Since: 3.4.0

typeof
typeof(expr) - Return DDL-formatted type string for the data type of the input.
Examples:
> SELECT typeof(1);
 int
> SELECT typeof(array(1));
 array<int>

Since: 3.0.0

ucase
ucase(str) - Returns str with all characters changed to uppercase.
Examples:
> SELECT ucase('SparkSql');
 SPARKSQL

Since: 1.0.1

unbase64
unbase64(str) - Converts the argument from a base 64 string str to a binary.
Examples:
> SELECT unbase64('U3BhcmsgU1FM');
 Spark SQL

Since: 1.5.0

unhex
unhex(expr) - Converts hexadecimal expr to binary.
Examples:
> SELECT decode(unhex('537061726B2053514C'), 'UTF-8');
 Spark SQL

Since: 1.5.0

unix_date
unix_date(date) - Returns the number of days since 1970-01-01.
Examples:
> SELECT unix_date(DATE("1970-01-02"));
 1

Since: 3.1.0

unix_micros
unix_micros(timestamp) - Returns the number of microseconds since 1970-01-01 00:00:00 UTC.
Examples:
> SELECT unix_micros(TIMESTAMP('1970-01-01 00:00:01Z'));
 1000000

Since: 3.1.0

unix_millis
unix_millis(timestamp) - Returns the number of milliseconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.
Examples:
> SELECT unix_millis(TIMESTAMP('1970-01-01 00:00:01Z'));
 1000

Since: 3.1.0

unix_seconds
unix_seconds(timestamp) - Returns the number of seconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.
Examples:
> SELECT unix_seconds(TIMESTAMP('1970-01-01 00:00:01Z'));
 1

Since: 3.1.0

unix_timestamp
unix_timestamp([timeExp[, fmt]]) - Returns the UNIX timestamp of current or specified time.
Arguments:

timeExp - A date/timestamp or string. If not provided, this defaults to current time.
fmt - Date/time format pattern to follow. Ignored if timeExp is not a string.
        Default value is "yyyy-MM-dd HH:mm:ss". See  Datetime Patterns
        for valid date and time format patterns.

Examples:
> SELECT unix_timestamp();
 1476884637
> SELECT unix_timestamp('2016-04-08', 'yyyy-MM-dd');
 1460041200

Since: 1.5.0

upper
upper(str) - Returns str with all characters changed to uppercase.
Examples:
> SELECT upper('SparkSql');
 SPARKSQL

Since: 1.0.1

url_decode
url_decode(str) - Decodes a str in 'application/x-www-form-urlencoded' format using a specific encoding scheme.
Arguments:

str - a string expression to decode

Examples:
> SELECT url_decode('https%3A%2F%2Fspark.apache.org');
 https://spark.apache.org

Since: 3.4.0

url_encode
url_encode(str) - Translates a string into 'application/x-www-form-urlencoded' format using a specific encoding scheme.
Arguments:
str - a string expression to be translated
Examples:
> SELECT url_encode('https://spark.apache.org');
 https%3A%2F%2Fspark.apache.org

Since: 3.4.0

user
user() - user name of current execution context.
Examples:
> SELECT user();
 mockingjay

Since: 3.2.0

uuid
uuid() - Returns an universally unique identifier (UUID) string. The value is returned as a canonical UUID 36-character string.
Examples:
> SELECT uuid();
 46707d92-02f4-4817-8116-a4c3b23e6266

Note:
The function is non-deterministic.
Since: 2.3.0

var_pop
var_pop(expr) - Returns the population variance calculated from values of a group.
Examples:
> SELECT var_pop(col) FROM VALUES (1), (2), (3) AS tab(col);
 0.6666666666666666

Since: 1.6.0

var_samp
var_samp(expr) - Returns the sample variance calculated from values of a group.
Examples:
> SELECT var_samp(col) FROM VALUES (1), (2), (3) AS tab(col);
 1.0

Since: 1.6.0

variance
variance(expr) - Returns the sample variance calculated from values of a group.
Examples:
> SELECT variance(col) FROM VALUES (1), (2), (3) AS tab(col);
 1.0

Since: 1.6.0

version
version() - Returns the Spark version. The string contains 2 fields, the first being a release version and the second being a git revision.
Examples:
> SELECT version();
 3.1.0 a6d6ea3efedbad14d99c24143834cd4e2e52fb40

Since: 3.0.0

weekday
weekday(date) - Returns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).
Examples:
> SELECT weekday('2009-07-30');
 3

Since: 2.4.0

weekofyear
weekofyear(date) - Returns the week of the year of the given date. A week is considered to start on a Monday and week 1 is the first week with >3 days.
Examples:
> SELECT weekofyear('2008-02-20');
 8

Since: 1.5.0

when
CASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END - When expr1 = true, returns expr2; else when expr3 = true, returns expr4; else returns expr5.
Arguments:

expr1, expr3 - the branch condition expressions should all be boolean type.
expr2, expr4, expr5 - the branch value expressions and else value expression should all be
    same type or coercible to a common type.

Examples:
> SELECT CASE WHEN 1 > 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;
 1.0
> SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;
 2.0
> SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 < 0 THEN 2.0 END;
 NULL

Since: 1.0.1

width_bucket
width_bucket(value, min_value, max_value, num_bucket) - Returns the bucket number to which
value would be assigned in an equiwidth histogram with num_bucket buckets,
in the range min_value to max_value."
Examples:
> SELECT width_bucket(5.3, 0.2, 10.6, 5);
 3
> SELECT width_bucket(-2.1, 1.3, 3.4, 3);
 0
> SELECT width_bucket(8.1, 0.0, 5.7, 4);
 5
> SELECT width_bucket(-0.9, 5.2, 0.5, 2);
 3
> SELECT width_bucket(INTERVAL '0' YEAR, INTERVAL '0' YEAR, INTERVAL '10' YEAR, 10);
 1
> SELECT width_bucket(INTERVAL '1' YEAR, INTERVAL '0' YEAR, INTERVAL '10' YEAR, 10);
 2
> SELECT width_bucket(INTERVAL '0' DAY, INTERVAL '0' DAY, INTERVAL '10' DAY, 10);
 1
> SELECT width_bucket(INTERVAL '1' DAY, INTERVAL '0' DAY, INTERVAL '10' DAY, 10);
 2

Since: 3.1.0

window
window(time_column, window_duration[, slide_duration[, start_time]]) - Bucketize rows into one or more time windows given a timestamp specifying column.
Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05).
Windows can support microsecond precision. Windows in the order of months are not supported.
See 'Window Operations on Event Time' in Structured Streaming guide doc for detailed explanation and examples.
Arguments:

time_column - The column or the expression to use as the timestamp for windowing by time. The time column must be of TimestampType.
window_duration - A string specifying the width of the window represented as "interval value".
  (See Interval Literal for more details.)
  Note that the duration is a fixed length of time, and does not vary over time according to a calendar.
slide_duration - A string specifying the sliding interval of the window represented as "interval value".
  A new window will be generated every slide_duration. Must be less than or equal to the window_duration.
  This duration is likewise absolute, and does not vary according to a calendar.
start_time - The offset with respect to 1970-01-01 00:00:00 UTC with which to start window intervals.
  For example, in order to have hourly tumbling windows that start 15 minutes past the hour,
  e.g. 12:15-13:15, 13:15-14:15... provide start_time as 15 minutes.

Examples:
> SELECT a, window.start, window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:06:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, window(b, '5 minutes') ORDER BY a, start;
  A1    2021-01-01 00:00:00 2021-01-01 00:05:00 2
  A1    2021-01-01 00:05:00 2021-01-01 00:10:00 1
  A2    2021-01-01 00:00:00 2021-01-01 00:05:00 1
> SELECT a, window.start, window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:06:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, window(b, '10 minutes', '5 minutes') ORDER BY a, start;
  A1    2020-12-31 23:55:00 2021-01-01 00:05:00 2
  A1    2021-01-01 00:00:00 2021-01-01 00:10:00 3
  A1    2021-01-01 00:05:00 2021-01-01 00:15:00 1
  A2    2020-12-31 23:55:00 2021-01-01 00:05:00 1
  A2    2021-01-01 00:00:00 2021-01-01 00:10:00 1

Since: 2.0.0

window_time
window_time(window_column) - Extract the time value from time/session window column which can be used for event time value of window.
The extracted time is (window.end - 1) which reflects the fact that the the aggregating
windows have exclusive upper bound - [start, end)
See 'Window Operations on Event Time' in Structured Streaming guide doc for detailed explanation and examples.
Arguments:

window_column - The column representing time/session window.

Examples:
> SELECT a, window.start as start, window.end as end, window_time(window), cnt FROM (SELECT a, window, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:06:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, window(b, '5 minutes') ORDER BY a, window.start);
  A1    2021-01-01 00:00:00 2021-01-01 00:05:00 2021-01-01 00:04:59.999999  2
  A1    2021-01-01 00:05:00 2021-01-01 00:10:00 2021-01-01 00:09:59.999999  1
  A2    2021-01-01 00:00:00 2021-01-01 00:05:00 2021-01-01 00:04:59.999999  1

Since: 3.4.0

xpath
xpath(xml, xpath) - Returns a string array of values within the nodes of xml that match the XPath expression.
Examples:
> SELECT xpath('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>','a/b/text()');
 ["b1","b2","b3"]
> SELECT xpath('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>','a/b');
 [null,null,null]

Since: 2.0.0

xpath_boolean
xpath_boolean(xml, xpath) - Returns true if the XPath expression evaluates to true, or if a matching node is found.
Examples:
> SELECT xpath_boolean('<a><b>1</b></a>','a/b');
 true

Since: 2.0.0

xpath_double
xpath_double(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.
Examples:
> SELECT xpath_double('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
 3.0

Since: 2.0.0

xpath_float
xpath_float(xml, xpath) - Returns a float value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.
Examples:
> SELECT xpath_float('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
 3.0

Since: 2.0.0

xpath_int
xpath_int(xml, xpath) - Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.
Examples:
> SELECT xpath_int('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
 3

Since: 2.0.0

xpath_long
xpath_long(xml, xpath) - Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.
Examples:
> SELECT xpath_long('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
 3

Since: 2.0.0

xpath_number
xpath_number(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.
Examples:
> SELECT xpath_number('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
 3.0

Since: 2.0.0

xpath_short
xpath_short(xml, xpath) - Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.
Examples:
> SELECT xpath_short('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
 3

Since: 2.0.0

xpath_string
xpath_string(xml, xpath) - Returns the text contents of the first xml node that matches the XPath expression.
Examples:
> SELECT xpath_string('<a><b>b</b><c>cc</c></a>','a/c');
 cc

Since: 2.0.0

xxhash64
xxhash64(expr1, expr2, ...) - Returns a 64-bit hash value of the arguments. Hash seed is 42.
Examples:
> SELECT xxhash64('Spark', array(123), 2);
 5602566077635097486

Since: 3.0.0

year
year(date) - Returns the year component of the date/timestamp.
Examples:
> SELECT year('2016-07-30');
 2016

Since: 1.5.0

zip_with
zip_with(left, right, func) - Merges the two given arrays, element-wise, into a single array using function. If one array is shorter, nulls are appended at the end to match the length of the longer array, before applying function.
Examples:
> SELECT zip_with(array(1, 2, 3), array('a', 'b', 'c'), (x, y) -> (y, x));
 [{"y":"a","x":1},{"y":"b","x":2},{"y":"c","x":3}]
> SELECT zip_with(array(1, 2), array(3, 4), (x, y) -> x + y);
 [4,6]
> SELECT zip_with(array('a', 'b', 'c'), array('d', 'e', 'f'), (x, y) -> concat(x, y));
 ["ad","be","cf"]

Since: 2.4.0

|
expr1 | expr2 - Returns the result of bitwise OR of expr1 and expr2.
Examples:
> SELECT 3 | 5;
 7

Since: 1.4.0

||
expr1 || expr2 - Returns the concatenation of expr1 and expr2.
Examples:
> SELECT 'Spark' || 'SQL';
 SparkSQL
> SELECT array(1, 2, 3) || array(4, 5) || array(6);
 [1,2,3,4,5,6]

Note:
|| for arrays is available since 2.4.0.
Since: 2.3.0

~
~ expr - Returns the result of bitwise NOT of expr.
Examples:
> SELECT ~ 0;
 -1

Since: 1.4.0









  Built with MkDocs using a theme provided by Read the Docs.






















  




Building Spark - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Building Spark

Building Apache Spark 
Apache Maven 
Setting up Maven’s Memory Usage
build/mvn


Building a Runnable Distribution
Specifying the Hadoop Version and Enabling YARN
Building With Hive and JDBC Support
Packaging without Hadoop Dependencies for YARN
Building with Mesos support
Building with Kubernetes support
Building submodules individually
Building with Spark Connect support
Continuous Compilation
Building with SBT 
Setting up SBT’s Memory Usage


Speeding up Compilation
Encrypted Filesystems
IntelliJ IDEA or Eclipse


Running Tests 
Testing with SBT
Running Individual Tests
PySpark pip installable
PySpark Tests with Maven or SBT
Running R Tests
Running Docker-based Integration Test Suites
Change Scala Version
Running Jenkins tests with GitHub Enterprise 
Related environment variables
Building and testing on IPv6-only environment
Building with user-defined protoc





Building Apache Spark
Apache Maven
The Maven-based build is the build of reference for Apache Spark.
Building Spark using Maven requires Maven 3.9.6 and Java 8/11/17.
Spark requires Scala 2.12/2.13; support for Scala 2.11 was removed in Spark 3.0.0.
Setting up Maven’s Memory Usage
You’ll need to configure Maven to use more memory than usual by setting MAVEN_OPTS:
export MAVEN_OPTS="-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g"

(The ReservedCodeCacheSize setting is optional but recommended.)
If you don’t add these parameters to MAVEN_OPTS, you may see errors and warnings like the following:
[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.12/classes...
[ERROR] Java heap space -> [Help 1]

You can fix these problems by setting the MAVEN_OPTS variable as discussed before.
Note:

If using build/mvn with no MAVEN_OPTS set, the script will automatically add the above options to the MAVEN_OPTS environment variable.
The test phase of the Spark build will automatically add these options to MAVEN_OPTS, even when not using build/mvn.

build/mvn
Spark now comes packaged with a self-contained Maven installation to ease building and deployment of Spark from source located under the build/ directory. This script will automatically download and setup all necessary build requirements (Maven, Scala) locally within the build/ directory itself. It honors any mvn binary if present already, however, will pull down its own copy of Scala regardless to ensure proper version requirements are met. build/mvn execution acts as a pass through to the mvn call allowing easy transition from previous build methods. As an example, one can build a version of Spark as follows:
./build/mvn -DskipTests clean package

Other build examples can be found below.
Building a Runnable Distribution
To create a Spark distribution like those distributed by the
Spark Downloads page, and that is laid out so as
to be runnable, use ./dev/make-distribution.sh in the project root directory. It can be configured
with Maven profile settings and so on like the direct Maven build. Example:
./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phive -Phive-thriftserver -Pmesos -Pyarn -Pkubernetes

This will build Spark distribution along with Python pip and R packages. For more information on usage, run ./dev/make-distribution.sh --help
Specifying the Hadoop Version and Enabling YARN
You can specify the exact version of Hadoop to compile against through the hadoop.version property.
You can enable the yarn profile and optionally set the yarn.version property if it is different
from hadoop.version.
Example:
./build/mvn -Pyarn -Dhadoop.version=3.3.0 -DskipTests clean package

Building With Hive and JDBC Support
To enable Hive integration for Spark SQL along with its JDBC server and CLI,
add the -Phive and -Phive-thriftserver profiles to your existing build options.
By default Spark will build with Hive 2.3.9.
# With Hive 2.3.9 support
./build/mvn -Pyarn -Phive -Phive-thriftserver -DskipTests clean package

Packaging without Hadoop Dependencies for YARN
The assembly directory produced by mvn package will, by default, include all of Spark’s
dependencies, including Hadoop and some of its ecosystem projects. On YARN deployments, this
causes multiple versions of these to appear on executor classpaths: the version packaged in
the Spark assembly and the version on each node, included with yarn.application.classpath.
The hadoop-provided profile builds the assembly without including Hadoop-ecosystem projects,
like ZooKeeper and Hadoop itself.
Building with Mesos support
./build/mvn -Pmesos -DskipTests clean package

Building with Kubernetes support
./build/mvn -Pkubernetes -DskipTests clean package

Building submodules individually
It’s possible to build Spark submodules using the mvn -pl option.
For instance, you can build the Spark Streaming module using:
./build/mvn -pl :spark-streaming_2.12 clean install

where spark-streaming_2.12 is the artifactId as defined in streaming/pom.xml file.
Building with Spark Connect support
./build/mvn -Pconnect -DskipTests clean package

Continuous Compilation
We use the scala-maven-plugin which supports incremental and continuous compilation. E.g.
./build/mvn scala:cc

should run continuous compilation (i.e. wait for changes). However, this has not been tested
extensively. A couple of gotchas to note:


it only scans the paths src/main and src/test (see
docs), so it will only work
from within certain submodules that have that structure.


you’ll typically need to run mvn install from the project root for compilation within
specific submodules to work; this is because submodules that depend on other submodules do so via
the spark-parent module).


Thus, the full flow for running continuous-compilation of the core submodule may look more like:
$ ./build/mvn install
$ cd core
$ ../build/mvn scala:cc

Building with SBT
Maven is the official build tool recommended for packaging Spark, and is the build of reference.
But SBT is supported for day-to-day development since it can provide much faster iterative
compilation. More advanced developers may wish to use SBT.
The SBT build is derived from the Maven POM files, and so the same Maven profiles and variables
can be set to control the SBT build. For example:
./build/sbt package

To avoid the overhead of launching sbt each time you need to re-compile, you can launch sbt
in interactive mode by running build/sbt, and then run all build commands at the command
prompt.
Setting up SBT’s Memory Usage
Configure the JVM options for SBT in .jvmopts at the project root, for example:
-Xmx2g
-XX:ReservedCodeCacheSize=1g

For the meanings of these two options, please carefully read the Setting up Maven’s Memory Usage section.
Speeding up Compilation
Developers who compile Spark frequently may want to speed up compilation; e.g., by avoiding re-compilation of the
assembly JAR (for developers who build with SBT).  For more information about how to do this, refer to the
Useful Developer Tools page.
Encrypted Filesystems
When building on an encrypted filesystem (if your home directory is encrypted, for example), then the Spark build might fail with a “Filename too long” error. As a workaround, add the following in the configuration args of the scala-maven-plugin in the project pom.xml:
<arg>-Xmax-classfile-name</arg>
<arg>128</arg>

and in project/SparkBuild.scala add:
scalacOptions in Compile ++= Seq("-Xmax-classfile-name", "128"),

to the sharedSettings val. See also this PR if you are unsure of where to add these lines.
IntelliJ IDEA or Eclipse
For help in setting up IntelliJ IDEA or Eclipse for Spark development, and troubleshooting, refer to the
Useful Developer Tools page.
Running Tests
Tests are run by default via the ScalaTest Maven plugin.
Note that tests should not be run as root or an admin user.
The following is an example of a command to run the tests:
./build/mvn test

Testing with SBT
The following is an example of a command to run the tests:
./build/sbt test

Running Individual Tests
For information about how to run individual tests, refer to the
Useful Developer Tools page.
PySpark pip installable
If you are building Spark for use in a Python environment and you wish to pip install it, you will first need to build the Spark JARs as described above. Then you can construct an sdist package suitable for setup.py and pip installable package.
cd python; python setup.py sdist

Note: Due to packaging requirements you can not directly pip install from the Python directory, rather you must first build the sdist package as described above.
Alternatively, you can also run make-distribution with the –pip option.
PySpark Tests with Maven or SBT
If you are building PySpark and wish to run the PySpark tests you will need to build Spark with Hive support.
./build/mvn -DskipTests clean package -Phive
./python/run-tests

If you are building PySpark with SBT and wish to run the PySpark tests, you will need to build Spark with Hive support and also build the test components:
./build/sbt -Phive clean package
./build/sbt test:compile
./python/run-tests

The run-tests script also can be limited to a specific Python version or a specific module
./python/run-tests --python-executables=python --modules=pyspark-sql

Running R Tests
To run the SparkR tests you will need to install the knitr, rmarkdown, testthat, e1071 and survival packages first:
Rscript -e "install.packages(c('knitr', 'rmarkdown', 'devtools', 'testthat', 'e1071', 'survival'), repos='https://cloud.r-project.org/')"

You can run just the SparkR tests using the command:
./R/run-tests.sh

Running Docker-based Integration Test Suites
In order to run Docker integration tests, you have to install the docker engine on your box.
The instructions for installation can be found at the Docker site.
Once installed, the docker service needs to be started, if not already running.
On Linux, this can be done by sudo service docker start.
./build/mvn install -DskipTests
./build/mvn test -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.12

or
./build/sbt docker-integration-tests/test

Change Scala Version
When other versions of Scala like 2.13 are supported, it will be possible to build for that version.
Change the major Scala version using (e.g. 2.13):
./dev/change-scala-version.sh 2.13

Enable the profile (e.g. 2.13):
# For Maven
./build/mvn -Pscala-2.13 compile

# For sbt
./build/sbt -Pscala-2.13 compile

Running Jenkins tests with GitHub Enterprise
To run tests with Jenkins:
./dev/run-tests-jenkins

If use an individual repository or a repository on GitHub Enterprise, export below environment variables before running above command.
Related environment variables

Variable NameDefaultMeaning

SPARK_PROJECT_URL
https://github.com/apache/spark

    The Spark project URL of GitHub Enterprise.
  


GITHUB_API_BASE
https://api.github.com/repos/apache/spark

    The Spark project API server URL of GitHub Enterprise.
  


Building and testing on IPv6-only environment
Use Apache Spark GitBox URL because GitHub doesn’t support IPv6 yet.
https://gitbox.apache.org/repos/asf/spark.git

To build and run tests on IPv6-only environment, the following configurations are required.
export SPARK_LOCAL_HOSTNAME="your-IPv6-address" # e.g. '[2600:1700:232e:3de0:...]'
export DEFAULT_ARTIFACT_REPOSITORY=https://ipv6.repo1.maven.org/maven2/
export MAVEN_OPTS="-Djava.net.preferIPv6Addresses=true"
export SBT_OPTS="-Djava.net.preferIPv6Addresses=true"
export SERIAL_SBT_TESTS=1

Building with user-defined protoc
When the user cannot use the official protoc binary files to build the core module in the compilation environment, for example, compiling core module on CentOS 6 or CentOS 7 which the default glibc version is less than 2.14, we can try to compile and test by specifying the user-defined protoc binary files as follows:
export SPARK_PROTOC_EXEC_PATH=/path-to-protoc-exe
./build/mvn -Puser-defined-protoc -DskipDefaultProtoc clean package

or
export SPARK_PROTOC_EXEC_PATH=/path-to-protoc-exe
./build/sbt -Puser-defined-protoc clean package

The user-defined protoc binary files can be produced in the user’s compilation environment by source code compilation, for compilation steps, please refer to protobuf.




















  




Cluster Mode Overview - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Cluster Mode Overview
This document gives a short overview of how Spark runs on clusters, to make it easier to understand
the components involved. Read through the application submission guide
to learn about launching applications on a cluster.
Components
Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext
object in your main program (called the driver program).
Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers
(either Spark’s own standalone cluster manager, Mesos, YARN or Kubernetes), which allocate resources across
applications. Once connected, Spark acquires executors on nodes in the cluster, which are
processes that run computations and store data for your application.
Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to
the executors. Finally, SparkContext sends tasks to the executors to run.



There are several useful things to note about this architecture:

Each application gets its own executor processes, which stay up for the duration of the whole
application and run tasks in multiple threads. This has the benefit of isolating applications
from each other, on both the scheduling side (each driver schedules its own tasks) and executor
side (tasks from different applications run in different JVMs). However, it also means that
data cannot be shared across different Spark applications (instances of SparkContext) without
writing it to an external storage system.
Spark is agnostic to the underlying cluster manager. As long as it can acquire executor
processes, and these communicate with each other, it is relatively easy to run it even on a
cluster manager that also supports other applications (e.g. Mesos/YARN/Kubernetes).
The driver program must listen for and accept incoming connections from its executors throughout
its lifetime (e.g., see spark.driver.port in the network config
section). As such, the driver program must be network
addressable from the worker nodes.
Because the driver schedules tasks on the cluster, it should be run close to the worker
nodes, preferably on the same local area network. If you’d like to send requests to the
cluster remotely, it’s better to open an RPC to the driver and have it submit operations
from nearby than to run a driver far away from the worker nodes.

Cluster Manager Types
The system currently supports several cluster managers:

Standalone – a simple cluster manager included with Spark that makes it
easy to set up a cluster.
Apache Mesos – a general cluster manager that can also run Hadoop MapReduce
and service applications. (Deprecated)
Hadoop YARN – the resource manager in Hadoop 3.
Kubernetes – an open-source system for automating deployment, scaling,
and management of containerized applications.

Submitting Applications
Applications can be submitted to a cluster of any type using the spark-submit script.
The application submission guide describes how to do this.
Monitoring
Each driver program has a web UI, typically on port 4040, that displays information about running
tasks, executors, and storage usage. Simply go to http://<driver-node>:4040 in a web browser to
access this UI. The monitoring guide also describes other monitoring options.
Job Scheduling
Spark gives control over resource allocation both across applications (at the level of the cluster
manager) and within applications (if multiple computations are happening on the same SparkContext).
The job scheduling overview describes this in more detail.
Glossary
The following table summarizes terms you’ll see used to refer to cluster concepts:


TermMeaning



Application
User program built on Spark. Consists of a driver program and executors on the cluster.


Application jar

        A jar containing the user's Spark application. In some cases users will want to create
        an "uber jar" containing their application along with its dependencies. The user's jar
        should never include Hadoop or Spark libraries, however, these will be added at runtime.
      


Driver program
The process running the main() function of the application and creating the SparkContext


Cluster manager
An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN, Kubernetes)


Deploy mode
Distinguishes where the driver process runs. In "cluster" mode, the framework launches
        the driver inside of the cluster. In "client" mode, the submitter launches the driver
        outside of the cluster.


Worker node
Any node that can run application code in the cluster


Executor
A process launched for an application on a worker node, that runs tasks and keeps data in memory
        or disk storage across them. Each application has its own executors.


Task
A unit of work that will be sent to one executor


Job
A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action
        (e.g. save, collect); you'll see this term used in the driver's logs.


Stage
Each job gets divided into smaller sets of tasks called stages that depend on each other
        (similar to the map and reduce stages in MapReduce); you'll see this term used in the driver's logs.























  




Configuration - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Spark Configuration

Spark Properties 
Dynamically Loading Spark Properties
Viewing Spark Properties
Available Properties 
Application Properties
Runtime Environment
Shuffle Behavior
Spark UI
Compression and Serialization
Memory Management
Execution Behavior
Executor Metrics
Networking
Scheduling
Barrier Execution Mode
Dynamic Allocation
Thread Configurations
Spark Connect 
Server Configuration


Security
Spark SQL 
Runtime SQL Configuration
Static SQL Configuration


Spark Streaming
SparkR
GraphX
Deploy
Cluster Managers 
YARN
Mesos
Kubernetes
Standalone Mode






Environment Variables
Configuring Logging
Overriding configuration directory
Inheriting Hadoop Cluster Configuration
Custom Hadoop/Hive Configuration
Custom Resource Scheduling and Configuration Overview
Stage Level Scheduling Overview
Push-based shuffle overview 
External Shuffle service(server) side configuration options
Client side configuration options



Spark provides three locations to configure the system:

Spark properties control most application parameters and can be set by using
a SparkConf object, or through Java
system properties.
Environment variables can be used to set per-machine settings, such as
the IP address, through the conf/spark-env.sh script on each node.
Logging can be configured through log4j2.properties.

Spark Properties
Spark properties control most application settings and are configured separately for each
application. These properties can be set directly on a
SparkConf passed to your
SparkContext. SparkConf allows you to configure some of the common properties
(e.g. master URL and application name), as well as arbitrary key-value pairs through the
set() method. For example, we could initialize an application with two threads as follows:
Note that we run with local[2], meaning two threads - which represents “minimal” parallelism,
which can help detect bugs that only exist when we run in a distributed context.
val conf = new SparkConf()
             .setMaster("local[2]")
             .setAppName("CountingSheep")
val sc = new SparkContext(conf)
Note that we can have more than 1 thread in local mode, and in cases like Spark Streaming, we may
actually require more than 1 thread to prevent any sort of starvation issues.
Properties that specify some time duration should be configured with a unit of time.
The following format is accepted:
25ms (milliseconds)
5s (seconds)
10m or 10min (minutes)
3h (hours)
5d (days)
1y (years)

Properties that specify a byte size should be configured with a unit of size.
The following format is accepted:
1b (bytes)
1k or 1kb (kibibytes = 1024 bytes)
1m or 1mb (mebibytes = 1024 kibibytes)
1g or 1gb (gibibytes = 1024 mebibytes)
1t or 1tb (tebibytes = 1024 gibibytes)
1p or 1pb (pebibytes = 1024 tebibytes)

While numbers without units are generally interpreted as bytes, a few are interpreted as KiB or MiB.
See documentation of individual configuration properties. Specifying units is desirable where
possible.
Dynamically Loading Spark Properties
In some cases, you may want to avoid hard-coding certain configurations in a SparkConf. For
instance, if you’d like to run the same application with different masters or different
amounts of memory. Spark allows you to simply create an empty conf:
val sc = new SparkContext(new SparkConf())
Then, you can supply configuration values at runtime:
./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false
  --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar
The Spark shell and spark-submit
tool support two ways to load configurations dynamically. The first is command line options,
such as --master, as shown above. spark-submit can accept any Spark property using the --conf/-c
flag, but uses special flags for properties that play a part in launching the Spark application.
Running ./bin/spark-submit --help will show the entire list of these options.
bin/spark-submit will also read configuration options from conf/spark-defaults.conf, in which
each line consists of a key and a value separated by whitespace. For example:
spark.master            spark://5.6.7.8:7077
spark.executor.memory   4g
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer

Any values specified as flags or in the properties file will be passed on to the application
and merged with those specified through SparkConf. Properties set directly on the SparkConf
take highest precedence, then flags passed to spark-submit or spark-shell, then options
in the spark-defaults.conf file. A few configuration keys have been renamed since earlier
versions of Spark; in such cases, the older key names are still accepted, but take lower
precedence than any instance of the newer key.
Spark properties mainly can be divided into two kinds: one is related to deploy, like
“spark.driver.memory”, “spark.executor.instances”, this kind of properties may not be affected when
setting programmatically through SparkConf in runtime, or the behavior is depending on which
cluster manager and deploy mode you choose, so it would be suggested to set through configuration
file or spark-submit command line options; another is mainly related to Spark runtime control,
like “spark.task.maxFailures”, this kind of properties can be set in either way.
Viewing Spark Properties
The application web UI at http://<driver>:4040 lists Spark properties in the “Environment” tab.
This is a useful place to check to make sure that your properties have been set correctly. Note
that only values explicitly specified through spark-defaults.conf, SparkConf, or the command
line will appear. For all other configuration properties, you can assume the default value is used.
Available Properties
Most of the properties that control internal settings have reasonable default values. Some
of the most common options to set are:
Application Properties

Property NameDefaultMeaningSince Version

spark.app.name
(none)

    The name of your application. This will appear in the UI and in log data.
  
0.9.0


spark.driver.cores
1

    Number of cores to use for the driver process, only in cluster mode.
  
1.3.0


spark.driver.maxResultSize
1g

    Limit of total size of serialized results of all partitions for each Spark action (e.g.
    collect) in bytes. Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total
    size is above this limit.
    Having a high limit may cause out-of-memory errors in driver (depends on spark.driver.memory
    and memory overhead of objects in JVM). Setting a proper limit can protect the driver from
    out-of-memory errors.
  
1.2.0


spark.driver.memory
1g

    Amount of memory to use for the driver process, i.e. where SparkContext is initialized, in the
    same format as JVM memory strings with a size unit suffix ("k", "m", "g" or "t")
    (e.g. 512m, 2g).
    
Note: In client mode, this config must not be set through the SparkConf
    directly in your application, because the driver JVM has already started at that point.
    Instead, please set this through the --driver-memory command line option
    or in your default properties file.
  
1.1.1


spark.driver.memoryOverhead
driverMemory * spark.driver.memoryOverheadFactor, with minimum of 384 

    Amount of non-heap memory to be allocated per driver process in cluster mode, in MiB unless
    otherwise specified. This is memory that accounts for things like VM overheads, interned strings,
    other native overheads, etc. This tends to grow with the container size (typically 6-10%).
    This option is currently supported on YARN, Mesos and Kubernetes.
    Note: Non-heap memory includes off-heap memory
    (when spark.memory.offHeap.enabled=true) and memory used by other driver processes
    (e.g. python process that goes with a PySpark driver) and memory used by other non-driver
    processes running in the same container. The maximum memory size of container to running
    driver is determined by the sum of spark.driver.memoryOverhead
    and spark.driver.memory.
  
2.3.0


spark.driver.memoryOverheadFactor
0.10

    Fraction of driver memory to be allocated as additional non-heap memory per driver process in cluster mode.
    This is memory that accounts for things like VM overheads, interned strings,
    other native overheads, etc. This tends to grow with the container size.
    This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to
    0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks
    commonly fail with "Memory Overhead Exceeded" errors. This preempts this error
    with a higher default.
    This value is ignored if spark.driver.memoryOverhead is set directly.
  
3.3.0


spark.driver.resource.{resourceName}.amount
0

    Amount of a particular resource type to use on the driver.
    If this is used, you must also specify the
    spark.driver.resource.{resourceName}.discoveryScript
    for the driver to find the resource on startup.
  
3.0.0


spark.driver.resource.{resourceName}.discoveryScript
None

    A script for the driver to run to discover a particular resource type. This should
    write to STDOUT a JSON string in the format of the ResourceInformation class. This has a
    name and an array of addresses. For a client-submitted driver, discovery script must assign
    different resource addresses to this driver comparing to other drivers on the same host.
  
3.0.0


spark.driver.resource.{resourceName}.vendor
None

    Vendor of the resources to use for the driver. This option is currently
    only supported on Kubernetes and is actually both the vendor and domain following
    the Kubernetes device plugin naming convention. (e.g. For GPUs on Kubernetes
    this config would be set to nvidia.com or amd.com)
  
3.0.0


spark.resources.discoveryPlugin
org.apache.spark.resource.ResourceDiscoveryScriptPlugin

    Comma-separated list of class names implementing
    org.apache.spark.api.resource.ResourceDiscoveryPlugin to load into the application.
    This is for advanced users to replace the resource discovery class with a
    custom implementation. Spark will try each class specified until one of them
    returns the resource information for that resource. It tries the discovery
    script last if none of the plugins return information for that resource.
  
3.0.0


spark.executor.memory
1g

    Amount of memory to use per executor process, in the same format as JVM memory strings with
    a size unit suffix ("k", "m", "g" or "t") (e.g. 512m, 2g).
  
0.7.0


spark.executor.pyspark.memory
Not set

    The amount of memory to be allocated to PySpark in each executor, in MiB
    unless otherwise specified.  If set, PySpark memory for an executor will be
    limited to this amount. If not set, Spark will not limit Python's memory use
    and it is up to the application to avoid exceeding the overhead memory space
    shared with other non-JVM processes. When PySpark is run in YARN or Kubernetes, this memory
    is added to executor resource requests.
    
Note: This feature is dependent on Python's `resource` module; therefore, the behaviors and
    limitations are inherited. For instance, Windows does not support resource limiting and actual
    resource is not limited on MacOS.
  
2.4.0


spark.executor.memoryOverhead
executorMemory * spark.executor.memoryOverheadFactor, with minimum of 384 

    Amount of additional memory to be allocated per executor process, in MiB unless otherwise specified.
    This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.
    This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes.
    
Note: Additional memory includes PySpark executor memory
    (when spark.executor.pyspark.memory is not configured) and memory used by other
    non-executor processes running in the same container. The maximum memory size of container to
    running executor is determined by the sum of spark.executor.memoryOverhead,
    spark.executor.memory, spark.memory.offHeap.size and
    spark.executor.pyspark.memory.
  
2.3.0


spark.executor.memoryOverheadFactor
0.10

    Fraction of executor memory to be allocated as additional non-heap memory per executor process.
    This is memory that accounts for things like VM overheads, interned strings,
    other native overheads, etc. This tends to grow with the container size.
    This value defaults to 0.10 except for Kubernetes non-JVM jobs, which defaults to
    0.40. This is done as non-JVM tasks need more non-JVM heap space and such tasks
    commonly fail with "Memory Overhead Exceeded" errors. This preempts this error
    with a higher default.
    This value is ignored if spark.executor.memoryOverhead is set directly.
  
3.3.0


spark.executor.resource.{resourceName}.amount
0

    Amount of a particular resource type to use per executor process.
    If this is used, you must also specify the
    spark.executor.resource.{resourceName}.discoveryScript
    for the executor to find the resource on startup.
  
3.0.0


spark.executor.resource.{resourceName}.discoveryScript
None

    A script for the executor to run to discover a particular resource type. This should
    write to STDOUT a JSON string in the format of the ResourceInformation class. This has a
    name and an array of addresses.
  
3.0.0


spark.executor.resource.{resourceName}.vendor
None

    Vendor of the resources to use for the executors. This option is currently
    only supported on Kubernetes and is actually both the vendor and domain following
    the Kubernetes device plugin naming convention. (e.g. For GPUs on Kubernetes
    this config would be set to nvidia.com or amd.com)
  
3.0.0


spark.extraListeners
(none)

    A comma-separated list of classes that implement SparkListener; when initializing
    SparkContext, instances of these classes will be created and registered with Spark's listener
    bus.  If a class has a single-argument constructor that accepts a SparkConf, that constructor
    will be called; otherwise, a zero-argument constructor will be called. If no valid constructor
    can be found, the SparkContext creation will fail with an exception.
  
1.3.0


spark.local.dir
/tmp

    Directory to use for "scratch" space in Spark, including map output files and RDDs that get
    stored on disk. This should be on a fast, local disk in your system. It can also be a
    comma-separated list of multiple directories on different disks.

    
Note: This will be overridden by SPARK_LOCAL_DIRS (Standalone), MESOS_SANDBOX (Mesos) or
    LOCAL_DIRS (YARN) environment variables set by the cluster manager.
  
0.5.0


spark.logConf
false

    Logs the effective SparkConf as INFO when a SparkContext is started.
  
0.9.0


spark.master
(none)

    The cluster manager to connect to. See the list of
     allowed master URL's.
  
0.9.0


spark.submit.deployMode
client

    The deploy mode of Spark driver program, either "client" or "cluster",
    Which means to launch driver program locally ("client")
    or remotely ("cluster") on one of the nodes inside the cluster.
  
1.5.0


spark.log.callerContext
(none)

    Application information that will be written into Yarn RM log/HDFS audit log when running on Yarn/HDFS.
    Its length depends on the Hadoop configuration hadoop.caller.context.max.size. It should be concise,
    and typically can have up to 50 characters.
  
2.2.0


spark.log.level
(none)

    When set, overrides any user-defined log settings as if calling
    SparkContext.setLogLevel() at Spark startup. Valid log levels include: "ALL", "DEBUG", "ERROR", "FATAL", "INFO", "OFF", "TRACE", "WARN".
  
3.5.0


spark.driver.supervise
false

    If true, restarts the driver automatically if it fails with a non-zero exit status.
    Only has effect in Spark standalone mode or Mesos cluster deploy mode.
  
1.3.0


spark.driver.log.dfsDir
(none)

    Base directory in which Spark driver logs are synced, if spark.driver.log.persistToDfs.enabled
    is true. Within this base directory, each application logs the driver logs to an application specific file.
    Users may want to set this to a unified location like an HDFS directory so driver log files can be persisted
    for later usage. This directory should allow any Spark user to read/write files and the Spark History Server
    user to delete files. Additionally, older logs from this directory are cleaned by the
    Spark History Server if
    spark.history.fs.driverlog.cleaner.enabled is true and, if they are older than max age configured
    by setting spark.history.fs.driverlog.cleaner.maxAge.
  
3.0.0


spark.driver.log.persistToDfs.enabled
false

    If true, spark application running in client mode will write driver logs to a persistent storage, configured
    in spark.driver.log.dfsDir. If spark.driver.log.dfsDir is not configured, driver logs
    will not be persisted. Additionally, enable the cleaner by setting spark.history.fs.driverlog.cleaner.enabled
    to true in Spark History Server.
  
3.0.0


spark.driver.log.layout
%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex

    The layout for the driver logs that are synced to spark.driver.log.dfsDir. If this is not configured,
    it uses the layout for the first appender defined in log4j2.properties. If that is also not configured, driver logs
    use the default layout.
  
3.0.0


spark.driver.log.allowErasureCoding
false

    Whether to allow driver logs to use erasure coding.  On HDFS, erasure coded files will not
    update as quickly as regular replicated files, so they make take longer to reflect changes
    written by the application. Note that even if this is true, Spark will still not force the
    file to use erasure coding, it will simply use file system defaults.
  
3.0.0


spark.decommission.enabled
false

    When decommission enabled, Spark will try its best to shut down the executor gracefully.
    Spark will try to migrate all the RDD blocks (controlled by spark.storage.decommission.rddBlocks.enabled)
    and shuffle blocks (controlled by spark.storage.decommission.shuffleBlocks.enabled) from the decommissioning
    executor to a remote executor when spark.storage.decommission.enabled is enabled.
    With decommission enabled, Spark will also decommission an executor instead of killing when spark.dynamicAllocation.enabled enabled.
  
3.1.0


spark.executor.decommission.killInterval
(none)

    Duration after which a decommissioned executor will be killed forcefully by an outside (e.g. non-spark) service.
  
3.1.0


spark.executor.decommission.forceKillTimeout
(none)

    Duration after which a Spark will force a decommissioning executor to exit.
    This should be set to a high value in most situations as low values will prevent block migrations from having enough time to complete.
  
3.2.0


spark.executor.decommission.signal
PWR

    The signal that used to trigger the executor to start decommission.
  
3.2.0


spark.executor.maxNumFailures
numExecutors * 2, with minimum of 3

    The maximum number of executor failures before failing the application.
    This configuration only takes effect on YARN, or Kubernetes when 
    `spark.kubernetes.allocation.pods.allocator` is set to 'direct'.
  
3.5.0


spark.executor.failuresValidityInterval
(none)

    Interval after which executor failures will be considered independent and
    not accumulate towards the attempt count.
    This configuration only takes effect on YARN, or Kubernetes when 
    `spark.kubernetes.allocation.pods.allocator` is set to 'direct'.
  
3.5.0


Apart from these, the following properties are also available, and may be useful in some situations:
Runtime Environment

Property NameDefaultMeaningSince Version

spark.driver.extraClassPath
(none)

    Extra classpath entries to prepend to the classpath of the driver.

    Note: In client mode, this config must not be set through the SparkConf
    directly in your application, because the driver JVM has already started at that point.
    Instead, please set this through the --driver-class-path command line option or in
    your default properties file.
  
1.0.0


spark.driver.defaultJavaOptions
(none)

    A string of default JVM options to prepend to spark.driver.extraJavaOptions.
    This is intended to be set by administrators.

    For instance, GC settings or other logging.
    Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap
    size settings can be set with spark.driver.memory in the cluster mode and through
    the --driver-memory command line option in the client mode.

    Note: In client mode, this config must not be set through the SparkConf
    directly in your application, because the driver JVM has already started at that point.
    Instead, please set this through the --driver-java-options command line option or in
    your default properties file.
  
3.0.0


spark.driver.extraJavaOptions
(none)

    A string of extra JVM options to pass to the driver. This is intended to be set by users.

    For instance, GC settings or other logging.
    Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap
    size settings can be set with spark.driver.memory in the cluster mode and through
    the --driver-memory command line option in the client mode.

    Note: In client mode, this config must not be set through the SparkConf
    directly in your application, because the driver JVM has already started at that point.
    Instead, please set this through the --driver-java-options command line option or in
    your default properties file.

    spark.driver.defaultJavaOptions will be prepended to this configuration.
  
1.0.0


spark.driver.extraLibraryPath
(none)

    Set a special library path to use when launching the driver JVM.

    Note: In client mode, this config must not be set through the SparkConf
    directly in your application, because the driver JVM has already started at that point.
    Instead, please set this through the --driver-library-path command line option or in
    your default properties file.
  
1.0.0


spark.driver.userClassPathFirst
false

    (Experimental) Whether to give user-added jars precedence over Spark's own jars when loading
    classes in the driver. This feature can be used to mitigate conflicts between Spark's
    dependencies and user dependencies. It is currently an experimental feature.

    This is used in cluster mode only.
  
1.3.0


spark.executor.extraClassPath
(none)

    Extra classpath entries to prepend to the classpath of executors. This exists primarily for
    backwards-compatibility with older versions of Spark. Users typically should not need to set
    this option.
  
1.0.0


spark.executor.defaultJavaOptions
(none)

    A string of default JVM options to prepend to spark.executor.extraJavaOptions.
    This is intended to be set by administrators.

    For instance, GC settings or other logging.
    Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this
    option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file
    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.

    The following symbols, if present will be interpolated:  will be replaced by
    application ID and  will be replaced by executor ID. For example, to enable
    verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of:
    -verbose:gc -Xloggc:/tmp/-.gc

3.0.0


spark.executor.extraJavaOptions
(none)

    A string of extra JVM options to pass to executors. This is intended to be set by users.

    For instance, GC settings or other logging.
    Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this
    option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file
    used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.

    The following symbols, if present will be interpolated:  will be replaced by
    application ID and  will be replaced by executor ID. For example, to enable
    verbose gc logging to a file named for the executor ID of the app in /tmp, pass a 'value' of:
    -verbose:gc -Xloggc:/tmp/-.gc
spark.executor.defaultJavaOptions will be prepended to this configuration.
  
1.0.0


spark.executor.extraLibraryPath
(none)

    Set a special library path to use when launching executor JVM's.
  
1.0.0


spark.executor.logs.rolling.maxRetainedFiles
-1

    Sets the number of latest rolling log files that are going to be retained by the system.
    Older log files will be deleted. Disabled by default.
  
1.1.0


spark.executor.logs.rolling.enableCompression
false

    Enable executor log compression. If it is enabled, the rolled executor logs will be compressed.
    Disabled by default.
  
2.0.2


spark.executor.logs.rolling.maxSize
1024 * 1024

    Set the max size of the file in bytes by which the executor logs will be rolled over.
    Rolling is disabled by default. See spark.executor.logs.rolling.maxRetainedFiles
    for automatic cleaning of old logs.
  
1.4.0


spark.executor.logs.rolling.strategy
(none)

    Set the strategy of rolling of executor logs. By default it is disabled. It can
    be set to "time" (time-based rolling) or "size" (size-based rolling). For "time",
    use spark.executor.logs.rolling.time.interval to set the rolling interval.
    For "size", use spark.executor.logs.rolling.maxSize to set
    the maximum file size for rolling.
  
1.1.0


spark.executor.logs.rolling.time.interval
daily

    Set the time interval by which the executor logs will be rolled over.
    Rolling is disabled by default. Valid values are daily, hourly, minutely or
    any interval in seconds. See spark.executor.logs.rolling.maxRetainedFiles
    for automatic cleaning of old logs.
  
1.1.0


spark.executor.userClassPathFirst
false

    (Experimental) Same functionality as spark.driver.userClassPathFirst, but
    applied to executor instances.
  
1.3.0


spark.executorEnv.[EnvironmentVariableName]
(none)

    Add the environment variable specified by EnvironmentVariableName to the Executor
    process. The user can specify multiple of these to set multiple environment variables.
  
0.9.0


spark.redaction.regex
(?i)secret|password|token|access[.]key

    Regex to decide which Spark configuration properties and environment variables in driver and
    executor environments contain sensitive information. When this regex matches a property key or
    value, the value is redacted from the environment UI and various logs like YARN and event logs.
  
2.1.2


spark.redaction.string.regex
(none)

    Regex to decide which parts of strings produced by Spark contain sensitive information.
    When this regex matches a string part, that string part is replaced by a dummy value.
    This is currently used to redact the output of SQL explain commands.
  
2.2.0


spark.python.profile
false

    Enable profiling in Python worker, the profile result will show up by sc.show_profiles(),
    or it will be displayed before the driver exits. It also can be dumped into disk by
    sc.dump_profiles(path). If some of the profile results had been displayed manually,
    they will not be displayed automatically before driver exiting.

    By default the pyspark.profiler.BasicProfiler will be used, but this can be overridden by
    passing a profiler class in as a parameter to the SparkContext constructor.
  
1.2.0


spark.python.profile.dump
(none)

    The directory which is used to dump the profile result before driver exiting.
    The results will be dumped as separated file for each RDD. They can be loaded
    by pstats.Stats(). If this is specified, the profile result will not be displayed
    automatically.
  
1.2.0


spark.python.worker.memory
512m

    Amount of memory to use per python worker process during aggregation, in the same
    format as JVM memory strings with a size unit suffix ("k", "m", "g" or "t")
    (e.g. 512m, 2g).
    If the memory used during aggregation goes above this amount, it will spill the data into disks.
  
1.1.0


spark.python.worker.reuse
true

    Reuse Python worker or not. If yes, it will use a fixed number of Python workers,
    does not need to fork() a Python process for every task. It will be very useful
    if there is a large broadcast, then the broadcast will not need to be transferred
    from JVM to Python worker for every task.
  
1.2.0


spark.files


    Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed.
  
1.0.0


spark.submit.pyFiles


    Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed.
  
1.0.1


spark.jars


    Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.
  
0.9.0


spark.jars.packages


    Comma-separated list of Maven coordinates of jars to include on the driver and executor
    classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings
    is given artifacts will be resolved according to the configuration in the file, otherwise artifacts
    will be searched for in the local maven repo, then maven central and finally any additional remote
    repositories given by the command-line option --repositories. For more details, see
    Advanced Dependency Management.
  
1.5.0


spark.jars.excludes


    Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies
    provided in spark.jars.packages to avoid dependency conflicts.
  
1.5.0


spark.jars.ivy


    Path to specify the Ivy user directory, used for the local Ivy cache and package files from
    spark.jars.packages. This will override the Ivy property ivy.default.ivy.user.dir
    which defaults to ~/.ivy2.
  
1.3.0


spark.jars.ivySettings


    Path to an Ivy settings file to customize resolution of jars specified using spark.jars.packages
    instead of the built-in defaults, such as maven central. Additional repositories given by the command-line
    option --repositories or spark.jars.repositories will also be included.
    Useful for allowing Spark to resolve artifacts from behind a firewall e.g. via an in-house
    artifact server like Artifactory. Details on the settings file format can be
    found at Settings Files.
    Only paths with file:// scheme are supported. Paths without a scheme are assumed to have
    a file:// scheme.
    
    When running in YARN cluster mode, this file will also be localized to the remote driver for dependency
    resolution within SparkContext#addJar

2.2.0


spark.jars.repositories


    Comma-separated list of additional remote repositories to search for the maven coordinates
    given with --packages or spark.jars.packages.
  
2.3.0


spark.archives


    Comma-separated list of archives to be extracted into the working directory of each executor.
    .jar, .tar.gz, .tgz and .zip are supported. You can specify the directory name to unpack via
    adding # after the file name to unpack, for example, file.zip#directory.
    This configuration is experimental.
  
3.1.0


spark.pyspark.driver.python


    Python binary executable to use for PySpark in driver.
    (default is spark.pyspark.python)
  
2.1.0


spark.pyspark.python


    Python binary executable to use for PySpark in both driver and executors.
  
2.1.0


Shuffle Behavior

Property NameDefaultMeaningSince Version

spark.reducer.maxSizeInFlight
48m

    Maximum size of map outputs to fetch simultaneously from each reduce task, in MiB unless
    otherwise specified. Since each output requires us to create a buffer to receive it, this
    represents a fixed memory overhead per reduce task, so keep it small unless you have a
    large amount of memory.
  
1.4.0


spark.reducer.maxReqsInFlight
Int.MaxValue

    This configuration limits the number of remote requests to fetch blocks at any given point.
    When the number of hosts in the cluster increase, it might lead to very large number
    of inbound connections to one or more nodes, causing the workers to fail under load.
    By allowing it to limit the number of fetch requests, this scenario can be mitigated.
  
2.0.0


spark.reducer.maxBlocksInFlightPerAddress
Int.MaxValue

    This configuration limits the number of remote blocks being fetched per reduce task from a
    given host port. When a large number of blocks are being requested from a given address in a
    single fetch or simultaneously, this could crash the serving executor or Node Manager. This
    is especially useful to reduce the load on the Node Manager when external shuffle is enabled.
    You can mitigate this issue by setting it to a lower value.
  
2.2.1


spark.shuffle.compress
true

    Whether to compress map output files. Generally a good idea. Compression will use
    spark.io.compression.codec.
  
0.6.0


spark.shuffle.file.buffer
32k

    Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise
    specified. These buffers reduce the number of disk seeks and system calls made in creating
    intermediate shuffle files.
  
1.4.0


spark.shuffle.unsafe.file.output.buffer
32k

    The file system for this buffer size after each partition is written in unsafe shuffle writer.
    In KiB unless otherwise specified.
  
2.3.0


spark.shuffle.spill.diskWriteBufferSize
1024 * 1024

    The buffer size, in bytes, to use when writing the sorted records to an on-disk file.
  
2.3.0


spark.shuffle.io.maxRetries
3

    (Netty only) Fetches that fail due to IO-related exceptions are automatically retried if this is
    set to a non-zero value. This retry logic helps stabilize large shuffles in the face of long GC
    pauses or transient network connectivity issues.
  
1.2.0


spark.shuffle.io.numConnectionsPerPeer
1

    (Netty only) Connections between hosts are reused in order to reduce connection buildup for
    large clusters. For clusters with many hard disks and few hosts, this may result in insufficient
    concurrency to saturate all disks, and so users may consider increasing this value.
  
1.2.1


spark.shuffle.io.preferDirectBufs
true

    (Netty only) Off-heap buffers are used to reduce garbage collection during shuffle and cache
    block transfer. For environments where off-heap memory is tightly limited, users may wish to
    turn this off to force all allocations from Netty to be on-heap.
  
1.2.0


spark.shuffle.io.retryWait
5s

    (Netty only) How long to wait between retries of fetches. The maximum delay caused by retrying
    is 15 seconds by default, calculated as maxRetries * retryWait.
  
1.2.1


spark.shuffle.io.backLog
-1

    Length of the accept queue for the shuffle service. For large applications, this value may
    need to be increased, so that incoming connections are not dropped if the service cannot keep
    up with a large number of connections arriving in a short period of time. This needs to
    be configured wherever the shuffle service itself is running, which may be outside of the
    application (see spark.shuffle.service.enabled option below). If set below 1,
    will fallback to OS default defined by Netty's io.netty.util.NetUtil#SOMAXCONN.
  
1.1.1


spark.shuffle.io.connectionTimeout
value of spark.network.timeout

    Timeout for the established connections between shuffle servers and clients to be marked
    as idled and closed if there are still outstanding fetch requests but no traffic no the channel
    for at least `connectionTimeout`.
  
1.2.0


spark.shuffle.io.connectionCreationTimeout
value of spark.shuffle.io.connectionTimeout

    Timeout for establishing a connection between the shuffle servers and clients.
  
3.2.0


spark.shuffle.service.enabled
false

    Enables the external shuffle service. This service preserves the shuffle files written by
    executors e.g. so that executors can be safely removed, or so that shuffle fetches can continue in
    the event of executor failure. The external shuffle service must be set up in order to enable it. See
    dynamic allocation
    configuration and setup documentation for more information.
  
1.2.0


spark.shuffle.service.port
7337

    Port on which the external shuffle service will run.
  
1.2.0


spark.shuffle.service.name
spark_shuffle

    The configured name of the Spark shuffle service the client should communicate with.
    This must match the name used to configure the Shuffle within the YARN NodeManager configuration
    (yarn.nodemanager.aux-services). Only takes effect
    when spark.shuffle.service.enabled is set to true.
  
3.2.0


spark.shuffle.service.index.cache.size
100m

    Cache entries limited to the specified memory footprint, in bytes unless otherwise specified.
  
2.3.0


spark.shuffle.service.removeShuffle
false

    Whether to use the ExternalShuffleService for deleting shuffle blocks for
    deallocated executors when the shuffle is no longer needed. Without this enabled,
    shuffle data on executors that are deallocated will remain on disk until the
    application ends.
  
3.3.0


spark.shuffle.maxChunksBeingTransferred
Long.MAX_VALUE

    The max number of chunks allowed to be transferred at the same time on shuffle service.
    Note that new incoming connections will be closed when the max number is hit. The client will
    retry according to the shuffle retry configs (see spark.shuffle.io.maxRetries and
    spark.shuffle.io.retryWait), if those limits are reached the task will fail with
    fetch failure.
  
2.3.0


spark.shuffle.sort.bypassMergeThreshold
200

    (Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no
    map-side aggregation and there are at most this many reduce partitions.
  
1.1.1


spark.shuffle.sort.io.plugin.class
org.apache.spark.shuffle.sort.io.LocalDiskShuffleDataIO

    Name of the class to use for shuffle IO.
  
3.0.0


spark.shuffle.spill.compress
true

    Whether to compress data spilled during shuffles. Compression will use
    spark.io.compression.codec.
  
0.9.0


spark.shuffle.accurateBlockThreshold
100 * 1024 * 1024

    Threshold in bytes above which the size of shuffle blocks in HighlyCompressedMapStatus is
    accurately recorded. This helps to prevent OOM by avoiding underestimating shuffle
    block size when fetch shuffle blocks.
  
2.2.1


spark.shuffle.registration.timeout
5000

    Timeout in milliseconds for registration to the external shuffle service.
  
2.3.0


spark.shuffle.registration.maxAttempts
3

    When we fail to register to the external shuffle service, we will retry for maxAttempts times.
  
2.3.0


spark.shuffle.reduceLocality.enabled
true

    Whether to compute locality preferences for reduce tasks.
  
1.5.0


spark.shuffle.mapOutput.minSizeForBroadcast
512k

    The size at which we use Broadcast to send the map output statuses to the executors.
  
2.0.0


spark.shuffle.detectCorrupt
true

    Whether to detect any corruption in fetched blocks.
  
2.2.0


spark.shuffle.detectCorrupt.useExtraMemory
false

    If enabled, part of a compressed/encrypted stream will be de-compressed/de-crypted by using extra memory
    to detect early corruption. Any IOException thrown will cause the task to be retried once
    and if it fails again with same exception, then FetchFailedException will be thrown to retry previous stage.
  
3.0.0


spark.shuffle.useOldFetchProtocol
false

    Whether to use the old protocol while doing the shuffle block fetching. It is only enabled while we need the
    compatibility in the scenario of new Spark version job fetching shuffle blocks from old version external shuffle service.
  
3.0.0


spark.shuffle.readHostLocalDisk
true

    If enabled (and spark.shuffle.useOldFetchProtocol is disabled, shuffle blocks requested from those block managers
    which are running on the same host are read from the disk directly instead of being fetched as remote blocks over the network.
  
3.0.0


spark.files.io.connectionTimeout
value of spark.network.timeout

    Timeout for the established connections for fetching files in Spark RPC environments to be marked
    as idled and closed if there are still outstanding files being downloaded but no traffic no the channel
    for at least `connectionTimeout`.
  
1.6.0


spark.files.io.connectionCreationTimeout
value of spark.files.io.connectionTimeout

    Timeout for establishing a connection for fetching files in Spark RPC environments.
  
3.2.0


spark.shuffle.checksum.enabled
true

    Whether to calculate the checksum of shuffle data. If enabled, Spark will calculate the checksum values for each partition
    data within the map output file and store the values in a checksum file on the disk. When there's shuffle data corruption
    detected, Spark will try to diagnose the cause (e.g., network issue, disk issue, etc.) of the corruption by using the checksum file.
  
3.2.0


spark.shuffle.checksum.algorithm
ADLER32

    The algorithm is used to calculate the shuffle checksum. Currently, it only supports built-in algorithms of JDK, e.g., ADLER32, CRC32.
  
3.2.0


spark.shuffle.service.fetch.rdd.enabled
false

    Whether to use the ExternalShuffleService for fetching disk persisted RDD blocks.
    In case of dynamic allocation if this feature is enabled executors having only disk
    persisted blocks are considered idle after
    spark.dynamicAllocation.executorIdleTimeout and will be released accordingly.
  
3.0.0


spark.shuffle.service.db.enabled
true

    Whether to use db in ExternalShuffleService. Note that this only affects standalone mode.
  
3.0.0


spark.shuffle.service.db.backend
LEVELDB

    Specifies a disk-based store used in shuffle service local db. Setting as LEVELDB or ROCKSDB.
  
3.4.0


Spark UI

Property NameDefaultMeaningSince Version

spark.eventLog.logBlockUpdates.enabled
false

    Whether to log events for every block update, if spark.eventLog.enabled is true.
    *Warning*: This will increase the size of the event log considerably.
  
2.3.0


spark.eventLog.longForm.enabled
false

    If true, use the long form of call sites in the event log. Otherwise use the short form.
  
2.4.0


spark.eventLog.compress
false

    Whether to compress logged events, if spark.eventLog.enabled is true.
  
1.0.0


spark.eventLog.compression.codec
zstd

    The codec to compress logged events. By default, Spark provides four codecs:
    lz4, lzf, snappy, and zstd.
    You can also use fully qualified class names to specify the codec, e.g.
    org.apache.spark.io.LZ4CompressionCodec,
    org.apache.spark.io.LZFCompressionCodec,
    org.apache.spark.io.SnappyCompressionCodec,
    and org.apache.spark.io.ZStdCompressionCodec.
  
3.0.0


spark.eventLog.erasureCoding.enabled
false

    Whether to allow event logs to use erasure coding, or turn erasure coding off, regardless of
    filesystem defaults.  On HDFS, erasure coded files will not update as quickly as regular
    replicated files, so the application updates will take longer to appear in the History Server.
    Note that even if this is true, Spark will still not force the file to use erasure coding, it
    will simply use filesystem defaults.
  
3.0.0


spark.eventLog.dir
file:///tmp/spark-events

    Base directory in which Spark events are logged, if spark.eventLog.enabled is true.
    Within this base directory, Spark creates a sub-directory for each application, and logs the
    events specific to the application in this directory. Users may want to set this to
    a unified location like an HDFS directory so history files can be read by the history server.
  
1.0.0


spark.eventLog.enabled
false

    Whether to log Spark events, useful for reconstructing the Web UI after the application has
    finished.
  
1.0.0


spark.eventLog.overwrite
false

    Whether to overwrite any existing files.
  
1.0.0


spark.eventLog.buffer.kb
100k

    Buffer size to use when writing to output streams, in KiB unless otherwise specified.
  
1.0.0


spark.eventLog.rolling.enabled
false

    Whether rolling over event log files is enabled. If set to true, it cuts down each event
    log file to the configured size.
  
3.0.0


spark.eventLog.rolling.maxFileSize
128m

    When spark.eventLog.rolling.enabled=true, specifies the max size of event log file before it's rolled over.
  
3.0.0


spark.ui.dagGraph.retainedRootRDDs
Int.MaxValue

    How many DAG graph nodes the Spark UI and status APIs remember before garbage collecting.
  
2.1.0


spark.ui.enabled
true

    Whether to run the web UI for the Spark application.
  
1.1.1


spark.ui.store.path
None

    Local directory where to cache application information for live UI.
    By default this is not set, meaning all application information will be kept in memory.
  
3.4.0


spark.ui.killEnabled
true

    Allows jobs and stages to be killed from the web UI.
  
1.0.0


spark.ui.liveUpdate.period
100ms

    How often to update live entities. -1 means "never update" when replaying applications,
    meaning only the last write will happen. For live applications, this avoids a few
    operations that we can live without when rapidly processing incoming task events.
  
2.3.0


spark.ui.liveUpdate.minFlushPeriod
1s

    Minimum time elapsed before stale UI data is flushed. This avoids UI staleness when incoming
    task events are not fired frequently.
  
2.4.2


spark.ui.port
4040

    Port for your application's dashboard, which shows memory and workload data.
  
0.7.0


spark.ui.retainedJobs
1000

    How many jobs the Spark UI and status APIs remember before garbage collecting.
    This is a target maximum, and fewer elements may be retained in some circumstances.
  
1.2.0


spark.ui.retainedStages
1000

    How many stages the Spark UI and status APIs remember before garbage collecting.
    This is a target maximum, and fewer elements may be retained in some circumstances.
  
0.9.0


spark.ui.retainedTasks
100000

    How many tasks in one stage the Spark UI and status APIs remember before garbage collecting.
    This is a target maximum, and fewer elements may be retained in some circumstances.
  
2.0.1


spark.ui.reverseProxy
false

    Enable running Spark Master as reverse proxy for worker and application UIs. In this mode, Spark master will reverse proxy the worker and application UIs to enable access without requiring direct access to their hosts. Use it with caution, as worker and application UI will not be accessible directly, you will only be able to access them through spark master/proxy public URL. This setting affects all the workers and application UIs running in the cluster and must be set on all the workers, drivers and masters.
  
2.1.0


spark.ui.reverseProxyUrl


    If the Spark UI should be served through another front-end reverse proxy, this is the URL
    for accessing the Spark master UI through that reverse proxy.
    This is useful when running proxy for authentication e.g. an OAuth proxy. The URL may contain
    a path prefix, like http://mydomain.com/path/to/spark/, allowing you to serve the
    UI for multiple Spark clusters and other web applications through the same virtual host and
    port.
    Normally, this should be an absolute URL including scheme (http/https), host and port.
    It is possible to specify a relative URL starting with "/" here. In this case, all URLs
    generated by the Spark UI and Spark REST APIs will be server-relative links -- this will still
    work, as the entire Spark UI is served through the same host and port.
    The setting affects link generation in the Spark UI, but the front-end reverse proxy
    is responsible for
    
stripping a path prefix before forwarding the request,
rewriting redirects which point directly to the Spark master,
redirecting access from http://mydomain.com/path/to/spark to
      http://mydomain.com/path/to/spark/ (trailing slash after path prefix); otherwise
      relative links on the master page do not work correctly.

    This setting affects all the workers and application UIs running in the cluster and must be set
    identically on all the workers, drivers and masters. In is only effective when
    spark.ui.reverseProxy is turned on. This setting is not needed when the Spark
    master web UI is directly reachable.
    Note that the value of the setting can't contain the keyword `proxy` or `history` after split by "/". Spark UI relies on both keywords for getting REST API endpoints from URIs.
  
2.1.0


spark.ui.proxyRedirectUri


    Where to address redirects when Spark is running behind a proxy. This will make Spark
    modify redirect responses so they point to the proxy server, instead of the Spark UI's own
    address. This should be only the address of the server, without any prefix paths for the
    application; the prefix should be set either by the proxy server itself (by adding the
    X-Forwarded-Context request header), or by setting the proxy base in the Spark
    app's configuration.
  
3.0.0


spark.ui.showConsoleProgress
false

    Show the progress bar in the console. The progress bar shows the progress of stages
    that run for longer than 500ms. If multiple stages run at the same time, multiple
    progress bars will be displayed on the same line.
    
Note: In shell environment, the default value of spark.ui.showConsoleProgress is true.
  
1.2.1


spark.ui.custom.executor.log.url
(none)

    Specifies custom spark executor log URL for supporting external log service instead of using cluster
    managers' application log URLs in Spark UI. Spark will support some path variables via patterns
    which can vary on cluster manager. Please check the documentation for your cluster manager to
    see which patterns are supported, if any. 
    Please note that this configuration also replaces original log urls in event log,
    which will be also effective when accessing the application on history server. The new log urls must be
    permanent, otherwise you might have dead link for executor log urls.
    
    For now, only YARN and K8s cluster manager supports this configuration
  
3.0.0


spark.worker.ui.retainedExecutors
1000

    How many finished executors the Spark UI and status APIs remember before garbage collecting.
  
1.5.0


spark.worker.ui.retainedDrivers
1000

    How many finished drivers the Spark UI and status APIs remember before garbage collecting.
  
1.5.0


spark.sql.ui.retainedExecutions
1000

    How many finished executions the Spark UI and status APIs remember before garbage collecting.
  
1.5.0


spark.streaming.ui.retainedBatches
1000

    How many finished batches the Spark UI and status APIs remember before garbage collecting.
  
1.0.0


spark.ui.retainedDeadExecutors
100

    How many dead executors the Spark UI and status APIs remember before garbage collecting.
  
2.0.0


spark.ui.filters
None

    Comma separated list of filter class names to apply to the Spark Web UI. The filter should be a
    standard 
    javax servlet Filter.

    Filter parameters can also be specified in the configuration, by setting config entries
    of the form spark.<class name of filter>.param.<param name>=<value>
For example:
    spark.ui.filters=com.test.filter1
spark.com.test.filter1.param.name1=foo
spark.com.test.filter1.param.name2=bar

1.0.0


spark.ui.requestHeaderSize
8k

    The maximum allowed size for a HTTP request header, in bytes unless otherwise specified.
    This setting applies for the Spark History Server too.
  
2.2.3


spark.ui.timelineEnabled
true

    Whether to display event timeline data on UI pages.
  
3.4.0


spark.ui.timeline.executors.maximum
250

    The maximum number of executors shown in the event timeline.
  
3.2.0


spark.ui.timeline.jobs.maximum
500

    The maximum number of jobs shown in the event timeline.
  
3.2.0


spark.ui.timeline.stages.maximum
500

    The maximum number of stages shown in the event timeline.
  
3.2.0


spark.ui.timeline.tasks.maximum
1000

    The maximum number of tasks shown in the event timeline.
  
1.4.0


spark.appStatusStore.diskStoreDir
None

    Local directory where to store diagnostic information of SQL executions. This configuration is only for live UI.
  
3.4.0


Compression and Serialization

Property NameDefaultMeaningSince Version

spark.broadcast.compress
true

    Whether to compress broadcast variables before sending them. Generally a good idea.
    Compression will use spark.io.compression.codec.
  
0.6.0


spark.checkpoint.compress
false

    Whether to compress RDD checkpoints. Generally a good idea.
    Compression will use spark.io.compression.codec.
  
2.2.0


spark.io.compression.codec
lz4

    The codec used to compress internal data such as RDD partitions, event log, broadcast variables
    and shuffle outputs. By default, Spark provides four codecs: lz4, lzf,
    snappy, and zstd. You can also use fully qualified class names to specify the codec,
    e.g.
    org.apache.spark.io.LZ4CompressionCodec,
    org.apache.spark.io.LZFCompressionCodec,
    org.apache.spark.io.SnappyCompressionCodec,
    and org.apache.spark.io.ZStdCompressionCodec.
  
0.8.0


spark.io.compression.lz4.blockSize
32k

    Block size used in LZ4 compression, in the case when LZ4 compression codec
    is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used.
    Default unit is bytes, unless otherwise specified. This configuration only applies to
    `spark.io.compression.codec`.
  
1.4.0


spark.io.compression.snappy.blockSize
32k

    Block size in Snappy compression, in the case when Snappy compression codec is used.
    Lowering this block size will also lower shuffle memory usage when Snappy is used.
    Default unit is bytes, unless otherwise specified. This configuration only applies
    to `spark.io.compression.codec`.
  
1.4.0


spark.io.compression.zstd.level
1

    Compression level for Zstd compression codec. Increasing the compression level will result in better
    compression at the expense of more CPU and memory. This configuration only applies to
    `spark.io.compression.codec`.
  
2.3.0


spark.io.compression.zstd.bufferSize
32k

    Buffer size in bytes used in Zstd compression, in the case when Zstd compression codec
    is used. Lowering this size will lower the shuffle memory usage when Zstd is used, but it
    might increase the compression cost because of excessive JNI call overhead. This
    configuration only applies to `spark.io.compression.codec`.
  
2.3.0


spark.io.compression.zstd.bufferPool.enabled
true

    If true, enable buffer pool of ZSTD JNI library.
  
3.2.0


spark.kryo.classesToRegister
(none)

    If you use Kryo serialization, give a comma-separated list of custom class names to register
    with Kryo.
    See the tuning guide for more details.
  
1.2.0


spark.kryo.referenceTracking
true

    Whether to track references to the same object when serializing data with Kryo, which is
    necessary if your object graphs have loops and useful for efficiency if they contain multiple
    copies of the same object. Can be disabled to improve performance if you know this is not the
    case.
  
0.8.0


spark.kryo.registrationRequired
false

    Whether to require registration with Kryo. If set to 'true', Kryo will throw an exception
    if an unregistered class is serialized. If set to false (the default), Kryo will write
    unregistered class names along with each object. Writing class names can cause
    significant performance overhead, so enabling this option can enforce strictly that a
    user has not omitted classes from registration.
  
1.1.0


spark.kryo.registrator
(none)

    If you use Kryo serialization, give a comma-separated list of classes that register your custom classes with Kryo. This
    property is useful if you need to register your classes in a custom way, e.g. to specify a custom
    field serializer. Otherwise spark.kryo.classesToRegister is simpler. It should be
    set to classes that extend
    
KryoRegistrator.
    See the tuning guide for more details.
  
0.5.0


spark.kryo.unsafe
true

    Whether to use unsafe based Kryo serializer. Can be
    substantially faster by using Unsafe Based IO.
  
2.1.0


spark.kryoserializer.buffer.max
64m

    Maximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified.
    This must be larger than any object you attempt to serialize and must be less than 2048m.
    Increase this if you get a "buffer limit exceeded" exception inside Kryo.
  
1.4.0


spark.kryoserializer.buffer
64k

    Initial size of Kryo's serialization buffer, in KiB unless otherwise specified.
    Note that there will be one buffer per core on each worker. This buffer will grow up to
    spark.kryoserializer.buffer.max if needed.
  
1.4.0


spark.rdd.compress
false

    Whether to compress serialized RDD partitions (e.g. for
    StorageLevel.MEMORY_ONLY_SER in Java
    and Scala or StorageLevel.MEMORY_ONLY in Python).
    Can save substantial space at the cost of some extra CPU time.
    Compression will use spark.io.compression.codec.
  
0.6.0


spark.serializer

    org.apache.spark.serializer.JavaSerializer
  

    Class to use for serializing objects that will be sent over the network or need to be cached
    in serialized form. The default of Java serialization works with any Serializable Java object
    but is quite slow, so we recommend using
    org.apache.spark.serializer.KryoSerializer and configuring Kryo serialization
    when speed is necessary. Can be any subclass of
    
org.apache.spark.Serializer.
  
0.5.0


spark.serializer.objectStreamReset
100

    When serializing using org.apache.spark.serializer.JavaSerializer, the serializer caches
    objects to prevent writing redundant data, however that stops garbage collection of those
    objects. By calling 'reset' you flush that info from the serializer, and allow old
    objects to be collected. To turn off this periodic reset set it to -1.
    By default it will reset the serializer every 100 objects.
  
1.0.0


Memory Management

Property NameDefaultMeaningSince Version

spark.memory.fraction
0.6

    Fraction of (heap space - 300MB) used for execution and storage. The lower this is, the
    more frequently spills and cached data eviction occur. The purpose of this config is to set
    aside memory for internal metadata, user data structures, and imprecise size estimation
    in the case of sparse, unusually large records. Leaving this at the default value is
    recommended. For more detail, including important information about correctly tuning JVM
    garbage collection when increasing this value, see
    this description.
  
1.6.0


spark.memory.storageFraction
0.5

    Amount of storage memory immune to eviction, expressed as a fraction of the size of the
    region set aside by spark.memory.fraction. The higher this is, the less
    working memory may be available to execution and tasks may spill to disk more often.
    Leaving this at the default value is recommended. For more detail, see
    this description.
  
1.6.0


spark.memory.offHeap.enabled
false

    If true, Spark will attempt to use off-heap memory for certain operations. If off-heap memory
    use is enabled, then spark.memory.offHeap.size must be positive.
  
1.6.0


spark.memory.offHeap.size
0

    The absolute amount of memory which can be used for off-heap allocation, in bytes unless otherwise specified.
    This setting has no impact on heap memory usage, so if your executors' total memory consumption
    must fit within some hard limit then be sure to shrink your JVM heap size accordingly.
    This must be set to a positive value when spark.memory.offHeap.enabled=true.
  
1.6.0


spark.storage.unrollMemoryThreshold
1024 * 1024

    Initial memory to request before unrolling any block.
  
1.1.0


spark.storage.replication.proactive
true

    Enables proactive block replication for RDD blocks. Cached RDD block replicas lost due to
    executor failures are replenished if there are any existing available replicas. This tries
    to get the replication level of the block to the initial number.
  
2.2.0


spark.storage.localDiskByExecutors.cacheSize
1000

    The max number of executors for which the local dirs are stored. This size is both applied for the driver and
    both for the executors side to avoid having an unbounded store. This cache will be used to avoid the network
    in case of fetching disk persisted RDD blocks or shuffle blocks (when spark.shuffle.readHostLocalDisk is set) from the same host.
  
3.0.0


spark.cleaner.periodicGC.interval
30min

    Controls how often to trigger a garbage collection.
    This context cleaner triggers cleanups only when weak references are garbage collected.
    In long-running applications with large driver JVMs, where there is little memory pressure
    on the driver, this may happen very occasionally or not at all. Not cleaning at all may
    lead to executors running out of disk space after a while.
  
1.6.0


spark.cleaner.referenceTracking
true

    Enables or disables context cleaning.
  
1.0.0


spark.cleaner.referenceTracking.blocking
true

    Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by
    spark.cleaner.referenceTracking.blocking.shuffle Spark property).
  
1.0.0


spark.cleaner.referenceTracking.blocking.shuffle
false

    Controls whether the cleaning thread should block on shuffle cleanup tasks.
  
1.1.1


spark.cleaner.referenceTracking.cleanCheckpoints
false

    Controls whether to clean checkpoint files if the reference is out of scope.
  
1.4.0


Execution Behavior

Property NameDefaultMeaningSince Version

spark.broadcast.blockSize
4m

    Size of each piece of a block for TorrentBroadcastFactory, in KiB unless otherwise
    specified. Too large a value decreases parallelism during broadcast (makes it slower); however,
    if it is too small, BlockManager might take a performance hit.
  
0.5.0


spark.broadcast.checksum
true

    Whether to enable checksum for broadcast. If enabled, broadcasts will include a checksum, which can
    help detect corrupted blocks, at the cost of computing and sending a little more data. It's possible
    to disable it if the network has other mechanisms to guarantee data won't be corrupted during broadcast.
  
2.1.1


spark.broadcast.UDFCompressionThreshold
1 * 1024 * 1024

    The threshold at which user-defined functions (UDFs) and Python RDD commands are compressed by broadcast in bytes unless otherwise specified.
  
3.0.0


spark.executor.cores

    1 in YARN mode, all the available cores on the worker in
    standalone and Mesos coarse-grained modes.
  

    The number of cores to use on each executor.

    In standalone and Mesos coarse-grained modes, for more detail, see
    this description.
  
1.0.0


spark.default.parallelism

    For distributed shuffle operations like reduceByKey and join, the
    largest number of partitions in a parent RDD.  For operations like parallelize
    with no parent RDDs, it depends on the cluster manager:
    
Local mode: number of cores on the local machine
Mesos fine grained mode: 8
Others: total number of cores on all executor nodes or 2, whichever is larger



    Default number of partitions in RDDs returned by transformations like join,
    reduceByKey, and parallelize when not set by user.
  
0.5.0


spark.executor.heartbeatInterval
10s

    Interval between each executor's heartbeats to the driver.  Heartbeats let
    the driver know that the executor is still alive and update it with metrics for in-progress
    tasks. spark.executor.heartbeatInterval should be significantly less than
    spark.network.timeout
  
1.1.0


spark.files.fetchTimeout
60s

    Communication timeout to use when fetching files added through SparkContext.addFile() from
    the driver.
  
1.0.0


spark.files.useFetchCache
true

    If set to true (default), file fetching will use a local cache that is shared by executors
    that belong to the same application, which can improve task launching performance when
    running many executors on the same host. If set to false, these caching optimizations will
    be disabled and all executors will fetch their own copies of files. This optimization may be
    disabled in order to use Spark local directories that reside on NFS filesystems (see
    SPARK-6313 for more details).
  
1.2.2


spark.files.overwrite
false

    Whether to overwrite any files which exist at the startup. Users can not overwrite the files added by
    SparkContext.addFile or SparkContext.addJar before even if this option is set
    true.
  
1.0.0


spark.files.ignoreCorruptFiles
false

    Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted or
    non-existing files and contents that have been read will still be returned.
  
2.1.0


spark.files.ignoreMissingFiles
false

    Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and
    the contents that have been read will still be returned.
  
2.4.0


spark.files.maxPartitionBytes
134217728 (128 MiB)

    The maximum number of bytes to pack into a single partition when reading files.
  
2.1.0


spark.files.openCostInBytes
4194304 (4 MiB)

    The estimated cost to open a file, measured by the number of bytes could be scanned at the same
    time. This is used when putting multiple files into a partition. It is better to overestimate,
    then the partitions with small files will be faster than partitions with bigger files.
  
2.1.0


spark.hadoop.cloneConf
false

    If set to true, clones a new Hadoop Configuration object for each task.  This
    option should be enabled to work around Configuration thread-safety issues (see
    SPARK-2546 for more details).
    This is disabled by default in order to avoid unexpected performance regressions for jobs that
    are not affected by these issues.
  
1.0.3


spark.hadoop.validateOutputSpecs
true

    If set to true, validates the output specification (e.g. checking if the output directory already exists)
    used in saveAsHadoopFile and other variants. This can be disabled to silence exceptions due to pre-existing
    output directories. We recommend that users do not disable this except if trying to achieve compatibility
    with previous versions of Spark. Simply use Hadoop's FileSystem API to delete output directories by hand.
    This setting is ignored for jobs generated through Spark Streaming's StreamingContext, since data may
    need to be rewritten to pre-existing output directories during checkpoint recovery.
  
1.0.1


spark.storage.memoryMapThreshold
2m

    Size of a block above which Spark memory maps when reading a block from disk. Default unit is bytes,
    unless specified otherwise. This prevents Spark from memory mapping very small blocks. In general,
    memory mapping has high overhead for blocks close to or below the page size of the operating system.
  
0.9.2


spark.storage.decommission.enabled
false

    Whether to decommission the block manager when decommissioning executor.
  
3.1.0


spark.storage.decommission.shuffleBlocks.enabled
true

    Whether to transfer shuffle blocks during block manager decommissioning. Requires a migratable shuffle resolver
    (like sort based shuffle).
  
3.1.0


spark.storage.decommission.shuffleBlocks.maxThreads
8

    Maximum number of threads to use in migrating shuffle files.
  
3.1.0


spark.storage.decommission.rddBlocks.enabled
true

    Whether to transfer RDD blocks during block manager decommissioning.
  
3.1.0


spark.storage.decommission.fallbackStorage.path
(none)

    The location for fallback storage during block manager decommissioning. For example, s3a://spark-storage/.
    In case of empty, fallback storage is disabled. The storage should be managed by TTL because Spark will not clean it up.
  
3.1.0


spark.storage.decommission.fallbackStorage.cleanUp
false

    If true, Spark cleans up its fallback storage data during shutting down.
  
3.2.0


spark.storage.decommission.shuffleBlocks.maxDiskSize
(none)

    Maximum disk space to use to store shuffle blocks before rejecting remote shuffle blocks.
    Rejecting remote shuffle blocks means that an executor will not receive any shuffle migrations,
    and if there are no other executors available for migration then shuffle blocks will be lost unless
    spark.storage.decommission.fallbackStorage.path is configured.
  
3.2.0


spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version
1

    The file output committer algorithm version, valid algorithm version number: 1 or 2.
    Note that 2 may cause a correctness issue like MAPREDUCE-7282.
  
2.2.0


Executor Metrics

Property NameDefaultMeaningSince Version

spark.eventLog.logStageExecutorMetrics
false

    Whether to write per-stage peaks of executor metrics (for each executor) to the event log.
    
Note: The metrics are polled (collected) and sent in the executor heartbeat,
    and this is always done; this configuration is only to determine if aggregated metric peaks
    are written to the event log.
  
3.0.0


spark.executor.processTreeMetrics.enabled
false

    Whether to collect process tree metrics (from the /proc filesystem) when collecting
    executor metrics.
    
Note: The process tree metrics are collected only if the /proc filesystem
    exists.
  
3.0.0


spark.executor.metrics.pollingInterval
0

    How often to collect executor metrics (in milliseconds).
    
    If 0, the polling is done on executor heartbeats (thus at the heartbeat interval,
    specified by spark.executor.heartbeatInterval).
    If positive, the polling is done at this interval.
  
3.0.0


spark.eventLog.gcMetrics.youngGenerationGarbageCollectors
Copy,PS Scavenge,ParNew,G1 Young Generation

    Names of supported young generation garbage collector. A name usually is the return of GarbageCollectorMXBean.getName.
    The built-in young generation garbage collectors are Copy,PS Scavenge,ParNew,G1 Young Generation.
  
3.0.0


spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
MarkSweepCompact,PS MarkSweep,ConcurrentMarkSweep,G1 Old Generation

    Names of supported old generation garbage collector. A name usually is the return of GarbageCollectorMXBean.getName.
    The built-in old generation garbage collectors are MarkSweepCompact,PS MarkSweep,ConcurrentMarkSweep,G1 Old Generation.
  
3.0.0


spark.executor.metrics.fileSystemSchemes
file,hdfs

    The file system schemes to report in executor metrics.
  
3.1.0


Networking

Property NameDefaultMeaningSince Version

spark.rpc.message.maxSize
128

    Maximum message size (in MiB) to allow in "control plane" communication; generally only applies to map
    output size information sent between executors and the driver. Increase this if you are running
    jobs with many thousands of map and reduce tasks and see messages about the RPC message size.
  
2.0.0


spark.blockManager.port
(random)

    Port for all block managers to listen on. These exist on both the driver and the executors.
  
1.1.0


spark.driver.blockManager.port
(value of spark.blockManager.port)

    Driver-specific port for the block manager to listen on, for cases where it cannot use the same
    configuration as executors.
  
2.1.0


spark.driver.bindAddress
(value of spark.driver.host)

    Hostname or IP address where to bind listening sockets. This config overrides the SPARK_LOCAL_IP
    environment variable (see below).

    It also allows a different address from the local one to be advertised to executors or external systems.
    This is useful, for example, when running containers with bridged networking. For this to properly work,
    the different ports used by the driver (RPC, block manager and UI) need to be forwarded from the
    container's host.
  
2.1.0


spark.driver.host
(local hostname)

    Hostname or IP address for the driver.
    This is used for communicating with the executors and the standalone Master.
  
0.7.0


spark.driver.port
(random)

    Port for the driver to listen on.
    This is used for communicating with the executors and the standalone Master.
  
0.7.0


spark.rpc.io.backLog
64

    Length of the accept queue for the RPC server. For large applications, this value may
    need to be increased, so that incoming connections are not dropped when a large number of
    connections arrives in a short period of time.
  
3.0.0


spark.network.timeout
120s

    Default timeout for all network interactions. This config will be used in place of
    spark.storage.blockManagerHeartbeatTimeoutMs,
    spark.shuffle.io.connectionTimeout, spark.rpc.askTimeout or
    spark.rpc.lookupTimeout if they are not configured.
  
1.3.0


spark.network.timeoutInterval
60s

    Interval for the driver to check and expire dead executors.
  
1.3.2


spark.network.io.preferDirectBufs
true

    If enabled then off-heap buffer allocations are preferred by the shared allocators.
    Off-heap buffers are used to reduce garbage collection during shuffle and cache
    block transfer. For environments where off-heap memory is tightly limited, users may wish to
    turn this off to force all allocations to be on-heap.
  
3.0.0


spark.port.maxRetries
16

    Maximum number of retries when binding to a port before giving up.
    When a port is given a specific value (non 0), each subsequent retry will
    increment the port used in the previous attempt by 1 before retrying. This
    essentially allows it to try a range of ports from the start port specified
    to port + maxRetries.
  
1.1.1


spark.rpc.askTimeout
spark.network.timeout

    Duration for an RPC ask operation to wait before timing out.
  
1.4.0


spark.rpc.lookupTimeout
120s

    Duration for an RPC remote endpoint lookup operation to wait before timing out.
  
1.4.0


spark.network.maxRemoteBlockSizeFetchToMem
200m

    Remote block will be fetched to disk when size of the block is above this threshold
    in bytes. This is to avoid a giant request takes too much memory. Note this
    configuration will affect both shuffle fetch and block manager remote block fetch.
    For users who enabled external shuffle service, this feature can only work when
    external shuffle service is at least 2.3.0.
  
3.0.0


spark.rpc.io.connectionTimeout
value of spark.network.timeout

    Timeout for the established connections between RPC peers to be marked as idled and closed
    if there are outstanding RPC requests but no traffic on the channel for at least
    `connectionTimeout`.
  
1.2.0


spark.rpc.io.connectionCreationTimeout
value of spark.rpc.io.connectionTimeout

    Timeout for establishing a connection between RPC peers.
  
3.2.0


Scheduling

Property NameDefaultMeaningSince Version

spark.cores.max
(not set)

    When running on a standalone deploy cluster or a
    Mesos cluster in "coarse-grained"
    sharing mode, the maximum amount of CPU cores to request for the application from
    across the cluster (not from each machine). If not set, the default will be
    spark.deploy.defaultCores on Spark's standalone cluster manager, or
    infinite (all available cores) on Mesos.
  
0.6.0


spark.locality.wait
3s

    How long to wait to launch a data-local task before giving up and launching it
    on a less-local node. The same wait will be used to step through multiple locality levels
    (process-local, node-local, rack-local and then any). It is also possible to customize the
    waiting time for each level by setting spark.locality.wait.node, etc.
    You should increase this setting if your tasks are long and see poor locality, but the
    default usually works well.
  
0.5.0


spark.locality.wait.node
spark.locality.wait

    Customize the locality wait for node locality. For example, you can set this to 0 to skip
    node locality and search immediately for rack locality (if your cluster has rack information).
  
0.8.0


spark.locality.wait.process
spark.locality.wait

    Customize the locality wait for process locality. This affects tasks that attempt to access
    cached data in a particular executor process.
  
0.8.0


spark.locality.wait.rack
spark.locality.wait

    Customize the locality wait for rack locality.
  
0.8.0


spark.scheduler.maxRegisteredResourcesWaitingTime
30s

    Maximum amount of time to wait for resources to register before scheduling begins.
  
1.1.1


spark.scheduler.minRegisteredResourcesRatio
0.8 for KUBERNETES mode; 0.8 for YARN mode; 0.0 for standalone mode and Mesos coarse-grained mode

    The minimum ratio of registered resources (registered resources / total expected resources)
    (resources are executors in yarn mode and Kubernetes mode, CPU cores in standalone mode and Mesos coarse-grained
     mode ['spark.cores.max' value is total expected resources for Mesos coarse-grained mode] )
    to wait for before scheduling begins. Specified as a double between 0.0 and 1.0.
    Regardless of whether the minimum ratio of resources has been reached,
    the maximum amount of time it will wait before scheduling begins is controlled by config
    spark.scheduler.maxRegisteredResourcesWaitingTime.
  
1.1.1


spark.scheduler.mode
FIFO

    The scheduling mode between
    jobs submitted to the same SparkContext. Can be set to FAIR
    to use fair sharing instead of queueing jobs one after another. Useful for
    multi-user services.
  
0.8.0


spark.scheduler.revive.interval
1s

    The interval length for the scheduler to revive the worker resource offers to run tasks.
  
0.8.1


spark.scheduler.listenerbus.eventqueue.capacity
10000

    The default capacity for event queues. Spark will try to initialize an event queue
    using capacity specified by `spark.scheduler.listenerbus.eventqueue.queueName.capacity`
    first. If it's not configured, Spark will use the default capacity specified by this
    config. Note that capacity must be greater than 0. Consider increasing value (e.g. 20000)
    if listener events are dropped. Increasing this value may result in the driver using more memory.
  
2.3.0


spark.scheduler.listenerbus.eventqueue.shared.capacity
spark.scheduler.listenerbus.eventqueue.capacity

    Capacity for shared event queue in Spark listener bus, which hold events for external listener(s)
    that register to the listener bus. Consider increasing value, if the listener events corresponding
    to shared queue are dropped. Increasing this value may result in the driver using more memory.
  
3.0.0


spark.scheduler.listenerbus.eventqueue.appStatus.capacity
spark.scheduler.listenerbus.eventqueue.capacity

    Capacity for appStatus event queue, which hold events for internal application status listeners.
    Consider increasing value, if the listener events corresponding to appStatus queue are dropped.
    Increasing this value may result in the driver using more memory.
  
3.0.0


spark.scheduler.listenerbus.eventqueue.executorManagement.capacity
spark.scheduler.listenerbus.eventqueue.capacity

    Capacity for executorManagement event queue in Spark listener bus, which hold events for internal
    executor management listeners. Consider increasing value if the listener events corresponding to
    executorManagement queue are dropped. Increasing this value may result in the driver using more memory.
  
3.0.0


spark.scheduler.listenerbus.eventqueue.eventLog.capacity
spark.scheduler.listenerbus.eventqueue.capacity

    Capacity for eventLog queue in Spark listener bus, which hold events for Event logging listeners
    that write events to eventLogs. Consider increasing value if the listener events corresponding to eventLog queue
    are dropped. Increasing this value may result in the driver using more memory.
  
3.0.0


spark.scheduler.listenerbus.eventqueue.streams.capacity
spark.scheduler.listenerbus.eventqueue.capacity

    Capacity for streams queue in Spark listener bus, which hold events for internal streaming listener.
    Consider increasing value if the listener events corresponding to streams queue are dropped. Increasing
    this value may result in the driver using more memory.
  
3.0.0


spark.scheduler.resource.profileMergeConflicts
false

    If set to "true", Spark will merge ResourceProfiles when different profiles are specified
    in RDDs that get combined into a single stage. When they are merged, Spark chooses the maximum of
    each resource and creates a new ResourceProfile. The default of false results in Spark throwing
    an exception if multiple different ResourceProfiles are found in RDDs going into the same stage.
  
3.1.0


spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout
120s

    The timeout in seconds to wait to acquire a new executor and schedule a task before aborting a
    TaskSet which is unschedulable because all executors are excluded due to task failures.
  
2.4.1


spark.standalone.submit.waitAppCompletion
false

    If set to true, Spark will merge ResourceProfiles when different profiles are specified in RDDs that get combined into a single stage.
    When they are merged, Spark chooses the maximum of each resource and creates a new ResourceProfile.
    The default of false results in Spark throwing an exception if multiple different ResourceProfiles are found in RDDs going into the same stage.
  
3.1.0


spark.excludeOnFailure.enabled

    false
  

    If set to "true", prevent Spark from scheduling tasks on executors that have been excluded
    due to too many task failures. The algorithm used to exclude executors and nodes can be further
    controlled by the other "spark.excludeOnFailure" configuration options.
  
2.1.0


spark.excludeOnFailure.timeout
1h

    (Experimental) How long a node or executor is excluded for the entire application, before it
    is unconditionally removed from the excludelist to attempt running new tasks.
  
2.1.0


spark.excludeOnFailure.task.maxTaskAttemptsPerExecutor
1

    (Experimental) For a given task, how many times it can be retried on one executor before the
    executor is excluded for that task.
  
2.1.0


spark.excludeOnFailure.task.maxTaskAttemptsPerNode
2

    (Experimental) For a given task, how many times it can be retried on one node, before the entire
    node is excluded for that task.
  
2.1.0


spark.excludeOnFailure.stage.maxFailedTasksPerExecutor
2

    (Experimental) How many different tasks must fail on one executor, within one stage, before the
    executor is excluded for that stage.
  
2.1.0


spark.excludeOnFailure.stage.maxFailedExecutorsPerNode
2

    (Experimental) How many different executors are marked as excluded for a given stage, before
    the entire node is marked as failed for the stage.
  
2.1.0


spark.excludeOnFailure.application.maxFailedTasksPerExecutor
2

    (Experimental) How many different tasks must fail on one executor, in successful task sets,
    before the executor is excluded for the entire application.  Excluded executors will
    be automatically added back to the pool of available resources after the timeout specified by
    spark.excludeOnFailure.timeout.  Note that with dynamic allocation, though, the executors
    may get marked as idle and be reclaimed by the cluster manager.
  
2.2.0


spark.excludeOnFailure.application.maxFailedExecutorsPerNode
2

    (Experimental) How many different executors must be excluded for the entire application,
    before the node is excluded for the entire application.  Excluded nodes will
    be automatically added back to the pool of available resources after the timeout specified by
    spark.excludeOnFailure.timeout.  Note that with dynamic allocation, though, the
    executors on the node may get marked as idle and be reclaimed by the cluster manager.
  
2.2.0


spark.excludeOnFailure.killExcludedExecutors
false

    (Experimental) If set to "true", allow Spark to automatically kill the executors
    when they are excluded on fetch failure or excluded for the entire application,
    as controlled by spark.killExcludedExecutors.application.*. Note that, when an entire node is added
    excluded, all of the executors on that node will be killed.
  
2.2.0


spark.excludeOnFailure.application.fetchFailure.enabled
false

    (Experimental) If set to "true", Spark will exclude the executor immediately when a fetch
    failure happens. If external shuffle service is enabled, then the whole node will be
    excluded.
  
2.3.0


spark.speculation
false

    If set to "true", performs speculative execution of tasks. This means if one or more tasks are
    running slowly in a stage, they will be re-launched.
  
0.6.0


spark.speculation.interval
100ms

    How often Spark will check for tasks to speculate.
  
0.6.0


spark.speculation.multiplier
1.5

    How many times slower a task is than the median to be considered for speculation.
  
0.6.0


spark.speculation.quantile
0.75

    Fraction of tasks which must be complete before speculation is enabled for a particular stage.
  
0.6.0


spark.speculation.minTaskRuntime
100ms

    Minimum amount of time a task runs before being considered for speculation.
    This can be used to avoid launching speculative copies of tasks that are very short.
  
3.2.0


spark.speculation.task.duration.threshold
None

    Task duration after which scheduler would try to speculative run the task. If provided, tasks
    would be speculatively run if current stage contains less tasks than or equal to the number of
    slots on a single executor and the task is taking longer time than the threshold. This config
    helps speculate stage with very few tasks. Regular speculation configs may also apply if the
    executor slots are large enough. E.g. tasks might be re-launched if there are enough successful
    runs even though the threshold hasn't been reached. The number of slots is computed based on
    the conf values of spark.executor.cores and spark.task.cpus minimum 1.
    Default unit is bytes, unless otherwise specified.
  
3.0.0


spark.speculation.efficiency.processRateMultiplier
0.75

    A multiplier that used when evaluating inefficient tasks. The higher the multiplier
    is, the more tasks will be possibly considered as inefficient.
  
3.4.0


spark.speculation.efficiency.longRunTaskFactor
2

    A task will be speculated anyway as long as its duration has exceeded the value of multiplying
    the factor and the time threshold (either be spark.speculation.multiplier
    * successfulTaskDurations.median or spark.speculation.minTaskRuntime) regardless
    of it's data process rate is good or not. This avoids missing the inefficient tasks when task
    slow isn't related to data process rate.
  
3.4.0


spark.speculation.efficiency.enabled
true

    When set to true, spark will evaluate the efficiency of task processing through the stage task
    metrics or its duration, and only need to speculate the inefficient tasks. A task is inefficient
    when 1)its data process rate is less than the average data process rate of all successful tasks
    in the stage multiplied by a multiplier or 2)its duration has exceeded the value of multiplying
     spark.speculation.efficiency.longRunTaskFactor and the time threshold (either be
     spark.speculation.multiplier * successfulTaskDurations.median or
    spark.speculation.minTaskRuntime).
  
3.4.0


spark.task.cpus
1

    Number of cores to allocate for each task.
  
0.5.0


spark.task.resource.{resourceName}.amount
1

    Amount of a particular resource type to allocate for each task, note that this can be a double.
    If this is specified you must also provide the executor config
    spark.executor.resource.{resourceName}.amount and any corresponding discovery configs
    so that your executors are created with that resource type. In addition to whole amounts,
    a fractional amount (for example, 0.25, which means 1/4th of a resource) may be specified.
    Fractional amounts must be less than or equal to 0.5, or in other words, the minimum amount of
    resource sharing is 2 tasks per resource. Additionally, fractional amounts are floored
    in order to assign resource slots (e.g. a 0.2222 configuration, or 1/0.2222 slots will become
    4 tasks/resource, not 5).
  
3.0.0


spark.task.maxFailures
4

    Number of continuous failures of any particular task before giving up on the job.
    The total number of failures spread across different tasks will not cause the job
    to fail; a particular task has to fail this number of attempts continuously.
    If any attempt succeeds, the failure count for the task will be reset.
    Should be greater than or equal to 1. Number of allowed retries = this value - 1.
  
0.8.0


spark.task.reaper.enabled
false

    Enables monitoring of killed / interrupted tasks. When set to true, any task which is killed
    will be monitored by the executor until that task actually finishes executing. See the other
    spark.task.reaper.* configurations for details on how to control the exact behavior
    of this monitoring. When set to false (the default), task killing will use an older code
    path which lacks such monitoring.
  
2.0.3


spark.task.reaper.pollingInterval
10s

    When spark.task.reaper.enabled = true, this setting controls the frequency at which
    executors will poll the status of killed tasks. If a killed task is still running when polled
    then a warning will be logged and, by default, a thread-dump of the task will be logged
    (this thread dump can be disabled via the spark.task.reaper.threadDump setting,
    which is documented below).
  
2.0.3


spark.task.reaper.threadDump
true

    When spark.task.reaper.enabled = true, this setting controls whether task thread
    dumps are logged during periodic polling of killed tasks. Set this to false to disable
    collection of thread dumps.
  
2.0.3


spark.task.reaper.killTimeout
-1

    When spark.task.reaper.enabled = true, this setting specifies a timeout after
    which the executor JVM will kill itself if a killed task has not stopped running. The default
    value, -1, disables this mechanism and prevents the executor from self-destructing. The purpose
    of this setting is to act as a safety-net to prevent runaway noncancellable tasks from rendering
    an executor unusable.
  
2.0.3


spark.stage.maxConsecutiveAttempts
4

    Number of consecutive stage attempts allowed before a stage is aborted.
  
2.2.0


spark.stage.ignoreDecommissionFetchFailure
false

    Whether ignore stage fetch failure caused by executor decommission when
    count spark.stage.maxConsecutiveAttempts

3.4.0


Barrier Execution Mode

Property NameDefaultMeaningSince Version

spark.barrier.sync.timeout
365d

    The timeout in seconds for each barrier() call from a barrier task. If the
    coordinator didn't receive all the sync messages from barrier tasks within the
    configured time, throw a SparkException to fail all the tasks. The default value is set
    to 31536000(3600 * 24 * 365) so the barrier() call shall wait for one year.
  
2.4.0


spark.scheduler.barrier.maxConcurrentTasksCheck.interval
15s

    Time in seconds to wait between a max concurrent tasks check failure and the next
    check. A max concurrent tasks check ensures the cluster can launch more concurrent
    tasks than required by a barrier stage on job submitted. The check can fail in case
    a cluster has just started and not enough executors have registered, so we wait for a
    little while and try to perform the check again. If the check fails more than a
    configured max failure times for a job then fail current job submission. Note this
    config only applies to jobs that contain one or more barrier stages, we won't perform
    the check on non-barrier jobs.
  
2.4.0


spark.scheduler.barrier.maxConcurrentTasksCheck.maxFailures
40

    Number of max concurrent tasks check failures allowed before fail a job submission.
    A max concurrent tasks check ensures the cluster can launch more concurrent tasks than
    required by a barrier stage on job submitted. The check can fail in case a cluster
    has just started and not enough executors have registered, so we wait for a little
    while and try to perform the check again. If the check fails more than a configured
    max failure times for a job then fail current job submission. Note this config only
    applies to jobs that contain one or more barrier stages, we won't perform the check on
    non-barrier jobs.
  
2.4.0


Dynamic Allocation

Property NameDefaultMeaningSince Version

spark.dynamicAllocation.enabled
false

    Whether to use dynamic resource allocation, which scales the number of executors registered
    with this application up and down based on the workload.
    For more detail, see the description
    here.
    
    This requires one of the following conditions: 
    1) enabling external shuffle service through spark.shuffle.service.enabled, or
    2) enabling shuffle tracking through spark.dynamicAllocation.shuffleTracking.enabled, or
    3) enabling shuffle blocks decommission through spark.decommission.enabled and spark.storage.decommission.shuffleBlocks.enabled, or
    4) (Experimental) configuring spark.shuffle.sort.io.plugin.class to use a custom ShuffleDataIO who's ShuffleDriverComponents supports reliable storage.
    The following configurations are also relevant:
    spark.dynamicAllocation.minExecutors,
    spark.dynamicAllocation.maxExecutors, and
    spark.dynamicAllocation.initialExecutors
spark.dynamicAllocation.executorAllocationRatio

1.2.0


spark.dynamicAllocation.executorIdleTimeout
60s

    If dynamic allocation is enabled and an executor has been idle for more than this duration,
    the executor will be removed. For more detail, see this
    description.
  
1.2.0


spark.dynamicAllocation.cachedExecutorIdleTimeout
infinity

    If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration,
    the executor will be removed. For more details, see this
    description.
  
1.4.0


spark.dynamicAllocation.initialExecutors
spark.dynamicAllocation.minExecutors

    Initial number of executors to run if dynamic allocation is enabled.
    
    If `--num-executors` (or `spark.executor.instances`) is set and larger than this value, it will
    be used as the initial number of executors.
  
1.3.0


spark.dynamicAllocation.maxExecutors
infinity

    Upper bound for the number of executors if dynamic allocation is enabled.
  
1.2.0


spark.dynamicAllocation.minExecutors
0

    Lower bound for the number of executors if dynamic allocation is enabled.
  
1.2.0


spark.dynamicAllocation.executorAllocationRatio
1

    By default, the dynamic allocation will request enough executors to maximize the
    parallelism according to the number of tasks to process. While this minimizes the
    latency of the job, with small tasks this setting can waste a lot of resources due to
    executor allocation overhead, as some executor might not even do any work.
    This setting allows to set a ratio that will be used to reduce the number of
    executors w.r.t. full parallelism.
    Defaults to 1.0 to give maximum parallelism.
    0.5 will divide the target number of executors by 2
    The target number of executors computed by the dynamicAllocation can still be overridden
    by the spark.dynamicAllocation.minExecutors and
    spark.dynamicAllocation.maxExecutors settings
  
2.4.0


spark.dynamicAllocation.schedulerBacklogTimeout
1s

    If dynamic allocation is enabled and there have been pending tasks backlogged for more than
    this duration, new executors will be requested. For more detail, see this
    description.
  
1.2.0


spark.dynamicAllocation.sustainedSchedulerBacklogTimeout
schedulerBacklogTimeout

    Same as spark.dynamicAllocation.schedulerBacklogTimeout, but used only for
    subsequent executor requests. For more detail, see this
    description.
  
1.2.0


spark.dynamicAllocation.shuffleTracking.enabled
true

    Enables shuffle file tracking for executors, which allows dynamic allocation
    without the need for an external shuffle service. This option will try to keep alive executors
    that are storing shuffle data for active jobs.
  
3.0.0


spark.dynamicAllocation.shuffleTracking.timeout
infinity

    When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle
    data. The default value means that Spark will rely on the shuffles being garbage collected to be
    able to release executors. If for some reason garbage collection is not cleaning up shuffles
    quickly enough, this option can be used to control when to time out executors even when they are
    storing shuffle data.
  
3.0.0


Thread Configurations
Depending on jobs and cluster configurations, we can set number of threads in several places in Spark to utilize
available resources efficiently to get better performance. Prior to Spark 3.0, these thread configurations apply
to all roles of Spark, such as driver, executor, worker and master. From Spark 3.0, we can configure threads in
finer granularity starting from driver and executor. Take RPC module as example in below table. For other modules,
like shuffle, just replace “rpc” with “shuffle” in the property names except
spark.{driver|executor}.rpc.netty.dispatcher.numThreads, which is only for RPC module.

Property NameDefaultMeaningSince Version

spark.{driver|executor}.rpc.io.serverThreads

    Fall back on spark.rpc.io.serverThreads

Number of threads used in the server thread pool
1.6.0


spark.{driver|executor}.rpc.io.clientThreads

    Fall back on spark.rpc.io.clientThreads

Number of threads used in the client thread pool
1.6.0


spark.{driver|executor}.rpc.netty.dispatcher.numThreads

    Fall back on spark.rpc.netty.dispatcher.numThreads

Number of threads used in RPC message dispatcher thread pool
3.0.0


The default value for number of thread-related config keys is the minimum of the number of cores requested for
the driver or executor, or, in the absence of that value, the number of cores available for the JVM (with a hardcoded upper limit of 8).
Spark Connect
Server Configuration
Server configurations are set in Spark Connect server, for example, when you start the Spark Connect server with ./sbin/start-connect-server.sh.
They are typically set via the config file and command-line options with --conf/-c.

Property NameDefaultMeaningSince Version

spark.connect.grpc.binding.port

    15002
  
Port for Spark Connect server to bind.
3.4.0


spark.connect.grpc.interceptor.classes

    (none)
  
Comma separated list of class names that must implement the io.grpc.ServerInterceptor interface
3.4.0


spark.connect.grpc.arrow.maxBatchSize

    4m
  
When using Apache Arrow, limit the maximum size of one arrow batch that can be sent from server side to client side. Currently, we conservatively use 70% of it because the size is not accurate but estimated.
3.4.0


spark.connect.grpc.maxInboundMessageSize

    134217728
  
Sets the maximum inbound message size for the gRPC requests. Requests with a larger payload will fail.
3.4.0


spark.connect.extensions.relation.classes

    (none)
  
Comma separated list of classes that implement the trait org.apache.spark.sql.connect.plugin.RelationPlugin to support custom
Relation types in proto.
3.4.0


spark.connect.extensions.expression.classes

    (none)
  
Comma separated list of classes that implement the trait
org.apache.spark.sql.connect.plugin.ExpressionPlugin to support custom
Expression types in proto.
3.4.0


spark.connect.extensions.command.classes

    (none)
  
Comma separated list of classes that implement the trait
org.apache.spark.sql.connect.plugin.CommandPlugin to support custom
Command types in proto.
3.4.0


Security
Please refer to the Security page for available options on how to secure different
Spark subsystems.
Spark SQL
Runtime SQL Configuration
Runtime SQL configurations are per-session, mutable Spark SQL configurations. They can be set with initial values by the config file
and command-line options with --conf/-c prefixed, or by setting SparkConf that are used to create SparkSession.
Also, they can be set and queried by SET commands and rest to their initial values by RESET command,
or by SparkSession.conf’s setter and getter methods in runtime.

Property NameDefaultMeaningSince Version

spark.sql.adaptive.advisoryPartitionSizeInBytes
(value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize)
The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.
3.0.0


spark.sql.adaptive.autoBroadcastJoinThreshold
(none)
Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. The default value is same with spark.sql.autoBroadcastJoinThreshold. Note that, this config is used only in adaptive framework.
3.2.0


spark.sql.adaptive.coalescePartitions.enabled
true
When true and 'spark.sql.adaptive.enabled' is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid too many small tasks.
3.0.0


spark.sql.adaptive.coalescePartitions.initialPartitionNum
(none)
The initial number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when 'spark.sql.adaptive.enabled' and 'spark.sql.adaptive.coalescePartitions.enabled' are both true.
3.0.0


spark.sql.adaptive.coalescePartitions.minPartitionSize
1MB
The minimum size of shuffle partitions after coalescing. This is useful when the adaptively calculated target size is too small during partition coalescing.
3.2.0


spark.sql.adaptive.coalescePartitions.parallelismFirst
true
When true, Spark does not respect the target size specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes' (default 64MB) when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark cluster. The calculated size is usually smaller than the configured target size. This is to maximize the parallelism and avoid performance regression when enabling adaptive query execution. It's recommended to set this config to false and respect the configured target size.
3.2.0


spark.sql.adaptive.customCostEvaluatorClass
(none)
The custom cost evaluator class to be used for adaptive execution. If not being set, Spark will use its own SimpleCostEvaluator by default.
3.2.0


spark.sql.adaptive.enabled
true
When true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.
1.6.0


spark.sql.adaptive.forceOptimizeSkewedJoin
false
When true, force enable OptimizeSkewedJoin even if it introduces extra shuffle.
3.3.0


spark.sql.adaptive.localShuffleReader.enabled
true
When true and 'spark.sql.adaptive.enabled' is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.
3.0.0


spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold
0b
Configures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than spark.sql.adaptive.advisoryPartitionSizeInBytes and all the partition size are not larger than this config, join selection prefer to use shuffled hash join instead of sort merge join regardless of the value of spark.sql.join.preferSortMergeJoin.
3.2.0


spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled
true
When true and 'spark.sql.adaptive.enabled' is true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by 'spark.sql.adaptive.advisoryPartitionSizeInBytes'), to avoid data skew.
3.2.0


spark.sql.adaptive.optimizer.excludedRules
(none)
Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.
3.1.0


spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor
0.2
A partition will be merged during splitting if its size is small than this factor multiply spark.sql.adaptive.advisoryPartitionSizeInBytes.
3.3.0


spark.sql.adaptive.skewJoin.enabled
true
When true and 'spark.sql.adaptive.enabled' is true, Spark dynamically handles skew in shuffled join (sort-merge and shuffled hash) by splitting (and replicating if needed) skewed partitions.
3.0.0


spark.sql.adaptive.skewJoin.skewedPartitionFactor
5.0
A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes'
3.0.0


spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes
256MB
A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than 'spark.sql.adaptive.skewJoin.skewedPartitionFactor' multiplying the median partition size. Ideally this config should be set larger than 'spark.sql.adaptive.advisoryPartitionSizeInBytes'.
3.0.0


spark.sql.allowNamedFunctionArguments
true
If true, Spark will turn on support for named parameters for all functions that has it implemented.
3.5.0


spark.sql.ansi.doubleQuotedIdentifiers
false
When true and 'spark.sql.ansi.enabled' is true, Spark SQL reads literals enclosed in double quoted (") as identifiers. When false they are read as string literals.
3.4.0


spark.sql.ansi.enabled
false
When true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results when the inputs to a SQL operator/function are invalid.For full details of this dialect, you can find them in the section "ANSI Compliance" of Spark's documentation. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL's style
3.0.0


spark.sql.ansi.enforceReservedKeywords
false
When true and 'spark.sql.ansi.enabled' is true, the Spark SQL parser enforces the ANSI reserved keywords and forbids SQL queries that use reserved keywords as alias names and/or identifiers for table, view, function, etc.
3.3.0


spark.sql.ansi.relationPrecedence
false
When true and 'spark.sql.ansi.enabled' is true, JOIN takes precedence over comma when combining relation. For example, t1, t2 JOIN t3 should result to t1 X (t2 X t3). If the config is false, the result is (t1 X t2) X t3.
3.4.0


spark.sql.autoBroadcastJoinThreshold
10MB
Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join.  By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan has been run, and file-based data source tables where the statistics are computed directly on the files of data.
1.1.0


spark.sql.avro.compression.codec
snappy
Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2, xz and zstandard. Default codec is snappy.
2.4.0


spark.sql.avro.deflate.level
-1
Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.
2.4.0


spark.sql.avro.filterPushdown.enabled
true
When true, enable filter pushdown to Avro datasource.
3.1.0


spark.sql.broadcastTimeout
300
Timeout in seconds for the broadcast wait time in broadcast joins.
1.3.0


spark.sql.bucketing.coalesceBucketsInJoin.enabled
false
When true, if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.
3.1.0


spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio
4
The ratio of the number of two buckets being coalesced should be less than or equal to this value for bucket coalescing to be applied. This configuration only has an effect when 'spark.sql.bucketing.coalesceBucketsInJoin.enabled' is set to true.
3.1.0


spark.sql.catalog.spark_catalog
(none)
A catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: spark_catalog. This catalog shares its identifier namespace with the spark_catalog and must be consistent with it; for example, if a table can be loaded by the spark_catalog, this catalog must also return the table metadata. To delegate operations to the spark_catalog, implementations can extend 'CatalogExtension'.
3.0.0


spark.sql.cbo.enabled
false
Enables CBO for estimation of plan statistics when set true.
2.2.0


spark.sql.cbo.joinReorder.dp.star.filter
false
Applies star-join filter heuristics to cost based join enumeration.
2.2.0


spark.sql.cbo.joinReorder.dp.threshold
12
The maximum number of joined nodes allowed in the dynamic programming algorithm.
2.2.0


spark.sql.cbo.joinReorder.enabled
false
Enables join reorder in CBO.
2.2.0


spark.sql.cbo.planStats.enabled
false
When true, the logical plan will fetch row counts and column statistics from catalog.
3.0.0


spark.sql.cbo.starSchemaDetection
false
When true, it enables join reordering based on star schema detection. 
2.2.0


spark.sql.charAsVarchar
false
When true, Spark replaces CHAR type with VARCHAR type in CREATE/REPLACE/ALTER TABLE commands, so that newly created/updated tables will not have CHAR type columns/fields. Existing tables with CHAR type columns/fields are not affected by this config.
3.3.0


spark.sql.chunkBase64String.enabled
true
Whether to truncate string generated by the Base64 function. When true, base64 strings generated by the base64 function are chunked into lines of at most 76 characters. When false, the base64 strings are not chunked.
3.5.2


spark.sql.cli.print.header
false
When set to true, spark-sql CLI prints the names of the columns in query output.
3.2.0


spark.sql.columnNameOfCorruptRecord
_corrupt_record
The name of internal column for storing raw/un-parsed JSON and CSV records that fail to parse.
1.2.0


spark.sql.csv.filterPushdown.enabled
true
When true, enable filter pushdown to CSV datasource.
3.0.0


spark.sql.datetime.java8API.enabled
false
If the configuration property is set to true, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp and java.sql.Date are used for the same purpose.
3.0.0


spark.sql.debug.maxToStringFields
25
Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a "... N more fields" placeholder.
3.0.0


spark.sql.defaultCatalog
spark_catalog
Name of the default catalog. This will be the current catalog if users have not explicitly set the current catalog yet.
3.0.0


spark.sql.error.messageFormat
PRETTY
When PRETTY, the error message consists of textual representation of error class, message and query context. The MINIMAL and STANDARD formats are pretty JSON formats where STANDARD includes an additional JSON field message. This configuration property influences on error messages of Thrift Server and SQL CLI while running queries.
3.4.0


spark.sql.execution.arrow.enabled
false
(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.enabled'.)
2.3.0


spark.sql.execution.arrow.fallback.enabled
true
(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.fallback.enabled'.)
2.4.0


spark.sql.execution.arrow.localRelationThreshold
48MB
When converting Arrow batches to Spark DataFrame, local collections are used in the driver side if the byte size of Arrow batches is smaller than this threshold. Otherwise, the Arrow batches are sent and deserialized to Spark internal rows in the executors.
3.4.0


spark.sql.execution.arrow.maxRecordsPerBatch
10000
When using Apache Arrow, limit the maximum number of records that can be written to a single ArrowRecordBatch in memory. This configuration is not effective for the grouping API such as DataFrame(.cogroup).groupby.applyInPandas because each group becomes each ArrowRecordBatch. If set to zero or negative there is no limit.
2.3.0


spark.sql.execution.arrow.pyspark.enabled
(value of spark.sql.execution.arrow.enabled)
When true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to: 1. pyspark.sql.DataFrame.toPandas. 2. pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame or a NumPy ndarray. The following data type is unsupported: ArrayType of TimestampType.
3.0.0


spark.sql.execution.arrow.pyspark.fallback.enabled
(value of spark.sql.execution.arrow.fallback.enabled)
When true, optimizations enabled by 'spark.sql.execution.arrow.pyspark.enabled' will fallback automatically to non-optimized implementations if an error occurs.
3.0.0


spark.sql.execution.arrow.pyspark.selfDestruct.enabled
false
(Experimental) When true, make use of Apache Arrow's self-destruct and split-blocks options for columnar data transfers in PySpark, when converting from Arrow to Pandas. This reduces memory usage at the cost of some CPU time. This optimization applies to: pyspark.sql.DataFrame.toPandas when 'spark.sql.execution.arrow.pyspark.enabled' is set.
3.2.0


spark.sql.execution.arrow.sparkr.enabled
false
When true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to: 1. createDataFrame when its input is an R DataFrame 2. collect 3. dapply 4. gapply The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.
3.0.0


spark.sql.execution.pandas.structHandlingMode
legacy
The conversion mode of struct type when creating pandas DataFrame. When "legacy",1. when Arrow optimization is disabled, convert to Row object, 2. when Arrow optimization is enabled, convert to dict or raise an Exception if there are duplicated nested field names. When "row", convert to Row object regardless of Arrow optimization. When "dict", convert to dict and use suffixed key names, e.g., a_0, a_1, if there are duplicated nested field names, regardless of Arrow optimization.
3.5.0


spark.sql.execution.pandas.udf.buffer.size
(value of spark.buffer.size)
Same as spark.buffer.size but only applies to Pandas UDF executions. If it is not set, the fallback is spark.buffer.size. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.
3.0.0


spark.sql.execution.pyspark.udf.simplifiedTraceback.enabled
true
When true, the traceback from Python UDFs is simplified. It hides the Python worker, (de)serialization, etc from PySpark in tracebacks, and only shows the exception messages from UDFs. Note that this works only with CPython 3.7+.
3.1.0


spark.sql.execution.pythonUDF.arrow.enabled
false
Enable Arrow optimization in regular Python UDFs. This optimization can only be enabled when the given function takes at least one argument.
3.4.0


spark.sql.execution.pythonUDTF.arrow.enabled
false
Enable Arrow optimization for Python UDTFs.
3.5.0


spark.sql.execution.topKSortFallbackThreshold
2147483632
In SQL queries with a SORT followed by a LIMIT like 'SELECT x FROM t ORDER BY y LIMIT m', if m is under this threshold, do a top-K sort in memory, otherwise do a global sort which spills to disk if necessary.
2.4.0


spark.sql.files.ignoreCorruptFiles
false
Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
2.1.1


spark.sql.files.ignoreMissingFiles
false
Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
2.3.0


spark.sql.files.maxPartitionBytes
128MB
The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
2.0.0


spark.sql.files.maxPartitionNum
(none)
The suggested (not guaranteed) maximum number of split file partitions. If it is set, Spark will rescale each partition to make the number of partitions is close to this value if the initial number of partitions exceeds this value. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
3.5.0


spark.sql.files.maxRecordsPerFile
0
Maximum number of records to write out to a single file. If this value is zero or negative, there is no limit.
2.2.0


spark.sql.files.minPartitionNum
(none)
The suggested (not guaranteed) minimum number of split file partitions. If not set, the default value is spark.sql.leafNodeDefaultParallelism. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
3.1.0


spark.sql.function.concatBinaryAsString
false
When this option is set to false and all inputs are binary, functions.concat returns an output as binary. Otherwise, it returns as a string.
2.3.0


spark.sql.function.eltOutputAsString
false
When this option is set to false and all inputs are binary, elt returns an output as binary. Otherwise, it returns as a string.
2.3.0


spark.sql.groupByAliases
true
When true, aliases in a select list can be used in group by clauses. When false, an analysis exception is thrown in the case.
2.2.0


spark.sql.groupByOrdinal
true
When true, the ordinal numbers in group by clauses are treated as the position in the select list. When false, the ordinal numbers are ignored.
2.0.0


spark.sql.hive.convertInsertingPartitionedTable
true
When set to true, and spark.sql.hive.convertMetastoreParquet or spark.sql.hive.convertMetastoreOrc is true, the built-in ORC/Parquet writer is usedto process inserting into partitioned ORC/Parquet tables created by using the HiveSQL syntax.
3.0.0


spark.sql.hive.convertMetastoreCtas
true
When set to true,  Spark will try to use built-in data source writer instead of Hive serde in CTAS. This flag is effective only if spark.sql.hive.convertMetastoreParquet or spark.sql.hive.convertMetastoreOrc is enabled respectively for Parquet and ORC formats
3.0.0


spark.sql.hive.convertMetastoreInsertDir
true
When set to true,  Spark will try to use built-in data source writer instead of Hive serde in INSERT OVERWRITE DIRECTORY. This flag is effective only if spark.sql.hive.convertMetastoreParquet or spark.sql.hive.convertMetastoreOrc is enabled respectively for Parquet and ORC formats
3.3.0


spark.sql.hive.convertMetastoreOrc
true
When set to true, the built-in ORC reader and writer are used to process ORC tables created by using the HiveQL syntax, instead of Hive serde.
2.0.0


spark.sql.hive.convertMetastoreParquet
true
When set to true, the built-in Parquet reader and writer are used to process parquet tables created by using the HiveQL syntax, instead of Hive serde.
1.1.1


spark.sql.hive.convertMetastoreParquet.mergeSchema
false
When true, also tries to merge possibly different but compatible Parquet schemas in different Parquet data files. This configuration is only effective when "spark.sql.hive.convertMetastoreParquet" is true.
1.3.1


spark.sql.hive.dropPartitionByName.enabled
false
When true, Spark will get partition name rather than partition object to drop partition, which can improve the performance of drop partition.
3.4.0


spark.sql.hive.filesourcePartitionFileCacheSize
262144000
When nonzero, enable caching of partition file metadata in memory. All tables share a cache that can use up to specified num bytes for file metadata. This conf only has an effect when hive filesource partition management is enabled.
2.1.1


spark.sql.hive.manageFilesourcePartitions
true
When true, enable metastore partition management for file source tables as well. This includes both datasource and converted Hive tables. When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning when spark.sql.hive.metastorePartitionPruning is set to true.
2.1.1


spark.sql.hive.metastorePartitionPruning
true
When true, some predicates will be pushed down into the Hive metastore so that unmatching partitions can be eliminated earlier.
1.5.0


spark.sql.hive.metastorePartitionPruningFallbackOnException
false
Whether to fallback to get all partitions from Hive metastore and perform partition pruning on Spark client side, when encountering MetaException from the metastore. Note that Spark query performance may degrade if this is enabled and there are many partitions to be listed. If this is disabled, Spark will fail the query instead.
3.3.0


spark.sql.hive.metastorePartitionPruningFastFallback
false
When this config is enabled, if the predicates are not supported by Hive or Spark does fallback due to encountering MetaException from the metastore, Spark will instead prune partitions by getting the partition names first and then evaluating the filter expressions on the client side. Note that the predicates with TimeZoneAwareExpression is not supported.
3.3.0


spark.sql.hive.thriftServer.async
true
When set to true, Hive Thrift server executes SQL queries in an asynchronous way.
1.5.0


spark.sql.hive.verifyPartitionPath
false
When true, check all the partition paths under the table's root directory when reading data stored in HDFS. This configuration will be deprecated in the future releases and replaced by spark.files.ignoreMissingFiles.
1.4.0


spark.sql.inMemoryColumnarStorage.batchSize
10000
Controls the size of batches for columnar caching.  Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.
1.1.1


spark.sql.inMemoryColumnarStorage.compressed
true
When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.
1.0.1


spark.sql.inMemoryColumnarStorage.enableVectorizedReader
true
Enables vectorized reader for columnar caching.
2.3.1


spark.sql.json.filterPushdown.enabled
true
When true, enable filter pushdown to JSON datasource.
3.1.0


spark.sql.jsonGenerator.ignoreNullFields
true
Whether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.
3.0.0


spark.sql.leafNodeDefaultParallelism
(none)
The default parallelism of Spark SQL leaf nodes that produce data, such as the file scan node, the local data scan node, the range node, etc. The default value of this config is 'SparkContext#defaultParallelism'.
3.2.0


spark.sql.mapKeyDedupPolicy
EXCEPTION
The policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.
3.0.0


spark.sql.maven.additionalRemoteRepositories
https://maven-central.storage-download.googleapis.com/maven2/
A comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.
3.0.0


spark.sql.maxMetadataStringLength
100
Maximum number of characters to output for a metadata string. e.g. file location in DataSourceScanExec, every value will be abbreviated if exceed length.
3.1.0


spark.sql.maxPlanStringLength
2147483632
Maximum number of characters to output for a plan string.  If the plan is longer, further output will be truncated.  The default setting always generates a full plan.  Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.
3.0.0


spark.sql.maxSinglePartitionBytes
9223372036854775807b
The maximum number of bytes allowed for a single partition. Otherwise, The planner will introduce shuffle to improve parallelism.
3.4.0


spark.sql.optimizer.collapseProjectAlwaysInline
false
Whether to always collapse two adjacent projections and inline expressions even if it causes extra duplication.
3.3.0


spark.sql.optimizer.dynamicPartitionPruning.enabled
true
When true, we will generate predicate for partition column when it's used as join key
3.0.0


spark.sql.optimizer.enableCsvExpressionOptimization
true
Whether to optimize CSV expressions in SQL optimizer. It includes pruning unnecessary columns from from_csv.
3.2.0


spark.sql.optimizer.enableJsonExpressionOptimization
true
Whether to optimize JSON expressions in SQL optimizer. It includes pruning unnecessary columns from from_json, simplifying from_json + to_json, to_json + named_struct(from_json.col1, from_json.col2, ....).
3.1.0


spark.sql.optimizer.excludedRules
(none)
Configures a list of rules to be disabled in the optimizer, in which the rules are specified by their rule names and separated by comma. It is not guaranteed that all the rules in this configuration will eventually be excluded, as some rules are necessary for correctness. The optimizer will log the rules that have indeed been excluded.
2.4.0


spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold
10GB
Byte size threshold of the Bloom filter application side plan's aggregated scan size. Aggregated scan byte size of the Bloom filter application side needs to be over this value to inject a bloom filter.
3.3.0


spark.sql.optimizer.runtime.bloomFilter.creationSideThreshold
10MB
Size threshold of the bloom filter creation side plan. Estimated size needs to be under this value to try to inject bloom filter.
3.3.0


spark.sql.optimizer.runtime.bloomFilter.enabled
true
When true and if one side of a shuffle join has a selective predicate, we attempt to insert a bloom filter in the other side to reduce the amount of shuffle data.
3.3.0


spark.sql.optimizer.runtime.bloomFilter.expectedNumItems
1000000
The default number of expected items for the runtime bloomfilter
3.3.0


spark.sql.optimizer.runtime.bloomFilter.maxNumBits
67108864
The max number of bits to use for the runtime bloom filter
3.3.0


spark.sql.optimizer.runtime.bloomFilter.maxNumItems
4000000
The max allowed number of expected items for the runtime bloom filter
3.3.0


spark.sql.optimizer.runtime.bloomFilter.numBits
8388608
The default number of bits to use for the runtime bloom filter
3.3.0


spark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled
true
Enables runtime group filtering for group-based row-level operations. Data sources that replace groups of data (e.g. files, partitions) may prune entire groups using provided data source filters when planning a row-level operation scan. However, such filtering is limited as not all expressions can be converted into data source filters and some expressions can only be evaluated by Spark (e.g. subqueries). Since rewriting groups is expensive, Spark can execute a query at runtime to find what records match the condition of the row-level operation. The information about matching records will be passed back to the row-level operation scan, allowing data sources to discard groups that don't have to be rewritten.
3.4.0


spark.sql.optimizer.runtimeFilter.number.threshold
10
The total number of injected runtime filters (non-DPP) for a single query. This is to prevent driver OOMs with too many Bloom filters.
3.3.0


spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled
false
When true and if one side of a shuffle join has a selective predicate, we attempt to insert a semi join in the other side to reduce the amount of shuffle data.
3.3.0


spark.sql.orc.aggregatePushdown
false
If true, aggregates will be pushed down to ORC for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any ORC file footer, exception would be thrown.
3.3.0


spark.sql.orc.columnarReaderBatchSize
4096
The number of rows to include in a orc vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.
2.4.0


spark.sql.orc.columnarWriterBatchSize
1024
The number of rows to include in a orc vectorized writer batch. The number should be carefully chosen to minimize overhead and avoid OOMs in writing data.
3.4.0


spark.sql.orc.compression.codec
snappy
Sets the compression codec used when writing ORC files. If either compression or orc.compress is specified in the table-specific options/properties, the precedence would be compression, orc.compress, spark.sql.orc.compression.codec.Acceptable values include: none, uncompressed, snappy, zlib, lzo, zstd, lz4.
2.3.0


spark.sql.orc.enableNestedColumnVectorizedReader
true
Enables vectorized orc decoding for nested column.
3.2.0


spark.sql.orc.enableVectorizedReader
true
Enables vectorized orc decoding.
2.3.0


spark.sql.orc.filterPushdown
true
When true, enable filter pushdown for ORC files.
1.4.0


spark.sql.orc.mergeSchema
false
When true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.
3.0.0


spark.sql.orderByOrdinal
true
When true, the ordinal numbers are treated as the position in the select list. When false, the ordinal numbers in order/sort by clause are ignored.
2.0.0


spark.sql.parquet.aggregatePushdown
false
If true, aggregates will be pushed down to Parquet for optimization. Support MIN, MAX and COUNT as aggregate expression. For MIN/MAX, support boolean, integer, float and date type. For COUNT, support all data types. If statistics is missing from any Parquet file footer, exception would be thrown.
3.3.0


spark.sql.parquet.binaryAsString
false
Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.
1.1.1


spark.sql.parquet.columnarReaderBatchSize
4096
The number of rows to include in a parquet vectorized reader batch. The number should be carefully chosen to minimize overhead and avoid OOMs in reading data.
2.4.0


spark.sql.parquet.compression.codec
snappy
Sets the compression codec used when writing Parquet files. If either compression or parquet.compression is specified in the table-specific options/properties, the precedence would be compression, parquet.compression, spark.sql.parquet.compression.codec. Acceptable values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4raw, lz4_raw, zstd.
1.1.1


spark.sql.parquet.enableNestedColumnVectorizedReader
true
Enables vectorized Parquet decoding for nested columns (e.g., struct, list, map). Requires spark.sql.parquet.enableVectorizedReader to be enabled.
3.3.0


spark.sql.parquet.enableVectorizedReader
true
Enables vectorized parquet decoding.
2.0.0


spark.sql.parquet.fieldId.read.enabled
false
Field ID is a native field of the Parquet schema spec. When enabled, Parquet readers will use field IDs (if present) in the requested Spark schema to look up Parquet fields instead of using column names
3.3.0


spark.sql.parquet.fieldId.read.ignoreMissing
false
When the Parquet file doesn't have any field IDs but the Spark read schema is using field IDs to read, we will silently return nulls when this flag is enabled, or error otherwise.
3.3.0


spark.sql.parquet.fieldId.write.enabled
true
Field ID is a native field of the Parquet schema spec. When enabled, Parquet writers will populate the field Id metadata (if present) in the Spark schema to the Parquet schema.
3.3.0


spark.sql.parquet.filterPushdown
true
Enables Parquet filter push-down optimization when set to true.
1.2.0


spark.sql.parquet.inferTimestampNTZ.enabled
true
When enabled, Parquet timestamp columns with annotation isAdjustedToUTC = false are inferred as TIMESTAMP_NTZ type during schema inference. Otherwise, all the Parquet timestamp columns are inferred as TIMESTAMP_LTZ types. Note that Spark writes the output schema into Parquet's footer metadata on file writing and leverages it on file reading. Thus this configuration only affects the schema inference on Parquet files which are not written by Spark.
3.4.0


spark.sql.parquet.int96AsTimestamp
true
Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.
1.3.0


spark.sql.parquet.int96TimestampConversion
false
This controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.  This is necessary because Impala stores INT96 data with a different timezone offset than Hive & Spark.
2.3.0


spark.sql.parquet.mergeSchema
false
When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.
1.5.0


spark.sql.parquet.outputTimestampType
INT96
Sets which Parquet timestamp type to use when Spark writes data to Parquet files. INT96 is a non-standard but commonly used timestamp type in Parquet. TIMESTAMP_MICROS is a standard timestamp type in Parquet, which stores number of microseconds from the Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which means Spark has to truncate the microsecond portion of its timestamp value.
2.3.0


spark.sql.parquet.recordLevelFilter.enabled
false
If true, enables Parquet's native record-level filtering using the pushed down filters. This configuration only has an effect when 'spark.sql.parquet.filterPushdown' is enabled and the vectorized reader is not used. You can ensure the vectorized reader is not used by setting 'spark.sql.parquet.enableVectorizedReader' to false.
2.3.0


spark.sql.parquet.respectSummaryFiles
false
When true, we make assumption that all part-files of Parquet are consistent with summary files and we will ignore them when merging schema. Otherwise, if this is false, which is the default, we will merge all part-files. This should be considered as expert-only option, and shouldn't be enabled before knowing what it means exactly.
1.5.0


spark.sql.parquet.writeLegacyFormat
false
If true, data will be written in a way of Spark 1.4 and earlier. For example, decimal values will be written in Apache Parquet's fixed-length byte array format, which other systems such as Apache Hive and Apache Impala use. If false, the newer format in Parquet will be used. For example, decimals will be written in int-based format. If Parquet output is intended for use with systems that do not support this newer format, set to true.
1.6.0


spark.sql.parser.quotedRegexColumnNames
false
When true, quoted Identifiers (using backticks) in SELECT statement are interpreted as regular expressions.
2.3.0


spark.sql.pivotMaxValues
10000
When doing a pivot without specifying values for the pivot column this is the maximum number of (distinct) values that will be collected without error.
1.6.0


spark.sql.pyspark.inferNestedDictAsStruct.enabled
false
PySpark's SparkSession.createDataFrame infers the nested dict as a map by default. When it set to true, it infers the nested dict as a struct.
3.3.0


spark.sql.pyspark.jvmStacktrace.enabled
false
When true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled to hide JVM stacktrace and shows a Python-friendly exception only. Note that this is independent from log level settings.
3.0.0


spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled
false
PySpark's SparkSession.createDataFrame infers the element type of an array from all values in the array by default. If this config is set to true, it restores the legacy behavior of only inferring the type from the first array element.
3.4.0


spark.sql.readSideCharPadding
true
When true, Spark applies string padding when reading CHAR type columns/fields, in addition to the write-side padding. This config is true by default to better enforce CHAR type semantic in cases such as external tables.
3.4.0


spark.sql.redaction.options.regex
(?i)url
Regex to decide which keys in a Spark SQL command's options map contain sensitive information. The values of options whose names that match this regex will be redacted in the explain output. This redaction is applied on top of the global redaction configuration defined by spark.redaction.regex.
2.2.2


spark.sql.redaction.string.regex
(value of spark.redaction.string.regex)
Regex to decide which parts of strings produced by Spark contain sensitive information. When this regex matches a string part, that string part is replaced by a dummy value. This is currently used to redact the output of SQL explain commands. When this conf is not set, the value from spark.redaction.string.regex is used.
2.3.0


spark.sql.repl.eagerEval.enabled
false
Enables eager evaluation or not. When true, the top K rows of Dataset will be displayed if and only if the REPL supports the eager evaluation. Currently, the eager evaluation is supported in PySpark and SparkR. In PySpark, for the notebooks like Jupyter, the HTML table (generated by repr_html) will be returned. For plain Python REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned outputs are showed similar to R data.frame would.
2.4.0


spark.sql.repl.eagerEval.maxNumRows
20
The max number of rows that are returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true. The valid range of this config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).
2.4.0


spark.sql.repl.eagerEval.truncate
20
The max number of characters for each cell that is returned by eager evaluation. This only takes effect when spark.sql.repl.eagerEval.enabled is set to true.
2.4.0


spark.sql.session.localRelationCacheThreshold
67108864
The threshold for the size in bytes of local relations to be cached at the driver side after serialization.
3.5.0


spark.sql.session.timeZone
(value of local timezone)
The ID of session local timezone in the format of either region-based zone IDs or zone offsets. Region IDs must have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in the format '(+|-)HH', '(+|-)HH:mm' or '(+|-)HH:mm:ss', e.g '-08', '+01:00' or '-13:33:33'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'. Other short names are not recommended to use because they can be ambiguous.
2.2.0


spark.sql.shuffle.partitions
200
The default number of partitions to use when shuffling data for joins or aggregations. Note: For structured streaming, this configuration cannot be changed between query restarts from the same checkpoint location.
1.1.0


spark.sql.shuffledHashJoinFactor
3
The shuffle hash join can be selected if the data size of small side multiplied by this factor is still smaller than the large side.
3.3.0


spark.sql.sources.bucketing.autoBucketedScan.enabled
true
When true, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan. Note when 'spark.sql.sources.bucketing.enabled' is set to false, this configuration does not take any effect.
3.1.0


spark.sql.sources.bucketing.enabled
true
When false, we will treat bucketed table as normal table
2.0.0


spark.sql.sources.bucketing.maxBuckets
100000
The maximum number of buckets allowed.
2.4.0


spark.sql.sources.default
parquet
The default data source to use in input/output.
1.3.0


spark.sql.sources.parallelPartitionDiscovery.threshold
32
The maximum number of paths allowed for listing files at driver side. If the number of detected paths exceeds this value during partition discovery, it tries to list the files with another Spark distributed job. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
1.5.0


spark.sql.sources.partitionColumnTypeInference.enabled
true
When true, automatically infer the data types for partitioned columns.
1.5.0


spark.sql.sources.partitionOverwriteMode
STATIC
When INSERT OVERWRITE a partitioned data source table, we currently support 2 modes: static and dynamic. In static mode, Spark deletes all the partitions that match the partition specification(e.g. PARTITION(a=1,b)) in the INSERT statement, before overwriting. In dynamic mode, Spark doesn't delete partitions ahead, and only overwrite those partitions that have data written into it at runtime. By default we use static mode to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode. This can also be set as an output option for a data source using key partitionOverwriteMode (which takes precedence over this setting), e.g. dataframe.write.option("partitionOverwriteMode", "dynamic").save(path).
2.3.0


spark.sql.sources.v2.bucketing.enabled
false
Similar to spark.sql.sources.bucketing.enabled, this config is used to enable bucketing for V2 data sources. When turned on, Spark will recognize the specific distribution reported by a V2 data source through SupportsReportPartitioning, and will try to avoid shuffle if necessary.
3.3.0


spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled
false
During a storage-partitioned join, whether to allow input partitions to be partially clustered, when both sides of the join are of KeyGroupedPartitioning. At planning time, Spark will pick the side with less data size based on table statistics, group and replicate them to match the other side. This is an optimization on skew join and can help to reduce data skewness when certain partitions are assigned large amount of data. This config requires both spark.sql.sources.v2.bucketing.enabled and spark.sql.sources.v2.bucketing.pushPartValues.enabled to be enabled
3.4.0


spark.sql.sources.v2.bucketing.pushPartValues.enabled
false
Whether to pushdown common partition values when spark.sql.sources.v2.bucketing.enabled is enabled. When turned on, if both sides of a join are of KeyGroupedPartitioning and if they share compatible partition keys, even if they don't have the exact same partition values, Spark will calculate a superset of partition values and pushdown that info to scan nodes, which will use empty partitions for the missing partition values on either side. This could help to eliminate unnecessary shuffles
3.4.0


spark.sql.statistics.fallBackToHdfs
false
When true, it will fall back to HDFS if the table statistics are not available from table metadata. This is useful in determining if a table is small enough to use broadcast joins. This flag is effective only for non-partitioned Hive tables. For non-partitioned data source tables, it will be automatically recalculated if table statistics are not available. For partitioned data source and partitioned Hive tables, It is 'spark.sql.defaultSizeInBytes' if table statistics are not available.
2.0.0


spark.sql.statistics.histogram.enabled
false
Generates histograms when computing column statistics if enabled. Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.
2.3.0


spark.sql.statistics.size.autoUpdate.enabled
false
Enables automatic update for table size once table's data is changed. Note that if the total number of files of the table is very large, this can be expensive and slow down data change commands.
2.3.0


spark.sql.storeAssignmentPolicy
ANSI
When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting string to int or double to boolean. With legacy policy, Spark allows the type coercion as long as it is a valid Cast, which is very loose. e.g. converting string to int or double to boolean is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting double to int or decimal to double is not allowed.
3.0.0


spark.sql.streaming.checkpointLocation
(none)
The default location for storing checkpoint data for streaming queries.
2.0.0


spark.sql.streaming.continuous.epochBacklogQueueSize
10000
The max number of entries to be stored in queue to wait for late epochs. If this parameter is exceeded by the size of the queue, stream will stop with an error.
3.0.0


spark.sql.streaming.disabledV2Writers

A comma-separated list of fully qualified data source register class names for which StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.
2.3.1


spark.sql.streaming.fileSource.cleaner.numThreads
1
Number of threads used in the file source completed file cleaner.
3.0.0


spark.sql.streaming.forceDeleteTempCheckpointLocation
false
When true, enable temporary checkpoint locations force delete.
3.0.0


spark.sql.streaming.metricsEnabled
false
Whether Dropwizard/Codahale metrics will be reported for active streaming queries.
2.0.2


spark.sql.streaming.multipleWatermarkPolicy
min
Policy to calculate the global watermark value when there are multiple watermark operators in a streaming query. The default value is 'min' which chooses the minimum watermark reported across multiple operators. Other alternative value is 'max' which chooses the maximum across multiple operators. Note: This configuration cannot be changed between query restarts from the same checkpoint location.
2.4.0


spark.sql.streaming.noDataMicroBatches.enabled
true
Whether streaming micro-batch engine will execute batches without data for eager state management for stateful streaming queries.
2.4.1


spark.sql.streaming.numRecentProgressUpdates
100
The number of progress updates to retain for a streaming query
2.1.1


spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition
false
When true, streaming session window sorts and merge sessions in local partition prior to shuffle. This is to reduce the rows to shuffle, but only beneficial when there're lots of rows in a batch being assigned to same sessions.
3.2.0


spark.sql.streaming.stateStore.stateSchemaCheck
true
When true, Spark will validate the state schema against schema on existing state and fail query if it's incompatible.
3.1.0


spark.sql.streaming.stopActiveRunOnRestart
true
Running multiple runs of the same streaming query concurrently is not supported. If we find a concurrent active run for a streaming query (in the same or different SparkSessions on the same cluster) and this flag is true, we will stop the old streaming query run to start the new one.
3.0.0


spark.sql.streaming.stopTimeout
0
How long to wait in milliseconds for the streaming execution thread to stop when calling the streaming query's stop() method. 0 or negative values wait indefinitely.
3.0.0


spark.sql.thriftServer.interruptOnCancel
true
When true, all running tasks will be interrupted if one cancels a query. When false, all running tasks will remain until finished.
3.2.0


spark.sql.thriftServer.queryTimeout
0ms
Set a query duration timeout in seconds in Thrift Server. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. If timeout values are set for each statement via java.sql.Statement.setQueryTimeout and they are smaller than this configuration value, they take precedence. If you set this timeout and prefer to cancel the queries right away without waiting task to finish, consider enabling spark.sql.thriftServer.interruptOnCancel together.
3.1.0


spark.sql.thriftserver.scheduler.pool
(none)
Set a Fair Scheduler pool for a JDBC client session.
1.1.1


spark.sql.thriftserver.ui.retainedSessions
200
The number of SQL client sessions kept in the JDBC/ODBC web UI history.
1.4.0


spark.sql.thriftserver.ui.retainedStatements
200
The number of SQL statements kept in the JDBC/ODBC web UI history.
1.4.0


spark.sql.timestampType
TIMESTAMP_LTZ
Configures the default timestamp type of Spark SQL, including SQL DDL, Cast clause, type literal and the schema inference of data sources. Setting the configuration as TIMESTAMP_NTZ will use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as TIMESTAMP_LTZ will use TIMESTAMP WITH LOCAL TIME ZONE. Before the 3.4.0 release, Spark only supports the TIMESTAMP WITH LOCAL TIME ZONE type.
3.4.0


spark.sql.tvf.allowMultipleTableArguments.enabled
false
When true, allows multiple table arguments for table-valued functions, receiving the cartesian product of all the rows of these tables.
3.5.0


spark.sql.ui.explainMode
formatted
Configures the query explain mode used in the Spark SQL UI. The value can be 'simple', 'extended', 'codegen', 'cost', or 'formatted'. The default value is 'formatted'.
3.1.0


spark.sql.variable.substitute
true
This enables substitution using syntax like ${var}, ${system:var}, and ${env:var}.
2.0.0


Static SQL Configuration
Static SQL configurations are cross-session, immutable Spark SQL configurations. They can be set with final values by the config file
and command-line options with --conf/-c prefixed, or by setting SparkConf that are used to create SparkSession.
External users can query the static sql config values via SparkSession.conf or via set command, e.g. SET spark.sql.extensions;, but cannot set/unset them.

Property NameDefaultMeaningSince Version

spark.sql.cache.serializer
org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer
The name of a class that implements org.apache.spark.sql.columnar.CachedBatchSerializer. It will be used to translate SQL data into a format that can more efficiently be cached. The underlying API is subject to change so use with caution. Multiple classes cannot be specified. The class must have a no-arg constructor.
3.1.0


spark.sql.catalog.spark_catalog.defaultDatabase
default
The default database for session catalog.
3.4.0


spark.sql.event.truncate.length
2147483647
Threshold of SQL length beyond which it will be truncated before adding to event. Defaults to no truncation. If set to 0, callsite will be logged instead.
3.0.0


spark.sql.extensions
(none)
A comma-separated list of classes that implement Function1[SparkSessionExtensions, Unit] used to configure Spark Session extensions. The classes must have a no-args constructor. If multiple extensions are specified, they are applied in the specified order. For the case of rules and planner strategies, they are applied in the specified order. For the case of parsers, the last parser is used and each parser can delegate to its predecessor. For the case of function name conflicts, the last registered function name is used.
2.2.0


spark.sql.hive.metastore.barrierPrefixes

A comma separated list of class prefixes that should explicitly be reloaded for each version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a prefix that typically would be shared (i.e. org.apache.spark.*).
1.4.0


spark.sql.hive.metastore.jars
builtin
Location of the jars that should be used to instantiate the HiveMetastoreClient.
This property can be one of four options:
1. "builtin"
  Use Hive 2.3.9, which is bundled with the Spark assembly when
  -Phive is enabled. When this option is chosen,
  spark.sql.hive.metastore.version must be either
  2.3.9 or not defined.
2. "maven"
  Use Hive jars of specified version downloaded from Maven repositories.
3. "path"
  Use Hive jars configured by spark.sql.hive.metastore.jars.path
  in comma separated format. Support both local or remote paths.The provided jars
  should be the same version as spark.sql.hive.metastore.version.
4. A classpath in the standard format for both Hive and Hadoop. The provided jars
  should be the same version as spark.sql.hive.metastore.version.
1.4.0


spark.sql.hive.metastore.jars.path

Comma-separated paths of the jars that used to instantiate the HiveMetastoreClient.
This configuration is useful only when spark.sql.hive.metastore.jars is set as path.
The paths can be any of the following format:
1. file://path/to/jar/foo.jar
2. hdfs://nameservice/path/to/jar/foo.jar
3. /path/to/jar/ (path without URI scheme follow conf fs.defaultFS's URI schema)
4. [http/https/ftp]://path/to/jar/foo.jar
Note that 1, 2, and 3 support wildcard. For example:
1. file://path/to/jar/,file://path2/to/jar//.jar
2. hdfs://nameservice/path/to/jar/,hdfs://nameservice2/path/to/jar//.jar
3.1.0


spark.sql.hive.metastore.sharedPrefixes
com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc
A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared. For example, custom appenders that are used by log4j.
1.4.0


spark.sql.hive.metastore.version
2.3.9
Version of the Hive metastore. Available options are 0.12.0 through 2.3.9 and 3.0.0 through 3.1.3.
1.4.0


spark.sql.hive.thriftServer.singleSession
false
When set to true, Hive Thrift server is running in a single session mode. All the JDBC/ODBC connections share the temporary views, function registries, SQL configuration and the current database.
1.6.0


spark.sql.hive.version
2.3.9
The compiled, a.k.a, builtin Hive version of the Spark distribution bundled with. Note that, this a read-only conf and only used to report the built-in hive version. If you want a different metastore client for Spark to call, please refer to spark.sql.hive.metastore.version.
1.1.1


spark.sql.metadataCacheTTLSeconds
-1000ms
Time-to-live (TTL) value for the metadata caches: partition file metadata cache and session catalog cache. This configuration only has an effect when this value having a positive value (> 0). It also requires setting 'spark.sql.catalogImplementation' to hive, setting 'spark.sql.hive.filesourcePartitionFileCacheSize' > 0 and setting 'spark.sql.hive.manageFilesourcePartitions' to true to be applied to the partition file metadata cache.
3.1.0


spark.sql.queryExecutionListeners
(none)
List of class names implementing QueryExecutionListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.
2.3.0


spark.sql.sources.disabledJdbcConnProviderList

Configures a list of JDBC connection providers, which are disabled. The list contains the name of the JDBC connection providers separated by comma.
3.1.0


spark.sql.streaming.streamingQueryListeners
(none)
List of class names implementing StreamingQueryListener that will be automatically added to newly created sessions. The classes should have either a no-arg constructor, or a constructor that expects a SparkConf argument.
2.4.0


spark.sql.streaming.ui.enabled
true
Whether to run the Structured Streaming Web UI for the Spark application when the Spark Web UI is enabled.
3.0.0


spark.sql.streaming.ui.retainedProgressUpdates
100
The number of progress updates to retain for a streaming query for Structured Streaming UI.
3.0.0


spark.sql.streaming.ui.retainedQueries
100
The number of inactive queries to retain for Structured Streaming UI.
3.0.0


spark.sql.ui.retainedExecutions
1000
Number of executions to retain in the Spark UI.
1.5.0


spark.sql.warehouse.dir
(value of $PWD/spark-warehouse)
The default location for managed databases and tables.
2.0.0


Spark Streaming

Property NameDefaultMeaningSince Version

spark.streaming.backpressure.enabled
false

    Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5).
    This enables the Spark Streaming to control the receiving rate based on the
    current batch scheduling delays and processing times so that the system receives
    only as fast as the system can process. Internally, this dynamically sets the
    maximum receiving rate of receivers. This rate is upper bounded by the values
    spark.streaming.receiver.maxRate and spark.streaming.kafka.maxRatePerPartition
    if they are set (see below).
  
1.5.0


spark.streaming.backpressure.initialRate
not set

    This is the initial maximum receiving rate at which each receiver will receive data for the
    first batch when the backpressure mechanism is enabled.
  
2.0.0


spark.streaming.blockInterval
200ms

    Interval at which data received by Spark Streaming receivers is chunked
    into blocks of data before storing them in Spark. Minimum recommended - 50 ms. See the
    performance
     tuning section in the Spark Streaming programming guide for more details.
  
0.8.0


spark.streaming.receiver.maxRate
not set

    Maximum rate (number of records per second) at which each receiver will receive data.
    Effectively, each stream will consume at most this number of records per second.
    Setting this configuration to 0 or a negative number will put no limit on the rate.
    See the deployment guide
    in the Spark Streaming programming guide for mode details.
  
1.0.2


spark.streaming.receiver.writeAheadLog.enable
false

    Enable write-ahead logs for receivers. All the input data received through receivers
    will be saved to write-ahead logs that will allow it to be recovered after driver failures.
    See the deployment guide
    in the Spark Streaming programming guide for more details.
  
1.2.1


spark.streaming.unpersist
true

    Force RDDs generated and persisted by Spark Streaming to be automatically unpersisted from
    Spark's memory. The raw input data received by Spark Streaming is also automatically cleared.
    Setting this to false will allow the raw data and persisted RDDs to be accessible outside the
    streaming application as they will not be cleared automatically. But it comes at the cost of
    higher memory usage in Spark.
  
0.9.0


spark.streaming.stopGracefullyOnShutdown
false

    If true, Spark shuts down the StreamingContext gracefully on JVM
    shutdown rather than immediately.
  
1.4.0


spark.streaming.kafka.maxRatePerPartition
not set

    Maximum rate (number of records per second) at which data will be read from each Kafka
    partition when using the new Kafka direct stream API. See the
    Kafka Integration guide
    for more details.
  
1.3.0


spark.streaming.kafka.minRatePerPartition
1

    Minimum rate (number of records per second) at which data will be read from each Kafka
    partition when using the new Kafka direct stream API.
  
2.4.0


spark.streaming.ui.retainedBatches
1000

    How many batches the Spark Streaming UI and status APIs remember before garbage collecting.
  
1.0.0


spark.streaming.driver.writeAheadLog.closeFileAfterWrite
false

    Whether to close the file after writing a write-ahead log record on the driver. Set this to 'true'
    when you want to use S3 (or any file system that does not support flushing) for the metadata WAL
    on the driver.
  
1.6.0


spark.streaming.receiver.writeAheadLog.closeFileAfterWrite
false

    Whether to close the file after writing a write-ahead log record on the receivers. Set this to 'true'
    when you want to use S3 (or any file system that does not support flushing) for the data WAL
    on the receivers.
  
1.6.0


SparkR

Property NameDefaultMeaningSince Version

spark.r.numRBackendThreads
2

    Number of threads used by RBackend to handle RPC calls from SparkR package.
  
1.4.0


spark.r.command
Rscript

    Executable for executing R scripts in cluster modes for both driver and workers.
  
1.5.3


spark.r.driver.command
spark.r.command

    Executable for executing R scripts in client modes for driver. Ignored in cluster modes.
  
1.5.3


spark.r.shell.command
R

    Executable for executing sparkR shell in client modes for driver. Ignored in cluster modes. It is the same as environment variable SPARKR_DRIVER_R, but take precedence over it.
    spark.r.shell.command is used for sparkR shell while spark.r.driver.command is used for running R script.
  
2.1.0


spark.r.backendConnectionTimeout
6000

    Connection timeout set by R process on its connection to RBackend in seconds.
  
2.1.0


spark.r.heartBeatInterval
100

    Interval for heartbeats sent from SparkR backend to R process to prevent connection timeout.
  
2.1.0


GraphX

Property NameDefaultMeaningSince Version

spark.graphx.pregel.checkpointInterval
-1

    Checkpoint interval for graph and message in Pregel. It used to avoid stackOverflowError due to long lineage chains
  after lots of iterations. The checkpoint is disabled by default.
  
2.2.0


Deploy

Property NameDefaultMeaningSince Version

spark.deploy.recoveryMode
NONE
The recovery mode setting to recover submitted Spark jobs with cluster mode when it failed and relaunches.
    This is only applicable for cluster mode when running with Standalone or Mesos.
0.8.1


spark.deploy.zookeeper.url
None
When `spark.deploy.recoveryMode` is set to ZOOKEEPER, this configuration is used to set the zookeeper URL to connect to.
0.8.1


spark.deploy.zookeeper.dir
None
When `spark.deploy.recoveryMode` is set to ZOOKEEPER, this configuration is used to set the zookeeper directory to store recovery state.
0.8.1


Cluster Managers
Each cluster manager in Spark has additional configuration options. Configurations
can be found on the pages for each mode:
YARN
Mesos
Kubernetes
Standalone Mode
Environment Variables
Certain Spark settings can be configured through environment variables, which are read from the
conf/spark-env.sh script in the directory where Spark is installed (or conf/spark-env.cmd on
Windows). In Standalone and Mesos modes, this file can give machine specific information such as
hostnames. It is also sourced when running local Spark applications or submission scripts.
Note that conf/spark-env.sh does not exist by default when Spark is installed. However, you can
copy conf/spark-env.sh.template to create it. Make sure you make the copy executable.
The following variables can be set in spark-env.sh:

Environment VariableMeaning

JAVA_HOME
Location where Java is installed (if it's not on your default PATH).


PYSPARK_PYTHON
Python binary executable to use for PySpark in both driver and workers (default is python3 if available, otherwise python).
    Property spark.pyspark.python take precedence if it is set


PYSPARK_DRIVER_PYTHON
Python binary executable to use for PySpark in driver only (default is PYSPARK_PYTHON).
    Property spark.pyspark.driver.python take precedence if it is set


SPARKR_DRIVER_R
R binary executable to use for SparkR shell (default is R).
    Property spark.r.shell.command take precedence if it is set


SPARK_LOCAL_IP
IP address of the machine to bind to.


SPARK_PUBLIC_DNS
Hostname your Spark program will advertise to other machines.


In addition to the above, there are also options for setting up the Spark
standalone cluster scripts, such as number of cores
to use on each machine and maximum memory.
Since spark-env.sh is a shell script, some of these can be set programmatically – for example, you might
compute SPARK_LOCAL_IP by looking up the IP of a specific network interface.
Note: When running Spark on YARN in cluster mode, environment variables need to be set using the spark.yarn.appMasterEnv.[EnvironmentVariableName] property in your conf/spark-defaults.conf file.  Environment variables that are set in spark-env.sh will not be reflected in the YARN Application Master process in cluster mode.  See the YARN-related Spark Properties for more information.
Configuring Logging
Spark uses log4j for logging. You can configure it by adding a
log4j2.properties file in the conf directory. One way to start is to copy the existing
log4j2.properties.template located there.
By default, Spark adds 1 record to the MDC (Mapped Diagnostic Context): mdc.taskName, which shows something
like task 1.0 in stage 0.0. You can add %X{mdc.taskName} to your patternLayout in
order to print it in the logs.
Moreover, you can use spark.sparkContext.setLocalProperty(s"mdc.$name", "value") to add user specific data into MDC.
The key in MDC will be the string of “mdc.$name”.
Overriding configuration directory
To specify a different configuration directory other than the default “SPARK_HOME/conf”,
you can set SPARK_CONF_DIR. Spark will use the configuration files (spark-defaults.conf, spark-env.sh, log4j2.properties, etc)
from this directory.
Inheriting Hadoop Cluster Configuration
If you plan to read and write from HDFS using Spark, there are two Hadoop configuration files that
should be included on Spark’s classpath:

hdfs-site.xml, which provides default behaviors for the HDFS client.
core-site.xml, which sets the default filesystem name.

The location of these configuration files varies across Hadoop versions, but
a common location is inside of /etc/hadoop/conf. Some tools create
configurations on-the-fly, but offer a mechanism to download copies of them.
To make these files visible to Spark, set HADOOP_CONF_DIR in $SPARK_HOME/conf/spark-env.sh
to a location containing the configuration files.
Custom Hadoop/Hive Configuration
If your Spark application is interacting with Hadoop, Hive, or both, there are probably Hadoop/Hive
configuration files in Spark’s classpath.
Multiple running applications might require different Hadoop/Hive client side configurations.
You can copy and modify hdfs-site.xml, core-site.xml, yarn-site.xml, hive-site.xml in
Spark’s classpath for each application. In a Spark cluster running on YARN, these configuration
files are set cluster-wide, and cannot safely be changed by the application.
The better choice is to use spark hadoop properties in the form of spark.hadoop.*, and use
spark hive properties in the form of spark.hive.*.
For example, adding configuration “spark.hadoop.abc.def=xyz” represents adding hadoop property “abc.def=xyz”,
and adding configuration “spark.hive.abc=xyz” represents adding hive property “hive.abc=xyz”.
They can be considered as same as normal spark properties which can be set in $SPARK_HOME/conf/spark-defaults.conf
In some cases, you may want to avoid hard-coding certain configurations in a SparkConf. For
instance, Spark allows you to simply create an empty conf and set spark/spark hadoop/spark hive properties.
val conf = new SparkConf().set("spark.hadoop.abc.def", "xyz")
val sc = new SparkContext(conf)
Also, you can modify or add configurations at runtime:
./bin/spark-submit \
  --name "My app" \
  --master local[4] \
  --conf spark.eventLog.enabled=false \
  --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
  --conf spark.hadoop.abc.def=xyz \
  --conf spark.hive.abc=xyz
  myApp.jar
Custom Resource Scheduling and Configuration Overview
GPUs and other accelerators have been widely used for accelerating special workloads, e.g.,
deep learning and signal processing. Spark now supports requesting and scheduling generic resources, such as GPUs, with a few caveats. The current implementation requires that the resource have addresses that can be allocated by the scheduler. It requires your cluster manager to support and be properly configured with the resources.
There are configurations available to request resources for the driver: spark.driver.resource.{resourceName}.amount, request resources for the executor(s): spark.executor.resource.{resourceName}.amount and specify the requirements for each task: spark.task.resource.{resourceName}.amount. The spark.driver.resource.{resourceName}.discoveryScript config is required on YARN, Kubernetes and a client side Driver on Spark Standalone. spark.executor.resource.{resourceName}.discoveryScript config is required for YARN and Kubernetes. Kubernetes also requires spark.driver.resource.{resourceName}.vendor and/or spark.executor.resource.{resourceName}.vendor. See the config descriptions above for more information on each.
Spark will use the configurations specified to first request containers with the corresponding resources from the cluster manager. Once it gets the container, Spark launches an Executor in that container which will discover what resources the container has and the addresses associated with each resource. The Executor will register with the Driver and report back the resources available to that Executor. The Spark scheduler can then schedule tasks to each Executor and assign specific resource addresses based on the resource requirements the user specified. The user can see the resources assigned to a task using the TaskContext.get().resources api. On the driver, the user can see the resources assigned with the SparkContext resources call. It’s then up to the user to use the assignedaddresses to do the processing they want or pass those into the ML/AI framework they are using.
See your cluster manager specific page for requirements and details on each of - YARN, Kubernetes and Standalone Mode. It is currently not available with Mesos or local mode. And please also note that local-cluster mode with multiple workers is not supported(see Standalone documentation).
Stage Level Scheduling Overview
The stage level scheduling feature allows users to specify task and executor resource requirements at the stage level. This allows for different stages to run with executors that have different resources. A prime example of this is one ETL stage runs with executors with just CPUs, the next stage is an ML stage that needs GPUs. Stage level scheduling allows for user to request different executors that have GPUs when the ML stage runs rather then having to acquire executors with GPUs at the start of the application and them be idle while the ETL stage is being run.
This is only available for the RDD API in Scala, Java, and Python.  It is available on YARN, Kubernetes and Standalone when dynamic allocation is enabled. When dynamic allocation is disabled, it allows users to specify different task resource requirements at stage level, and this is supported on YARN, Kubernetes and Standalone cluster right now. See the YARN page or Kubernetes page or Standalone page for more implementation details.
See the RDD.withResources and ResourceProfileBuilder API’s for using this feature. When dynamic allocation is disabled, tasks with different task resource requirements will share executors with DEFAULT_RESOURCE_PROFILE. While when dynamic allocation is enabled, the current implementation acquires new executors for each ResourceProfile  created and currently has to be an exact match. Spark does not try to fit tasks into an executor that require a different ResourceProfile than the executor was created with. Executors that are not in use will idle timeout with the dynamic allocation logic. The default configuration for this feature is to only allow one ResourceProfile per stage. If the user associates more then 1 ResourceProfile to an RDD, Spark will throw an exception by default. See config spark.scheduler.resource.profileMergeConflicts to control that behavior. The current merge strategy Spark implements when spark.scheduler.resource.profileMergeConflicts is enabled is a simple max of each resource within the conflicting ResourceProfiles. Spark will create a new ResourceProfile with the max of each of the resources.
Push-based shuffle overview
Push-based shuffle helps improve the reliability and performance of spark shuffle. It takes a best-effort approach to push the shuffle blocks generated by the map tasks to remote external shuffle services to be merged per shuffle partition. Reduce tasks fetch a combination of merged shuffle partitions and original shuffle blocks as their input data, resulting in converting small random disk reads by external shuffle services into large sequential reads. Possibility of better data locality for reduce tasks additionally helps minimize network IO. Push-based shuffle takes priority over batch fetch for some scenarios, like partition coalesce when merged output is available.
 Push-based shuffle improves performance for long running jobs/queries which involves large disk I/O during shuffle. Currently it is not well suited for jobs/queries which runs quickly dealing with lesser amount of shuffle data. This will be further improved in the future releases.
  Currently push-based shuffle is only supported for Spark on YARN with external shuffle service. 
External Shuffle service(server) side configuration options

Property NameDefaultMeaningSince Version

spark.shuffle.push.server.mergedShuffleFileManagerImpl

org.apache.spark.network.shuffle.NoOpMergedShuffleFileManager


    Class name of the implementation of MergedShuffleFileManager that manages push-based shuffle. This acts as a server side config to disable or enable push-based shuffle. By default, push-based shuffle is disabled at the server side.  To enable push-based shuffle on the server side, set this config to org.apache.spark.network.shuffle.RemoteBlockPushResolver

3.2.0


spark.shuffle.push.server.minChunkSizeInMergedShuffleFile
2m

 The minimum size of a chunk when dividing a merged shuffle file into multiple chunks during push-based shuffle. A merged shuffle file consists of multiple small shuffle blocks. Fetching the complete merged shuffle file in a single disk I/O increases the memory requirements for both the clients and the external shuffle services. Instead, the external shuffle service serves the merged file in MB-sized chunks. This configuration controls how big a chunk can get. A corresponding index file for each merged shuffle file will be generated indicating chunk boundaries. 
 Setting this too high would increase the memory requirements on both the clients and the external shuffle service. 
 Setting this too low would increase the overall number of RPC requests to external shuffle service unnecessarily.

3.2.0


spark.shuffle.push.server.mergedIndexCacheSize
100m

    The maximum size of cache in memory which could be used in push-based shuffle for storing merged index files. This cache is in addition to the one configured via spark.shuffle.service.index.cache.size.
  
3.2.0


Client side configuration options

Property NameDefaultMeaningSince Version

spark.shuffle.push.enabled
false

    Set to true to enable push-based shuffle on the client side and works in conjunction with the server side flag spark.shuffle.push.server.mergedShuffleFileManagerImpl.
  
3.2.0


spark.shuffle.push.finalize.timeout
10s

    The amount of time driver waits in seconds, after all mappers have finished for a given shuffle map stage, before it sends merge finalize requests to remote external shuffle services. This gives the external shuffle services extra time to merge blocks. Setting this too long could potentially lead to performance regression.
  
3.2.0


spark.shuffle.push.maxRetainedMergerLocations
500

    Maximum number of merger locations cached for push-based shuffle. Currently, merger locations are hosts of external shuffle services responsible for handling pushed blocks, merging them and serving merged blocks for later shuffle fetch.
  
3.2.0


spark.shuffle.push.mergersMinThresholdRatio
0.05

    Ratio used to compute the minimum number of shuffle merger locations required for a stage based on the number of partitions for the reducer stage. For example, a reduce stage which has 100 partitions and uses the default value 0.05 requires at least 5 unique merger locations to enable push-based shuffle.
  
3.2.0


spark.shuffle.push.mergersMinStaticThreshold
5

    The static threshold for number of shuffle push merger locations should be available in order to enable push-based shuffle for a stage. Note this config works in conjunction with spark.shuffle.push.mergersMinThresholdRatio. Maximum of spark.shuffle.push.mergersMinStaticThreshold and spark.shuffle.push.mergersMinThresholdRatio ratio number of mergers needed to enable push-based shuffle for a stage. For example: with 1000 partitions for the child stage with spark.shuffle.push.mergersMinStaticThreshold as 5 and spark.shuffle.push.mergersMinThresholdRatio set to 0.05, we would need at least 50 mergers to enable push-based shuffle for that stage.
  
3.2.0


spark.shuffle.push.numPushThreads
(none)

    Specify the number of threads in the block pusher pool. These threads assist in creating connections and pushing blocks to remote external shuffle services.
    By default, the threadpool size is equal to the number of spark executor cores.
  
3.2.0


spark.shuffle.push.maxBlockSizeToPush
1m

 The max size of an individual block to push to the remote external shuffle services. Blocks larger than this threshold are not pushed to be merged remotely. These shuffle blocks will be fetched in the original manner. 
 Setting this too high would result in more blocks to be pushed to remote external shuffle services but those are already efficiently fetched with the existing mechanisms resulting in additional overhead of pushing the large blocks to remote external shuffle services. It is recommended to set spark.shuffle.push.maxBlockSizeToPush lesser than spark.shuffle.push.maxBlockBatchSize config's value. 
 Setting this too low would result in lesser number of blocks getting merged and directly fetched from mapper external shuffle service results in higher small random reads affecting overall disk I/O performance. 

3.2.0


spark.shuffle.push.maxBlockBatchSize
3m

    The max size of a batch of shuffle blocks to be grouped into a single push request. Default is set to 3m in order to keep it slightly higher than spark.storage.memoryMapThreshold default which is 2m as it is very likely that each batch of block gets memory mapped which incurs higher overhead.
  
3.2.0


spark.shuffle.push.merge.finalizeThreads
8

    Number of threads used by driver to finalize shuffle merge. Since it could potentially take seconds for a large shuffle to finalize,
    having multiple threads helps driver to handle concurrent shuffle merge finalize requests when push-based shuffle is enabled.
  
3.3.0


spark.shuffle.push.minShuffleSizeToWait
500m

    Driver will wait for merge finalization to complete only if total shuffle data size is more than this threshold. If total shuffle size is less, driver will immediately finalize the shuffle output.
  
3.3.0


spark.shuffle.push.minCompletedPushRatio
1.0

    Fraction of minimum map partitions that should be push complete before driver starts shuffle merge finalization during push based shuffle.
  
3.3.0






















  




GraphX - Spark 3.5.5 Documentation



















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











GraphX Programming Guide

Overview
Getting Started
The Property Graph 
Example Property Graph


Graph Operators 
Summary List of Operators
Property Operators
Structural Operators
Join Operators
Neighborhood Aggregation 
Aggregate Messages (aggregateMessages)
Map Reduce Triplets Transition Guide (Legacy)
Computing Degree Information
Collecting Neighbors


Caching and Uncaching


Pregel API
Graph Builders
Vertex and Edge RDDs 
VertexRDDs
EdgeRDDs


Optimized Representation
Graph Algorithms 
PageRank
Connected Components
Triangle Counting


Examples






Overview
GraphX is a new component in Spark for graphs and graph-parallel computation. At a high level,
GraphX extends the Spark RDD by introducing a
new Graph abstraction: a directed multigraph with properties
attached to each vertex and edge.  To support graph computation, GraphX exposes a set of fundamental
operators (e.g., subgraph, joinVertices, and
aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and
builders to simplify graph analytics tasks.
Getting Started
To get started you first need to import Spark and GraphX into your project, as follows:
import org.apache.spark._
import org.apache.spark.graphx._
// To make some of the examples work we will also need RDD
import org.apache.spark.rdd.RDD
If you are not using the Spark shell you will also need a SparkContext.  To learn more about
getting started with Spark refer to the Spark Quick Start Guide.

The Property Graph
The property graph is a directed multigraph
with user defined objects attached to each vertex and edge.  A directed multigraph is a directed
graph with potentially multiple parallel edges sharing the same source and destination vertex.  The
ability to support parallel edges simplifies modeling scenarios where there can be multiple
relationships (e.g., co-worker and friend) between the same vertices.  Each vertex is keyed by a
unique 64-bit long identifier (VertexId).  GraphX does not impose any ordering constraints on
the vertex identifiers.  Similarly, edges have corresponding source and destination vertex
identifiers.
The property graph is parameterized over the vertex (VD) and edge (ED) types.  These
are the types of the objects associated with each vertex and edge respectively.

GraphX optimizes the representation of vertex and edge types when they are primitive data types
(e.g., int, double, etc…) reducing the in memory footprint by storing them in specialized
arrays.

In some cases it may be desirable to have vertices with different property types in the same graph.
This can be accomplished through inheritance.  For example to model users and products as a
bipartite graph we might do the following:
class VertexProperty()
case class UserProperty(val name: String) extends VertexProperty
case class ProductProperty(val name: String, val price: Double) extends VertexProperty
// The graph might then have the type:
var graph: Graph[VertexProperty, String] = null
Like RDDs, property graphs are immutable, distributed, and fault-tolerant.  Changes to the values or
structure of the graph are accomplished by producing a new graph with the desired changes.  Note
that substantial parts of the original graph (i.e., unaffected structure, attributes, and indices)
are reused in the new graph reducing the cost of this inherently functional data structure.  The
graph is partitioned across the executors using a range of vertex partitioning heuristics.  As with
RDDs, each partition of the graph can be recreated on a different machine in the event of a failure.
Logically the property graph corresponds to a pair of typed collections (RDDs) encoding the
properties for each vertex and edge.  As a consequence, the graph class contains members to access
the vertices and edges of the graph:
class Graph[VD, ED] {
  val vertices: VertexRDD[VD]
  val edges: EdgeRDD[ED]
}
The classes VertexRDD[VD] and EdgeRDD[ED] extend and are optimized versions of RDD[(VertexId,
VD)] and RDD[Edge[ED]] respectively.  Both VertexRDD[VD] and EdgeRDD[ED] provide  additional
functionality built around graph computation and leverage internal optimizations.  We discuss the
VertexRDDVertexRDD and EdgeRDDEdgeRDD API in greater detail in the section on vertex and edge
RDDs but for now they can be thought of as simply RDDs of the form:
RDD[(VertexId, VD)] and RDD[Edge[ED]].
Example Property Graph
Suppose we want to construct a property graph consisting of the various collaborators on the GraphX
project. The vertex property might contain the username and occupation.  We could annotate edges
with a string describing the relationships between collaborators:




The resulting graph would have the type signature:
val userGraph: Graph[(String, String), String]
There are numerous ways to construct a property graph from raw files, RDDs, and even synthetic
generators and these are discussed in more detail in the section on
graph builders.  Probably the most general method is to use the
Graph object.  For example the following
code constructs a graph from a collection of RDDs:
// Assume the SparkContext has already been constructed
val sc: SparkContext
// Create an RDD for the vertices
val users: RDD[(VertexId, (String, String))] =
  sc.parallelize(Seq((3L, ("rxin", "student")), (7L, ("jgonzal", "postdoc")),
                       (5L, ("franklin", "prof")), (2L, ("istoica", "prof"))))
// Create an RDD for edges
val relationships: RDD[Edge[String]] =
  sc.parallelize(Seq(Edge(3L, 7L, "collab"),    Edge(5L, 3L, "advisor"),
                       Edge(2L, 5L, "colleague"), Edge(5L, 7L, "pi")))
// Define a default user in case there are relationship with missing user
val defaultUser = ("John Doe", "Missing")
// Build the initial Graph
val graph = Graph(users, relationships, defaultUser)
In the above example we make use of the Edge case class. Edges have a srcId and a
dstId corresponding to the source and destination vertex identifiers. In addition, the Edge
class has an attr member which stores the edge property.
We can deconstruct a graph into the respective vertex and edge views by using the graph.vertices
and graph.edges members respectively.
val graph: Graph[(String, String), String] // Constructed from above
// Count all users which are postdocs
graph.vertices.filter { case (id, (name, pos)) => pos == "postdoc" }.count
// Count all the edges where src > dst
graph.edges.filter(e => e.srcId > e.dstId).count

Note that graph.vertices returns an VertexRDD[(String, String)] which extends
RDD[(VertexId, (String, String))] and so we use the scala case expression to deconstruct the
tuple.  On the other hand, graph.edges returns an EdgeRDD containing Edge[String] objects.
We could have also used the case class type constructor as in the following:

graph.edges.filter { case Edge(src, dst, prop) => src > dst }.count
In addition to the vertex and edge views of the property graph, GraphX also exposes a triplet view.
The triplet view logically joins the vertex and edge properties yielding an
RDD[EdgeTriplet[VD, ED]] containing instances of the EdgeTriplet class. This
join can be expressed in the following SQL expression:
SELECT src.id, dst.id, src.attr, e.attr, dst.attr
FROM edges AS e LEFT JOIN vertices AS src, vertices AS dst
ON e.srcId = src.Id AND e.dstId = dst.Id
or graphically as:




The EdgeTriplet class extends the Edge class by adding the srcAttr and
dstAttr members which contain the source and destination properties respectively. We can use the
triplet view of a graph to render a collection of strings describing relationships between users.
val graph: Graph[(String, String), String] // Constructed from above
// Use the triplets view to create an RDD of facts.
val facts: RDD[String] =
  graph.triplets.map(triplet =>
    triplet.srcAttr._1 + " is the " + triplet.attr + " of " + triplet.dstAttr._1)
facts.collect.foreach(println(_))
Graph Operators
Just as RDDs have basic operations like map, filter, and reduceByKey, property graphs also
have a collection of basic operators that take user defined functions and produce new graphs with
transformed properties and structure.  The core operators that have optimized implementations are
defined in Graph and convenient operators that are expressed as a compositions of the
core operators are defined in GraphOps.  However, thanks to Scala implicits the
operators in GraphOps are automatically available as members of Graph.  For example, we can
compute the in-degree of each vertex (defined in GraphOps) by the following:
val graph: Graph[(String, String), String]
// Use the implicit GraphOps.inDegrees operator
val inDegrees: VertexRDD[Int] = graph.inDegrees
The reason for differentiating between core graph operations and GraphOps is to be
able to support different graph representations in the future.  Each graph representation must
provide implementations of the core operations and reuse many of the useful operations defined in
GraphOps.
Summary List of Operators
The following is a quick summary of the functionality defined in both Graph and
GraphOps but presented as members of Graph for simplicity. Note that some function
signatures have been simplified (e.g., default arguments and type constraints removed) and some more
advanced functionality has been removed so please consult the API docs for the official list of
operations.
/** Summary of the functionality in the property graph */
class Graph[VD, ED] {
  // Information about the Graph ===================================================================
  val numEdges: Long
  val numVertices: Long
  val inDegrees: VertexRDD[Int]
  val outDegrees: VertexRDD[Int]
  val degrees: VertexRDD[Int]
  // Views of the graph as collections =============================================================
  val vertices: VertexRDD[VD]
  val edges: EdgeRDD[ED]
  val triplets: RDD[EdgeTriplet[VD, ED]]
  // Functions for caching graphs ==================================================================
  def persist(newLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, ED]
  def cache(): Graph[VD, ED]
  def unpersistVertices(blocking: Boolean = false): Graph[VD, ED]
  // Change the partitioning heuristic  ============================================================
  def partitionBy(partitionStrategy: PartitionStrategy): Graph[VD, ED]
  // Transform vertex and edge attributes ==========================================================
  def mapVertices[VD2](map: (VertexId, VD) => VD2): Graph[VD2, ED]
  def mapEdges[ED2](map: Edge[ED] => ED2): Graph[VD, ED2]
  def mapEdges[ED2](map: (PartitionID, Iterator[Edge[ED]]) => Iterator[ED2]): Graph[VD, ED2]
  def mapTriplets[ED2](map: EdgeTriplet[VD, ED] => ED2): Graph[VD, ED2]
  def mapTriplets[ED2](map: (PartitionID, Iterator[EdgeTriplet[VD, ED]]) => Iterator[ED2])
    : Graph[VD, ED2]
  // Modify the graph structure ====================================================================
  def reverse: Graph[VD, ED]
  def subgraph(
      epred: EdgeTriplet[VD,ED] => Boolean = (x => true),
      vpred: (VertexId, VD) => Boolean = ((v, d) => true))
    : Graph[VD, ED]
  def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED]
  def groupEdges(merge: (ED, ED) => ED): Graph[VD, ED]
  // Join RDDs with the graph ======================================================================
  def joinVertices[U](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) => VD): Graph[VD, ED]
  def outerJoinVertices[U, VD2](other: RDD[(VertexId, U)])
      (mapFunc: (VertexId, VD, Option[U]) => VD2)
    : Graph[VD2, ED]
  // Aggregate information about adjacent triplets =================================================
  def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]
  def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]]
  def aggregateMessages[Msg: ClassTag](
      sendMsg: EdgeContext[VD, ED, Msg] => Unit,
      mergeMsg: (Msg, Msg) => Msg,
      tripletFields: TripletFields = TripletFields.All)
    : VertexRDD[A]
  // Iterative graph-parallel computation ==========================================================
  def pregel[A](initialMsg: A, maxIterations: Int, activeDirection: EdgeDirection)(
      vprog: (VertexId, VD, A) => VD,
      sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],
      mergeMsg: (A, A) => A)
    : Graph[VD, ED]
  // Basic graph algorithms ========================================================================
  def pageRank(tol: Double, resetProb: Double = 0.15): Graph[Double, Double]
  def connectedComponents(): Graph[VertexId, ED]
  def triangleCount(): Graph[Int, ED]
  def stronglyConnectedComponents(numIter: Int): Graph[VertexId, ED]
}
Property Operators
Like the RDD map operator, the property graph contains the following:
class Graph[VD, ED] {
  def mapVertices[VD2](map: (VertexId, VD) => VD2): Graph[VD2, ED]
  def mapEdges[ED2](map: Edge[ED] => ED2): Graph[VD, ED2]
  def mapTriplets[ED2](map: EdgeTriplet[VD, ED] => ED2): Graph[VD, ED2]
}
Each of these operators yields a new graph with the vertex or edge properties modified by the user
defined map function.

Note that in each case the graph structure is unaffected. This is a key feature of these operators
which allows the resulting graph to reuse the structural indices of the original graph. The
following snippets are logically equivalent, but the first one does not preserve the structural
indices and would not benefit from the GraphX system optimizations:

val newVertices = graph.vertices.map { case (id, attr) => (id, mapUdf(id, attr)) }
val newGraph = Graph(newVertices, graph.edges)

Instead, use mapVertices to preserve the indices:

val newGraph = graph.mapVertices((id, attr) => mapUdf(id, attr))
These operators are often used to initialize the graph for a particular computation or project away
unnecessary properties.  For example, given a graph with the out degrees as the vertex properties
(we describe how to construct such a graph later), we initialize it for PageRank:
// Given a graph where the vertex property is the out degree
val inputGraph: Graph[Int, String] =
  graph.outerJoinVertices(graph.outDegrees)((vid, _, degOpt) => degOpt.getOrElse(0))
// Construct a graph where each edge contains the weight
// and each vertex is the initial PageRank
val outputGraph: Graph[Double, Double] =
  inputGraph.mapTriplets(triplet => 1.0 / triplet.srcAttr).mapVertices((id, _) => 1.0)

Structural Operators
Currently GraphX supports only a simple set of commonly used structural operators and we expect to
add more in the future.  The following is a list of the basic structural operators.
class Graph[VD, ED] {
  def reverse: Graph[VD, ED]
  def subgraph(epred: EdgeTriplet[VD,ED] => Boolean,
               vpred: (VertexId, VD) => Boolean): Graph[VD, ED]
  def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED]
  def groupEdges(merge: (ED, ED) => ED): Graph[VD,ED]
}
The reverse operator returns a new graph with all the edge directions reversed.
This can be useful when, for example, trying to compute the inverse PageRank.  Because the reverse
operation does not modify vertex or edge properties or change the number of edges, it can be
implemented efficiently without data movement or duplication.
The subgraph operator takes vertex and edge predicates and returns the graph
containing only the vertices that satisfy the vertex predicate (evaluate to true) and edges that
satisfy the edge predicate and connect vertices that satisfy the vertex predicate.  The subgraph
operator can be used in number of situations to restrict the graph to the vertices and edges of
interest or eliminate broken links. For example in the following code we remove broken links:
// Create an RDD for the vertices
val users: RDD[(VertexId, (String, String))] =
  sc.parallelize(Seq((3L, ("rxin", "student")), (7L, ("jgonzal", "postdoc")),
                       (5L, ("franklin", "prof")), (2L, ("istoica", "prof")),
                       (4L, ("peter", "student"))))
// Create an RDD for edges
val relationships: RDD[Edge[String]] =
  sc.parallelize(Seq(Edge(3L, 7L, "collab"),    Edge(5L, 3L, "advisor"),
                       Edge(2L, 5L, "colleague"), Edge(5L, 7L, "pi"),
                       Edge(4L, 0L, "student"),   Edge(5L, 0L, "colleague")))
// Define a default user in case there are relationship with missing user
val defaultUser = ("John Doe", "Missing")
// Build the initial Graph
val graph = Graph(users, relationships, defaultUser)
// Notice that there is a user 0 (for which we have no information) connected to users
// 4 (peter) and 5 (franklin).
graph.triplets.map(
  triplet => triplet.srcAttr._1 + " is the " + triplet.attr + " of " + triplet.dstAttr._1
).collect.foreach(println(_))
// Remove missing vertices as well as the edges to connected to them
val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != "Missing")
// The valid subgraph will disconnect users 4 and 5 by removing user 0
validGraph.vertices.collect.foreach(println(_))
validGraph.triplets.map(
  triplet => triplet.srcAttr._1 + " is the " + triplet.attr + " of " + triplet.dstAttr._1
).collect.foreach(println(_))

Note in the above example only the vertex predicate is provided.  The subgraph operator defaults
to true if the vertex or edge predicates are not provided.

The mask operator constructs a subgraph by returning a graph that contains the
vertices and edges that are also found in the input graph.  This can be used in conjunction with the
subgraph operator to restrict a graph based on the properties in another related graph.  For
example, we might run connected components using the graph with missing vertices and then restrict
the answer to the valid subgraph.
// Run Connected Components
val ccGraph = graph.connectedComponents() // No longer contains missing field
// Remove missing vertices as well as the edges to connected to them
val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != "Missing")
// Restrict the answer to the valid subgraph
val validCCGraph = ccGraph.mask(validGraph)
The groupEdges operator merges parallel edges (i.e., duplicate edges between
pairs of vertices) in the multigraph.  In many numerical applications, parallel edges can be added
(their weights combined) into a single edge thereby reducing the size of the graph.

Join Operators
In many cases it is necessary to join data from external collections (RDDs) with graphs.  For
example, we might have extra user properties that we want to merge with an existing graph or we
might want to pull vertex properties from one graph into another.  These tasks can be accomplished
using the join operators. Below we list the key join operators:
class Graph[VD, ED] {
  def joinVertices[U](table: RDD[(VertexId, U)])(map: (VertexId, VD, U) => VD)
    : Graph[VD, ED]
  def outerJoinVertices[U, VD2](table: RDD[(VertexId, U)])(map: (VertexId, VD, Option[U]) => VD2)
    : Graph[VD2, ED]
}
The joinVertices operator joins the vertices with the input RDD and
returns a new graph with the vertex properties obtained by applying the user defined map function
to the result of the joined vertices.  Vertices without a matching value in the RDD retain their
original value.

Note that if the RDD contains more than one value for a given vertex only one will be used.  It
is therefore recommended that the input RDD be made unique using the following which will
also pre-index the resulting values to substantially accelerate the subsequent join.

val nonUniqueCosts: RDD[(VertexId, Double)]
val uniqueCosts: VertexRDD[Double] =
  graph.vertices.aggregateUsingIndex(nonUnique, (a,b) => a + b)
val joinedGraph = graph.joinVertices(uniqueCosts)(
  (id, oldCost, extraCost) => oldCost + extraCost)
The more general outerJoinVertices behaves similarly to joinVertices
except that the user defined map function is applied to all vertices and can change the vertex
property type.  Because not all vertices may have a matching value in the input RDD the map
function takes an Option type.  For example, we can set up a graph for PageRank by initializing
vertex properties with their outDegree.
val outDegrees: VertexRDD[Int] = graph.outDegrees
val degreeGraph = graph.outerJoinVertices(outDegrees) { (id, oldAttr, outDegOpt) =>
  outDegOpt match {
    case Some(outDeg) => outDeg
    case None => 0 // No outDegree means zero outDegree
  }
}

You may have noticed the multiple parameter lists (e.g., f(a)(b)) curried function pattern used
in the above examples.  While we could have equally written f(a)(b) as f(a,b) this would mean
that type inference on b would not depend on a.  As a consequence, the user would need to
provide type annotation for the user defined function:

val joinedGraph = graph.joinVertices(uniqueCosts,
  (id: VertexId, oldCost: Double, extraCost: Double) => oldCost + extraCost)



Neighborhood Aggregation
A key step in many graph analytics tasks is aggregating information about the neighborhood of each
vertex.
For example, we might want to know the number of followers each user has or the average age of
the followers of each user.  Many iterative graph algorithms (e.g., PageRank, Shortest Path, and
connected components) repeatedly aggregate properties of neighboring vertices (e.g., current
PageRank Value, shortest path to the source, and smallest reachable vertex id).

To improve performance the primary aggregation operator changed from
graph.mapReduceTriplets to the new graph.AggregateMessages.  While the changes in the API are
relatively small, we provide a transition guide below.


Aggregate Messages (aggregateMessages)
The core aggregation operation in GraphX is aggregateMessages.
This operator applies a user defined sendMsg function to each edge triplet in the graph
and then uses the mergeMsg function to aggregate those messages at their destination vertex.
class Graph[VD, ED] {
  def aggregateMessages[Msg: ClassTag](
      sendMsg: EdgeContext[VD, ED, Msg] => Unit,
      mergeMsg: (Msg, Msg) => Msg,
      tripletFields: TripletFields = TripletFields.All)
    : VertexRDD[Msg]
}
The user defined sendMsg function takes an EdgeContext, which exposes the
source and destination attributes along with the edge attribute and functions
(sendToSrc, and sendToDst) to send
messages to the source and destination attributes.  Think of sendMsg as the map
function in map-reduce.
The user defined mergeMsg function takes two messages destined to the same vertex and
yields a single message.  Think of mergeMsg as the reduce function in map-reduce.
The  aggregateMessages operator returns a VertexRDD[Msg]
containing the aggregate message (of type Msg) destined to each vertex.  Vertices that did not
receive a message are not included in the returned VertexRDDVertexRDD.

In addition, aggregateMessages takes an optional
tripletsFields which indicates what data is accessed in the EdgeContext
(i.e., the source vertex attribute but not the destination vertex attribute).
The possible options for the tripletsFields are defined in TripletFields and
the default value is TripletFields.All which indicates that the user
defined sendMsg function may access any of the fields in the EdgeContext.
The tripletFields argument can be used to notify GraphX that only part of the
EdgeContext will be needed allowing GraphX to select an optimized join strategy.
For example if we are computing the average age of the followers of each user we would only require
the source field and so we would use TripletFields.Src to indicate that we
only require the source field

In earlier versions of GraphX we used byte code inspection to infer the
TripletFields however we have found that bytecode inspection to be
slightly unreliable and instead opted for more explicit user control.

In the following example we use the aggregateMessages operator to
compute the average age of the more senior followers of each user.
import org.apache.spark.graphx.{Graph, VertexRDD}
import org.apache.spark.graphx.util.GraphGenerators

// Create a graph with "age" as the vertex property.
// Here we use a random graph for simplicity.
val graph: Graph[Double, Int] =
  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) => id.toDouble )
// Compute the number of older followers and their total age
val olderFollowers: VertexRDD[(Int, Double)] = graph.aggregateMessages[(Int, Double)](
  triplet => { // Map Function
    if (triplet.srcAttr > triplet.dstAttr) {
      // Send message to destination vertex containing counter and age
      triplet.sendToDst((1, triplet.srcAttr))
    }
  },
  // Add counter and age
  (a, b) => (a._1 + b._1, a._2 + b._2) // Reduce Function
)
// Divide total age by number of older followers to get average age of older followers
val avgAgeOfOlderFollowers: VertexRDD[Double] =
  olderFollowers.mapValues( (id, value) =>
    value match { case (count, totalAge) => totalAge / count } )
// Display the results
avgAgeOfOlderFollowers.collect.foreach(println(_))
Find full example code at "examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala" in the Spark repo.

The aggregateMessages operation performs optimally when the messages (and the sums of
messages) are constant sized (e.g., floats and addition instead of lists and concatenation).


Map Reduce Triplets Transition Guide (Legacy)
In earlier versions of GraphX neighborhood aggregation was accomplished using the
mapReduceTriplets operator:
class Graph[VD, ED] {
  def mapReduceTriplets[Msg](
      map: EdgeTriplet[VD, ED] => Iterator[(VertexId, Msg)],
      reduce: (Msg, Msg) => Msg)
    : VertexRDD[Msg]
}
The mapReduceTriplets operator takes a user defined map function which
is applied to each triplet and can yield messages which are aggregated using the user defined
reduce function.
However, we found the user of the returned iterator to be expensive and it inhibited our ability to
apply additional optimizations (e.g., local vertex renumbering).
In aggregateMessages we introduced the EdgeContext which exposes the
triplet fields and also functions to explicitly send messages to the source and destination vertex.
Furthermore we removed bytecode inspection and instead require the user to indicate what fields
in the triplet are actually required.
The following code block using mapReduceTriplets:
val graph: Graph[Int, Float] = ...
def msgFun(triplet: Triplet[Int, Float]): Iterator[(Int, String)] = {
  Iterator((triplet.dstId, "Hi"))
}
def reduceFun(a: String, b: String): String = a + " " + b
val result = graph.mapReduceTriplets[String](msgFun, reduceFun)
can be rewritten using aggregateMessages as:
val graph: Graph[Int, Float] = ...
def msgFun(triplet: EdgeContext[Int, Float, String]) {
  triplet.sendToDst("Hi")
}
def reduceFun(a: String, b: String): String = a + " " + b
val result = graph.aggregateMessages[String](msgFun, reduceFun)
Computing Degree Information
A common aggregation task is computing the degree of each vertex: the number of edges adjacent to
each vertex.  In the context of directed graphs it is often necessary to know the in-degree, 
out-degree, and the total degree of each vertex.  The  GraphOps class contains a
collection of operators to compute the degrees of each vertex.  For example in the following we
compute the max in, out, and total degrees:
// Define a reduce operation to compute the highest degree vertex
def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = {
  if (a._2 > b._2) a else b
}
// Compute the max degrees
val maxInDegree: (VertexId, Int)  = graph.inDegrees.reduce(max)
val maxOutDegree: (VertexId, Int) = graph.outDegrees.reduce(max)
val maxDegrees: (VertexId, Int)   = graph.degrees.reduce(max)
Collecting Neighbors
In some cases it may be easier to express computation by collecting neighboring vertices and their
attributes at each vertex. This can be easily accomplished using the
collectNeighborIds and the
collectNeighbors operators.
class GraphOps[VD, ED] {
  def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]
  def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[ Array[(VertexId, VD)] ]
}

These operators can be quite costly as they duplicate information and require
substantial communication.  If possible try expressing the same computation using the
aggregateMessages  operator directly.

Caching and Uncaching
In Spark, RDDs are not persisted in memory by default. To avoid recomputation, they must be explicitly cached when using them multiple times (see the Spark Programming Guide). Graphs in GraphX behave the same way. When using a graph multiple times, make sure to call Graph.cache() on it first.
In iterative computations, uncaching may also be necessary for best performance. By default, cached RDDs and graphs will remain in memory until memory pressure forces them to be evicted in LRU order. For iterative computation, intermediate results from previous iterations will fill up the cache. Though they will eventually be evicted, the unnecessary data stored in memory will slow down garbage collection. It would be more efficient to uncache intermediate results as soon as they are no longer necessary. This involves materializing (caching and forcing) a graph or RDD every iteration, uncaching all other datasets, and only using the materialized dataset in future iterations. However, because graphs are composed of multiple RDDs, it can be difficult to unpersist them correctly. For iterative computation we recommend using the Pregel API, which correctly unpersists intermediate results.

Pregel API
Graphs are inherently recursive data structures as properties of vertices depend on properties of
their neighbors which in turn depend on properties of their neighbors.  As a
consequence many important graph algorithms iteratively recompute the properties of each vertex
until a fixed-point condition is reached.  A range of graph-parallel abstractions have been proposed
to express these iterative algorithms.  GraphX exposes a variant of the Pregel API.
At a high level the Pregel operator in GraphX is a bulk-synchronous parallel messaging abstraction
constrained to the topology of the graph.  The Pregel operator executes in a series of super steps
in which vertices receive the sum of their inbound messages from the previous super step, compute
a new value for the vertex property, and then send messages to neighboring vertices in the next
super step.  Unlike Pregel, messages are computed in parallel as a
function of the edge triplet and the message computation has access to both the source and
destination vertex attributes.  Vertices that do not receive a message are skipped within a super
step.  The Pregel operator terminates iteration and returns the final graph when there are no
messages remaining.

Note, unlike more standard Pregel implementations, vertices in GraphX can only send messages to
neighboring vertices and the message construction is done in parallel using a user defined
messaging function.  These constraints allow additional optimization within GraphX.

The following is the type signature of the Pregel operator as well as a sketch
of its implementation (note: to avoid stackOverflowError due to long lineage chains, pregel support periodically
checkpoint graph and messages by setting “spark.graphx.pregel.checkpointInterval” to a positive number,
say 10. And set checkpoint directory as well using SparkContext.setCheckpointDir(directory: String)):
class GraphOps[VD, ED] {
  def pregel[A]
      (initialMsg: A,
       maxIter: Int = Int.MaxValue,
       activeDir: EdgeDirection = EdgeDirection.Out)
      (vprog: (VertexId, VD, A) => VD,
       sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],
       mergeMsg: (A, A) => A)
    : Graph[VD, ED] = {
    // Receive the initial message at each vertex
    var g = mapVertices( (vid, vdata) => vprog(vid, vdata, initialMsg) ).cache()

    // compute the messages
    var messages = GraphXUtils.mapReduceTriplets(g, sendMsg, mergeMsg)
    var activeMessages = messages.count()
    // Loop until no messages remain or maxIterations is achieved
    var i = 0
    while (activeMessages > 0 && i < maxIterations) {
      // Receive the messages and update the vertices.
      g = g.joinVertices(messages)(vprog).cache()
      val oldMessages = messages
      // Send new messages, skipping edges where neither side received a message. We must cache
      // messages so it can be materialized on the next line, allowing us to uncache the previous
      // iteration.
      messages = GraphXUtils.mapReduceTriplets(
        g, sendMsg, mergeMsg, Some((oldMessages, activeDirection))).cache()
      activeMessages = messages.count()
      i += 1
    }
    g
  }
}
Notice that Pregel takes two argument lists (i.e., graph.pregel(list1)(list2)).  The first
argument list contains configuration parameters including the initial message, the maximum number of
iterations, and the edge direction in which to send messages (by default along out edges).  The
second argument list contains the user defined functions for receiving messages (the vertex program
vprog), computing messages (sendMsg), and combining messages mergeMsg.
We can use the Pregel operator to express computation such as single source
shortest path in the following example.
import org.apache.spark.graphx.{Graph, VertexId}
import org.apache.spark.graphx.util.GraphGenerators

// A graph with edge attributes containing distances
val graph: Graph[Long, Double] =
  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapEdges(e => e.attr.toDouble)
val sourceId: VertexId = 42 // The ultimate source
// Initialize the graph such that all vertices except the root have distance infinity.
val initialGraph = graph.mapVertices((id, _) =>
    if (id == sourceId) 0.0 else Double.PositiveInfinity)
val sssp = initialGraph.pregel(Double.PositiveInfinity)(
  (id, dist, newDist) => math.min(dist, newDist), // Vertex Program
  triplet => {  // Send Message
    if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {
      Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))
    } else {
      Iterator.empty
    }
  },
  (a, b) => math.min(a, b) // Merge Message
)
println(sssp.vertices.collect.mkString("\n"))
Find full example code at "examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala" in the Spark repo.

Graph Builders
GraphX provides several ways of building a graph from a collection of vertices and edges in an RDD or on disk. None of the graph builders repartitions the graph’s edges by default; instead, edges are left in their default partitions (such as their original blocks in HDFS). Graph.groupEdges requires the graph to be repartitioned because it assumes identical edges will be colocated on the same partition, so you must call Graph.partitionBy before calling groupEdges.
object GraphLoader {
  def edgeListFile(
      sc: SparkContext,
      path: String,
      canonicalOrientation: Boolean = false,
      minEdgePartitions: Int = 1)
    : Graph[Int, Int]
}
GraphLoader.edgeListFile provides a way to load a graph from a list of edges on disk. It parses an adjacency list of (source vertex ID, destination vertex ID) pairs of the following form, skipping comment lines that begin with #:
# This is a comment
2 1
4 1
1 2

It creates a Graph from the specified edges, automatically creating any vertices mentioned by edges. All vertex and edge attributes default to 1. The canonicalOrientation argument allows reorienting edges in the positive direction (srcId < dstId), which is required by the connected components algorithm. The minEdgePartitions argument specifies the minimum number of edge partitions to generate; there may be more edge partitions than specified if, for example, the HDFS file has more blocks.
object Graph {
  def apply[VD, ED](
      vertices: RDD[(VertexId, VD)],
      edges: RDD[Edge[ED]],
      defaultVertexAttr: VD = null)
    : Graph[VD, ED]

  def fromEdges[VD, ED](
      edges: RDD[Edge[ED]],
      defaultValue: VD): Graph[VD, ED]

  def fromEdgeTuples[VD](
      rawEdges: RDD[(VertexId, VertexId)],
      defaultValue: VD,
      uniqueEdges: Option[PartitionStrategy] = None): Graph[VD, Int]

}
Graph.apply allows creating a graph from RDDs of vertices and edges. Duplicate vertices are picked arbitrarily and vertices found in the edge RDD but not the vertex RDD are assigned the default attribute.
Graph.fromEdges allows creating a graph from only an RDD of edges, automatically creating any vertices mentioned by edges and assigning them the default value.
Graph.fromEdgeTuples allows creating a graph from only an RDD of edge tuples, assigning the edges the value 1, and automatically creating any vertices mentioned by edges and assigning them the default value. It also supports deduplicating the edges; to deduplicate, pass Some of a PartitionStrategy as the uniqueEdges parameter (for example, uniqueEdges = Some(PartitionStrategy.RandomVertexCut)). A partition strategy is necessary to colocate identical edges on the same partition so they can be deduplicated.

Vertex and Edge RDDs
GraphX exposes RDD views of the vertices and edges stored within the graph.  However, because
GraphX maintains the vertices and edges in optimized data structures and these data structures
provide additional functionality, the vertices and edges are returned as VertexRDDVertexRDD and EdgeRDDEdgeRDD
respectively.  In this section we review some of the additional useful functionality in these types.
Note that this is just an incomplete list, please refer to the API docs for the official list of operations.
VertexRDDs
The VertexRDD[A] extends RDD[(VertexId, A)] and adds the additional constraint that each
VertexId occurs only once.  Moreover, VertexRDD[A] represents a set of vertices each with an
attribute of type A.  Internally, this is achieved by storing the vertex attributes in a reusable
hash-map data-structure.  As a consequence if two VertexRDDs are derived from the same base
VertexRDDVertexRDD (e.g., by filter or mapValues) they can be joined in constant time without hash
evaluations. To leverage this indexed data structure, the VertexRDDVertexRDD exposes the following
additional functionality:
class VertexRDD[VD] extends RDD[(VertexId, VD)] {
  // Filter the vertex set but preserves the internal index
  def filter(pred: Tuple2[VertexId, VD] => Boolean): VertexRDD[VD]
  // Transform the values without changing the ids (preserves the internal index)
  def mapValues[VD2](map: VD => VD2): VertexRDD[VD2]
  def mapValues[VD2](map: (VertexId, VD) => VD2): VertexRDD[VD2]
  // Show only vertices unique to this set based on their VertexId's
  def minus(other: RDD[(VertexId, VD)])
  // Remove vertices from this set that appear in the other set
  def diff(other: VertexRDD[VD]): VertexRDD[VD]
  // Join operators that take advantage of the internal indexing to accelerate joins (substantially)
  def leftJoin[VD2, VD3](other: RDD[(VertexId, VD2)])(f: (VertexId, VD, Option[VD2]) => VD3): VertexRDD[VD3]
  def innerJoin[U, VD2](other: RDD[(VertexId, U)])(f: (VertexId, VD, U) => VD2): VertexRDD[VD2]
  // Use the index on this RDD to accelerate a `reduceByKey` operation on the input RDD.
  def aggregateUsingIndex[VD2](other: RDD[(VertexId, VD2)], reduceFunc: (VD2, VD2) => VD2): VertexRDD[VD2]
}
Notice, for example,  how the filter operator returns a VertexRDDVertexRDD.  Filter is actually
implemented using a BitSet thereby reusing the index and preserving the ability to do fast joins
with other VertexRDDs.  Likewise, the mapValues operators do not allow the map function to
change the VertexId thereby enabling the same HashMap data structures to be reused.  Both the
leftJoin and innerJoin are able to identify when joining two VertexRDDs derived from the same
HashMap and implement the join by linear scan rather than costly point lookups.
The aggregateUsingIndex operator is useful for efficient construction of a new VertexRDDVertexRDD from an
RDD[(VertexId, A)].  Conceptually, if I have constructed a VertexRDD[B] over a set of vertices,
which is a super-set of the vertices in some RDD[(VertexId, A)] then I can reuse the index to
both aggregate and then subsequently index the RDD[(VertexId, A)].  For example:
val setA: VertexRDD[Int] = VertexRDD(sc.parallelize(0L until 100L).map(id => (id, 1)))
val rddB: RDD[(VertexId, Double)] = sc.parallelize(0L until 100L).flatMap(id => List((id, 1.0), (id, 2.0)))
// There should be 200 entries in rddB
rddB.count
val setB: VertexRDD[Double] = setA.aggregateUsingIndex(rddB, _ + _)
// There should be 100 entries in setB
setB.count
// Joining A and B should now be fast!
val setC: VertexRDD[Double] = setA.innerJoin(setB)((id, a, b) => a + b)
EdgeRDDs
The EdgeRDD[ED], which extends RDD[Edge[ED]] organizes the edges in blocks partitioned using one
of the various partitioning strategies defined in PartitionStrategy.  Within
each partition, edge attributes and adjacency structure, are stored separately enabling maximum
reuse when changing attribute values.
The three additional functions exposed by the EdgeRDDEdgeRDD are:
// Transform the edge attributes while preserving the structure
def mapValues[ED2](f: Edge[ED] => ED2): EdgeRDD[ED2]
// Reverse the edges reusing both attributes and structure
def reverse: EdgeRDD[ED]
// Join two `EdgeRDD`s partitioned using the same partitioning strategy.
def innerJoin[ED2, ED3](other: EdgeRDD[ED2])(f: (VertexId, VertexId, ED, ED2) => ED3): EdgeRDD[ED3]
In most applications we have found that operations on the EdgeRDDEdgeRDD are accomplished through the
graph operators or rely on operations defined in the base RDD class.
Optimized Representation
While a detailed description of the optimizations used in the GraphX representation of distributed
graphs is beyond the scope of this guide, some high-level understanding may aid in the design of
scalable algorithms as well as optimal use of the API.  GraphX adopts a vertex-cut approach to
distributed graph partitioning:




Rather than splitting graphs along edges, GraphX partitions the graph along vertices which can
reduce both the communication and storage overhead.  Logically, this corresponds to assigning edges
to machines and allowing vertices to span multiple machines.  The exact method of assigning edges
depends on the PartitionStrategy and there are several tradeoffs to the
various heuristics.  Users can choose between different strategies by repartitioning the graph with
the Graph.partitionBy operator.  The default partitioning strategy is to use
the initial partitioning of the edges as provided on graph construction.  However, users can easily
switch to 2D-partitioning or other heuristics included in GraphX.




Once the edges have been partitioned the key challenge to efficient graph-parallel computation is
efficiently joining vertex attributes with the edges.  Because real-world graphs typically have more
edges than vertices, we move vertex attributes to the edges.  Because not all partitions will
contain edges adjacent to all vertices we internally maintain a routing table which identifies where
to broadcast vertices when implementing the join required for operations like triplets and
aggregateMessages.

Graph Algorithms
GraphX includes a set of graph algorithms to simplify analytics tasks. The algorithms are contained in the org.apache.spark.graphx.lib package and can be accessed directly as methods on Graph via GraphOps. This section describes the algorithms and how they are used.

PageRank
PageRank measures the importance of each vertex in a graph, assuming an edge from u to v represents an endorsement of v’s importance by u. For example, if a Twitter user is followed by many others, the user will be ranked highly.
GraphX comes with static and dynamic implementations of PageRank as methods on the PageRank object. Static PageRank runs for a fixed number of iterations, while dynamic PageRank runs until the ranks converge (i.e., stop changing by more than a specified tolerance). GraphOps allows calling these algorithms directly as methods on Graph.
GraphX also includes an example social network dataset that we can run PageRank on. A set of users is given in data/graphx/users.txt, and a set of relationships between users is given in data/graphx/followers.txt. We compute the PageRank of each user as follows:
import org.apache.spark.graphx.GraphLoader

// Load the edges as a graph
val graph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt")
// Run PageRank
val ranks = graph.pageRank(0.0001).vertices
// Join the ranks with the usernames
val users = sc.textFile("data/graphx/users.txt").map { line =>
  val fields = line.split(",")
  (fields(0).toLong, fields(1))
}
val ranksByUsername = users.join(ranks).map {
  case (id, (username, rank)) => (username, rank)
}
// Print the result
println(ranksByUsername.collect().mkString("\n"))
Find full example code at "examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala" in the Spark repo.
Connected Components
The connected components algorithm labels each connected component of the graph with the ID of its lowest-numbered vertex. For example, in a social network, connected components can approximate clusters. GraphX contains an implementation of the algorithm in the ConnectedComponents object, and we compute the connected components of the example social network dataset from the PageRank section as follows:
import org.apache.spark.graphx.GraphLoader

// Load the graph as in the PageRank example
val graph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt")
// Find the connected components
val cc = graph.connectedComponents().vertices
// Join the connected components with the usernames
val users = sc.textFile("data/graphx/users.txt").map { line =>
  val fields = line.split(",")
  (fields(0).toLong, fields(1))
}
val ccByUsername = users.join(cc).map {
  case (id, (username, cc)) => (username, cc)
}
// Print the result
println(ccByUsername.collect().mkString("\n"))
Find full example code at "examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala" in the Spark repo.
Triangle Counting
A vertex is part of a triangle when it has two adjacent vertices with an edge between them. GraphX implements a triangle counting algorithm in the TriangleCount object that determines the number of triangles passing through each vertex, providing a measure of clustering. We compute the triangle count of the social network dataset from the PageRank section. Note that TriangleCount requires the edges to be in canonical orientation (srcId < dstId) and the graph to be partitioned using Graph.partitionBy.
import org.apache.spark.graphx.{GraphLoader, PartitionStrategy}

// Load the edges in canonical order and partition the graph for triangle count
val graph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt", true)
  .partitionBy(PartitionStrategy.RandomVertexCut)
// Find the triangle count for each vertex
val triCounts = graph.triangleCount().vertices
// Join the triangle counts with the usernames
val users = sc.textFile("data/graphx/users.txt").map { line =>
  val fields = line.split(",")
  (fields(0).toLong, fields(1))
}
val triCountByUsername = users.join(triCounts).map { case (id, (username, tc)) =>
  (username, tc)
}
// Print the result
println(triCountByUsername.collect().mkString("\n"))
Find full example code at "examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala" in the Spark repo.
Examples
Suppose I want to build a graph from some text files, restrict the graph
to important relationships and users, run page-rank on the subgraph, and
then finally return attributes associated with the top users.  I can do
all of this in just a few lines with GraphX:
import org.apache.spark.graphx.GraphLoader

// Load my user data and parse into tuples of user id and attribute list
val users = (sc.textFile("data/graphx/users.txt")
  .map(line => line.split(",")).map( parts => (parts.head.toLong, parts.tail) ))

// Parse the edge data which is already in userId -> userId format
val followerGraph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt")

// Attach the user attributes
val graph = followerGraph.outerJoinVertices(users) {
  case (uid, deg, Some(attrList)) => attrList
  // Some users may not have attributes so we set them as empty
  case (uid, deg, None) => Array.empty[String]
}

// Restrict the graph to users with usernames and names
val subgraph = graph.subgraph(vpred = (vid, attr) => attr.size == 2)

// Compute the PageRank
val pagerankGraph = subgraph.pageRank(0.001)

// Get the attributes of the top pagerank users
val userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) {
  case (uid, attrList, Some(pr)) => (pr, attrList.toList)
  case (uid, attrList, None) => (0.0, attrList.toList)
}

println(userInfoWithPageRank.vertices.top(5)(Ordering.by(_._2._1)).mkString("\n"))
Find full example code at "examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala" in the Spark repo.




















  




Hardware Provisioning - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Hardware Provisioning
A common question received by Spark developers is how to configure hardware for it. While the right
hardware will depend on the situation, we make the following recommendations.
Storage Systems
Because most Spark jobs will likely have to read input data from an external storage system (e.g.
the Hadoop File System, or HBase), it is important to place it as close to this system as
possible. We recommend the following:


If at all possible, run Spark on the same nodes as HDFS. The simplest way is to set up a Spark
standalone mode cluster on the same nodes, and configure Spark and
Hadoop’s memory and CPU usage to avoid interference (for Hadoop, the relevant options are
mapred.child.java.opts for the per-task memory and mapreduce.tasktracker.map.tasks.maximum
and mapreduce.tasktracker.reduce.tasks.maximum for number of tasks). Alternatively, you can run
Hadoop and Spark on a common cluster manager like Mesos or
Hadoop YARN.


If this is not possible, run Spark on different nodes in the same local-area network as HDFS.


For low-latency data stores like HBase, it may be preferable to run computing jobs on different
nodes than the storage system to avoid interference.


Local Disks
While Spark can perform a lot of its computation in memory, it still uses local disks to store
data that doesn’t fit in RAM, as well as to preserve intermediate output between stages. We
recommend having 4-8 disks per node, configured without RAID (just as separate mount points).
In Linux, mount the disks with the noatime option
to reduce unnecessary writes. In Spark, configure the spark.local.dir
variable to be a comma-separated list of the local disks. If you are running HDFS, it’s fine to
use the same disks as HDFS.
Memory
In general, Spark can run well with anywhere from 8 GiB to hundreds of gigabytes of memory per
machine. In all cases, we recommend allocating only at most 75% of the memory for Spark; leave the
rest for the operating system and buffer cache.
How much memory you will need will depend on your application. To determine how much your
application uses for a certain dataset size, load part of your dataset in a Spark RDD and use the
Storage tab of Spark’s monitoring UI (http://<driver-node>:4040) to see its size in memory.
Note that memory usage is greatly affected by storage level and serialization format – see
the tuning guide for tips on how to reduce it.
Finally, note that the Java VM does not always behave well with more than 200 GiB of RAM. If you
purchase machines with more RAM than this, you can launch multiple executors in a single node. In
Spark’s standalone mode, a worker is responsible for launching multiple
executors according to its available memory and cores, and each executor will be launched in a
separate Java VM.
Network
In our experience, when the data is in memory, a lot of Spark applications are network-bound.
Using a 10 Gigabit or higher network is the best way to make these applications faster.
This is especially true for “distributed reduce” applications such as group-bys, reduce-bys, and
SQL joins. In any given application, you can see how much data Spark shuffles across the network
from the application’s monitoring UI (http://<driver-node>:4040).
CPU Cores
Spark scales well to tens of CPU cores per machine because it performs minimal sharing between
threads. You should likely provision at least 8-16 cores per machine. Depending on the CPU
cost of your workload, you may also need more: once data is in memory, most applications are
either CPU- or network-bound.




















  




Overview - Spark 3.5.5 Documentation



















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Apache Spark - A Unified engine for large-scale data analytics


                  Apache Spark is a unified analytics engine for large-scale data processing.
                  It provides high-level APIs in Java, Scala, Python and R,
                  and an optimized engine that supports general execution graphs.
                  It also supports a rich set of higher-level tools including
                  Spark SQL for SQL and structured data processing,
                  pandas API on Spark for pandas workloads,
                  MLlib for machine learning,
                  GraphX for graph processing,
                   and Structured Streaming
                   for incremental computation and stream processing.
                




Downloading
Get Spark from the downloads page of the project website. This documentation is for Spark version 3.5.5. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions.
Users can also download a “Hadoop free” binary and run Spark with any Hadoop version
by augmenting Spark’s classpath.
Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI.
If you’d like to build Spark from
source, visit Building Spark.
Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.
Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.8+, and R 3.5+.
Java 8 prior to version 8u371 support is deprecated as of Spark 3.5.0.
When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for.
For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.
For Java 11, setting -Dio.netty.tryReflectionSetAccessible=true is required for the Apache Arrow library. This prevents the java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available error when Apache Arrow uses Netty internally.
Running the Examples and Shell
Spark comes with several sample programs. Python, Scala, Java, and R examples are in the
examples/src/main directory.
To run Spark interactively in a Python interpreter, use
bin/pyspark:
./bin/pyspark --master "local[2]"

Sample applications are provided in Python. For example:
./bin/spark-submit examples/src/main/python/pi.py 10

To run one of the Scala or Java sample programs, use
bin/run-example <class> [params] in the top-level Spark directory. (Behind the scenes, this
invokes the more general
spark-submit script for
launching applications). For example,
./bin/run-example SparkPi 10

You can also run Spark interactively through a modified version of the Scala shell. This is a
great way to learn the framework.
./bin/spark-shell --master "local[2]"

The --master option specifies the
master URL for a distributed cluster, or local to run
locally with one thread, or local[N] to run locally with N threads. You should start by using
local for testing. For a full list of options, run the Spark shell with the --help option.
Since version 1.4, Spark has provided an R API (only the DataFrame APIs are included).
To run Spark interactively in an R interpreter, use bin/sparkR:
./bin/sparkR --master "local[2]"

Example applications are also provided in R. For example:
./bin/spark-submit examples/src/main/r/dataframe.R

Running Spark Client Applications Anywhere with Spark Connect
Spark Connect is a new client-server architecture introduced in Spark 3.4 that decouples Spark
client applications and allows remote connectivity to Spark clusters. The separation between
client and server allows Spark and its open ecosystem to be leveraged from anywhere, embedded
in any application. In Spark 3.4, Spark Connect provides DataFrame API coverage for PySpark and
DataFrame/Dataset API support in Scala.
To learn more about Spark Connect and how to use it, see Spark Connect Overview.
Launching on a Cluster
The Spark cluster mode overview explains the key concepts in running on a cluster.
Spark can run both by itself, or over several existing cluster managers. It currently provides several
options for deployment:

Standalone Deploy Mode: simplest way to deploy Spark on a private cluster
Apache Mesos (deprecated)
Hadoop YARN
Kubernetes

Where to Go from Here
Programming Guides:

Quick Start: a quick introduction to the Spark API; start here!
RDD Programming Guide: overview of Spark basics - RDDs (core but old API), accumulators, and broadcast variables
Spark SQL, Datasets, and DataFrames: processing structured data with relational queries (newer API than RDDs)
Structured Streaming: processing structured data streams with relation queries (using Datasets and DataFrames, newer API than DStreams)
Spark Streaming: processing data streams using DStreams (old API)
MLlib: applying machine learning algorithms
GraphX: processing graphs
SparkR: processing data with Spark in R
PySpark: processing data with Spark in Python
Spark SQL CLI: processing data with SQL on the command line

API Docs:

Spark Scala API (Scaladoc)
Spark Java API (Javadoc)
Spark Python API (Sphinx)
Spark R API (Roxygen2)
Spark SQL, Built-in Functions (MkDocs)

Deployment Guides:

Cluster Overview: overview of concepts and components when running on a cluster
Submitting Applications: packaging and deploying applications
Deployment modes:
    
Amazon EC2: scripts that let you launch a cluster on EC2 in about 5 minutes
Standalone Deploy Mode: launch a standalone cluster quickly without a third-party cluster manager
Mesos: deploy a private cluster using
  Apache Mesos
YARN: deploy Spark on top of Hadoop NextGen (YARN)
Kubernetes: deploy Spark on top of Kubernetes



Other Documents:

Configuration: customize Spark via its configuration system
Monitoring: track the behavior of your applications
Tuning Guide: best practices to optimize performance and memory use
Job Scheduling: scheduling resources across and within Spark applications
Security: Spark security support
Hardware Provisioning: recommendations for cluster hardware
Integration with other storage systems:
    
Cloud Infrastructures
OpenStack Swift


Migration Guide: Migration guides for Spark components
Building Spark: build Spark using the Maven system
Contributing to Spark
Third Party Projects: related third party Spark projects

External Resources:

Spark Homepage
Spark Community resources, including local meetups
StackOverflow tag apache-spark
Mailing Lists: ask questions about Spark here
AMP Camps: a series of training camps at UC Berkeley that featured talks and
exercises about Spark, Spark Streaming, Mesos, and more. Videos,
are available online for free.
Code Examples: more are also available in the examples subfolder of Spark (Scala,
 Java,
 Python,
 R)





















  




Job Scheduling - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Job Scheduling

Overview
Scheduling Across Applications 
Dynamic Resource Allocation 
Caveats
Configuration and Setup
Resource Allocation Policy 
Request Policy
Remove Policy


Graceful Decommission of Executors




Scheduling Within an Application 
Fair Scheduler Pools
Default Behavior of Pools
Configuring Pool Properties
Scheduling using JDBC Connections
Concurrent Jobs in PySpark



Overview
Spark has several facilities for scheduling resources between computations. First, recall that, as described
in the cluster mode overview, each Spark application (instance of SparkContext)
runs an independent set of executor processes. The cluster managers that Spark runs on provide
facilities for scheduling across applications. Second,
within each Spark application, multiple “jobs” (Spark actions) may be running concurrently
if they were submitted by different threads. This is common if your application is serving requests
over the network. Spark includes a fair scheduler to schedule resources within each SparkContext.
Scheduling Across Applications
When running on a cluster, each Spark application gets an independent set of executor JVMs that only
run tasks and store data for that application. If multiple users need to share your cluster, there are
different options to manage allocation, depending on the cluster manager.
The simplest option, available on all cluster managers, is static partitioning of resources. With
this approach, each application is given a maximum amount of resources it can use and holds onto them
for its whole duration. This is the approach used in Spark’s standalone
and YARN modes, as well as the
coarse-grained Mesos mode.
Resource allocation can be configured as follows, based on the cluster type:

Standalone mode: By default, applications submitted to the standalone mode cluster will run in
FIFO (first-in-first-out) order, and each application will try to use all available nodes. You can limit
the number of nodes an application uses by setting the spark.cores.max configuration property in it,
or change the default for applications that don’t set this setting through spark.deploy.defaultCores.
Finally, in addition to controlling cores, each application’s spark.executor.memory setting controls
its memory use.
Mesos: To use static partitioning on Mesos, set the spark.mesos.coarse configuration property to true,
and optionally set spark.cores.max to limit each application’s resource share as in the standalone mode.
You should also set spark.executor.memory to control the executor memory.
YARN: The --num-executors option to the Spark YARN client controls how many executors it will allocate
on the cluster (spark.executor.instances as configuration property), while --executor-memory
(spark.executor.memory configuration property) and --executor-cores (spark.executor.cores configuration
property) control the resources per executor. For more information, see the
YARN Spark Properties.

A second option available on Mesos is dynamic sharing of CPU cores. In this mode, each Spark application
still has a fixed and independent memory allocation (set by spark.executor.memory), but when the
application is not running tasks on a machine, other applications may run tasks on those cores. This mode
is useful when you expect large numbers of not overly active applications, such as shell sessions from
separate users. However, it comes with a risk of less predictable latency, because it may take a while for
an application to gain back cores on one node when it has work to do. To use this mode, simply use a
mesos:// URL and set spark.mesos.coarse to false.
Note that none of the modes currently provide memory sharing across applications. If you would like to share
data this way, we recommend running a single server application that can serve multiple requests by querying
the same RDDs.
Dynamic Resource Allocation
Spark provides a mechanism to dynamically adjust the resources your application occupies based
on the workload. This means that your application may give resources back to the cluster if they
are no longer used and request them again later when there is demand. This feature is particularly
useful if multiple applications share resources in your Spark cluster.
This feature is disabled by default and available on all coarse-grained cluster managers, i.e.
standalone mode, YARN mode,
Mesos coarse-grained mode and K8s mode.
Caveats

In standalone mode, without explicitly setting spark.executor.cores, each executor will get all the available cores of a worker. In this case, when dynamic allocation enabled, spark will possibly acquire much more executors than expected. When you want to use dynamic allocation in standalone mode, you are recommended to explicitly set cores for each executor before the issue SPARK-30299 got fixed.

Configuration and Setup
There are several ways for using this feature.
Regardless of which approach you choose, your application must set spark.dynamicAllocation.enabled to true first, additionally,

your application must set spark.shuffle.service.enabled to true after you set up an external shuffle service on each worker node in the same cluster, or
your application must set spark.dynamicAllocation.shuffleTracking.enabled to true, or
your application must set both spark.decommission.enabled and spark.storage.decommission.shuffleBlocks.enabled to true, or
your application must configure spark.shuffle.sort.io.plugin.class to use a custom ShuffleDataIO who’s ShuffleDriverComponents supports reliable storage.

The purpose of the external shuffle service or the shuffle tracking or the ShuffleDriverComponents supports reliable storage is to allow executors to be removed
without deleting shuffle files written by them (more detail described
below). While it is simple to enable shuffle tracking, the way to set up the external shuffle service varies across cluster managers:
In standalone mode, simply start your workers with spark.shuffle.service.enabled set to true.
In Mesos coarse-grained mode, run $SPARK_HOME/sbin/start-mesos-shuffle-service.sh on all
worker nodes with spark.shuffle.service.enabled set to true. For instance, you may do so
through Marathon.
In YARN mode, follow the instructions here.
All other relevant configurations are optional and under the spark.dynamicAllocation.* and
spark.shuffle.service.* namespaces. For more detail, see the
configurations page.
Resource Allocation Policy
At a high level, Spark should relinquish executors when they are no longer used and acquire
executors when they are needed. Since there is no definitive way to predict whether an executor
that is about to be removed will run a task in the near future, or whether a new executor that is
about to be added will actually be idle, we need a set of heuristics to determine when to remove
and request executors.
Request Policy
A Spark application with dynamic allocation enabled requests additional executors when it has
pending tasks waiting to be scheduled. This condition necessarily implies that the existing set
of executors is insufficient to simultaneously saturate all tasks that have been submitted but
not yet finished.
Spark requests executors in rounds. The actual request is triggered when there have been pending
tasks for spark.dynamicAllocation.schedulerBacklogTimeout seconds, and then triggered again
every spark.dynamicAllocation.sustainedSchedulerBacklogTimeout seconds thereafter if the queue
of pending tasks persists. Additionally, the number of executors requested in each round increases
exponentially from the previous round. For instance, an application will add 1 executor in the
first round, and then 2, 4, 8 and so on executors in the subsequent rounds.
The motivation for an exponential increase policy is twofold. First, an application should request
executors cautiously in the beginning in case it turns out that only a few additional executors is
sufficient. This echoes the justification for TCP slow start. Second, the application should be
able to ramp up its resource usage in a timely manner in case it turns out that many executors are
actually needed.
Remove Policy
The policy for removing executors is much simpler. A Spark application removes an executor when
it has been idle for more than spark.dynamicAllocation.executorIdleTimeout seconds. Note that,
under most circumstances, this condition is mutually exclusive with the request condition, in that
an executor should not be idle if there are still pending tasks to be scheduled.
Graceful Decommission of Executors
Before dynamic allocation, if a Spark executor exits when the associated application has also exited 
then all state associated with the executor is no longer needed and can be safely discarded. 
With dynamic allocation, however, the application is still running when an executor is explicitly 
removed. If the application attempts to access state stored in or written by the executor, it will 
have to perform a recompute the state. Thus, Spark needs a mechanism to decommission an executor 
gracefully by preserving its state before removing it.
This requirement is especially important for shuffles. During a shuffle, the Spark executor first
writes its own map outputs locally to disk, and then acts as the server for those files when other
executors attempt to fetch them. In the event of stragglers, which are tasks that run for much
longer than their peers, dynamic allocation may remove an executor before the shuffle completes,
in which case the shuffle files written by that executor must be recomputed unnecessarily.
The solution for preserving shuffle files is to use an external shuffle service, also introduced
in Spark 1.2. This service refers to a long-running process that runs on each node of your cluster
independently of your Spark applications and their executors. If the service is enabled, Spark
executors will fetch shuffle files from the service instead of from each other. This means any
shuffle state written by an executor may continue to be served beyond the executor’s lifetime.
In addition to writing shuffle files, executors also cache data either on disk or in memory.
When an executor is removed, however, all cached data will no longer be accessible.  To mitigate this,
by default executors containing cached data are never removed.  You can configure this behavior with
spark.dynamicAllocation.cachedExecutorIdleTimeout. When set spark.shuffle.service.fetch.rdd.enabled
to true, Spark can use ExternalShuffleService for fetching disk persisted RDD blocks. In case of 
dynamic allocation if this feature is enabled executors having only disk persisted blocks are considered
idle after spark.dynamicAllocation.executorIdleTimeout and will be released accordingly. In future releases,
the cached data may be preserved through an off-heap storage similar in spirit to how shuffle files are preserved 
through the external shuffle service.
Scheduling Within an Application
Inside a given Spark application (SparkContext instance), multiple parallel jobs can run simultaneously if
they were submitted from separate threads. By “job”, in this section, we mean a Spark action (e.g. save,
collect) and any tasks that need to run to evaluate that action. Spark’s scheduler is fully thread-safe
and supports this use case to enable applications that serve multiple requests (e.g. queries for
multiple users).
By default, Spark’s scheduler runs jobs in FIFO fashion. Each job is divided into “stages” (e.g. map and
reduce phases), and the first job gets priority on all available resources while its stages have tasks to
launch, then the second job gets priority, etc. If the jobs at the head of the queue don’t need to use
the whole cluster, later jobs can start to run right away, but if the jobs at the head of the queue are
large, then later jobs may be delayed significantly.
Starting in Spark 0.8, it is also possible to configure fair sharing between jobs. Under fair sharing,
Spark assigns tasks between jobs in a “round robin” fashion, so that all jobs get a roughly equal share
of cluster resources. This means that short jobs submitted while a long job is running can start receiving
resources right away and still get good response times, without waiting for the long job to finish. This
mode is best for multi-user settings.
This feature is disabled by default and available on all coarse-grained cluster managers, i.e.
standalone mode, YARN mode,
K8s mode and Mesos coarse-grained mode.
To enable the fair scheduler, simply set the spark.scheduler.mode property to FAIR when configuring
a SparkContext:
val conf = new SparkConf().setMaster(...).setAppName(...)
conf.set("spark.scheduler.mode", "FAIR")
val sc = new SparkContext(conf)
Fair Scheduler Pools
The fair scheduler also supports grouping jobs into pools, and setting different scheduling options
(e.g. weight) for each pool. This can be useful to create a “high-priority” pool for more important jobs,
for example, or to group the jobs of each user together and give users equal shares regardless of how
many concurrent jobs they have instead of giving jobs equal shares. This approach is modeled after the
Hadoop Fair Scheduler.
Without any intervention, newly submitted jobs go into a default pool, but jobs’ pools can be set by
adding the spark.scheduler.pool “local property” to the SparkContext in the thread that’s submitting them.
This is done as follows:
// Assuming sc is your SparkContext variable
sc.setLocalProperty("spark.scheduler.pool", "pool1")
After setting this local property, all jobs submitted within this thread (by calls in this thread
to RDD.save, count, collect, etc) will use this pool name. The setting is per-thread to make
it easy to have a thread run multiple jobs on behalf of the same user. If you’d like to clear the
pool that a thread is associated with, simply call:
sc.setLocalProperty("spark.scheduler.pool", null)
Default Behavior of Pools
By default, each pool gets an equal share of the cluster (also equal in share to each job in the default
pool), but inside each pool, jobs run in FIFO order. For example, if you create one pool per user, this
means that each user will get an equal share of the cluster, and that each user’s queries will run in
order instead of later queries taking resources from that user’s earlier ones.
Configuring Pool Properties
Specific pools’ properties can also be modified through a configuration file. Each pool supports three
properties:

schedulingMode: This can be FIFO or FAIR, to control whether jobs within the pool queue up behind
each other (the default) or share the pool’s resources fairly.
weight: This controls the pool’s share of the cluster relative to other pools. By default, all pools
have a weight of 1. If you give a specific pool a weight of 2, for example, it will get 2x more
resources as other active pools. Setting a high weight such as 1000 also makes it possible to implement
priority between pools—in essence, the weight-1000 pool will always get to launch tasks first
whenever it has jobs active.
minShare: Apart from an overall weight, each pool can be given a minimum shares (as a number of
CPU cores) that the administrator would like it to have. The fair scheduler always attempts to meet
all active pools’ minimum shares before redistributing extra resources according to the weights.
The minShare property can, therefore, be another way to ensure that a pool can always get up to a
certain number of resources (e.g. 10 cores) quickly without giving it a high priority for the rest
of the cluster. By default, each pool’s minShare is 0.

The pool properties can be set by creating an XML file, similar to conf/fairscheduler.xml.template,
and either putting a file named fairscheduler.xml on the classpath, or setting spark.scheduler.allocation.file property in your
SparkConf. The file path respects the hadoop configuration and can either be a local file path or HDFS file path.
// scheduler file at local
conf.set("spark.scheduler.allocation.file", "file:///path/to/file")
// scheduler file at hdfs
conf.set("spark.scheduler.allocation.file", "hdfs:///path/to/file")
The format of the XML file is simply a <pool> element for each pool, with different elements
within it for the various settings. For example:
<?xml version="1.0"?>
<allocations>
  <pool name="production">
    <schedulingMode>FAIR</schedulingMode>
    <weight>1</weight>
    <minShare>2</minShare>
  </pool>
  <pool name="test">
    <schedulingMode>FIFO</schedulingMode>
    <weight>2</weight>
    <minShare>3</minShare>
  </pool>
</allocations>
A full example is also available in conf/fairscheduler.xml.template. Note that any pools not
configured in the XML file will simply get default values for all settings (scheduling mode FIFO,
weight 1, and minShare 0).
Scheduling using JDBC Connections
To set a Fair Scheduler pool for a JDBC client session,
users can set the spark.sql.thriftserver.scheduler.pool variable:
SET spark.sql.thriftserver.scheduler.pool=accounting;
Concurrent Jobs in PySpark
PySpark, by default, does not support to synchronize PVM threads with JVM threads and 
launching multiple jobs in multiple PVM threads does not guarantee to launch each job
in each corresponding JVM thread. Due to this limitation, it is unable to set a different job group
via sc.setJobGroup in a separate PVM thread, which also disallows to cancel the job via sc.cancelJobGroup
later.
pyspark.InheritableThread is recommended to use together for a PVM thread to inherit the inheritable attributes
 such as local properties in a JVM thread.




















  




Migration Guide - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Migration Guide
This page documents sections of the migration guide for each component in order
for users to migrate effectively.

Spark Core
SQL, Datasets, and DataFrame
Structured Streaming
MLlib (Machine Learning)
PySpark (Python on Spark)
SparkR (R on Spark)





















  




MLlib: Main Guide - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












MLlib: Main Guide



            
                Basic statistics
            
        



            
                Data sources
            
        



            
                Pipelines
            
        



            
                Extracting, transforming and selecting features
            
        



            
                Classification and Regression
            
        



            
                Clustering
            
        



            
                Collaborative filtering
            
        



            
                Frequent Pattern Mining
            
        



            
                Model selection and tuning
            
        



            
                Advanced topics
            
        


MLlib: RDD-based API Guide



            
                Data types
            
        



            
                Basic statistics
            
        



            
                Classification and regression
            
        



            
                Collaborative filtering
            
        



            
                Clustering
            
        



            
                Dimensionality reduction
            
        



            
                Feature extraction and transformation
            
        



            
                Frequent pattern mining
            
        



            
                Evaluation metrics
            
        



            
                PMML model export
            
        



            
                Optimization (developer)
            
        







Machine Learning Library (MLlib) Guide
MLlib is Spark’s machine learning (ML) library.
Its goal is to make practical machine learning scalable and easy.
At a high level, it provides tools such as:

ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering
Featurization: feature extraction, transformation, dimensionality reduction, and selection
Pipelines: tools for constructing, evaluating, and tuning ML Pipelines
Persistence: saving and load algorithms, models, and Pipelines
Utilities: linear algebra, statistics, data handling, etc.

Announcement: DataFrame-based API is primary API
The MLlib RDD-based API is now in maintenance mode.
As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode.
The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.
What are the implications?

MLlib will still support the RDD-based API in spark.mllib with bug fixes.
MLlib will not add new features to the RDD-based API.
In the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.

Why is MLlib switching to the DataFrame-based API?

DataFrames provide a more user-friendly API than RDDs.  The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.
The DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.
DataFrames facilitate practical ML Pipelines, particularly feature transformations.  See the Pipelines guide for details.

What is “Spark ML”?

“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API.
This is majorly due to the org.apache.spark.ml Scala package name used by the DataFrame-based API, 
and the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.

Is MLlib deprecated?

No. MLlib includes both the RDD-based API and the DataFrame-based API.
The RDD-based API is now in maintenance mode.
But neither API is deprecated, nor MLlib as a whole.

Dependencies
MLlib uses linear algebra packages Breeze and dev.ludovic.netlib for optimised numerical processing1. Those packages may call native acceleration libraries such as Intel MKL or OpenBLAS if they are available as system libraries or in runtime library paths.
However, native acceleration libraries can’t be distributed with Spark. See MLlib Linear Algebra Acceleration Guide for how to enable accelerated linear algebra processing. If accelerated native libraries are not enabled, you will see a warning message like below and a pure JVM implementation will be used instead:
WARNING: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS

To use MLlib in Python, you will need NumPy version 1.4 or newer.
Highlights in 3.0
The list below highlights some of the new features and enhancements added to MLlib in the 3.0
release of Spark:

Multiple columns support was added to Binarizer (SPARK-23578), StringIndexer (SPARK-11215), StopWordsRemover (SPARK-29808) and PySpark QuantileDiscretizer (SPARK-22796).
Tree-Based Feature Transformation was added
(SPARK-13677).
Two new evaluators MultilabelClassificationEvaluator (SPARK-16692) and RankingEvaluator (SPARK-28045) were added.
Sample weights support was added in DecisionTreeClassifier/Regressor (SPARK-19591), RandomForestClassifier/Regressor (SPARK-9478), GBTClassifier/Regressor (SPARK-9612),  MulticlassClassificationEvaluator (SPARK-24101), RegressionEvaluator (SPARK-24102), BinaryClassificationEvaluator (SPARK-24103), BisectingKMeans (SPARK-30351), KMeans (SPARK-29967) and GaussianMixture (SPARK-30102).
R API for PowerIterationClustering was added
(SPARK-19827).
Added Spark ML listener for tracking ML pipeline status
(SPARK-23674).
Fit with validation set was added to Gradient Boosted Trees in Python
(SPARK-24333).
RobustScaler transformer was added
(SPARK-28399).
Factorization Machines classifier and regressor were added
(SPARK-29224).
Gaussian Naive Bayes Classifier (SPARK-16872) and Complement Naive Bayes Classifier (SPARK-29942) were added.
ML function parity between Scala and Python
(SPARK-28958).
predictRaw is made public in all the Classification models. predictProbability is made public in all the Classification models except LinearSVCModel
(SPARK-30358).

Migration Guide
The migration guide is now archived on this page.



To learn more about the benefits and background of system optimised natives, you may wish to
watch Sam Halliday’s ScalaX talk on High Performance Linear Algebra in Scala. ↩























  




Monitoring and Instrumentation - Spark 3.5.5 Documentation



















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Monitoring and Instrumentation

Web Interfaces 
Viewing After the Fact 
Environment Variables
Applying compaction on rolling event log files
Spark History Server Configuration Options


REST API 
Executor Task Metrics
Executor Metrics
API Versioning Policy




Metrics 
List of available metrics providers 
Component instance = Driver
Component instance = Executor
Source = JVM Source
Component instance = applicationMaster
Component instance = mesos_cluster
Component instance = master
Component instance = ApplicationSource
Component instance = worker
Component instance = shuffleService




Advanced Instrumentation

There are several ways to monitor Spark applications: web UIs, metrics, and external instrumentation.
Web Interfaces
Every SparkContext launches a Web UI, by default on port 4040, that
displays useful information about the application. This includes:

A list of scheduler stages and tasks
A summary of RDD sizes and memory usage
Environmental information.
Information about the running executors

You can access this interface by simply opening http://<driver-node>:4040 in a web browser.
If multiple SparkContexts are running on the same host, they will bind to successive ports
beginning with 4040 (4041, 4042, etc).
Note that this information is only available for the duration of the application by default.
To view the web UI after the fact, set spark.eventLog.enabled to true before starting the
application. This configures Spark to log Spark events that encode the information displayed
in the UI to persisted storage.
Viewing After the Fact
It is still possible to construct the UI of an application through Spark’s history server,
provided that the application’s event logs exist.
You can start the history server by executing:
./sbin/start-history-server.sh

This creates a web interface at http://<server-url>:18080 by default, listing incomplete
and completed applications and attempts.
When using the file-system provider class (see spark.history.provider below), the base logging
directory must be supplied in the spark.history.fs.logDirectory configuration option,
and should contain sub-directories that each represents an application’s event logs.
The spark jobs themselves must be configured to log events, and to log them to the same shared,
writable directory. For example, if the server was configured with a log directory of
hdfs://namenode/shared/spark-logs, then the client-side options would be:
spark.eventLog.enabled true
spark.eventLog.dir hdfs://namenode/shared/spark-logs

The history server can be configured as follows:
Environment Variables

Environment VariableMeaning

SPARK_DAEMON_MEMORY
Memory to allocate to the history server (default: 1g).


SPARK_DAEMON_JAVA_OPTS
JVM options for the history server (default: none).


SPARK_DAEMON_CLASSPATH
Classpath for the history server (default: none).


SPARK_PUBLIC_DNS

      The public address for the history server. If this is not set, links to application history
      may use the internal address of the server, resulting in broken links (default: none).
    


SPARK_HISTORY_OPTS

spark.history.* configuration options for the history server (default: none).
    


Applying compaction on rolling event log files
A long-running application (e.g. streaming) can bring a huge single event log file which may cost a lot to maintain and
also requires a bunch of resource to replay per each update in Spark History Server.
Enabling spark.eventLog.rolling.enabled and spark.eventLog.rolling.maxFileSize would
let you have rolling event log files instead of single huge event log file which may help some scenarios on its own,
but it still doesn’t help you reducing the overall size of logs.
Spark History Server can apply compaction on the rolling event log files to reduce the overall size of
logs, via setting the configuration spark.history.fs.eventLog.rolling.maxFilesToRetain on the
Spark History Server.
Details will be described below, but please note in prior that compaction is LOSSY operation.
Compaction will discard some events which will be no longer seen on UI - you may want to check which events will be discarded
before enabling the option.
When the compaction happens, the History Server lists all the available event log files for the application, and considers
the event log files having less index than the file with smallest index which will be retained as target of compaction.
For example, if the application A has 5 event log files and spark.history.fs.eventLog.rolling.maxFilesToRetain is set to 2, then first 3 log files will be selected to be compacted.
Once it selects the target, it analyzes them to figure out which events can be excluded, and rewrites them
into one compact file with discarding events which are decided to exclude.
The compaction tries to exclude the events which point to the outdated data. As of now, below describes the candidates of events to be excluded:

Events for the job which is finished, and related stage/tasks events
Events for the executor which is terminated
Events for the SQL execution which is finished, and related job/stage/tasks events

Once rewriting is done, original log files will be deleted, via best-effort manner. The History Server may not be able to delete
the original log files, but it will not affect the operation of the History Server.
Please note that Spark History Server may not compact the old event log files if figures out not a lot of space
would be reduced during compaction. For streaming query we normally expect compaction
will run as each micro-batch will trigger one or more jobs which will be finished shortly, but compaction won’t run
in many cases for batch query.
Please also note that this is a new feature introduced in Spark 3.0, and may not be completely stable. Under some circumstances,
the compaction may exclude more events than you expect, leading some UI issues on History Server for the application.
Use it with caution.
Spark History Server Configuration Options
Security options for the Spark History Server are covered more detail in the
Security page.



Property Name
Default
Meaning
Since Version



spark.history.provider
org.apache.spark.deploy.history.FsHistoryProvider
Name of the class implementing the application history backend. Currently there is only
    one implementation, provided by Spark, which looks for application logs stored in the
    file system.
1.1.0


spark.history.fs.logDirectory
file:/tmp/spark-events

    For the filesystem history provider, the URL to the directory containing application event
    logs to load. This can be a local file:// path,
    an HDFS path hdfs://namenode/shared/spark-logs
    or that of an alternative filesystem supported by the Hadoop APIs.
    
1.1.0


spark.history.fs.update.interval
10s

      The period at which the filesystem history provider checks for new or
      updated logs in the log directory. A shorter interval detects new applications faster,
      at the expense of more server load re-reading updated applications.
      As soon as an update has completed, listings of the completed and incomplete applications
      will reflect the changes.
    
1.4.0


spark.history.retainedApplications
50

      The number of applications to retain UI data for in the cache. If this cap is exceeded, then
      the oldest applications will be removed from the cache. If an application is not in the cache,
      it will have to be loaded from disk if it is accessed from the UI.
    
1.0.0


spark.history.ui.maxApplications
Int.MaxValue

      The number of applications to display on the history summary page. Application UIs are still
      available by accessing their URLs directly even if they are not displayed on the history summary page.
    
2.0.1


spark.history.ui.port
18080

      The port to which the web interface of the history server binds.
    
1.0.0


spark.history.kerberos.enabled
false

      Indicates whether the history server should use kerberos to login. This is required
      if the history server is accessing HDFS files on a secure Hadoop cluster.
    
1.0.1


spark.history.kerberos.principal
(none)

      When spark.history.kerberos.enabled=true, specifies kerberos principal name for the History Server.
    
1.0.1


spark.history.kerberos.keytab
(none)

      When spark.history.kerberos.enabled=true, specifies location of the kerberos keytab file for the History Server.
    
1.0.1


spark.history.fs.cleaner.enabled
false

      Specifies whether the History Server should periodically clean up event logs from storage.
    
1.4.0


spark.history.fs.cleaner.interval
1d

      When spark.history.fs.cleaner.enabled=true, specifies how often the filesystem job history cleaner checks for files to delete.
      Files are deleted if at least one of two conditions holds.
      First, they're deleted if they're older than spark.history.fs.cleaner.maxAge.
      They are also deleted if the number of files is more than
      spark.history.fs.cleaner.maxNum, Spark tries to clean up the completed attempts
      from the applications based on the order of their oldest attempt time.
    
1.4.0


spark.history.fs.cleaner.maxAge
7d

      When spark.history.fs.cleaner.enabled=true, job history files older than this will be deleted when the filesystem history cleaner runs.
    
1.4.0


spark.history.fs.cleaner.maxNum
Int.MaxValue

      When spark.history.fs.cleaner.enabled=true, specifies the maximum number of files in the event log directory.
      Spark tries to clean up the completed attempt logs to maintain the log directory under this limit.
      This should be smaller than the underlying file system limit like
      `dfs.namenode.fs-limits.max-directory-items` in HDFS.
    
3.0.0


spark.history.fs.endEventReparseChunkSize
1m

      How many bytes to parse at the end of log files looking for the end event.
      This is used to speed up generation of application listings by skipping unnecessary
      parts of event log files. It can be disabled by setting this config to 0.
    
2.4.0


spark.history.fs.inProgressOptimization.enabled
true

      Enable optimized handling of in-progress logs. This option may leave finished
      applications that fail to rename their event logs listed as in-progress.
    
2.4.0


spark.history.fs.driverlog.cleaner.enabled
spark.history.fs.cleaner.enabled

      Specifies whether the History Server should periodically clean up driver logs from storage.
    
3.0.0


spark.history.fs.driverlog.cleaner.interval
spark.history.fs.cleaner.interval

      When spark.history.fs.driverlog.cleaner.enabled=true, specifies how often the filesystem driver log cleaner checks for files to delete.
      Files are only deleted if they are older than spark.history.fs.driverlog.cleaner.maxAge

3.0.0


spark.history.fs.driverlog.cleaner.maxAge
spark.history.fs.cleaner.maxAge

      When spark.history.fs.driverlog.cleaner.enabled=true, driver log files older than this will be deleted when the driver log cleaner runs.
    
3.0.0


spark.history.fs.numReplayThreads
25% of available cores

      Number of threads that will be used by history server to process event logs.
    
2.0.0


spark.history.store.maxDiskUsage
10g

      Maximum disk usage for the local directory where the cache application history information
      are stored.
    
2.3.0


spark.history.store.path
(none)

        Local directory where to cache application history data. If set, the history
        server will store application data on disk instead of keeping it in memory. The data
        written to disk will be re-used in the event of a history server restart.
    
2.3.0


spark.history.store.serializer
JSON

        Serializer for writing/reading in-memory UI objects to/from disk-based KV Store; JSON or PROTOBUF.
        JSON serializer is the only choice before Spark 3.4.0, thus it is the default value.
        PROTOBUF serializer is fast and compact, compared to the JSON serializer.
    
3.4.0


spark.history.custom.executor.log.url
(none)

        Specifies custom spark executor log URL for supporting external log service instead of using cluster
        managers' application log URLs in the history server. Spark will support some path variables via patterns
        which can vary on cluster manager. Please check the documentation for your cluster manager to
        see which patterns are supported, if any. This configuration has no effect on a live application, it only
        affects the history server.
        
        For now, only YARN mode supports this configuration
    
3.0.0


spark.history.custom.executor.log.url.applyIncompleteApplication
true

        Specifies whether to apply custom spark executor log URL to incomplete applications as well.
        If executor logs for running applications should be provided as origin log URLs, set this to `false`.
        Please note that incomplete applications may include applications which didn't shutdown gracefully.
        Even this is set to `true`, this configuration has no effect on a live application, it only affects the history server.
    
3.0.0


spark.history.fs.eventLog.rolling.maxFilesToRetain
Int.MaxValue

      The maximum number of event log files which will be retained as non-compacted. By default,
      all event log files will be retained. The lowest value is 1 for technical reason.
      Please read the section of "Applying compaction of old event log files" for more details.
    
3.0.0


spark.history.store.hybridStore.enabled
false

      Whether to use HybridStore as the store when parsing event logs. HybridStore will first write data
      to an in-memory store and having a background thread that dumps data to a disk store after the writing
      to in-memory store is completed.
    
3.1.0


spark.history.store.hybridStore.maxMemoryUsage
2g

      Maximum memory space that can be used to create HybridStore. The HybridStore co-uses the heap memory,
      so the heap memory should be increased through the memory option for SHS if the HybridStore is enabled.
    
3.1.0


spark.history.store.hybridStore.diskBackend
ROCKSDB

      Specifies a disk-based store used in hybrid store; LEVELDB or ROCKSDB.
    
3.3.0


spark.history.fs.update.batchSize
Int.MaxValue

      Specifies the batch size for updating new eventlog files.
      This controls each scan process to be completed within a reasonable time, and such
      prevent the initial scan from running too long and blocking new eventlog files to
      be scanned in time in large environments.
    
3.4.0


Note that in all of these UIs, the tables are sortable by clicking their headers,
making it easy to identify slow tasks, data skew, etc.
Note


The history server displays both completed and incomplete Spark jobs. If an application makes
multiple attempts after failures, the failed attempts will be displayed, as well as any ongoing
incomplete attempt or the final successful attempt.


Incomplete applications are only updated intermittently. The time between updates is defined
by the interval between checks for changed files (spark.history.fs.update.interval).
On larger clusters, the update interval may be set to large values.
The way to view a running application is actually to view its own web UI.


Applications which exited without registering themselves as completed will be listed
as incomplete —even though they are no longer running. This can happen if an application
crashes.


One way to signal the completion of a Spark job is to stop the Spark Context
explicitly (sc.stop()), or in Python using the with SparkContext() as sc: construct
to handle the Spark Context setup and tear down.


REST API
In addition to viewing the metrics in the UI, they are also available as JSON.  This gives developers
an easy way to create new visualizations and monitoring tools for Spark.  The JSON is available for
both running applications, and in the history server.  The endpoints are mounted at /api/v1.  For example,
for the history server, they would typically be accessible at http://<server-url>:18080/api/v1, and
for a running application, at http://localhost:4040/api/v1.
In the API, an application is referenced by its application ID, [app-id].
When running on YARN, each application may have multiple attempts, but there are attempt IDs
only for applications in cluster mode, not applications in client mode. Applications in YARN cluster mode
can be identified by their [attempt-id]. In the API listed below, when running in YARN cluster mode,
[app-id] will actually be [base-app-id]/[attempt-id], where [base-app-id] is the YARN application ID.

EndpointMeaning

/applications
A list of all applications.
    
?status=[completed|running] list only applications in the chosen state.
    
?minDate=[date] earliest start date/time to list.
    
?maxDate=[date] latest start date/time to list.
    
?minEndDate=[date] earliest end date/time to list.
    
?maxEndDate=[date] latest end date/time to list.
    
?limit=[limit] limits the number of applications listed.
    Examples:
    ?minDate=2015-02-10
?minDate=2015-02-03T16:42:40.000GMT
?maxDate=2015-02-11T20:41:30.000GMT
?minEndDate=2015-02-12
?minEndDate=2015-02-12T09:15:10.000GMT
?maxEndDate=2015-02-14T16:30:45.000GMT
?limit=10


/applications/[app-id]/jobs

      A list of all jobs for a given application.
      ?status=[running|succeeded|failed|unknown] list only jobs in the specific state.
    


/applications/[app-id]/jobs/[job-id]
Details for the given job.


/applications/[app-id]/stages

      A list of all stages for a given application.
        ?status=[active|complete|pending|failed] list only stages in the given state.
        ?details=true lists all stages with the task data.
        ?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING] lists only those tasks with the specified task status. Query parameter taskStatus takes effect only when details=true. This also supports multiple taskStatus such as ?details=true&taskStatus=SUCCESS&taskStatus=FAILED which will return all tasks matching any of specified task status.
        ?withSummaries=true lists stages with task metrics distribution and executor metrics distribution.
        ?quantiles=0.0,0.25,0.5,0.75,1.0 summarize the metrics with the given quantiles. Query parameter quantiles takes effect only when withSummaries=true. Default value is 0.0,0.25,0.5,0.75,1.0.
    


/applications/[app-id]/stages/[stage-id]

      A list of all attempts for the given stage.
        ?details=true lists all attempts with the task data for the given stage.
        ?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING] lists only those tasks with the specified task status. Query parameter taskStatus takes effect only when details=true. This also supports multiple taskStatus such as ?details=true&taskStatus=SUCCESS&taskStatus=FAILED which will return all tasks matching any of specified task status.
        ?withSummaries=true lists task metrics distribution and executor metrics distribution of each attempt.
        ?quantiles=0.0,0.25,0.5,0.75,1.0 summarize the metrics with the given quantiles. Query parameter quantiles takes effect only when withSummaries=true. Default value is 0.0,0.25,0.5,0.75,1.0.
      Example:
        ?details=true
?details=true&taskStatus=RUNNING
?withSummaries=true
?details=true&withSummaries=true&quantiles=0.01,0.5,0.99



/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]

      Details for the given stage attempt.
        ?details=true lists all task data for the given stage attempt.
        ?taskStatus=[RUNNING|SUCCESS|FAILED|KILLED|PENDING] lists only those tasks with the specified task status. Query parameter taskStatus takes effect only when details=true. This also supports multiple taskStatus such as ?details=true&taskStatus=SUCCESS&taskStatus=FAILED which will return all tasks matching any of specified task status.
        ?withSummaries=true lists task metrics distribution and executor metrics distribution for the given stage attempt.
        ?quantiles=0.0,0.25,0.5,0.75,1.0 summarize the metrics with the given quantiles. Query parameter quantiles takes effect only when withSummaries=true. Default value is 0.0,0.25,0.5,0.75,1.0.
      Example:
        ?details=true
?details=true&taskStatus=RUNNING
?withSummaries=true
?details=true&withSummaries=true&quantiles=0.01,0.5,0.99



/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskSummary

      Summary metrics of all tasks in the given stage attempt.
      ?quantiles summarize the metrics with the given quantiles.
      Example: ?quantiles=0.01,0.5,0.99



/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList

       A list of all tasks for the given stage attempt.
      ?offset=[offset]&length=[len] list tasks in the given range.
      ?sortBy=[runtime|-runtime] sort the tasks.
      ?status=[running|success|killed|failed|unknown] list only tasks in the state.
      Example: ?offset=10&length=50&sortBy=runtime&status=running



/applications/[app-id]/executors
A list of all active executors for the given application.


/applications/[app-id]/executors/[executor-id]/threads

      Stack traces of all the threads running within the given active executor.
      Not available via the history server.
    


/applications/[app-id]/allexecutors
A list of all(active and dead) executors for the given application.


/applications/[app-id]/storage/rdd
A list of stored RDDs for the given application.


/applications/[app-id]/storage/rdd/[rdd-id]
Details for the storage status of a given RDD.


/applications/[base-app-id]/logs
Download the event logs for all attempts of the given application as files within
    a zip file.
    


/applications/[base-app-id]/[attempt-id]/logs
Download the event logs for a specific application attempt as a zip file.


/applications/[app-id]/streaming/statistics
Statistics for the streaming context.


/applications/[app-id]/streaming/receivers
A list of all streaming receivers.


/applications/[app-id]/streaming/receivers/[stream-id]
Details of the given receiver.


/applications/[app-id]/streaming/batches
A list of all retained batches.


/applications/[app-id]/streaming/batches/[batch-id]
Details of the given batch.


/applications/[app-id]/streaming/batches/[batch-id]/operations
A list of all output operations of the given batch.


/applications/[app-id]/streaming/batches/[batch-id]/operations/[outputOp-id]
Details of the given operation and given batch.


/applications/[app-id]/sql
A list of all queries for a given application.
    
?details=[true (default) | false] lists/hides details of Spark plan nodes.
    
?planDescription=[true (default) | false] enables/disables Physical planDescription on demand when Physical Plan size is high.
    
?offset=[offset]&length=[len] lists queries in the given range.
    


/applications/[app-id]/sql/[execution-id]
Details for the given query.
    
?details=[true (default) | false] lists/hides metric details in addition to given query details.
    
?planDescription=[true (default) | false] enables/disables Physical planDescription on demand for the given query when Physical Plan size is high.
    


/applications/[app-id]/environment
Environment details of the given application.


/version
Get the current spark version.


The number of jobs and stages which can be retrieved is constrained by the same retention
mechanism of the standalone Spark UI; "spark.ui.retainedJobs" defines the threshold
value triggering garbage collection on jobs, and spark.ui.retainedStages that for stages.
Note that the garbage collection takes place on playback: it is possible to retrieve
more entries by increasing these values and restarting the history server.
Executor Task Metrics
The REST API exposes the values of the Task Metrics collected by Spark executors with the granularity
of task execution. The metrics can be used for performance troubleshooting and workload characterization.
A list of the available metrics, with a short description:



Spark Executor Task Metric name
Short description



executorRunTime
Elapsed time the executor spent running this task. This includes time fetching shuffle data.
    The value is expressed in milliseconds.


executorCpuTime
CPU time the executor spent running this task. This includes time fetching shuffle data.
    The value is expressed in nanoseconds.


executorDeserializeTime
Elapsed time spent to deserialize this task. The value is expressed in milliseconds.


executorDeserializeCpuTime
CPU time taken on the executor to deserialize this task. The value is expressed
    in nanoseconds.


resultSize
The number of bytes this task transmitted back to the driver as the TaskResult.


jvmGCTime
Elapsed time the JVM spent in garbage collection while executing this task.
    The value is expressed in milliseconds.


resultSerializationTime
Elapsed time spent serializing the task result. The value is expressed in milliseconds.


memoryBytesSpilled
The number of in-memory bytes spilled by this task.


diskBytesSpilled
The number of on-disk bytes spilled by this task.


peakExecutionMemory
Peak memory used by internal data structures created during shuffles, aggregations and
        joins. The value of this accumulator should be approximately the sum of the peak sizes
        across all such data structures created in this task. For SQL jobs, this only tracks all
         unsafe operators and ExternalSort.


inputMetrics.*
Metrics related to reading data from org.apache.spark.rdd.HadoopRDD
    or from persisted data.


    .bytesRead
Total number of bytes read.


    .recordsRead
Total number of records read.


outputMetrics.*
Metrics related to writing data externally (e.g. to a distributed filesystem),
    defined only in tasks with output.


    .bytesWritten
Total number of bytes written


    .recordsWritten
Total number of records written


shuffleReadMetrics.*
Metrics related to shuffle read operations.


    .recordsRead
Number of records read in shuffle operations


    .remoteBlocksFetched
Number of remote blocks fetched in shuffle operations


    .localBlocksFetched
Number of local (as opposed to read from a remote executor) blocks fetched
    in shuffle operations


    .totalBlocksFetched
Number of blocks fetched in shuffle operations (both local and remote)


    .remoteBytesRead
Number of remote bytes read in shuffle operations


    .localBytesRead
Number of bytes read in shuffle operations from local disk (as opposed to
    read from a remote executor)


    .totalBytesRead
Number of bytes read in shuffle operations (both local and remote)


    .remoteBytesReadToDisk
Number of remote bytes read to disk in shuffle operations.
    Large blocks are fetched to disk in shuffle read operations, as opposed to
    being read into memory, which is the default behavior.


    .fetchWaitTime
Time the task spent waiting for remote shuffle blocks.
        This only includes the time blocking on shuffle input data.
        For instance if block B is being fetched while the task is still not finished
        processing block A, it is not considered to be blocking on block B.
        The value is expressed in milliseconds.


shuffleWriteMetrics.*
Metrics related to operations writing shuffle data.


    .bytesWritten
Number of bytes written in shuffle operations


    .recordsWritten
Number of records written in shuffle operations


    .writeTime
Time spent blocking on writes to disk or buffer cache. The value is expressed
     in nanoseconds.


Executor Metrics
Executor-level metrics are sent from each executor to the driver as part of the Heartbeat to describe the performance metrics of Executor itself like JVM heap memory, GC information.
Executor metric values and their measured memory peak values per executor are exposed via the REST API in JSON format and in Prometheus format.
The JSON end point is exposed at: /applications/[app-id]/executors, and the Prometheus endpoint at: /metrics/executors/prometheus.
The Prometheus endpoint is conditional to a configuration parameter: spark.ui.prometheus.enabled=true (the default is false).
In addition, aggregated per-stage peak values of the executor memory metrics are written to the event log if
spark.eventLog.logStageExecutorMetrics is true.
Executor memory metrics are also exposed via the Spark metrics system based on the Dropwizard metrics library.
A list of the available metrics, with a short description:


Executor Level Metric name
Short description



rddBlocks
RDD blocks in the block manager of this executor.


memoryUsed
Storage memory used by this executor.


diskUsed
Disk space used for RDD storage by this executor.


totalCores
Number of cores available in this executor.


maxTasks
Maximum number of tasks that can run concurrently in this executor.


activeTasks
Number of tasks currently executing.


failedTasks
Number of tasks that have failed in this executor.


completedTasks
Number of tasks that have completed in this executor.


totalTasks
Total number of tasks (running, failed and completed) in this executor.


totalDuration
Elapsed time the JVM spent executing tasks in this executor.
    The value is expressed in milliseconds.


totalGCTime
Elapsed time the JVM spent in garbage collection summed in this executor.
    The value is expressed in milliseconds.


totalInputBytes
Total input bytes summed in this executor.


totalShuffleRead
Total shuffle read bytes summed in this executor.


totalShuffleWrite
Total shuffle write bytes summed in this executor.


maxMemory
Total amount of memory available for storage, in bytes.


memoryMetrics.*
Current value of memory metrics:


    .usedOnHeapStorageMemory
Used on heap memory currently for storage, in bytes.


    .usedOffHeapStorageMemory
Used off heap memory currently for storage, in bytes.


    .totalOnHeapStorageMemory
Total available on heap memory for storage, in bytes. This amount can vary over time,  on the MemoryManager implementation.


    .totalOffHeapStorageMemory
Total available off heap memory for storage, in bytes. This amount can vary over time, depending on the MemoryManager implementation.


peakMemoryMetrics.*
Peak value of memory (and GC) metrics:


    .JVMHeapMemory
Peak memory usage of the heap that is used for object allocation.
    The heap consists of one or more memory pools. The used and committed size of the returned memory usage is the sum of those values of all heap memory pools whereas the init and max size of the returned memory usage represents the setting of the heap memory which may not be the sum of those of all heap memory pools.
    The amount of used memory in the returned memory usage is the amount of memory occupied by both live objects and garbage objects that have not been collected, if any.


    .JVMOffHeapMemory
Peak memory usage of non-heap memory that is used by the Java virtual machine. The non-heap memory consists of one or more memory pools. The used and committed size of the returned memory usage is the sum of those values of all non-heap memory pools whereas the init and max size of the returned memory usage represents the setting of the non-heap memory which may not be the sum of those of all non-heap memory pools.


    .OnHeapExecutionMemory
Peak on heap execution memory in use, in bytes.


    .OffHeapExecutionMemory
Peak off heap execution memory in use, in bytes.


    .OnHeapStorageMemory
Peak on heap storage memory in use, in bytes.


    .OffHeapStorageMemory
Peak off heap storage memory in use, in bytes.


    .OnHeapUnifiedMemory
Peak on heap memory (execution and storage).


    .OffHeapUnifiedMemory
Peak off heap memory (execution and storage).


    .DirectPoolMemory
Peak memory that the JVM is using for direct buffer pool (java.lang.management.BufferPoolMXBean)


    .MappedPoolMemory
Peak memory that the JVM is using for mapped buffer pool (java.lang.management.BufferPoolMXBean)


    .ProcessTreeJVMVMemory
Virtual memory size in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.


    .ProcessTreeJVMRSSMemory
Resident Set Size: number of pages the process has
      in real memory.  This is just the pages which count
      toward text, data, or stack space.  This does not
      include pages which have not been demand-loaded in,
      or which are swapped out. Enabled if spark.executor.processTreeMetrics.enabled is true.


    .ProcessTreePythonVMemory
Virtual memory size for Python in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.


    .ProcessTreePythonRSSMemory
Resident Set Size for Python. Enabled if spark.executor.processTreeMetrics.enabled is true.


    .ProcessTreeOtherVMemory
Virtual memory size for other kind of process in bytes. Enabled if spark.executor.processTreeMetrics.enabled is true.


    .ProcessTreeOtherRSSMemory
Resident Set Size for other kind of process. Enabled if spark.executor.processTreeMetrics.enabled is true.


    .MinorGCCount
Total minor GC count. For example, the garbage collector is one of     Copy, PS Scavenge, ParNew, G1 Young Generation and so on.


    .MinorGCTime
Elapsed total minor GC time.
    The value is expressed in milliseconds.


    .MajorGCCount
Total major GC count. For example, the garbage collector is one of     MarkSweepCompact, PS MarkSweep, ConcurrentMarkSweep, G1 Old Generation and so on.


    .MajorGCTime
Elapsed total major GC time.
    The value is expressed in milliseconds.


The computation of RSS and Vmem are based on proc(5)
API Versioning Policy
These endpoints have been strongly versioned to make it easier to develop applications on top.
 In particular, Spark guarantees:

Endpoints will never be removed from one version
Individual fields will never be removed for any given endpoint
New endpoints may be added
New fields may be added to existing endpoints
New versions of the api may be added in the future as a separate endpoint (e.g., api/v2).  New versions are not required to be backwards compatible.
Api versions may be dropped, but only after at least one minor release of co-existing with a new api version.

Note that even when examining the UI of running applications, the applications/[app-id] portion is
still required, though there is only one application available.  E.g. to see the list of jobs for the
running app, you would go to http://localhost:4040/api/v1/applications/[app-id]/jobs.  This is to
keep the paths consistent in both modes.
Metrics
Spark has a configurable metrics system based on the
Dropwizard Metrics Library.
This allows users to report Spark metrics to a variety of sinks including HTTP, JMX, and CSV
files. The metrics are generated by sources embedded in the Spark code base. They
provide instrumentation for specific activities and Spark components.
The metrics system is configured via a configuration file that Spark expects to be present
at $SPARK_HOME/conf/metrics.properties. A custom file location can be specified via the
spark.metrics.conf configuration property.
Instead of using the configuration file, a set of configuration parameters with prefix
spark.metrics.conf. can be used.
By default, the root namespace used for driver or executor metrics is
the value of spark.app.id. However, often times, users want to be able to track the metrics
across apps for driver and executors, which is hard to do with application ID
(i.e. spark.app.id) since it changes with every invocation of the app. For such use cases,
a custom namespace can be specified for metrics reporting using spark.metrics.namespace
configuration property.
If, say, users wanted to set the metrics namespace to the name of the application, they
can set the spark.metrics.namespace property to a value like ${spark.app.name}. This value is
then expanded appropriately by Spark and is used as the root namespace of the metrics system.
Non-driver and executor metrics are never prefixed with spark.app.id, nor does the
spark.metrics.namespace property have any such affect on such metrics.
Spark’s metrics are decoupled into different
instances corresponding to Spark components. Within each instance, you can configure a
set of sinks to which metrics are reported. The following instances are currently supported:

master: The Spark standalone master process.
applications: A component within the master which reports on various applications.
worker: A Spark standalone worker process.
executor: A Spark executor.
driver: The Spark driver process (the process in which your SparkContext is created).
shuffleService: The Spark shuffle service.
applicationMaster: The Spark ApplicationMaster when running on YARN.
mesos_cluster: The Spark cluster scheduler when running on Mesos.

Each instance can report to zero or more sinks. Sinks are contained in the
org.apache.spark.metrics.sink package:

ConsoleSink: Logs metrics information to the console.
CSVSink: Exports metrics data to CSV files at regular intervals.
JmxSink: Registers metrics for viewing in a JMX console.
MetricsServlet: Adds a servlet within the existing Spark UI to serve metrics data as JSON data.
PrometheusServlet: (Experimental) Adds a servlet within the existing Spark UI to serve metrics data in Prometheus format.
GraphiteSink: Sends metrics to a Graphite node.
Slf4jSink: Sends metrics to slf4j as log entries.
StatsdSink: Sends metrics to a StatsD node.

Spark also supports a Ganglia sink which is not included in the default build due to
licensing restrictions:

GangliaSink: Sends metrics to a Ganglia node or multicast group.

To install the GangliaSink you’ll need to perform a custom build of Spark. Note that
by embedding this library you will include LGPL-licensed
code in your Spark package. For sbt users, set the
SPARK_GANGLIA_LGPL environment variable before building. For Maven users, enable
the -Pspark-ganglia-lgpl profile. In addition to modifying the cluster’s Spark build
user applications will need to link to the spark-ganglia-lgpl artifact.
The syntax of the metrics configuration file and the parameters available for each sink are defined
in an example configuration file,
$SPARK_HOME/conf/metrics.properties.template.
When using Spark configuration parameters instead of the metrics configuration file, the relevant
parameter names are composed by the prefix spark.metrics.conf. followed by the configuration
details, i.e. the parameters take the following form:
spark.metrics.conf.[instance|*].sink.[sink_name].[parameter_name].
This example shows a list of Spark configuration parameters for a Graphite sink:
"spark.metrics.conf.*.sink.graphite.class"="org.apache.spark.metrics.sink.GraphiteSink"
"spark.metrics.conf.*.sink.graphite.host"="graphiteEndPoint_hostName>"
"spark.metrics.conf.*.sink.graphite.port"=<graphite_listening_port>
"spark.metrics.conf.*.sink.graphite.period"=10
"spark.metrics.conf.*.sink.graphite.unit"=seconds
"spark.metrics.conf.*.sink.graphite.prefix"="optional_prefix"
"spark.metrics.conf.*.sink.graphite.regex"="optional_regex_to_send_matching_metrics"

Default values of the Spark metrics configuration are as follows:
"*.sink.servlet.class" = "org.apache.spark.metrics.sink.MetricsServlet"
"*.sink.servlet.path" = "/metrics/json"
"master.sink.servlet.path" = "/metrics/master/json"
"applications.sink.servlet.path" = "/metrics/applications/json"

Additional sources can be configured using the metrics configuration file or the configuration
parameter spark.metrics.conf.[component_name].source.jvm.class=[source_name]. At present the
JVM source is the only available optional source. For example the following configuration parameter
activates the JVM source:
"spark.metrics.conf.*.source.jvm.class"="org.apache.spark.metrics.source.JvmSource"
List of available metrics providers
Metrics used by Spark are of multiple types: gauge, counter, histogram, meter and timer,
see Dropwizard library documentation for details.
The following list of components and metrics reports the name and some details about the available metrics,
grouped per component instance and source namespace.
The most common time of metrics used in Spark instrumentation are gauges and counters.
Counters can be recognized as they have the .count suffix. Timers, meters and histograms are annotated
in the list, the rest of the list elements are metrics of type gauge.
The large majority of metrics are active as soon as their parent component instance is configured,
some metrics require also to be enabled via an additional configuration parameter, the details are
reported in the list.
Component instance = Driver
This is the component with the largest amount of instrumented metrics

namespace=BlockManager
    
disk.diskSpaceUsed_MB
memory.maxMem_MB
memory.maxOffHeapMem_MB
memory.maxOnHeapMem_MB
memory.memUsed_MB
memory.offHeapMemUsed_MB
memory.onHeapMemUsed_MB
memory.remainingMem_MB
memory.remainingOffHeapMem_MB
memory.remainingOnHeapMem_MB


namespace=HiveExternalCatalog
    
note: these metrics are conditional to a configuration parameter:
spark.metrics.staticSources.enabled (default is true)
fileCacheHits.count
filesDiscovered.count
hiveClientCalls.count
parallelListingJobCount.count
partitionsFetched.count


namespace=CodeGenerator
    
note: these metrics are conditional to a configuration parameter:
spark.metrics.staticSources.enabled (default is true)
compilationTime (histogram)
generatedClassSize (histogram)
generatedMethodSize (histogram)
sourceCodeSize (histogram)


namespace=DAGScheduler
    
job.activeJobs
job.allJobs
messageProcessingTime (timer)
stage.failedStages
stage.runningStages
stage.waitingStages


namespace=LiveListenerBus
    
listenerProcessingTime.org.apache.spark.HeartbeatReceiver (timer)
listenerProcessingTime.org.apache.spark.scheduler.EventLoggingListener (timer)
listenerProcessingTime.org.apache.spark.status.AppStatusListener (timer)
numEventsPosted.count
queue.appStatus.listenerProcessingTime (timer)
queue.appStatus.numDroppedEvents.count
queue.appStatus.size
queue.eventLog.listenerProcessingTime (timer)
queue.eventLog.numDroppedEvents.count
queue.eventLog.size
queue.executorManagement.listenerProcessingTime (timer)


namespace=appStatus (all metrics of type=counter)
    
note: Introduced in Spark 3.0. Conditional to a configuration parameter:
 spark.metrics.appStatusSource.enabled (default is false)
stages.failedStages.count
stages.skippedStages.count
stages.completedStages.count
tasks.blackListedExecutors.count // deprecated use excludedExecutors instead
tasks.excludedExecutors.count
tasks.completedTasks.count
tasks.failedTasks.count
tasks.killedTasks.count
tasks.skippedTasks.count
tasks.unblackListedExecutors.count // deprecated use unexcludedExecutors instead
tasks.unexcludedExecutors.count
jobs.succeededJobs
jobs.failedJobs
jobDuration


namespace=AccumulatorSource
    
note: User-configurable sources to attach accumulators to metric system
DoubleAccumulatorSource
LongAccumulatorSource


namespace=spark.streaming
    
note: This applies to Spark Structured Streaming only. Conditional to a configuration
parameter: spark.sql.streaming.metricsEnabled=true (default is false)
eventTime-watermark
inputRate-total
latency
processingRate-total
states-rowsTotal
states-usedBytes


namespace=JVMCPU
    
jvmCpuTime


namespace=executor
    
note: These metrics are available in the driver in local mode only.
A full list of available metrics in this
namespace can be found in the corresponding entry for the Executor component instance.


namespace=ExecutorMetrics
    
note: these metrics are conditional to a configuration parameter:
spark.metrics.executorMetricsSource.enabled (default is true)
This source contains memory-related metrics. A full list of available metrics in this
namespace can be found in the corresponding entry for the Executor component instance.


namespace=ExecutorAllocationManager
    
note: these metrics are only emitted when using dynamic allocation. Conditional to a configuration
parameter spark.dynamicAllocation.enabled (default is false)
executors.numberExecutorsToAdd
executors.numberExecutorsPendingToRemove
executors.numberAllExecutors
executors.numberTargetExecutors
executors.numberMaxNeededExecutors
executors.numberDecommissioningExecutors
executors.numberExecutorsGracefullyDecommissioned.count
executors.numberExecutorsDecommissionUnfinished.count
executors.numberExecutorsExitedUnexpectedly.count
executors.numberExecutorsKilledByDriver.count


namespace=plugin.<Plugin Class Name>
    
Optional namespace(s). Metrics in this namespace are defined by user-supplied code, and
configured using the Spark plugin API. See “Advanced Instrumentation” below for how to load
custom plugins into Spark.



Component instance = Executor
These metrics are exposed by Spark executors.

namespace=executor (metrics are of type counter or gauge)
    
notes:

spark.executor.metrics.fileSystemSchemes (default: file,hdfs) determines the exposed file system metrics.


bytesRead.count
bytesWritten.count
cpuTime.count
deserializeCpuTime.count
deserializeTime.count
diskBytesSpilled.count
filesystem.file.largeRead_ops
filesystem.file.read_bytes
filesystem.file.read_ops
filesystem.file.write_bytes
filesystem.file.write_ops
filesystem.hdfs.largeRead_ops
filesystem.hdfs.read_bytes
filesystem.hdfs.read_ops
filesystem.hdfs.write_bytes
filesystem.hdfs.write_ops
jvmGCTime.count
memoryBytesSpilled.count
recordsRead.count
recordsWritten.count
resultSerializationTime.count
resultSize.count
runTime.count
shuffleBytesWritten.count
shuffleFetchWaitTime.count
shuffleLocalBlocksFetched.count
shuffleLocalBytesRead.count
shuffleRecordsRead.count
shuffleRecordsWritten.count
shuffleRemoteBlocksFetched.count
shuffleRemoteBytesRead.count
shuffleRemoteBytesReadToDisk.count
shuffleTotalBytesRead.count
shuffleWriteTime.count
succeededTasks.count
threadpool.activeTasks
threadpool.completeTasks
threadpool.currentPool_size
threadpool.maxPool_size
threadpool.startedTasks


namespace=ExecutorMetrics
    
notes:

These metrics are conditional to a configuration parameter:
spark.metrics.executorMetricsSource.enabled (default value is true)
ExecutorMetrics are updated as part of heartbeat processes scheduled
 for the executors and for the driver at regular intervals: spark.executor.heartbeatInterval (default value is 10 seconds)
An optional faster polling mechanism is available for executor memory metrics,
 it can be activated by setting a polling interval (in milliseconds) using the configuration parameter spark.executor.metrics.pollingInterval


JVMHeapMemory
JVMOffHeapMemory
OnHeapExecutionMemory
OnHeapStorageMemory
OnHeapUnifiedMemory
OffHeapExecutionMemory
OffHeapStorageMemory
OffHeapUnifiedMemory
DirectPoolMemory
MappedPoolMemory
MinorGCCount
MinorGCTime
MajorGCCount
MajorGCTime
“ProcessTree*” metric counters:
        
ProcessTreeJVMVMemory
ProcessTreeJVMRSSMemory
ProcessTreePythonVMemory
ProcessTreePythonRSSMemory
ProcessTreeOtherVMemory
ProcessTreeOtherRSSMemory
note: “ProcessTree” metrics are collected only under certain conditions.
The conditions are the logical AND of the following: /proc filesystem exists,
spark.executor.processTreeMetrics.enabled=true.
“ProcessTree” metrics report 0 when those conditions are not met.




namespace=JVMCPU
    
jvmCpuTime


namespace=NettyBlockTransfer
    
shuffle-client.usedDirectMemory
shuffle-client.usedHeapMemory
shuffle-server.usedDirectMemory
shuffle-server.usedHeapMemory


namespace=HiveExternalCatalog
    
note: these metrics are conditional to a configuration parameter:
spark.metrics.staticSources.enabled (default is true)
fileCacheHits.count
filesDiscovered.count
hiveClientCalls.count
parallelListingJobCount.count
partitionsFetched.count


namespace=CodeGenerator
    
note: these metrics are conditional to a configuration parameter:
spark.metrics.staticSources.enabled (default is true)
compilationTime (histogram)
generatedClassSize (histogram)
generatedMethodSize (histogram)
sourceCodeSize (histogram)


namespace=plugin.<Plugin Class Name>
    
Optional namespace(s). Metrics in this namespace are defined by user-supplied code, and
configured using the Spark plugin API. See “Advanced Instrumentation” below for how to load
custom plugins into Spark.



Source = JVM Source
Notes:

Activate this source by setting the relevant metrics.properties file entry or the
  configuration parameter:spark.metrics.conf.*.source.jvm.class=org.apache.spark.metrics.source.JvmSource
These metrics are conditional to a configuration parameter:
spark.metrics.staticSources.enabled (default is true)
This source is available for driver and executor instances and is also available for other instances.
This source provides information on JVM metrics using the
  Dropwizard/Codahale Metric Sets for JVM instrumentation
   and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet.

Component instance = applicationMaster
Note: applies when running on YARN

numContainersPendingAllocate
numExecutorsFailed
numExecutorsRunning
numLocalityAwareTasks
numReleasedContainers

Component instance = mesos_cluster
Note: applies when running on mesos

waitingDrivers
launchedDrivers
retryDrivers

Component instance = master
Note: applies when running in Spark standalone as master

workers
aliveWorkers
apps
waitingApps

Component instance = ApplicationSource
Note: applies when running in Spark standalone as master

status
runtime_ms
cores

Component instance = worker
Note: applies when running in Spark standalone as worker

executors
coresUsed
memUsed_MB
coresFree
memFree_MB

Component instance = shuffleService
Note: applies to the shuffle service

blockTransferRate (meter) - rate of blocks being transferred
blockTransferMessageRate (meter) - rate of block transfer messages,
i.e. if batch fetches are enabled, this represents number of batches rather than number of blocks
blockTransferRateBytes (meter)
blockTransferAvgTime_1min (gauge - 1-minute moving average)
numActiveConnections.count
numRegisteredConnections.count
numCaughtExceptions.count
openBlockRequestLatencyMillis (histogram)
registerExecutorRequestLatencyMillis (histogram)
registeredExecutorsSize
shuffle-server.usedDirectMemory

shuffle-server.usedHeapMemory

note: the metrics below apply when the server side configuration
spark.shuffle.push.server.mergedShuffleFileManagerImpl is set to
org.apache.spark.network.shuffle.MergedShuffleFileManager for Push-Based Shuffle
blockBytesWritten - size of the pushed block data written to file in bytes
blockAppendCollisions - number of shuffle push blocks collided in shuffle services
as another block for the same reduce partition were being written
lateBlockPushes - number of shuffle push blocks that are received in shuffle service
after the specific shuffle merge has been finalized
deferredBlocks - number of the current deferred block parts buffered in memory
deferredBlockBytes - size of the current deferred block parts buffered in memory
staleBlockPushes - number of stale shuffle block push requests
ignoredBlockBytes - size of the pushed block data that was transferred to ESS, but ignored.
The pushed block data are considered as ignored when: 1. it was received after the shuffle
was finalized; 2. when a push request is for a duplicate block; 3. ESS was unable to write the block.

Advanced Instrumentation
Several external tools can be used to help profile the performance of Spark jobs:

Cluster-wide monitoring tools, such as Ganglia, can provide
insight into overall cluster utilization and resource bottlenecks. For instance, a Ganglia
dashboard can quickly reveal whether a particular workload is disk bound, network bound, or
CPU bound.
OS profiling tools such as dstat,
iostat, and iotop
can provide fine-grained profiling on individual nodes.
JVM utilities such as jstack for providing stack traces, jmap for creating heap-dumps,
jstat for reporting time-series statistics and jconsole for visually exploring various JVM
properties are useful for those comfortable with JVM internals.

Spark also provides a plugin API so that custom instrumentation code can be added to Spark
applications. There are two configuration keys available for loading plugins into Spark:

spark.plugins
spark.plugins.defaultList

Both take a comma-separated list of class names that implement the
org.apache.spark.api.plugin.SparkPlugin interface. The two names exist so that it’s
possible for one list to be placed in the Spark default config file, allowing users to
easily add other plugins from the command line without overwriting the config file’s list. Duplicate
plugins are ignored.




















  




Quick Start - Spark 3.5.5 Documentation



















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Quick Start

Interactive Analysis with the Spark Shell 
Basics
More on Dataset Operations
Caching


Self-Contained Applications
Where to Go from Here

This tutorial provides a quick introduction to using Spark. We will first introduce the API through Spark’s
interactive shell (in Python or Scala),
then show how to write applications in Java, Scala, and Python.
To follow along with this guide, first, download a packaged release of Spark from the
Spark website. Since we won’t be using HDFS,
you can download a package for any version of Hadoop.
Note that, before Spark 2.0, the main programming interface of Spark was the Resilient Distributed Dataset (RDD). After Spark 2.0, RDDs are replaced by Dataset, which is strongly-typed like an RDD, but with richer optimizations under the hood. The RDD interface is still supported, and you can get a more detailed reference at the RDD programming guide. However, we highly recommend you to switch to use Dataset, which has better performance than RDD. See the SQL programming guide to get more information about Dataset.
Interactive Analysis with the Spark Shell
Basics
Spark’s shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively.
It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries)
or Python. Start it by running the following in the Spark directory:


./bin/pyspark
 
Or if PySpark is installed with pip in your current environment:
pyspark
 
Spark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets. Due to Python’s dynamic nature, we don’t need the Dataset to be strongly-typed in Python. As a result, all Datasets in Python are Dataset[Row], and we call it DataFrame to be consistent with the data frame concept in Pandas and R. Let’s make a new DataFrame from the text of the README file in the Spark source directory:
>>> textFile = spark.read.text("README.md")
You can get values from DataFrame directly, by calling some actions, or transform the DataFrame to get a new one. For more details, please read the API doc.
>>> textFile.count()  # Number of rows in this DataFrame
126

>>> textFile.first()  # First row in this DataFrame
Row(value=u'# Apache Spark')
Now let’s transform this DataFrame to a new one. We call filter to return a new DataFrame with a subset of the lines in the file.
>>> linesWithSpark = textFile.filter(textFile.value.contains("Spark"))
We can chain together transformations and actions:
>>> textFile.filter(textFile.value.contains("Spark")).count()  # How many lines contain "Spark"?
15


./bin/spark-shell
 
Spark’s primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other Datasets. Let’s make a new Dataset from the text of the README file in the Spark source directory:
scala> val textFile = spark.read.textFile("README.md")
textFile: org.apache.spark.sql.Dataset[String] = [value: string]
You can get values from Dataset directly, by calling some actions, or transform the Dataset to get a new one. For more details, please read the API doc.
scala> textFile.count() // Number of items in this Dataset
res0: Long = 126 // May be different from yours as README.md will change over time, similar to other outputs

scala> textFile.first() // First item in this Dataset
res1: String = # Apache Spark
Now let’s transform this Dataset into a new one. We call filter to return a new Dataset with a subset of the items in the file.
scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]
We can chain together transformations and actions:
scala> textFile.filter(line => line.contains("Spark")).count() // How many lines contain "Spark"?
res3: Long = 15


More on Dataset Operations
Dataset actions and transformations can be used for more complex computations. Let’s say we want to find the line with the most words:


>>> from pyspark.sql import functions as sf
>>> textFile.select(sf.size(sf.split(textFile.value, "\s+")).name("numWords")).agg(sf.max(sf.col("numWords"))).collect()
[Row(max(numWords)=15)]
This first maps a line to an integer value and aliases it as “numWords”, creating a new DataFrame. agg is called on that DataFrame to find the largest word count. The arguments to select and agg are both Column, we can use df.colName to get a column from a DataFrame. We can also import pyspark.sql.functions, which provides a lot of convenient functions to build a new Column from an old one.
One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:
>>> wordCounts = textFile.select(sf.explode(sf.split(textFile.value, "\s+")).alias("word")).groupBy("word").count()
Here, we use the explode function in select, to transform a Dataset of lines to a Dataset of words, and then combine groupBy and count to compute the per-word counts in the file as a DataFrame of 2 columns: “word” and “count”. To collect the word counts in our shell, we can call collect:
>>> wordCounts.collect()
[Row(word=u'online', count=1), Row(word=u'graphs', count=1), ...]


scala> textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)
res4: Int = 15
This first maps a line to an integer value, creating a new Dataset. reduce is called on that Dataset to find the largest word count. The arguments to map and reduce are Scala function literals (closures), and can use any language feature or Scala/Java library. For example, we can easily call functions declared elsewhere. We’ll use Math.max() function to make this code easier to understand:
scala> import java.lang.Math
import java.lang.Math

scala> textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))
res5: Int = 15
One common data flow pattern is MapReduce, as popularized by Hadoop. Spark can implement MapReduce flows easily:
scala> val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count()
wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]
Here, we call flatMap to transform a Dataset of lines to a Dataset of words, and then combine groupByKey and count to compute the per-word counts in the file as a Dataset of (String, Long) pairs. To collect the word counts in our shell, we can call collect:
scala> wordCounts.collect()
res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)


Caching
Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank. As a simple example, let’s mark our linesWithSpark dataset to be cached:


>>> linesWithSpark.cache()

>>> linesWithSpark.count()
15

>>> linesWithSpark.count()
15
It may seem silly to use Spark to explore and cache a 100-line text file. The interesting part is
that these same functions can be used on very large data sets, even when they are striped across
tens or hundreds of nodes. You can also do this interactively by connecting bin/pyspark to
a cluster, as described in the RDD programming guide.


scala> linesWithSpark.cache()
res7: linesWithSpark.type = [value: string]

scala> linesWithSpark.count()
res8: Long = 15

scala> linesWithSpark.count()
res9: Long = 15
It may seem silly to use Spark to explore and cache a 100-line text file. The interesting part is
that these same functions can be used on very large data sets, even when they are striped across
tens or hundreds of nodes. You can also do this interactively by connecting bin/spark-shell to
a cluster, as described in the RDD programming guide.


Self-Contained Applications
Suppose we wish to write a self-contained application using the Spark API. We will walk through a
simple application in Scala (with sbt), Java (with Maven), and Python (pip).


Now we will show how to write an application using the Python API (PySpark).
If you are building a packaged PySpark application or library you can add it to your setup.py file as:
    install_requires=[
        'pyspark==3.5.5'
    ]
As an example, we’ll create a simple Spark application, SimpleApp.py:
"""SimpleApp.py"""
from pyspark.sql import SparkSession

logFile = "YOUR_SPARK_HOME/README.md"  # Should be some file on your system
spark = SparkSession.builder.appName("SimpleApp").getOrCreate()
logData = spark.read.text(logFile).cache()

numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()

print("Lines with a: %i, lines with b: %i" % (numAs, numBs))

spark.stop()
This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a
text file.
Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is installed.
As with the Scala and Java examples, we use a SparkSession to create Datasets.
For applications that use custom classes or third-party libraries, we can also add code
dependencies to spark-submit through its --py-files argument by packaging them into a
.zip file (see spark-submit --help for details).
SimpleApp is simple enough that we do not need to specify any code dependencies.
We can run this application using the bin/spark-submit script:
# Use spark-submit to run your application
$ YOUR_SPARK_HOME/bin/spark-submit \
  --master local[4] \
  SimpleApp.py
...
Lines with a: 46, Lines with b: 23
If you have PySpark pip installed into your environment (e.g., pip install pyspark), you can run your application with the regular Python interpreter or use the provided ‘spark-submit’ as you prefer.
# Use the Python interpreter to run your application
$ python SimpleApp.py
...
Lines with a: 46, Lines with b: 23


We’ll create a very simple Spark application in Scala–so simple, in fact, that it’s
named SimpleApp.scala:
/* SimpleApp.scala */
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]): Unit = {
    val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
    val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println(s"Lines with a: $numAs, Lines with b: $numBs")
    spark.stop()
  }
}
Note that applications should define a main() method instead of extending scala.App.
Subclasses of scala.App may not work correctly.
This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the
Spark README. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is
installed. Unlike the earlier examples with the Spark shell, which initializes its own SparkSession,
we initialize a SparkSession as part of the program.
We call SparkSession.builder to construct a SparkSession, then set the application name, and finally call getOrCreate to get the SparkSession instance.
Our application depends on the Spark API, so we’ll also include an sbt configuration file,
build.sbt, which explains that Spark is a dependency. This file also adds a repository that
Spark depends on:
name := "Simple Project"

version := "1.0"

scalaVersion := "2.12.18"

libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.5.5"
For sbt to work correctly, we’ll need to layout SimpleApp.scala and build.sbt
according to the typical directory structure. Once that is in place, we can create a JAR package
containing the application’s code, then use the spark-submit script to run our program.
# Your directory layout should look like this
$ find .
.
./build.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala

# Package a jar containing your application
$ sbt package
...
[info] Packaging {..}/{..}/target/scala-2.12/simple-project_2.12-1.0.jar

# Use spark-submit to run your application
$ YOUR_SPARK_HOME/bin/spark-submit \
  --class "SimpleApp" \
  --master local[4] \
  target/scala-2.12/simple-project_2.12-1.0.jar
...
Lines with a: 46, Lines with b: 23


This example will use Maven to compile an application JAR, but any similar build system will work.
We’ll create a very simple Spark application, SimpleApp.java:
/* SimpleApp.java */
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;

public class SimpleApp {
  public static void main(String[] args) {
    String logFile = "YOUR_SPARK_HOME/README.md"; // Should be some file on your system
    SparkSession spark = SparkSession.builder().appName("Simple Application").getOrCreate();
    Dataset<String> logData = spark.read().textFile(logFile).cache();

    long numAs = logData.filter(s -> s.contains("a")).count();
    long numBs = logData.filter(s -> s.contains("b")).count();

    System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);

    spark.stop();
  }
}
This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the
Spark README. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is
installed. Unlike the earlier examples with the Spark shell, which initializes its own SparkSession,
we initialize a SparkSession as part of the program.
To build the program, we also write a Maven pom.xml file that lists Spark as a dependency.
Note that Spark artifacts are tagged with a Scala version.
<project>
  <groupId>edu.berkeley</groupId>
  <artifactId>simple-project</artifactId>
  <modelVersion>4.0.0</modelVersion>
  <name>Simple Project</name>
  <packaging>jar</packaging>
  <version>1.0</version>
  <dependencies>
    <dependency> <!-- Spark dependency -->
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_2.12</artifactId>
      <version>3.5.5</version>
      <scope>provided</scope>
    </dependency>
  </dependencies>
</project>
We lay out these files according to the canonical Maven directory structure:
$ find .
./pom.xml
./src
./src/main
./src/main/java
./src/main/java/SimpleApp.java
Now, we can package the application using Maven and execute it with ./bin/spark-submit.
# Package a JAR containing your application
$ mvn package
...
[INFO] Building jar: {..}/{..}/target/simple-project-1.0.jar

# Use spark-submit to run your application
$ YOUR_SPARK_HOME/bin/spark-submit \
  --class "SimpleApp" \
  --master local[4] \
  target/simple-project-1.0.jar
...
Lines with a: 46, Lines with b: 23


Other dependency management tools such as Conda and pip can be also used for custom classes or third-party libraries. See also Python Package Management.
Where to Go from Here
Congratulations on running your first Spark application!

For an in-depth overview of the API, start with the RDD programming guide and the SQL programming guide, or see “Programming Guides” menu for other components.
For running applications on a cluster, head to the deployment overview.
Finally, Spark includes several samples in the examples directory
(Scala,
 Java,
 Python,
 R).
You can run them as follows:

# For Scala and Java, use run-example:
./bin/run-example SparkPi

# For Python examples, use spark-submit directly:
./bin/spark-submit examples/src/main/python/pi.py

# For R examples, use spark-submit directly:
./bin/spark-submit examples/src/main/r/dataframe.R




















  




RDD Programming Guide - Spark 3.5.5 Documentation



















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











RDD Programming Guide

Overview
Linking with Spark
Initializing Spark 
Using the Shell


Resilient Distributed Datasets (RDDs) 
Parallelized Collections
External Datasets
RDD Operations 
Basics
Passing Functions to Spark
Understanding closures  
Example
Local vs. cluster modes
Printing elements of an RDD


Working with Key-Value Pairs
Transformations
Actions
Shuffle operations 
Background
Performance Impact




RDD Persistence 
Which Storage Level to Choose?
Removing Data




Shared Variables 
Broadcast Variables
Accumulators


Deploying to a Cluster
Launching Spark jobs from Java / Scala
Unit Testing
Where to Go from Here

Overview
At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.
A second abstraction in Spark is shared variables that can be used in parallel operations. By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. Sometimes, a variable needs to be shared across tasks, or between tasks and the driver program. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only “added” to, such as counters and sums.
This guide shows each of these features in each of Spark’s supported languages. It is easiest to follow
along with if you launch Spark’s interactive shell – either bin/spark-shell for the Scala shell or
bin/pyspark for the Python one.
Linking with Spark


Spark 3.5.5 works with Python 3.8+. It can use the standard CPython interpreter,
so C libraries like NumPy can be used. It also works with PyPy 7.3.6+.
Spark applications in Python can either be run with the bin/spark-submit script which includes Spark at runtime, or by including it in your setup.py as:
    install_requires=[
        'pyspark==3.5.5'
    ]
To run Spark applications in Python without pip installing PySpark, use the bin/spark-submit script located in the Spark directory.
This script will load Spark’s Java/Scala libraries and allow you to submit applications to a cluster.
You can also use bin/pyspark to launch an interactive Python shell.
If you wish to access HDFS data, you need to use a build of PySpark linking
to your version of HDFS.
Prebuilt packages are also available on the Spark homepage
for common HDFS versions.
Finally, you need to import some Spark classes into your program. Add the following line:
from pyspark import SparkContext, SparkConf
PySpark requires the same minor version of Python in both driver and workers. It uses the default python version in PATH,
you can specify which version of Python you want to use by PYSPARK_PYTHON, for example:
$ PYSPARK_PYTHON=python3.8 bin/pyspark
$ PYSPARK_PYTHON=/path-to-your-pypy/pypy bin/spark-submit examples/src/main/python/pi.py


Spark 3.5.5 is built and distributed to work with Scala 2.12
by default. (Spark can be built to work with other versions of Scala, too.) To write
applications in Scala, you will need to use a compatible Scala version (e.g. 2.12.X).
To write a Spark application, you need to add a Maven dependency on Spark. Spark is available through Maven Central at:
groupId = org.apache.spark
artifactId = spark-core_2.12
version = 3.5.5
 
In addition, if you wish to access an HDFS cluster, you need to add a dependency on
hadoop-client for your version of HDFS.
groupId = org.apache.hadoop
artifactId = hadoop-client
version = <your-hdfs-version>
 
Finally, you need to import some Spark classes into your program. Add the following lines:
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
(Before Spark 1.3.0, you need to explicitly import org.apache.spark.SparkContext._ to enable essential implicit conversions.)


Spark 3.5.5 supports
lambda expressions
for concisely writing functions, otherwise you can use the classes in the
org.apache.spark.api.java.function package.
Note that support for Java 7 was removed in Spark 2.2.0.
To write a Spark application in Java, you need to add a dependency on Spark. Spark is available through Maven Central at:
groupId = org.apache.spark
artifactId = spark-core_2.12
version = 3.5.5
 
In addition, if you wish to access an HDFS cluster, you need to add a dependency on
hadoop-client for your version of HDFS.
groupId = org.apache.hadoop
artifactId = hadoop-client
version = <your-hdfs-version>
 
Finally, you need to import some Spark classes into your program. Add the following lines:
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.SparkConf;


Initializing Spark


The first thing a Spark program must do is to create a SparkContext object, which tells Spark
how to access a cluster. To create a SparkContext you first need to build a SparkConf object
that contains information about your application.
conf = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf=conf)


The first thing a Spark program must do is to create a SparkContext object, which tells Spark
how to access a cluster. To create a SparkContext you first need to build a SparkConf object
that contains information about your application.
Only one SparkContext should be active per JVM. You must stop() the active SparkContext before creating a new one.
val conf = new SparkConf().setAppName(appName).setMaster(master)
new SparkContext(conf)


The first thing a Spark program must do is to create a JavaSparkContext object, which tells Spark
how to access a cluster. To create a SparkContext you first need to build a SparkConf object
that contains information about your application.
SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
JavaSparkContext sc = new JavaSparkContext(conf);


The appName parameter is a name for your application to show on the cluster UI.
master is a Spark, Mesos or YARN cluster URL,
or a special “local” string to run in local mode.
In practice, when running on a cluster, you will not want to hardcode master in the program,
but rather launch the application with spark-submit and
receive it there. However, for local testing and unit tests, you can pass “local” to run Spark
in-process.
Using the Shell


In the PySpark shell, a special interpreter-aware SparkContext is already created for you, in the
variable called sc. Making your own SparkContext will not work. You can set which master the
context connects to using the --master argument, and you can add Python .zip, .egg or .py files
to the runtime path by passing a comma-separated list to --py-files. For third-party Python dependencies,
see Python Package Management. You can also add dependencies
(e.g. Spark Packages) to your shell session by supplying a comma-separated list of Maven coordinates
to the --packages argument. Any additional repositories where dependencies might exist (e.g. Sonatype)
can be passed to the --repositories argument. For example, to run
bin/pyspark on exactly four cores, use:
$ ./bin/pyspark --master local[4]
Or, to also add code.py to the search path (in order to later be able to import code), use:
$ ./bin/pyspark --master local[4] --py-files code.py
For a complete list of options, run pyspark --help. Behind the scenes,
pyspark invokes the more general spark-submit script.
It is also possible to launch the PySpark shell in IPython, the
enhanced Python interpreter. PySpark works with IPython 1.0.0 and later. To
use IPython, set the PYSPARK_DRIVER_PYTHON variable to ipython when running bin/pyspark:
$ PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark
To use the Jupyter notebook (previously known as the IPython notebook),
$ PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark
You can customize the ipython or jupyter commands by setting PYSPARK_DRIVER_PYTHON_OPTS.
After the Jupyter Notebook server is launched, you can create a new notebook from
the “Files” tab. Inside the notebook, you can input the command %pylab inline as part of
your notebook before you start to try Spark from the Jupyter notebook.


In the Spark shell, a special interpreter-aware SparkContext is already created for you, in the
variable called sc. Making your own SparkContext will not work. You can set which master the
context connects to using the --master argument, and you can add JARs to the classpath
by passing a comma-separated list to the --jars argument. You can also add dependencies
(e.g. Spark Packages) to your shell session by supplying a comma-separated list of Maven coordinates
to the --packages argument. Any additional repositories where dependencies might exist (e.g. Sonatype)
can be passed to the --repositories argument. For example, to run bin/spark-shell on exactly
four cores, use:
$ ./bin/spark-shell --master local[4]
Or, to also add code.jar to its classpath, use:
$ ./bin/spark-shell --master local[4] --jars code.jar
To include a dependency using Maven coordinates:
$ ./bin/spark-shell --master local[4] --packages "org.example:example:0.1"
For a complete list of options, run spark-shell --help. Behind the scenes,
spark-shell invokes the more general spark-submit script.


Resilient Distributed Datasets (RDDs)
Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing
an existing collection in your driver program, or referencing a dataset in an external storage system, such as a
shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.
Parallelized Collections


Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
Once created, the distributed dataset (distData) can be operated on in parallel. For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list.
We describe operations on distributed datasets later on.


Parallelized collections are created by calling SparkContext’s parallelize method on an existing collection in your driver program (a Scala Seq). The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:
val data = Array(1, 2, 3, 4, 5)
val distData = sc.parallelize(data)
Once created, the distributed dataset (distData) can be operated on in parallel. For example, we might call distData.reduce((a, b) => a + b) to add up the elements of the array. We describe operations on distributed datasets later on.


Parallelized collections are created by calling JavaSparkContext’s parallelize method on an existing Collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:
List<Integer> data = Arrays.asList(1, 2, 3, 4, 5);
JavaRDD<Integer> distData = sc.parallelize(data);
Once created, the distributed dataset (distData) can be operated on in parallel. For example, we might call distData.reduce((a, b) -> a + b) to add up the elements of the list.
We describe operations on distributed datasets later on.


One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.
External Datasets


PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.
Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:
>>> distFile = sc.textFile("data.txt")
Once created, distFile can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the map and reduce operations as follows: distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b).
Some notes on reading files with Spark:


If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.


All of Spark’s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile("/my/directory"), textFile("/my/directory/*.txt"), and textFile("/my/directory/*.gz").


The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.


Apart from text files, Spark’s Python API also supports several other data formats:


SparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file.


RDD.saveAsPickleFile and SparkContext.pickleFile support saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10.


SequenceFile and Hadoop Input/Output Formats


Note this feature is currently marked Experimental and is intended for advanced users. It may be replaced in future with read/write support based on Spark SQL, in which case Spark SQL is the preferred approach.
Writable Support
PySpark SequenceFile support loads an RDD of key-value pairs within Java, converts Writables to base Java types, and pickles the
resulting Java objects using pickle. When saving an RDD of key-value pairs to SequenceFile,
PySpark does the reverse. It unpickles Python objects into Java objects and then converts them to Writables. The following
Writables are automatically converted:

Writable TypePython Type
Textstr
IntWritableint
FloatWritablefloat
DoubleWritablefloat
BooleanWritablebool
BytesWritablebytearray
NullWritableNone
MapWritabledict

Arrays are not handled out-of-the-box. Users need to specify custom ArrayWritable subtypes when reading or writing. When writing,
users also need to specify custom converters that convert arrays to custom ArrayWritable subtypes. When reading, the default
converter will convert custom ArrayWritable subtypes to Java Object[], which then get pickled to Python tuples. To get
Python array.array for arrays of primitive types, users need to specify custom converters.
Saving and Loading SequenceFiles
Similarly to text files, SequenceFiles can be saved and loaded by specifying the path. The key and value
classes can be specified, but for standard Writables this is not required.
>>> rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, "a" * x))
>>> rdd.saveAsSequenceFile("path/to/file")
>>> sorted(sc.sequenceFile("path/to/file").collect())
[(1, u'a'), (2, u'aa'), (3, u'aaa')]
Saving and Loading Other Hadoop Input/Output Formats
PySpark can also read any Hadoop InputFormat or write any Hadoop OutputFormat, for both ‘new’ and ‘old’ Hadoop MapReduce APIs.
If required, a Hadoop configuration can be passed in as a Python dict. Here is an example using the
Elasticsearch ESInputFormat:
$ ./bin/pyspark --jars /path/to/elasticsearch-hadoop.jar
>>> conf = {"es.resource" : "index/type"}  # assume Elasticsearch is running on localhost defaults
>>> rdd = sc.newAPIHadoopRDD("org.elasticsearch.hadoop.mr.EsInputFormat",
                             "org.apache.hadoop.io.NullWritable",
                             "org.elasticsearch.hadoop.mr.LinkedMapWritable",
                             conf=conf)
>>> rdd.first()  # the result is a MapWritable that is converted to a Python dict
(u'Elasticsearch ID',
 {u'field1': True,
  u'field2': u'Some Text',
  u'field3': 12345})
Note that, if the InputFormat simply depends on a Hadoop configuration and/or input path, and
the key and value classes can easily be converted according to the above table,
then this approach should work well for such cases.
If you have custom serialized binary data (such as loading data from Cassandra / HBase), then you will first need to
transform that data on the Scala/Java side to something which can be handled by pickle’s pickler.
A Converter trait is provided
for this. Simply extend this trait and implement your transformation code in the convert
method. Remember to ensure that this class, along with any dependencies required to access your InputFormat, are packaged into your Spark job jar and included on the PySpark
classpath.
See the Python examples and
the Converter examples
for examples of using Cassandra / HBase InputFormat and OutputFormat with custom converters.


Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.
Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:
scala> val distFile = sc.textFile("data.txt")
distFile: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[10] at textFile at <console>:26
Once created, distFile can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the map and reduce operations as follows: distFile.map(s => s.length).reduce((a, b) => a + b).
Some notes on reading files with Spark:


If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.


All of Spark’s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile("/my/directory"), textFile("/my/directory/*.txt"), and textFile("/my/directory/*.gz"). When multiple files are read, the order of the partitions depends on the order the files are returned from the filesystem. It may or may not, for example, follow the lexicographic ordering of the files by path. Within a partition, elements are ordered according to their order in the underlying file.


The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.


Apart from text files, Spark’s Scala API also supports several other data formats:


SparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file. Partitioning is determined by data locality which, in some cases, may result in too few partitions. For those cases, wholeTextFiles provides an optional second argument for controlling the minimal number of partitions.


For SequenceFiles, use SparkContext’s sequenceFile[K, V] method where K and V are the types of key and values in the file. These should be subclasses of Hadoop’s Writable interface, like IntWritable and Text. In addition, Spark allows you to specify native types for a few common Writables; for example, sequenceFile[Int, String] will automatically read IntWritables and Texts.


For other Hadoop InputFormats, you can use the SparkContext.hadoopRDD method, which takes an arbitrary JobConf and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use SparkContext.newAPIHadoopRDD for InputFormats based on the “new” MapReduce API (org.apache.hadoop.mapreduce).


RDD.saveAsObjectFile and SparkContext.objectFile support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.




Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.
Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:
JavaRDD<String> distFile = sc.textFile("data.txt");
Once created, distFile can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the map and reduce operations as follows: distFile.map(s -> s.length()).reduce((a, b) -> a + b).
Some notes on reading files with Spark:


If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.


All of Spark’s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile("/my/directory"), textFile("/my/directory/*.txt"), and textFile("/my/directory/*.gz").


The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.


Apart from text files, Spark’s Java API also supports several other data formats:


JavaSparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file.


For SequenceFiles, use SparkContext’s sequenceFile[K, V] method where K and V are the types of key and values in the file. These should be subclasses of Hadoop’s Writable interface, like IntWritable and Text.


For other Hadoop InputFormats, you can use the JavaSparkContext.hadoopRDD method, which takes an arbitrary JobConf and input format class, key class and value class. Set these the same way you would for a Hadoop job with your input source. You can also use JavaSparkContext.newAPIHadoopRDD for InputFormats based on the “new” MapReduce API (org.apache.hadoop.mapreduce).


JavaRDD.saveAsObjectFile and JavaSparkContext.objectFile support saving an RDD in a simple format consisting of serialized Java objects. While this is not as efficient as specialized formats like Avro, it offers an easy way to save any RDD.




RDD Operations
RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).
All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.
By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.
Basics


To illustrate RDD basics, consider the simple program below:
lines = sc.textFile("data.txt")
lineLengths = lines.map(lambda s: len(s))
totalLength = lineLengths.reduce(lambda a, b: a + b)
The first line defines a base RDD from an external file. This dataset is not loaded in memory or
otherwise acted on: lines is merely a pointer to the file.
The second line defines lineLengths as the result of a map transformation. Again, lineLengths
is not immediately computed, due to laziness.
Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks
to run on separate machines, and each machine runs both its part of the map and a local reduction,
returning only its answer to the driver program.
If we also wanted to use lineLengths again later, we could add:
lineLengths.persist()
before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed.


To illustrate RDD basics, consider the simple program below:
val lines = sc.textFile("data.txt")
val lineLengths = lines.map(s => s.length)
val totalLength = lineLengths.reduce((a, b) => a + b)
The first line defines a base RDD from an external file. This dataset is not loaded in memory or
otherwise acted on: lines is merely a pointer to the file.
The second line defines lineLengths as the result of a map transformation. Again, lineLengths
is not immediately computed, due to laziness.
Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks
to run on separate machines, and each machine runs both its part of the map and a local reduction,
returning only its answer to the driver program.
If we also wanted to use lineLengths again later, we could add:
lineLengths.persist()
before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed.


To illustrate RDD basics, consider the simple program below:
JavaRDD<String> lines = sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(s -> s.length());
int totalLength = lineLengths.reduce((a, b) -> a + b);
The first line defines a base RDD from an external file. This dataset is not loaded in memory or
otherwise acted on: lines is merely a pointer to the file.
The second line defines lineLengths as the result of a map transformation. Again, lineLengths
is not immediately computed, due to laziness.
Finally, we run reduce, which is an action. At this point Spark breaks the computation into tasks
to run on separate machines, and each machine runs both its part of the map and a local reduction,
returning only its answer to the driver program.
If we also wanted to use lineLengths again later, we could add:
lineLengths.persist(StorageLevel.MEMORY_ONLY());
before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed.


Passing Functions to Spark


Spark’s API relies heavily on passing functions in the driver program to run on the cluster.
There are three recommended ways to do this:

Lambda expressions,
for simple functions that can be written as an expression. (Lambdas do not support multi-statement
functions or statements that do not return a value.)
Local defs inside the function calling into Spark, for longer code.
Top-level functions in a module.

For example, to pass a longer function than can be supported using a lambda, consider
the code below:
"""MyScript.py"""
if __name__ == "__main__":
    def myFunc(s):
        words = s.split(" ")
        return len(words)

    sc = SparkContext(...)
    sc.textFile("file.txt").map(myFunc)
Note that while it is also possible to pass a reference to a method in a class instance (as opposed to
a singleton object), this requires sending the object that contains that class along with the method.
For example, consider:
class MyClass(object):
    def func(self, s):
        return s
    def doStuff(self, rdd):
        return rdd.map(self.func)
Here, if we create a new MyClass and call doStuff on it, the map inside there references the
func method of that MyClass instance, so the whole object needs to be sent to the cluster.
In a similar way, accessing fields of the outer object will reference the whole object:
class MyClass(object):
    def __init__(self):
        self.field = "Hello"
    def doStuff(self, rdd):
        return rdd.map(lambda s: self.field + s)
To avoid this issue, the simplest way is to copy field into a local variable instead
of accessing it externally:
def doStuff(self, rdd):
    field = self.field
    return rdd.map(lambda s: field + s)


Spark’s API relies heavily on passing functions in the driver program to run on the cluster.
There are two recommended ways to do this:

Anonymous function syntax,
which can be used for short pieces of code.
Static methods in a global singleton object. For example, you can define object MyFunctions and then
pass MyFunctions.func1, as follows:

object MyFunctions {
  def func1(s: String): String = { ... }
}

myRdd.map(MyFunctions.func1)
Note that while it is also possible to pass a reference to a method in a class instance (as opposed to
a singleton object), this requires sending the object that contains that class along with the method.
For example, consider:
class MyClass {
  def func1(s: String): String = { ... }
  def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(func1) }
}
Here, if we create a new MyClass instance and call doStuff on it, the map inside there references the
func1 method of that MyClass instance, so the whole object needs to be sent to the cluster. It is
similar to writing rdd.map(x => this.func1(x)).
In a similar way, accessing fields of the outer object will reference the whole object:
class MyClass {
  val field = "Hello"
  def doStuff(rdd: RDD[String]): RDD[String] = { rdd.map(x => field + x) }
}
is equivalent to writing rdd.map(x => this.field + x), which references all of this. To avoid this
issue, the simplest way is to copy field into a local variable instead of accessing it externally:
def doStuff(rdd: RDD[String]): RDD[String] = {
  val field_ = this.field
  rdd.map(x => field_ + x)
}


Spark’s API relies heavily on passing functions in the driver program to run on the cluster.
In Java, functions are represented by classes implementing the interfaces in the
org.apache.spark.api.java.function package.
There are two ways to create such functions:

Implement the Function interfaces in your own class, either as an anonymous inner class or a named one,
and pass an instance of it to Spark.
Use lambda expressions
to concisely define an implementation.

While much of this guide uses lambda syntax for conciseness, it is easy to use all the same APIs
in long-form. For example, we could have written our code above as follows:
JavaRDD<String> lines = sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(new Function<String, Integer>() {
  public Integer call(String s) { return s.length(); }
});
int totalLength = lineLengths.reduce(new Function2<Integer, Integer, Integer>() {
  public Integer call(Integer a, Integer b) { return a + b; }
});
Or, if writing the functions inline is unwieldy:
class GetLength implements Function<String, Integer> {
  public Integer call(String s) { return s.length(); }
}
class Sum implements Function2<Integer, Integer, Integer> {
  public Integer call(Integer a, Integer b) { return a + b; }
}

JavaRDD<String> lines = sc.textFile("data.txt");
JavaRDD<Integer> lineLengths = lines.map(new GetLength());
int totalLength = lineLengths.reduce(new Sum());
Note that anonymous inner classes in Java can also access variables in the enclosing scope as long
as they are marked final. Spark will ship copies of these variables to each worker node as it does
for other languages.


Understanding closures 
One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses foreach() to increment a counter, but similar issues can occur for other operations as well.
Example
Consider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in local mode (--master = local[n]) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):


counter = 0
rdd = sc.parallelize(data)

# Wrong: Don't do this!!
def increment_counter(x):
    global counter
    counter += x
rdd.foreach(increment_counter)

print("Counter value: ", counter)


var counter = 0
var rdd = sc.parallelize(data)

// Wrong: Don't do this!!
rdd.foreach(x => counter += x)

println("Counter value: " + counter)


int counter = 0;
JavaRDD<Integer> rdd = sc.parallelize(data);

// Wrong: Don't do this!!
rdd.foreach(x -> counter += x);

println("Counter value: " + counter);


Local vs. cluster modes
The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.
The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.
In local mode, in some circumstances, the foreach function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.
To ensure well-defined behavior in these sorts of scenarios one should use an Accumulator. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.
In general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.
Printing elements of an RDD
Another common idiom is attempting to print out the elements of an RDD using rdd.foreach(println) or rdd.map(println). On a single machine, this will generate the expected output and print all the RDD’s elements. However, in cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these! To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: rdd.collect().foreach(println). This can cause the driver to run out of memory, though, because collect() fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the take(): rdd.take(100).foreach(println).
Working with Key-Value Pairs


While most Spark operations work on RDDs containing any type of objects, a few special operations are
only available on RDDs of key-value pairs.
The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements
by a key.
In Python, these operations work on RDDs containing built-in Python tuples such as (1, 2).
Simply create such tuples and then call your desired operation.
For example, the following code uses the reduceByKey operation on key-value pairs to count how
many times each line of text occurs in a file:
lines = sc.textFile("data.txt")
pairs = lines.map(lambda s: (s, 1))
counts = pairs.reduceByKey(lambda a, b: a + b)
We could also use counts.sortByKey(), for example, to sort the pairs alphabetically, and finally
counts.collect() to bring them back to the driver program as a list of objects.


While most Spark operations work on RDDs containing any type of objects, a few special operations are
only available on RDDs of key-value pairs.
The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements
by a key.
In Scala, these operations are automatically available on RDDs containing
Tuple2 objects
(the built-in tuples in the language, created by simply writing (a, b)). The key-value pair operations are available in the
PairRDDFunctions class,
which automatically wraps around an RDD of tuples.
For example, the following code uses the reduceByKey operation on key-value pairs to count how
many times each line of text occurs in a file:
val lines = sc.textFile("data.txt")
val pairs = lines.map(s => (s, 1))
val counts = pairs.reduceByKey((a, b) => a + b)
We could also use counts.sortByKey(), for example, to sort the pairs alphabetically, and finally
counts.collect() to bring them back to the driver program as an array of objects.
Note: when using custom objects as the key in key-value pair operations, you must be sure that a
custom equals() method is accompanied with a matching hashCode() method.  For full details, see
the contract outlined in the Object.hashCode()
documentation.


While most Spark operations work on RDDs containing any type of objects, a few special operations are
only available on RDDs of key-value pairs.
The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements
by a key.
In Java, key-value pairs are represented using the
scala.Tuple2 class
from the Scala standard library. You can simply call new Tuple2(a, b) to create a tuple, and access
its fields later with tuple._1() and tuple._2().
RDDs of key-value pairs are represented by the
JavaPairRDD class. You can construct
JavaPairRDDs from JavaRDDs using special versions of the map operations, like
mapToPair and flatMapToPair. The JavaPairRDD will have both standard RDD functions and special
key-value ones.
For example, the following code uses the reduceByKey operation on key-value pairs to count how
many times each line of text occurs in a file:
JavaRDD<String> lines = sc.textFile("data.txt");
JavaPairRDD<String, Integer> pairs = lines.mapToPair(s -> new Tuple2(s, 1));
JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b);
We could also use counts.sortByKey(), for example, to sort the pairs alphabetically, and finally
counts.collect() to bring them back to the driver program as an array of objects.
Note: when using custom objects as the key in key-value pair operations, you must be sure that a
custom equals() method is accompanied with a matching hashCode() method.  For full details, see
the contract outlined in the Object.hashCode()
documentation.


Transformations
The following table lists some of the common transformations supported by Spark. Refer to the
RDD API doc
(Scala,
 Java,
 Python,
 R)
and pair RDD functions doc
(Scala,
 Java)
for details.

TransformationMeaning

 map(func) 
 Return a new distributed dataset formed by passing each element of the source through a function func. 


 filter(func) 
 Return a new dataset formed by selecting those elements of the source on which func returns true. 


 flatMap(func) 
 Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). 


 mapPartitions(func)  
 Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type
    Iterator<T> => Iterator<U> when running on an RDD of type T. 


 mapPartitionsWithIndex(func) 
 Similar to mapPartitions, but also provides func with an integer value representing the index of
  the partition, so func must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.
  


 sample(withReplacement, fraction, seed) 
 Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed. 


 union(otherDataset) 
 Return a new dataset that contains the union of the elements in the source dataset and the argument. 


 intersection(otherDataset) 
 Return a new RDD that contains the intersection of elements in the source dataset and the argument. 


 distinct([numPartitions])) 
 Return a new dataset that contains the distinct elements of the source dataset.


 groupByKey([numPartitions])  
 When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. 
Note: If you are grouping in order to perform an aggregation (such as a sum or
      average) over each key, using reduceByKey or aggregateByKey will yield much better
      performance.
    
Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.
      You can pass an optional numPartitions argument to set a different number of tasks.
  


 reduceByKey(func, [numPartitions])  
 When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. 


 aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])  
 When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. 


 sortByKey([ascending], [numPartitions])  
 When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.


 join(otherDataset, [numPartitions])  
 When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.
    Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.
  


 cogroup(otherDataset, [numPartitions])  
 When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith. 


 cartesian(otherDataset) 
 When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements). 


 pipe(command, [envVars]) 
 Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the
    process's stdin and lines output to its stdout are returned as an RDD of strings. 


 coalesce(numPartitions)  
 Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently
    after filtering down a large dataset. 


 repartition(numPartitions) 
 Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.
    This always shuffles all data over the network. 


 repartitionAndSortWithinPartitions(partitioner) 
 Repartition the RDD according to the given partitioner and, within each resulting partition,
  sort records by their keys. This is more efficient than calling repartition and then sorting within
  each partition because it can push the sorting down into the shuffle machinery. 


Actions
The following table lists some of the common actions supported by Spark. Refer to the
RDD API doc
(Scala,
 Java,
 Python,
 R)
and pair RDD functions doc
(Scala,
 Java)
for details.

ActionMeaning

 reduce(func) 
 Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. 


 collect() 
 Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. 


 count() 
 Return the number of elements in the dataset. 


 first() 
 Return the first element of the dataset (similar to take(1)). 


 take(n) 
 Return an array with the first n elements of the dataset. 


 takeSample(withReplacement, num, [seed]) 
 Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.


 takeOrdered(n, [ordering]) 
 Return the first n elements of the RDD using either their natural order or a custom comparator. 


 saveAsTextFile(path) 
 Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. 


 saveAsSequenceFile(path)  (Java and Scala) 
 Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also
   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). 


 saveAsObjectFile(path)  (Java and Scala) 
 Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using
    SparkContext.objectFile(). 


 countByKey()  
 Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. 


 foreach(func) 
 Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.
  Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures  for more details.


The Spark RDD API also exposes asynchronous versions of some actions, like foreachAsync for foreach, which immediately return a FutureAction to the caller instead of blocking on completion of the action. This can be used to manage or wait for the asynchronous execution of the action.
Shuffle operations
Certain operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s
mechanism for re-distributing data so that it’s grouped differently across partitions. This typically
involves copying data across executors and machines, making the shuffle a complex and
costly operation.
Background
To understand what happens during the shuffle, we can consider the example of the
reduceByKey operation. The reduceByKey operation generates a new RDD where all
values for a single key are combined into a tuple - the key and the result of executing a reduce
function against all values associated with that key. The challenge is that not all values for a
single key necessarily reside on the same partition, or even the same machine, but they must be
co-located to compute the result.
In Spark, data is generally not distributed across partitions to be in the necessary place for a
specific operation. During computations, a single task will operate on a single partition - thus, to
organize all the data for a single reduceByKey reduce task to execute, Spark needs to perform an
all-to-all operation. It must read from all partitions to find all the values for all keys,
and then bring together values across partitions to compute the final result for each key -
this is called the shuffle.
Although the set of elements in each partition of newly shuffled data will be deterministic, and so
is the ordering of partitions themselves, the ordering of these elements is not. If one desires predictably
ordered data following shuffle then it’s possible to use:

mapPartitions to sort each partition using, for example, .sorted
repartitionAndSortWithinPartitions to efficiently sort partitions while simultaneously repartitioning
sortBy to make a globally ordered RDD

Operations which can cause a shuffle include repartition operations like
repartition and coalesce, ‘ByKey operations
(except for counting) like groupByKey and reduceByKey, and
join operations like cogroup and join.
Performance Impact
The Shuffle is an expensive operation since it involves disk I/O, data serialization, and
network I/O. To organize data for the shuffle, Spark generates sets of tasks - map tasks to
organize the data, and a set of reduce tasks to aggregate it. This nomenclature comes from
MapReduce and does not directly relate to Spark’s map and reduce operations.
Internally, results from individual map tasks are kept in memory until they can’t fit. Then, these
are sorted based on the target partition and written to a single file. On the reduce side, tasks
read the relevant sorted blocks.
Certain shuffle operations can consume significant amounts of heap memory since they employ
in-memory data structures to organize records before or after transferring them. Specifically,
reduceByKey and aggregateByKey create these structures on the map side, and 'ByKey operations
generate these on the reduce side. When data does not fit in memory Spark will spill these tables
to disk, incurring the additional overhead of disk I/O and increased garbage collection.
Shuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files
are preserved until the corresponding RDDs are no longer used and are garbage collected.
This is done so the shuffle files don’t need to be re-created if the lineage is re-computed.
Garbage collection may happen only after a long period of time, if the application retains references
to these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may
consume a large amount of disk space. The temporary storage directory is specified by the
spark.local.dir configuration parameter when configuring the Spark context.
Shuffle behavior can be tuned by adjusting a variety of configuration parameters. See the
‘Shuffle Behavior’ section within the Spark Configuration Guide.
RDD Persistence
One of the most important capabilities in Spark is persisting (or caching) a dataset in memory
across operations. When you persist an RDD, each node stores any partitions of it that it computes in
memory and reuses them in other actions on that dataset (or datasets derived from it). This allows
future actions to be much faster (often by more than 10x). Caching is a key tool for
iterative algorithms and fast interactive use.
You can mark an RDD to be persisted using the persist() or cache() methods on it. The first time
it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant –
if any partition of an RDD is lost, it will automatically be recomputed using the transformations
that originally created it.
In addition, each persisted RDD can be stored using a different storage level, allowing you, for example,
to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space),
replicate it across nodes.
These levels are set by passing a
StorageLevel object (Scala,
Java,
Python)
to persist(). The cache() method is a shorthand for using the default storage level,
which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory). The full set of
storage levels is:

Storage LevelMeaning

 MEMORY_ONLY 
 Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will
    not be cached and will be recomputed on the fly each time they're needed. This is the default level. 


 MEMORY_AND_DISK 
 Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the
    partitions that don't fit on disk, and read them from there when they're needed. 


 MEMORY_ONLY_SER  (Java and Scala) 
 Store RDD as serialized Java objects (one byte array per partition).
    This is generally more space-efficient than deserialized objects, especially when using a
    fast serializer, but more CPU-intensive to read.
  


 MEMORY_AND_DISK_SER  (Java and Scala) 
 Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of
    recomputing them on the fly each time they're needed. 


 DISK_ONLY 
 Store the RDD partitions only on disk. 


 MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.  
 Same as the levels above, but replicate each partition on two cluster nodes. 


 OFF_HEAP (experimental) 
 Similar to MEMORY_ONLY_SER, but store the data in
    off-heap memory. This requires off-heap memory to be enabled. 


Note: In Python, stored objects will always be serialized with the Pickle library,
so it does not matter whether you choose a serialized level. The available storage levels in Python include MEMORY_ONLY, MEMORY_ONLY_2,
MEMORY_AND_DISK, MEMORY_AND_DISK_2, DISK_ONLY, DISK_ONLY_2, and DISK_ONLY_3.
Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call persist on the resulting RDD if they plan to reuse it.
Which Storage Level to Choose?
Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU
efficiency. We recommend going through the following process to select one:


If your RDDs fit comfortably with the default storage level (MEMORY_ONLY), leave them that way.
This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.


If not, try using MEMORY_ONLY_SER and selecting a fast serialization library to
make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)


Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter
a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from
disk.


Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve
requests from a web application). All the storage levels provide full fault tolerance by
recomputing lost data, but the replicated ones let you continue running tasks on the RDD without
waiting to recompute a lost partition.


Removing Data
Spark automatically monitors cache usage on each node and drops out old data partitions in a
least-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for
it to fall out of the cache, use the RDD.unpersist() method. Note that this method does not
block by default. To block until resources are freed, specify blocking=true when calling this method.
Shared Variables
Normally, when a function passed to a Spark operation (such as map or reduce) is executed on a
remote cluster node, it works on separate copies of all the variables used in the function. These
variables are copied to each machine, and no updates to the variables on the remote machine are
propagated back to the driver program. Supporting general, read-write shared variables across tasks
would be inefficient. However, Spark does provide two limited types of shared variables for two
common usage patterns: broadcast variables and accumulators.
Broadcast Variables
Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather
than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a
large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables
using efficient broadcast algorithms to reduce communication cost.
Spark actions are executed through a set of stages, separated by distributed “shuffle” operations.
Spark automatically broadcasts the common data needed by tasks within each stage. The data
broadcasted this way is cached in serialized form and deserialized before running each task. This
means that explicitly creating broadcast variables is only useful when tasks across multiple stages
need the same data or when caching the data in deserialized form is important.
Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The
broadcast variable is a wrapper around v, and its value can be accessed by calling the value
method. The code below shows this:


>>> broadcastVar = sc.broadcast([1, 2, 3])
<pyspark.broadcast.Broadcast object at 0x102789f10>

>>> broadcastVar.value
[1, 2, 3]


scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)

scala> broadcastVar.value
res0: Array[Int] = Array(1, 2, 3)


Broadcast<int[]> broadcastVar = sc.broadcast(new int[] {1, 2, 3});

broadcastVar.value();
// returns [1, 2, 3]


After the broadcast variable is created, it should be used instead of the value v in any functions
run on the cluster so that v is not shipped to the nodes more than once. In addition, the object
v should not be modified after it is broadcast in order to ensure that all nodes get the same
value of the broadcast variable (e.g. if the variable is shipped to a new node later).
To release the resources that the broadcast variable copied onto executors, call .unpersist().
If the broadcast is used again afterwards, it will be re-broadcast. To permanently release all
resources used by the broadcast variable, call .destroy(). The broadcast variable can’t be used
after that. Note that these methods do not block by default. To block until resources are freed,
specify blocking=true when calling them.
Accumulators
Accumulators are variables that are only “added” to through an associative and commutative operation and can
therefore be efficiently supported in parallel. They can be used to implement counters (as in
MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers
can add support for new types.
As a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance counter) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table.



Tracking accumulators in the UI can be useful for understanding the progress of
running stages (NOTE: this is not yet supported in Python).


An accumulator is created from an initial value v by calling SparkContext.accumulator(v). Tasks
running on a cluster can then add to it using the add method or the += operator. However, they cannot read its value.
Only the driver program can read the accumulator’s value, using its value method.
The code below shows an accumulator being used to add up the elements of an array:
>>> accum = sc.accumulator(0)
>>> accum
Accumulator<id=0, value=0>

>>> sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))
...
10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

>>> accum.value
10
While this code used the built-in support for accumulators of type Int, programmers can also
create their own types by subclassing AccumulatorParam.
The AccumulatorParam interface has two methods: zero for providing a “zero value” for your data
type, and addInPlace for adding two values together. For example, supposing we had a Vector class
representing mathematical vectors, we could write:
class VectorAccumulatorParam(AccumulatorParam):
    def zero(self, initialValue):
        return Vector.zeros(initialValue.size)

    def addInPlace(self, v1, v2):
        v1 += v2
        return v1

# Then, create an Accumulator of this type:
vecAccum = sc.accumulator(Vector(...), VectorAccumulatorParam())


A numeric accumulator can be created by calling SparkContext.longAccumulator() or SparkContext.doubleAccumulator()
to accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using
the add method.  However, they cannot read its value. Only the driver program can read the accumulator’s value,
using its value method.
The code below shows an accumulator being used to add up the elements of an array:
scala> val accum = sc.longAccumulator("My Accumulator")
accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)

scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))
...
10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

scala> accum.value
res2: Long = 10
While this code used the built-in support for accumulators of type Long, programmers can also
create their own types by subclassing AccumulatorV2.
The AccumulatorV2 abstract class has several methods which one has to override: reset for resetting
the accumulator to zero, add for adding another value into the accumulator,
merge for merging another same-type accumulator into this one. Other methods that must be overridden
are contained in the API documentation. For example, supposing we had a MyVector class
representing mathematical vectors, we could write:
class VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] {

  private val myVector: MyVector = MyVector.createZeroVector

  def reset(): Unit = {
    myVector.reset()
  }

  def add(v: MyVector): Unit = {
    myVector.add(v)
  }
  ...
}

// Then, create an Accumulator of this type:
val myVectorAcc = new VectorAccumulatorV2
// Then, register it into spark context:
sc.register(myVectorAcc, "MyVectorAcc1")
Note that, when programmers define their own type of AccumulatorV2, the resulting type can be different than that of the elements added.


A numeric accumulator can be created by calling SparkContext.longAccumulator() or SparkContext.doubleAccumulator()
to accumulate values of type Long or Double, respectively. Tasks running on a cluster can then add to it using
the add method.  However, they cannot read its value. Only the driver program can read the accumulator’s value,
using its value method.
The code below shows an accumulator being used to add up the elements of an array:
LongAccumulator accum = jsc.sc().longAccumulator();

sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -> accum.add(x));
// ...
// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

accum.value();
// returns 10
While this code used the built-in support for accumulators of type Long, programmers can also
create their own types by subclassing AccumulatorV2.
The AccumulatorV2 abstract class has several methods which one has to override: reset for resetting
the accumulator to zero, add for adding another value into the accumulator,
merge for merging another same-type accumulator into this one. Other methods that must be overridden
are contained in the API documentation. For example, supposing we had a MyVector class
representing mathematical vectors, we could write:
class VectorAccumulatorV2 implements AccumulatorV2<MyVector, MyVector> {

  private MyVector myVector = MyVector.createZeroVector();

  public void reset() {
    myVector.reset();
  }

  public void add(MyVector v) {
    myVector.add(v);
  }
  ...
}

// Then, create an Accumulator of this type:
VectorAccumulatorV2 myVectorAcc = new VectorAccumulatorV2();
// Then, register it into spark context:
jsc.sc().register(myVectorAcc, "MyVectorAcc1");
Note that, when programmers define their own type of AccumulatorV2, the resulting type can be different than that of the elements added.
Warning: When a Spark task finishes, Spark will try to merge the accumulated updates in this task to an accumulator.
If it fails, Spark will ignore the failure and still mark the task successful and continue to run other tasks. Hence,
a buggy accumulator will not impact a Spark job, but it may not get updated correctly although a Spark job is successful.


For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator
will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware
of that each task’s update may be applied more than once if tasks or job stages are re-executed.
Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map(). The below code fragment demonstrates this property:


accum = sc.accumulator(0)
def g(x):
    accum.add(x)
    return f(x)
data.map(g)
# Here, accum is still 0 because no actions have caused the `map` to be computed.


val accum = sc.longAccumulator
data.map { x => accum.add(x); x }
// Here, accum is still 0 because no actions have caused the map operation to be computed.


LongAccumulator accum = jsc.sc().longAccumulator();
data.map(x -> { accum.add(x); return f(x); });
// Here, accum is still 0 because no actions have caused the `map` to be computed.


Deploying to a Cluster
The application submission guide describes how to submit applications to a cluster.
In short, once you package your application into a JAR (for Java/Scala) or a set of .py or .zip files (for Python),
the bin/spark-submit script lets you submit it to any supported cluster manager.
Launching Spark jobs from Java / Scala
The org.apache.spark.launcher
package provides classes for launching Spark jobs as child processes using a simple Java API.
Unit Testing
Spark is friendly to unit testing with any popular unit test framework.
Simply create a SparkContext in your test with the master URL set to local, run your operations,
and then call SparkContext.stop() to tear it down.
Make sure you stop the context within a finally block or the test framework’s tearDown method,
as Spark does not support two contexts running concurrently in the same program.
Where to Go from Here
You can see some example Spark programs on the Spark website.
In addition, Spark includes several samples in the examples directory
(Scala,
 Java,
 Python,
 R).
You can run Java and Scala examples by passing the class name to Spark’s bin/run-example script; for instance:
./bin/run-example SparkPi

For Python examples, use spark-submit instead:
./bin/spark-submit examples/src/main/python/pi.py

For R examples, use spark-submit instead:
./bin/spark-submit examples/src/main/r/dataframe.R

For help on optimizing your programs, the configuration and
tuning guides provide information on best practices. They are especially important for
making sure that your data is stored in memory in an efficient format.
For help on deploying, the cluster mode overview describes the components involved
in distributed operation and supported cluster managers.
Finally, full API documentation is available in
Scala, Java, Python and R.




















  




Running Spark on Kubernetes - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Running Spark on Kubernetes

Security 
User Identity
Volume Mounts


Prerequisites
How it works
Submitting Applications to Kubernetes 
Docker Images
Cluster Mode
Client Mode 
Client Mode Networking
Client Mode Executor Pod Garbage Collection
Authentication Parameters


IPv4 and IPv6
Dependency Management
Secret Management
Pod Template
Using Kubernetes Volumes 
PVC-oriented executor pod allocation


Local Storage 
Using RAM for local storage


Introspection and Debugging 
Accessing Logs
Accessing Driver UI
Debugging


Kubernetes Features 
Configuration File
Contexts
Namespaces
RBAC


Spark Application Management
Future Work


Configuration 
Spark Properties
Pod template properties
Pod Metadata
Pod Spec
Container spec
Resource Allocation and Configuration Overview
Resource Level Scheduling Overview 
Priority Scheduling
Customized Kubernetes Schedulers for Spark on Kubernetes
Using Volcano as Customized Scheduler for Spark on Kubernetes 
Prerequisites
Build
Usage
Volcano Feature Step
Volcano PodGroup Template


Using Apache YuniKorn as Customized Scheduler for Spark on Kubernetes 
Prerequisites
Get started




Stage Level Scheduling Overview



Spark can run on clusters managed by Kubernetes. This feature makes use of native
Kubernetes scheduler that has been added to Spark.
Security
Security features like authentication are not enabled by default. When deploying a cluster that is open to the internet
or an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications
from running on the cluster.
Please see Spark Security and the specific security sections in this doc before running Spark.
User Identity
Images built from the project provided Dockerfiles contain a default USER directive with a default UID of 185.  This means that the resulting images will be running the Spark processes as this UID inside the container. Security conscious deployments should consider providing custom images with USER directives specifying their desired unprivileged UID and GID.  The resulting UID should include the root group in its supplementary groups in order to be able to run the Spark executables.  Users building their own images with the provided docker-image-tool.sh script can use the -u <uid> option to specify the desired UID.
Alternatively the Pod Template feature can be used to add a Security Context with a runAsUser to the pods that Spark submits.  This can be used to override the USER directives in the images themselves.  Please bear in mind that this requires cooperation from your users and as such may not be a suitable solution for shared environments.  Cluster administrators should use Pod Security Policies if they wish to limit the users that pods may run as.
Volume Mounts
As described later in this document under Using Kubernetes Volumes Spark on K8S provides configuration options that allow for mounting certain volume types into the driver and executor pods.  In particular it allows for hostPath volumes which as described in the Kubernetes documentation have known security vulnerabilities.
Cluster administrators should use Pod Security Policies to limit the ability to mount hostPath volumes appropriately for their environments.
Prerequisites

A running Kubernetes cluster at version >= 1.24 with access configured to it using
kubectl.  If you do not already have a working Kubernetes cluster,
you may set up a test cluster on your local machine using
minikube.
    
We recommend using the latest release of minikube with the DNS addon enabled.
Be aware that the default minikube configuration is not enough for running Spark applications.
We recommend 3 CPUs and 4g of memory to be able to start a simple Spark application with a single
executor.
Check kubernetes-client library’s version of your Spark environment, and its compatibility with your Kubernetes cluster’s version.


You must have appropriate permissions to list, create, edit and delete
pods in your cluster. You can verify that you can list these resources
by running kubectl auth can-i <list|create|edit|delete> pods.
    
The service account credentials used by the driver pods must be allowed to create pods, services and configmaps.


You must have Kubernetes DNS configured in your cluster.

How it works



spark-submit can be directly used to submit a Spark application to a Kubernetes cluster.
The submission mechanism works as follows:

Spark creates a Spark driver running within a Kubernetes pod.
The driver creates executors which are also running within Kubernetes pods and connects to them, and executes application code.
When the application completes, the executor pods terminate and are cleaned up, but the driver pod persists
logs and remains in “completed” state in the Kubernetes API until it’s eventually garbage collected or manually cleaned up.

Note that in the completed state, the driver pod does not use any computational or memory resources.
The driver and executor pod scheduling is handled by Kubernetes. Communication to the Kubernetes API is done via fabric8. It is possible to schedule the
driver and executor pods on a subset of available nodes through a node selector
using the configuration property for it. It will be possible to use more advanced
scheduling hints like node/pod affinities in a future release.
Submitting Applications to Kubernetes
Docker Images
Kubernetes requires users to supply images that can be deployed into containers within pods. The images are built to
be run in a container runtime environment that Kubernetes supports. Docker is a container runtime environment that is
frequently used with Kubernetes. Spark (starting with version 2.3) ships with a Dockerfile that can be used for this
purpose, or customized to match an individual application’s needs. It can be found in the kubernetes/dockerfiles/
directory.
Spark also ships with a bin/docker-image-tool.sh script that can be used to build and publish the Docker images to
use with the Kubernetes backend.
Example usage is:
$ ./bin/docker-image-tool.sh -r <repo> -t my-tag build
$ ./bin/docker-image-tool.sh -r <repo> -t my-tag push

This will build using the projects provided default Dockerfiles. To see more options available for customising the behaviour of this tool, including providing custom Dockerfiles, please run with the -h flag.
By default bin/docker-image-tool.sh builds docker image for running JVM jobs. You need to opt-in to build additional
language binding docker images.
Example usage is
# To build additional PySpark docker image
$ ./bin/docker-image-tool.sh -r <repo> -t my-tag -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build

# To build additional SparkR docker image
$ ./bin/docker-image-tool.sh -r <repo> -t my-tag -R ./kubernetes/dockerfiles/spark/bindings/R/Dockerfile build

You can also use the Apache Spark Docker images (such as apache/spark:<version>) directly.
Cluster Mode
To launch Spark Pi in cluster mode,
$ ./bin/spark-submit \
    --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=5 \
    --conf spark.kubernetes.container.image=<spark-image> \
    local:///path/to/examples.jar

The Spark master, specified either via passing the --master command line argument to spark-submit or by setting
spark.master in the application’s configuration, must be a URL with the format k8s://<api_server_host>:<k8s-apiserver-port>. The port must always be specified, even if it’s the HTTPS port 443. Prefixing the
master string with k8s:// will cause the Spark application to launch on the Kubernetes cluster, with the API server
being contacted at api_server_url. If no HTTP protocol is specified in the URL, it defaults to https. For example,
setting the master to k8s://example.com:443 is equivalent to setting it to k8s://https://example.com:443, but to
connect without TLS on a different port, the master would be set to k8s://http://example.com:8080.
In Kubernetes mode, the Spark application name that is specified by spark.app.name or the --name argument to
spark-submit is used by default to name the Kubernetes resources created like drivers and executors. So, application names
must consist of lower case alphanumeric characters, -, and .  and must start and end with an alphanumeric character.
If you have a Kubernetes cluster setup, one way to discover the apiserver URL is by executing kubectl cluster-info.
$ kubectl cluster-info
Kubernetes master is running at http://127.0.0.1:6443

In the above example, the specific Kubernetes cluster can be used with spark-submit by specifying
--master k8s://http://127.0.0.1:6443 as an argument to spark-submit. Additionally, it is also possible to use the
authenticating proxy, kubectl proxy to communicate to the Kubernetes API.
The local proxy can be started by:
$ kubectl proxy

If the local proxy is running at localhost:8001, --master k8s://http://127.0.0.1:8001 can be used as the argument to
spark-submit. Finally, notice that in the above example we specify a jar with a specific URI with a scheme of local://.
This URI is the location of the example jar that is already in the Docker image.
Client Mode
Starting with Spark 2.4.0, it is possible to run Spark applications on Kubernetes in client mode. When your application
runs in client mode, the driver can run inside a pod or on a physical host. When running an application in client mode,
it is recommended to account for the following factors:
Client Mode Networking
Spark executors must be able to connect to the Spark driver over a hostname and a port that is routable from the Spark
executors. The specific network configuration that will be required for Spark to work in client mode will vary per
setup. If you run your driver inside a Kubernetes pod, you can use a
headless service to allow your
driver pod to be routable from the executors by a stable hostname. When deploying your headless service, ensure that
the service’s label selector will only match the driver pod and no other pods; it is recommended to assign your driver
pod a sufficiently unique label and to use that label in the label selector of the headless service. Specify the driver’s
hostname via spark.driver.host and your spark driver’s port to spark.driver.port.
Client Mode Executor Pod Garbage Collection
If you run your Spark driver in a pod, it is highly recommended to set spark.kubernetes.driver.pod.name to the name of that pod.
When this property is set, the Spark scheduler will deploy the executor pods with an
OwnerReference, which in turn will
ensure that once the driver pod is deleted from the cluster, all of the application’s executor pods will also be deleted.
The driver will look for a pod with the given name in the namespace specified by spark.kubernetes.namespace, and
an OwnerReference pointing to that pod will be added to each executor pod’s OwnerReferences list. Be careful to avoid
setting the OwnerReference to a pod that is not actually that driver pod, or else the executors may be terminated
prematurely when the wrong pod is deleted.
If your application is not running inside a pod, or if spark.kubernetes.driver.pod.name is not set when your application is
actually running in a pod, keep in mind that the executor pods may not be properly deleted from the cluster when the
application exits. The Spark scheduler attempts to delete these pods, but if the network request to the API server fails
for any reason, these pods will remain in the cluster. The executor processes should exit when they cannot reach the
driver, so the executor pods should not consume compute resources (cpu and memory) in the cluster after your application
exits.
You may use spark.kubernetes.executor.podNamePrefix to fully control the executor pod names.
When this property is set, it’s highly recommended to make it unique across all jobs in the same namespace.
Authentication Parameters
Use the exact prefix spark.kubernetes.authenticate for Kubernetes authentication parameters in client mode.
IPv4 and IPv6
Starting with 3.4.0, Spark supports additionally IPv6-only environment via
IPv4/IPv6 dual-stack network
feature which enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.
According to the K8s cluster capability, spark.kubernetes.driver.service.ipFamilyPolicy and
spark.kubernetes.driver.service.ipFamilies can be one of SingleStack, PreferDualStack,
and RequireDualStack and one of IPv4, IPv6, IPv4,IPv6, and IPv6,IPv4 respectively.
By default, Spark uses spark.kubernetes.driver.service.ipFamilyPolicy=SingleStack and
spark.kubernetes.driver.service.ipFamilies=IPv4.
To use only IPv6, you can submit your jobs with the following.
...
    --conf spark.kubernetes.driver.service.ipFamilies=IPv6 \

In DualStack environment, you may need java.net.preferIPv6Addresses=true for JVM
and SPARK_PREFER_IPV6=true for Python additionally to use IPv6.
Dependency Management
If your application’s dependencies are all hosted in remote locations like HDFS or HTTP servers, they may be referred to
by their appropriate remote URIs. Also, application dependencies can be pre-mounted into custom-built Docker images.
Those dependencies can be added to the classpath by referencing them with local:// URIs and/or setting the
SPARK_EXTRA_CLASSPATH environment variable in your Dockerfiles. The local:// scheme is also required when referring to
dependencies in custom-built Docker images in spark-submit. We support dependencies from the submission
client’s local file system using the file:// scheme or without a scheme (using a full path), where the destination should be a Hadoop compatible filesystem.
A typical example of this using S3 is via passing the following options:
...
--packages org.apache.hadoop:hadoop-aws:3.2.2
--conf spark.kubernetes.file.upload.path=s3a://<s3-bucket>/path
--conf spark.hadoop.fs.s3a.access.key=...
--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
--conf spark.hadoop.fs.s3a.fast.upload=true
--conf spark.hadoop.fs.s3a.secret.key=....
--conf spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp
file:///full/path/to/app.jar

The app jar file will be uploaded to the S3 and then when the driver is launched it will be downloaded
to the driver pod and will be added to its classpath. Spark will generate a subdir under the upload path with a random name
to avoid conflicts with spark apps running in parallel. User could manage the subdirs created according to his needs.
The client scheme is supported for the application jar, and dependencies specified by properties spark.jars, spark.files and spark.archives.
Important: all client-side dependencies will be uploaded to the given path with a flat directory structure so
file names must be unique otherwise files will be overwritten. Also make sure in the derived k8s image default ivy dir
has the required access rights or modify the settings as above. The latter is also important if you use --packages in
cluster mode.
Secret Management
Kubernetes Secrets can be used to provide credentials for a
Spark application to access secured services. To mount a user-specified secret into the driver container, users can use
the configuration property of the form spark.kubernetes.driver.secrets.[SecretName]=<mount path>. Similarly, the
configuration property of the form spark.kubernetes.executor.secrets.[SecretName]=<mount path> can be used to mount a
user-specified secret into the executor containers. Note that it is assumed that the secret to be mounted is in the same
namespace as that of the driver and executor pods. For example, to mount a secret named spark-secret onto the path
/etc/secrets in both the driver and executor containers, add the following options to the spark-submit command:
--conf spark.kubernetes.driver.secrets.spark-secret=/etc/secrets
--conf spark.kubernetes.executor.secrets.spark-secret=/etc/secrets

To use a secret through an environment variable use the following options to the spark-submit command:
--conf spark.kubernetes.driver.secretKeyRef.ENV_NAME=name:key
--conf spark.kubernetes.executor.secretKeyRef.ENV_NAME=name:key

Pod Template
Kubernetes allows defining pods from template files.
Spark users can similarly use template files to define the driver or executor pod configurations that Spark configurations do not support.
To do so, specify the spark properties spark.kubernetes.driver.podTemplateFile and spark.kubernetes.executor.podTemplateFile
to point to files accessible to the spark-submit process.
--conf spark.kubernetes.driver.podTemplateFile=s3a://bucket/driver.yml
--conf spark.kubernetes.executor.podTemplateFile=s3a://bucket/executor.yml

To allow the driver pod access the executor pod template
file, the file will be automatically mounted onto a volume in the driver pod when it’s created.
Spark does not do any validation after unmarshalling these template files and relies on the Kubernetes API server for validation.
It is important to note that Spark is opinionated about certain pod configurations so there are values in the
pod template that will always be overwritten by Spark. Therefore, users of this feature should note that specifying
the pod template file only lets Spark start with a template pod instead of an empty pod during the pod-building process.
For details, see the full list of pod template values that will be overwritten by spark.
Pod template files can also define multiple containers. In such cases, you can use the spark properties
spark.kubernetes.driver.podTemplateContainerName and spark.kubernetes.executor.podTemplateContainerName
to indicate which container should be used as a basis for the driver or executor.
If not specified, or if the container name is not valid, Spark will assume that the first container in the list
will be the driver or executor container.
Using Kubernetes Volumes
Users can mount the following types of Kubernetes volumes into the driver and executor pods:

hostPath: mounts a file or directory from the host node’s filesystem into a pod.
emptyDir: an initially empty volume created when a pod is assigned to a node.
nfs: mounts an existing NFS(Network File System) into a pod.
persistentVolumeClaim: mounts a PersistentVolume into a pod.

NB: Please see the Security section of this document for security issues related to volume mounts.
To mount a volume of any of the types above into the driver pod, use the following configuration property:
--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path=<mount path>
--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly=<true|false>
--conf spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.subPath=<mount subPath>

Specifically, VolumeType can be one of the following values: hostPath, emptyDir, nfs and persistentVolumeClaim. VolumeName is the name you want to use for the volume under the volumes field in the pod specification.
Each supported type of volumes may have some specific configuration options, which can be specified using configuration properties of the following form:
spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].options.[OptionName]=<value>

For example, the server and path of a nfs with volume name images can be specified using the following properties:
spark.kubernetes.driver.volumes.nfs.images.options.server=example.com
spark.kubernetes.driver.volumes.nfs.images.options.path=/data

And, the claim name of a persistentVolumeClaim with volume name checkpointpvc can be specified using the following property:
spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=check-point-pvc-claim

The configuration properties for mounting volumes into the executor pods use prefix spark.kubernetes.executor. instead of spark.kubernetes.driver..
For example, you can mount a dynamically-created persistent volume claim per executor by using OnDemand as a claim name and storageClass and sizeLimit options like the following. This is useful in case of Dynamic Allocation.
spark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.claimName=OnDemand
spark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.storageClass=gp
spark.kubernetes.executor.volumes.persistentVolumeClaim.data.options.sizeLimit=500Gi
spark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.path=/data
spark.kubernetes.executor.volumes.persistentVolumeClaim.data.mount.readOnly=false

For a complete list of available options for each supported type of volumes, please refer to the Spark Properties section below.
PVC-oriented executor pod allocation
Since disks are one of the important resource types, Spark driver provides a fine-grained control
via a set of configurations. For example, by default, on-demand PVCs are owned by executors and
the lifecycle of PVCs are tightly coupled with its owner executors.
However, on-demand PVCs can be owned by driver and reused by another executors during the Spark job’s
lifetime with the following options. This reduces the overhead of PVC creation and deletion.
spark.kubernetes.driver.ownPersistentVolumeClaim=true
spark.kubernetes.driver.reusePersistentVolumeClaim=true

In addition, since Spark 3.4, Spark driver is able to do PVC-oriented executor allocation which means
Spark counts the total number of created PVCs which the job can have, and holds on a new executor creation
if the driver owns the maximum number of PVCs. This helps the transition of the existing PVC from one executor
to another executor.
spark.kubernetes.driver.waitToReusePersistentVolumeClaim=true

Local Storage
Spark supports using volumes to spill data during shuffles and other operations. To use a volume as local storage, the volume’s name should starts with spark-local-dir-, for example:
--conf spark.kubernetes.driver.volumes.[VolumeType].spark-local-dir-[VolumeName].mount.path=<mount path>
--conf spark.kubernetes.driver.volumes.[VolumeType].spark-local-dir-[VolumeName].mount.readOnly=false

Specifically, you can use persistent volume claims if the jobs require large shuffle and sorting operations in executors.
spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName=OnDemand
spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass=gp
spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit=500Gi
spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data
spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly=false

To enable shuffle data recovery feature via the built-in KubernetesLocalDiskShuffleDataIO plugin, we need to have the followings. You may want to enable spark.kubernetes.driver.waitToReusePersistentVolumeClaim additionally.
spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data/spark-x/executor-x
spark.shuffle.sort.io.plugin.class=org.apache.spark.shuffle.KubernetesLocalDiskShuffleDataIO

If no volume is set as local storage, Spark uses temporary scratch space to spill data to disk during shuffles and other operations. When using Kubernetes as the resource manager the pods will be created with an emptyDir volume mounted for each directory listed in spark.local.dir or the environment variable SPARK_LOCAL_DIRS .  If no directories are explicitly specified then a default directory is created and configured appropriately.
emptyDir volumes use the ephemeral storage feature of Kubernetes and do not persist beyond the life of the pod.
Using RAM for local storage
emptyDir volumes use the nodes backing storage for ephemeral storage by default, this behaviour may not be appropriate for some compute environments.  For example if you have diskless nodes with remote storage mounted over a network, having lots of executors doing IO to this remote storage may actually degrade performance.
In this case it may be desirable to set spark.kubernetes.local.dirs.tmpfs=true in your configuration which will cause the emptyDir volumes to be configured as tmpfs i.e. RAM backed volumes.  When configured like this Spark’s local storage usage will count towards your pods memory usage therefore you may wish to increase your memory requests by increasing the value of spark.{driver,executor}.memoryOverheadFactor as appropriate.
Introspection and Debugging
These are the different ways in which you can investigate a running/completed Spark application, monitor progress, and
take actions.
Accessing Logs
Logs can be accessed using the Kubernetes API and the kubectl CLI. When a Spark application is running, it’s possible
to stream logs from the application using:
$ kubectl -n=<namespace> logs -f <driver-pod-name>

The same logs can also be accessed through the
Kubernetes dashboard if installed on
the cluster.
When there exists a log collection system, you can expose it at Spark Driver Executors tab UI. For example,
spark.executorEnv.SPARK_EXECUTOR_ATTRIBUTE_APP_ID='$(SPARK_APPLICATION_ID)'
spark.executorEnv.SPARK_EXECUTOR_ATTRIBUTE_EXECUTOR_ID='$(SPARK_EXECUTOR_ID)'
spark.ui.custom.executor.log.url='https://log-server/log?appId=&execId='

Accessing Driver UI
The UI associated with any application can be accessed locally using
kubectl port-forward.
$ kubectl port-forward <driver-pod-name> 4040:4040

Then, the Spark driver UI can be accessed on http://localhost:4040.
Debugging
There may be several kinds of failures. If the Kubernetes API server rejects the request made from spark-submit, or the
connection is refused for a different reason, the submission logic should indicate the error encountered. However, if there
are errors during the running of the application, often, the best way to investigate may be through the Kubernetes CLI.
To get some basic information about the scheduling decisions made around the driver pod, you can run:
$ kubectl describe pod <spark-driver-pod>

If the pod has encountered a runtime error, the status can be probed further using:
$ kubectl logs <spark-driver-pod>

Status and logs of failed executor pods can be checked in similar ways. Finally, deleting the driver pod will clean up the entire spark
application, including all executors, associated service, etc. The driver pod can be thought of as the Kubernetes representation of
the Spark application.
Kubernetes Features
Configuration File
Your Kubernetes config file typically lives under .kube/config in your home directory or in a location specified by the KUBECONFIG environment variable.  Spark on Kubernetes will attempt to use this file to do an initial auto-configuration of the Kubernetes client used to interact with the Kubernetes cluster.  A variety of Spark configuration properties are provided that allow further customising the client configuration e.g. using an alternative authentication method.
Contexts
Kubernetes configuration files can contain multiple contexts that allow for switching between different clusters and/or user identities.  By default Spark on Kubernetes will use your current context (which can be checked by running kubectl config current-context) when doing the initial auto-configuration of the Kubernetes client.
In order to use an alternative context users can specify the desired context via the Spark configuration property spark.kubernetes.context e.g. spark.kubernetes.context=minikube.
Namespaces
Kubernetes has the concept of namespaces.
Namespaces are ways to divide cluster resources between multiple users (via resource quota). Spark on Kubernetes can
use namespaces to launch Spark applications. This can be made use of through the spark.kubernetes.namespace configuration.
Kubernetes allows using ResourceQuota to set limits on
resources, number of objects, etc on individual namespaces. Namespaces and ResourceQuota can be used in combination by
administrator to control sharing and resource allocation in a Kubernetes cluster running Spark applications.
RBAC
In Kubernetes clusters with RBAC enabled, users can configure
Kubernetes RBAC roles and service accounts used by the various Spark on Kubernetes components to access the Kubernetes
API server.
The Spark driver pod uses a Kubernetes service account to access the Kubernetes API server to create and watch executor
pods. The service account used by the driver pod must have the appropriate permission for the driver to be able to do
its work. Specifically, at minimum, the service account must be granted a
Role or ClusterRole that allows driver
pods to create pods and services. By default, the driver pod is automatically assigned the default service account in
the namespace specified by spark.kubernetes.namespace, if no service account is specified when the pod gets created.
Depending on the version and setup of Kubernetes deployed, this default service account may or may not have the role
that allows driver pods to create pods and services under the default Kubernetes
RBAC policies. Sometimes users may need to specify a custom
service account that has the right role granted. Spark on Kubernetes supports specifying a custom service account to
be used by the driver pod through the configuration property
spark.kubernetes.authenticate.driver.serviceAccountName=<service account name>. For example, to make the driver pod
use the spark service account, a user simply adds the following option to the spark-submit command:
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark

To create a custom service account, a user can use the kubectl create serviceaccount command. For example, the
following command creates a service account named spark:
$ kubectl create serviceaccount spark

To grant a service account a Role or ClusterRole, a RoleBinding or ClusterRoleBinding is needed. To create
a RoleBinding or ClusterRoleBinding, a user can use the kubectl create rolebinding (or clusterrolebinding
for ClusterRoleBinding) command. For example, the following command creates an edit ClusterRole in the default
namespace and grants it to the spark service account created above:
$ kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default

Note that a Role can only be used to grant access to resources (like pods) within a single namespace, whereas a
ClusterRole can be used to grant access to cluster-scoped resources (like nodes) as well as namespaced resources
(like pods) across all namespaces. For Spark on Kubernetes, since the driver always creates executor pods in the
same namespace, a Role is sufficient, although users may use a ClusterRole instead. For more information on
RBAC authorization and how to configure Kubernetes service accounts for pods, please refer to
Using RBAC Authorization and
Configure Service Accounts for Pods.
Spark Application Management
Kubernetes provides simple application management via the spark-submit CLI tool in cluster mode.
Users can kill a job by providing the submission ID that is printed when submitting their job.
The submission ID follows the format namespace:driver-pod-name.
If user omits the namespace then the namespace set in current k8s context is used.
For example if user has set a specific namespace as follows kubectl config set-context minikube --namespace=spark
then the spark namespace will be used by default. On the other hand, if there is no namespace added to the specific context
then all namespaces will be considered by default. That means operations will affect all Spark applications matching the given submission ID regardless of namespace.
Moreover, spark-submit for application management uses the same backend code that is used for submitting the driver, so the same properties
like spark.kubernetes.context etc., can be re-used.
For example:
$ spark-submit --kill spark:spark-pi-1547948636094-driver --master k8s://https://192.168.2.8:8443

Users also can list the application status by using the --status flag:
$ spark-submit --status spark:spark-pi-1547948636094-driver --master  k8s://https://192.168.2.8:8443

Both operations support glob patterns. For example user can run:
$ spark-submit --kill spark:spark-pi* --master  k8s://https://192.168.2.8:8443

The above will kill all application with the specific prefix.
User can specify the grace period for pod termination via the spark.kubernetes.appKillPodDeletionGracePeriod property,
using --conf as means to provide it (default value for all K8s pods is 30 secs).
Future Work
There are several Spark on Kubernetes features that are currently being worked on or planned to be worked on. Those features are expected to eventually make it into future versions of the spark-kubernetes integration.
Some of these include:

External Shuffle Service
Job Queues and Resource Management

Configuration
See the configuration page for information on Spark configurations.  The following configurations are specific to Spark on Kubernetes.
Spark Properties

Property NameDefaultMeaningSince Version

spark.kubernetes.context
(none)

    The context from the user Kubernetes configuration file used for the initial
    auto-configuration of the Kubernetes client library.  When not specified then
    the users current context is used.  NB: Many of the
    auto-configured settings can be overridden by the use of other Spark
    configuration properties e.g. spark.kubernetes.namespace.
  
3.0.0


spark.kubernetes.driver.master
https://kubernetes.default.svc

    The internal Kubernetes master (API server) address to be used for driver to request executors or
    'local[*]' for driver-pod-only mode.
  
3.0.0


spark.kubernetes.namespace
default

    The namespace that will be used for running the driver and executor pods.
  
2.3.0


spark.kubernetes.container.image
(none)

    Container image to use for the Spark application.
    This is usually of the form example.com/repo/spark:v1.0.0.
    This configuration is required and must be provided by the user, unless explicit
    images are provided for each different container type.
  
2.3.0


spark.kubernetes.driver.container.image
(value of spark.kubernetes.container.image)

    Custom container image to use for the driver.
  
2.3.0


spark.kubernetes.executor.container.image
(value of spark.kubernetes.container.image)

    Custom container image to use for executors.
  
2.3.0


spark.kubernetes.container.image.pullPolicy
IfNotPresent

    Container image pull policy used when pulling images within Kubernetes.
    Valid values are Always, Never, and IfNotPresent.
  
2.3.0


spark.kubernetes.container.image.pullSecrets


    Comma separated list of Kubernetes secrets used to pull images from private image registries.
  
2.4.0


spark.kubernetes.allocation.batch.size
5

    Number of pods to launch at once in each round of executor pod allocation.
  
2.3.0


spark.kubernetes.allocation.batch.delay
1s

    Time to wait between each round of executor pod allocation. Specifying values less than 1 second may lead to
    excessive CPU usage on the spark driver.
  
2.3.0


spark.kubernetes.authenticate.submission.caCertFile
(none)

    Path to the CA cert file for connecting to the Kubernetes API server over TLS when starting the driver. This file
    must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not provide
    a scheme). In client mode, use spark.kubernetes.authenticate.caCertFile instead.
  
2.3.0


spark.kubernetes.authenticate.submission.clientKeyFile
(none)

    Path to the client key file for authenticating against the Kubernetes API server when starting the driver. This file
    must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not provide
    a scheme). In client mode, use spark.kubernetes.authenticate.clientKeyFile instead.
  
2.3.0


spark.kubernetes.authenticate.submission.clientCertFile
(none)

    Path to the client cert file for authenticating against the Kubernetes API server when starting the driver. This
    file must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not
    provide a scheme). In client mode, use spark.kubernetes.authenticate.clientCertFile instead.
  
2.3.0


spark.kubernetes.authenticate.submission.oauthToken
(none)

    OAuth token to use when authenticating against the Kubernetes API server when starting the driver. Note
    that unlike the other authentication options, this is expected to be the exact string value of the token to use for
    the authentication. In client mode, use spark.kubernetes.authenticate.oauthToken instead.
  
2.3.0


spark.kubernetes.authenticate.submission.oauthTokenFile
(none)

    Path to the OAuth token file containing the token to use when authenticating against the Kubernetes API server when starting the driver.
    This file must be located on the submitting machine's disk. Specify this as a path as opposed to a URI (i.e. do not
    provide a scheme). In client mode, use spark.kubernetes.authenticate.oauthTokenFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.caCertFile
(none)

    Path to the CA cert file for connecting to the Kubernetes API server over TLS from the driver pod when requesting
    executors. This file must be located on the submitting machine's disk, and will be uploaded to the driver pod.
    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use
    spark.kubernetes.authenticate.caCertFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.clientKeyFile
(none)

    Path to the client key file for authenticating against the Kubernetes API server from the driver pod when requesting
    executors. This file must be located on the submitting machine's disk, and will be uploaded to the driver pod as
    a Kubernetes secret. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).
    In client mode, use spark.kubernetes.authenticate.clientKeyFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.clientCertFile
(none)

    Path to the client cert file for authenticating against the Kubernetes API server from the driver pod when
    requesting executors. This file must be located on the submitting machine's disk, and will be uploaded to the
    driver pod as a Kubernetes secret. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).
    In client mode, use spark.kubernetes.authenticate.clientCertFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.oauthToken
(none)

    OAuth token to use when authenticating against the Kubernetes API server from the driver pod when
    requesting executors. Note that unlike the other authentication options, this must be the exact string value of
    the token to use for the authentication. This token value is uploaded to the driver pod as a Kubernetes secret.
    In client mode, use spark.kubernetes.authenticate.oauthToken instead.
  
2.3.0


spark.kubernetes.authenticate.driver.oauthTokenFile
(none)

    Path to the OAuth token file containing the token to use when authenticating against the Kubernetes API server from the driver pod when
    requesting executors. Note that unlike the other authentication options, this file must contain the exact string value of
    the token to use for the authentication. This token value is uploaded to the driver pod as a secret. In client mode, use
    spark.kubernetes.authenticate.oauthTokenFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.mounted.caCertFile
(none)

    Path to the CA cert file for connecting to the Kubernetes API server over TLS from the driver pod when requesting
    executors. This path must be accessible from the driver pod.
    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use
    spark.kubernetes.authenticate.caCertFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.mounted.clientKeyFile
(none)

    Path to the client key file for authenticating against the Kubernetes API server from the driver pod when requesting
    executors. This path must be accessible from the driver pod.
    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use
    spark.kubernetes.authenticate.clientKeyFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.mounted.clientCertFile
(none)

    Path to the client cert file for authenticating against the Kubernetes API server from the driver pod when
    requesting executors. This path must be accessible from the driver pod.
    Specify this as a path as opposed to a URI (i.e. do not provide a scheme). In client mode, use
    spark.kubernetes.authenticate.clientCertFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.mounted.oauthTokenFile
(none)

    Path to the file containing the OAuth token to use when authenticating against the Kubernetes API server from the driver pod when
    requesting executors. This path must be accessible from the driver pod.
    Note that unlike the other authentication options, this file must contain the exact string value of the token to use
    for the authentication. In client mode, use spark.kubernetes.authenticate.oauthTokenFile instead.
  
2.3.0


spark.kubernetes.authenticate.driver.serviceAccountName
default

    Service account that is used when running the driver pod. The driver pod uses this service account when requesting
    executor pods from the API server. Note that this cannot be specified alongside a CA cert file, client key file,
    client cert file, and/or OAuth token. In client mode, use spark.kubernetes.authenticate.serviceAccountName instead.
  
2.3.0


spark.kubernetes.authenticate.executor.serviceAccountName
(value of spark.kubernetes.authenticate.driver.serviceAccountName)

    Service account that is used when running the executor pod.
    If this parameter is not setup, the fallback logic will use the driver's service account.
  
3.1.0


spark.kubernetes.authenticate.caCertFile
(none)

    In client mode, path to the CA cert file for connecting to the Kubernetes API server over TLS when
    requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).
  
2.4.0


spark.kubernetes.authenticate.clientKeyFile
(none)

    In client mode, path to the client key file for authenticating against the Kubernetes API server
    when requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).
  
2.4.0


spark.kubernetes.authenticate.clientCertFile
(none)

    In client mode, path to the client cert file for authenticating against the Kubernetes API server
    when requesting executors. Specify this as a path as opposed to a URI (i.e. do not provide a scheme).
  
2.4.0


spark.kubernetes.authenticate.oauthToken
(none)

    In client mode, the OAuth token to use when authenticating against the Kubernetes API server when
    requesting executors. Note that unlike the other authentication options, this must be the exact string value of
    the token to use for the authentication.
  
2.4.0


spark.kubernetes.authenticate.oauthTokenFile
(none)

    In client mode, path to the file containing the OAuth token to use when authenticating against the Kubernetes API
    server when requesting executors.
  
2.4.0


spark.kubernetes.driver.label.[LabelName]
(none)

    Add the label specified by LabelName to the driver pod.
    For example, spark.kubernetes.driver.label.something=true.
    Note that Spark also adds its own labels to the driver pod
    for bookkeeping purposes.
  
2.3.0


spark.kubernetes.driver.annotation.[AnnotationName]
(none)

    Add the Kubernetes annotation specified by AnnotationName to the driver pod.
    For example, spark.kubernetes.driver.annotation.something=true.
  
2.3.0


spark.kubernetes.driver.service.label.[LabelName]
(none)

    Add the Kubernetes label specified by LabelName to the driver service.
    For example, spark.kubernetes.driver.service.label.something=true.
    Note that Spark also adds its own labels to the driver service
    for bookkeeping purposes.
  
3.4.0


spark.kubernetes.driver.service.annotation.[AnnotationName]
(none)

    Add the Kubernetes annotation specified by AnnotationName to the driver service.
    For example, spark.kubernetes.driver.service.annotation.something=true.
  
3.0.0


spark.kubernetes.executor.label.[LabelName]
(none)

    Add the label specified by LabelName to the executor pods.
    For example, spark.kubernetes.executor.label.something=true.
    Note that Spark also adds its own labels to the executor pod
    for bookkeeping purposes.
  
2.3.0


spark.kubernetes.executor.annotation.[AnnotationName]
(none)

    Add the Kubernetes annotation specified by AnnotationName to the executor pods.
    For example, spark.kubernetes.executor.annotation.something=true.
  
2.3.0


spark.kubernetes.driver.pod.name
(none)

    Name of the driver pod. In cluster mode, if this is not set, the driver pod name is set to "spark.app.name"
    suffixed by the current timestamp to avoid name conflicts. In client mode, if your application is running
    inside a pod, it is highly recommended to set this to the name of the pod your driver is running in. Setting this
    value in client mode allows the driver to become the owner of its executor pods, which in turn allows the executor
    pods to be garbage collected by the cluster.
  
2.3.0


spark.kubernetes.executor.podNamePrefix
(none)

    Prefix to use in front of the executor pod names. It must conform the rules defined by the Kubernetes
    DNS Label Names.
    The prefix will be used to generate executor pod names in the form of $podNamePrefix-exec-$id, where the `id` is
    a positive int value, so the length of the `podNamePrefix` needs to be less than or equal to 47(= 63 - 10 - 6).
  
2.3.0


spark.kubernetes.submission.waitAppCompletion
true

    In cluster mode, whether to wait for the application to finish before exiting the launcher process.  When changed to
    false, the launcher has a "fire-and-forget" behavior when launching the Spark job.
  
2.3.0


spark.kubernetes.report.interval
1s

    Interval between reports of the current Spark job status in cluster mode.
  
2.3.0


spark.kubernetes.executor.apiPollingInterval
30s

    Interval between polls against the Kubernetes API server to inspect the state of executors.
  
2.4.0


spark.kubernetes.driver.request.cores
(none)

    Specify the cpu request for the driver pod. Values conform to the Kubernetes convention.
    Example values include 0.1, 500m, 1.5, 5, etc., with the definition of cpu units documented in CPU units.
    This takes precedence over spark.driver.cores for specifying the driver pod cpu request if set.
  
3.0.0


spark.kubernetes.driver.limit.cores
(none)

    Specify a hard cpu limit for the driver pod.
  
2.3.0


spark.kubernetes.executor.request.cores
(none)

    Specify the cpu request for each executor pod. Values conform to the Kubernetes convention.
    Example values include 0.1, 500m, 1.5, 5, etc., with the definition of cpu units documented in CPU units.
    This is distinct from spark.executor.cores: it is only used and takes precedence over spark.executor.cores for specifying the executor pod cpu request if set. Task
    parallelism, e.g., number of tasks an executor can run concurrently is not affected by this.
  
2.4.0


spark.kubernetes.executor.limit.cores
(none)

    Specify a hard cpu limit for each executor pod launched for the Spark Application.
  
2.3.0


spark.kubernetes.node.selector.[labelKey]
(none)

    Adds to the node selector of the driver pod and executor pods, with key labelKey and the value as the
    configuration's value. For example, setting spark.kubernetes.node.selector.identifier to myIdentifier
    will result in the driver pod and executors having a node selector with key identifier and value
     myIdentifier. Multiple node selector keys can be added by setting multiple configurations with this prefix.
  
2.3.0


spark.kubernetes.driver.node.selector.[labelKey]
(none)

    Adds to the driver node selector of the driver pod, with key labelKey and the value as the
    configuration's value. For example, setting spark.kubernetes.driver.node.selector.identifier to myIdentifier
    will result in the driver pod having a node selector with key identifier and value
     myIdentifier. Multiple driver node selector keys can be added by setting multiple configurations with this prefix.
  
3.3.0


spark.kubernetes.executor.node.selector.[labelKey]
(none)

    Adds to the executor node selector of the executor pods, with key labelKey and the value as the
    configuration's value. For example, setting spark.kubernetes.executor.node.selector.identifier to myIdentifier
    will result in the executors having a node selector with key identifier and value
     myIdentifier. Multiple executor node selector keys can be added by setting multiple configurations with this prefix.
  
3.3.0


spark.kubernetes.driverEnv.[EnvironmentVariableName]
(none)

    Add the environment variable specified by EnvironmentVariableName to
    the Driver process. The user can specify multiple of these to set multiple environment variables.
  
2.3.0


spark.kubernetes.driver.secrets.[SecretName]
(none)

   Add the Kubernetes Secret named SecretName to the driver pod on the path specified in the value. For example,
   spark.kubernetes.driver.secrets.spark-secret=/etc/secrets.
  
2.3.0


spark.kubernetes.executor.secrets.[SecretName]
(none)

   Add the Kubernetes Secret named SecretName to the executor pod on the path specified in the value. For example,
   spark.kubernetes.executor.secrets.spark-secret=/etc/secrets.
  
2.3.0


spark.kubernetes.driver.secretKeyRef.[EnvName]
(none)

   Add as an environment variable to the driver container with name EnvName (case sensitive), the value referenced by key  key  in the data of the referenced Kubernetes Secret. For example,
   spark.kubernetes.driver.secretKeyRef.ENV_VAR=spark-secret:key.
  
2.4.0


spark.kubernetes.executor.secretKeyRef.[EnvName]
(none)

   Add as an environment variable to the executor container with name EnvName (case sensitive), the value referenced by key  key  in the data of the referenced Kubernetes Secret. For example,
   spark.kubernetes.executor.secrets.ENV_VAR=spark-secret:key.
  
2.4.0


spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.path
(none)

   Add the Kubernetes Volume named VolumeName of the VolumeType type to the driver pod on the path specified in the value. For example,
   spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint.
  
2.4.0


spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.subPath
(none)

   Specifies a subpath to be mounted from the volume into the driver pod.
   spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint.
  
3.0.0


spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.readOnly
(none)

   Specify if the mounted volume is read only or not. For example,
   spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false.
  
2.4.0


spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].options.[OptionName]
(none)

   Configure Kubernetes Volume options passed to the Kubernetes with OptionName as key having specified value, must conform with Kubernetes option format. For example,
   spark.kubernetes.driver.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-pvc-claim.
  
2.4.0


spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.path
(none)

   Add the Kubernetes Volume named VolumeName of the VolumeType type to the executor pod on the path specified in the value. For example,
   spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.path=/checkpoint.
  
2.4.0


spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.subPath
(none)

   Specifies a subpath to be mounted from the volume into the executor pod.
   spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.subPath=checkpoint.
  
3.0.0


spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].mount.readOnly
false

   Specify if the mounted volume is read only or not. For example,
   spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.mount.readOnly=false.
  
2.4.0


spark.kubernetes.executor.volumes.[VolumeType].[VolumeName].options.[OptionName]
(none)

   Configure Kubernetes Volume options passed to the Kubernetes with OptionName as key having specified value. For example,
   spark.kubernetes.executor.volumes.persistentVolumeClaim.checkpointpvc.options.claimName=spark-pvc-claim.
  
2.4.0


spark.kubernetes.local.dirs.tmpfs
false

   Configure the emptyDir volumes used to back SPARK_LOCAL_DIRS within the Spark driver and executor pods to use tmpfs backing i.e. RAM.  See Local Storage earlier on this page
   for more discussion of this.
  
3.0.0


spark.kubernetes.memoryOverheadFactor
0.1

    This sets the Memory Overhead Factor that will allocate memory to non-JVM memory, which includes off-heap memory allocations, non-JVM tasks, various systems processes, and tmpfs-based local directories when spark.kubernetes.local.dirs.tmpfs is true. For JVM-based jobs this value will default to 0.10 and 0.40 for non-JVM jobs.
    This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly fail with "Memory Overhead Exceeded" errors. This preempts this error with a higher default.
    This will be overridden by the value set by spark.driver.memoryOverheadFactor and spark.executor.memoryOverheadFactor explicitly.
  
2.4.0


spark.kubernetes.pyspark.pythonVersion
"3"

   This sets the major Python version of the docker image used to run the driver and executor containers.
   It can be only "3". This configuration was deprecated from Spark 3.1.0, and is effectively no-op.
   Users should set 'spark.pyspark.python' and 'spark.pyspark.driver.python' configurations or
   'PYSPARK_PYTHON' and 'PYSPARK_DRIVER_PYTHON' environment variables.
  
2.4.0


spark.kubernetes.kerberos.krb5.path
(none)

   Specify the local location of the krb5.conf file to be mounted on the driver and executors for Kerberos interaction.
   It is important to note that the KDC defined needs to be visible from inside the containers.
  
3.0.0


spark.kubernetes.kerberos.krb5.configMapName
(none)

   Specify the name of the ConfigMap, containing the krb5.conf file, to be mounted on the driver and executors
   for Kerberos interaction. The KDC defined needs to be visible from inside the containers. The ConfigMap must also
   be in the same namespace of the driver and executor pods.
  
3.0.0


spark.kubernetes.hadoop.configMapName
(none)

    Specify the name of the ConfigMap, containing the HADOOP_CONF_DIR files, to be mounted on the driver
    and executors for custom Hadoop configuration.
  
3.0.0


spark.kubernetes.kerberos.tokenSecret.name
(none)

    Specify the name of the secret where your existing delegation tokens are stored. This removes the need for the job user
    to provide any kerberos credentials for launching a job.
  
3.0.0


spark.kubernetes.kerberos.tokenSecret.itemKey
(none)

    Specify the item key of the data where your existing delegation tokens are stored. This removes the need for the job user
    to provide any kerberos credentials for launching a job.
  
3.0.0


spark.kubernetes.driver.podTemplateFile
(none)

   Specify the local file that contains the driver pod template. For example
   spark.kubernetes.driver.podTemplateFile=/path/to/driver-pod-template.yaml

3.0.0


spark.kubernetes.driver.podTemplateContainerName
(none)

   Specify the container name to be used as a basis for the driver in the given pod template.
   For example spark.kubernetes.driver.podTemplateContainerName=spark-driver

3.0.0


spark.kubernetes.executor.podTemplateFile
(none)

   Specify the local file that contains the executor pod template. For example
   spark.kubernetes.executor.podTemplateFile=/path/to/executor-pod-template.yaml

3.0.0


spark.kubernetes.executor.podTemplateContainerName
(none)

   Specify the container name to be used as a basis for the executor in the given pod template.
   For example spark.kubernetes.executor.podTemplateContainerName=spark-executor

3.0.0


spark.kubernetes.executor.deleteOnTermination
true

  Specify whether executor pods should be deleted in case of failure or normal termination.
  
3.0.0


spark.kubernetes.executor.checkAllContainers
false

  Specify whether executor pods should be check all containers (including sidecars) or only the executor container when determining the pod status.
  
3.1.0


spark.kubernetes.submission.connectionTimeout
10000

    Connection timeout in milliseconds for the kubernetes client to use for starting the driver.
  
3.0.0


spark.kubernetes.submission.requestTimeout
10000

    Request timeout in milliseconds for the kubernetes client to use for starting the driver.
  
3.0.0


spark.kubernetes.trust.certificates
false

    If set to true then client can submit to kubernetes cluster only with token.
  
3.2.0


spark.kubernetes.driver.connectionTimeout
10000

    Connection timeout in milliseconds for the kubernetes client in driver to use when requesting executors.
  
3.0.0


spark.kubernetes.driver.requestTimeout
10000

    Request timeout in milliseconds for the kubernetes client in driver to use when requesting executors.
  
3.0.0


spark.kubernetes.appKillPodDeletionGracePeriod
(none)

  Specify the grace period in seconds when deleting a Spark application using spark-submit.
  
3.0.0


spark.kubernetes.dynamicAllocation.deleteGracePeriod
5s

    How long to wait for executors to shut down gracefully before a forceful kill.
  
3.0.0


spark.kubernetes.file.upload.path
(none)

    Path to store files at the spark submit side in cluster mode. For example:
    spark.kubernetes.file.upload.path=s3a://<s3-bucket>/path
    File should specified as file://path/to/file  or absolute path.
  
3.0.0


spark.kubernetes.executor.decommissionLabel
(none)

    Label to be applied to pods which are exiting or being decommissioned. Intended for use
    with pod disruption budgets, deletion costs, and similar.
  
3.3.0


spark.kubernetes.executor.decommissionLabelValue
(none)

    Value to be applied with the label when
    spark.kubernetes.executor.decommissionLabel is enabled.
  
3.3.0


spark.kubernetes.executor.scheduler.name
(none)

	Specify the scheduler name for each executor pod.
  
3.0.0


spark.kubernetes.driver.scheduler.name
(none)

    Specify the scheduler name for driver pod.
  
3.3.0


spark.kubernetes.scheduler.name
(none)

    Specify the scheduler name for driver and executor pods. If `spark.kubernetes.driver.scheduler.name` or
    `spark.kubernetes.executor.scheduler.name` is set, will override this.
  
3.3.0


spark.kubernetes.configMap.maxSize
1048576

    Max size limit for a config map.
    This is configurable as per limit on k8s server end.
  
3.1.0


spark.kubernetes.executor.missingPodDetectDelta
30s

    When a registered executor's POD is missing from the Kubernetes API server's polled
    list of PODs then this delta time is taken as the accepted time difference between the
    registration time and the time of the polling. After this time the POD is considered
    missing from the cluster and the executor will be removed.
  
3.1.1


spark.kubernetes.decommission.script
/opt/decom.sh

    The location of the script to use for graceful decommissioning.
  
3.2.0


spark.kubernetes.driver.service.deleteOnTermination
true

    If true, driver service will be deleted on Spark application termination. If false, it will be cleaned up when the driver pod is deletion.
  
3.2.0


spark.kubernetes.driver.service.ipFamilyPolicy
SingleStack

    K8s IP Family Policy for Driver Service. Valid values are
    SingleStack, PreferDualStack, and RequireDualStack.
  
3.4.0


spark.kubernetes.driver.service.ipFamilies
IPv4

    A list of IP families for K8s Driver Service. Valid values are
    IPv4 and IPv6.
  
3.4.0


spark.kubernetes.driver.ownPersistentVolumeClaim
true

    If true, driver pod becomes the owner of on-demand persistent volume claims instead of the executor pods
  
3.2.0


spark.kubernetes.driver.reusePersistentVolumeClaim
true

    If true, driver pod tries to reuse driver-owned on-demand persistent volume claims
    of the deleted executor pods if exists. This can be useful to reduce executor pod
    creation delay by skipping persistent volume creations. Note that a pod in
    `Terminating` pod status is not a deleted pod by definition and its resources
    including persistent volume claims are not reusable yet. Spark will create new
    persistent volume claims when there exists no reusable one. In other words, the total
    number of persistent volume claims can be larger than the number of running executors
    sometimes. This config requires spark.kubernetes.driver.ownPersistentVolumeClaim=true.

3.2.0


spark.kubernetes.driver.waitToReusePersistentVolumeClaim
false

    If true, driver pod counts the number of created on-demand persistent volume claims
    and wait if the number is greater than or equal to the total number of volumes which
    the Spark job is able to have. This config requires both
    spark.kubernetes.driver.ownPersistentVolumeClaim=true and
    spark.kubernetes.driver.reusePersistentVolumeClaim=true.

3.4.0


spark.kubernetes.executor.disableConfigMap
false

    If true, disable ConfigMap creation for executors.
  
3.2.0


spark.kubernetes.driver.pod.featureSteps
(none)

    Class names of an extra driver pod feature step implementing
    `KubernetesFeatureConfigStep`. This is a developer API. Comma separated.
    Runs after all of Spark internal feature steps. Since 3.3.0, your driver feature step
    can implement `KubernetesDriverCustomFeatureConfigStep` where the driver config
    is also available.
  
3.2.0


spark.kubernetes.executor.pod.featureSteps
(none)

    Class names of an extra executor pod feature step implementing
    `KubernetesFeatureConfigStep`. This is a developer API. Comma separated.
    Runs after all of Spark internal feature steps. Since 3.3.0, your executor feature step
    can implement `KubernetesExecutorCustomFeatureConfigStep` where the executor config
    is also available.
  
3.2.0


spark.kubernetes.allocation.maxPendingPods
Int.MaxValue

    Maximum number of pending PODs allowed during executor allocation for this
    application. Those newly requested executors which are unknown by Kubernetes yet are
    also counted into this limit as they will change into pending PODs by time.
    This limit is independent from the resource profiles as it limits the sum of all
    allocation for all the used resource profiles.
  
3.2.0


spark.kubernetes.allocation.pods.allocator
direct

    Allocator to use for pods. Possible values are direct (the default)
    and statefulset, or a full class name of a class implementing `AbstractPodsAllocator`.
    Future version may add Job or replicaset. This is a developer API and may change
    or be removed at anytime.
  
3.3.0


spark.kubernetes.allocation.executor.timeout
600s

    Time to wait before a newly created executor POD request, which does not reached
    the POD pending state yet, considered timedout and will be deleted.
  
3.1.0


spark.kubernetes.allocation.driver.readinessTimeout
1s

    Time to wait for driver pod to get ready before creating executor pods. This wait
    only happens on application start. If timeout happens, executor pods will still be
    created.
  
3.1.3


spark.kubernetes.executor.enablePollingWithResourceVersion
false

    If true, `resourceVersion` is set with `0` during invoking pod listing APIs
    in order to allow API Server-side caching. This should be used carefully.
  
3.3.0


spark.kubernetes.executor.eventProcessingInterval
1s

    Interval between successive inspection of executor events sent from the Kubernetes API.
  
2.4.0


spark.kubernetes.executor.rollInterval
0s

    Interval between executor roll operations. It's disabled by default with `0s`.
  
3.3.0


spark.kubernetes.executor.minTasksPerExecutorBeforeRolling
0

    The minimum number of tasks per executor before rolling.
    Spark will not roll executors whose total number of tasks is smaller
    than this configuration. The default value is zero.
  
3.3.0


spark.kubernetes.executor.rollPolicy
OUTLIER

    Executor roll policy: Valid values are ID, ADD_TIME, TOTAL_GC_TIME,
    TOTAL_DURATION, FAILED_TASKS, and OUTLIER (default).
    When executor roll happens, Spark uses this policy to choose
    an executor and decommission it. The built-in policies are based on executor summary
    and newly started executors are protected by spark.kubernetes.executor.minTasksPerExecutorBeforeRolling.
    ID policy chooses an executor with the smallest executor ID.
    ADD_TIME policy chooses an executor with the smallest add-time.
    TOTAL_GC_TIME policy chooses an executor with the biggest total task GC time.
    TOTAL_DURATION policy chooses an executor with the biggest total task time.
    AVERAGE_DURATION policy chooses an executor with the biggest average task time.
    FAILED_TASKS policy chooses an executor with the most number of failed tasks.
    OUTLIER policy chooses an executor with outstanding statistics which is bigger than
    at least two standard deviation from the mean in average task time,
    total task time, total task GC time, and the number of failed tasks if exists.
    If there is no outlier, it works like TOTAL_DURATION policy.
  
3.3.0


Pod template properties
See the below table for the full list of pod specifications that will be overwritten by spark.
Pod Metadata

Pod metadata keyModified valueDescription

name
Value of spark.kubernetes.driver.pod.name

    The driver pod name will be overwritten with either the configured or default value of
    spark.kubernetes.driver.pod.name. The executor pod names will be unaffected.
  


namespace
Value of spark.kubernetes.namespace

    Spark makes strong assumptions about the driver and executor namespaces. Both driver and executor namespaces will
    be replaced by either the configured or default spark conf value.
  


labels
Adds the labels from spark.kubernetes.{driver,executor}.label.*

    Spark will add additional labels specified by the spark configuration.
  


annotations
Adds the annotations from spark.kubernetes.{driver,executor}.annotation.*

    Spark will add additional annotations specified by the spark configuration.
  


Pod Spec

Pod spec keyModified valueDescription

imagePullSecrets
Adds image pull secrets from spark.kubernetes.container.image.pullSecrets

    Additional pull secrets will be added from the spark configuration to both executor pods.
  


nodeSelector
Adds node selectors from spark.kubernetes.node.selector.*

    Additional node selectors will be added from the spark configuration to both executor pods.
  


restartPolicy
"never"

    Spark assumes that both drivers and executors never restart.
  


serviceAccount
Value of spark.kubernetes.authenticate.driver.serviceAccountName

    Spark will override serviceAccount with the value of the spark configuration for only
    driver pods, and only if the spark configuration is specified. Executor pods will remain unaffected.
  


serviceAccountName
Value of spark.kubernetes.authenticate.driver.serviceAccountName

    Spark will override serviceAccountName with the value of the spark configuration for only
    driver pods, and only if the spark configuration is specified. Executor pods will remain unaffected.
  


volumes
Adds volumes from spark.kubernetes.{driver,executor}.volumes.[VolumeType].[VolumeName].mount.path

    Spark will add volumes as specified by the spark conf, as well as additional volumes necessary for passing
    spark conf and pod template files.
  


Container spec
The following affect the driver and executor containers. All other containers in the pod spec will be unaffected.

Container spec keyModified valueDescription

env
Adds env variables from spark.kubernetes.driverEnv.[EnvironmentVariableName]

    Spark will add driver env variables from spark.kubernetes.driverEnv.[EnvironmentVariableName], and
    executor env variables from spark.executorEnv.[EnvironmentVariableName].
  


image
Value of spark.kubernetes.{driver,executor}.container.image

    The image will be defined by the spark configurations.
  


imagePullPolicy
Value of spark.kubernetes.container.image.pullPolicy

    Spark will override the pull policy for both driver and executors.
  


name
See description

    The container name will be assigned by spark ("spark-kubernetes-driver" for the driver container, and
    "spark-kubernetes-executor" for each executor container) if not defined by the pod template. If the container is defined by the
    template, the template's name will be used.
  


resources
See description

    The cpu limits are set by spark.kubernetes.{driver,executor}.limit.cores. The cpu is set by
    spark.{driver,executor}.cores. The memory request and limit are set by summing the values of
    spark.{driver,executor}.memory and spark.{driver,executor}.memoryOverhead.
    Other resource limits are set by spark.{driver,executor}.resources.{resourceName}.* configs.
  


volumeMounts
Add volumes from spark.kubernetes.driver.volumes.[VolumeType].[VolumeName].mount.{path,readOnly}

    Spark will add volumes as specified by the spark conf, as well as additional volumes necessary for passing
    spark conf and pod template files.
  


Resource Allocation and Configuration Overview
Please make sure to have read the Custom Resource Scheduling and Configuration Overview section on the configuration page. This section only talks about the Kubernetes specific aspects of resource scheduling.
The user is responsible to properly configuring the Kubernetes cluster to have the resources available and ideally isolate each resource per container so that a resource is not shared between multiple containers. If the resource is not isolated the user is responsible for writing a discovery script so that the resource is not shared between containers. See the Kubernetes documentation for specifics on configuring Kubernetes with custom resources.
Spark automatically handles translating the Spark configs spark.{driver/executor}.resource.{resourceType} into the kubernetes configs as long as the Kubernetes resource type follows the Kubernetes device plugin format of vendor-domain/resourcetype. The user must specify the vendor using the spark.{driver/executor}.resource.{resourceType}.vendor config. The user does not need to explicitly add anything if you are using Pod templates. For reference and an example, you can see the Kubernetes documentation for scheduling GPUs. Spark only supports setting the resource limits.
Kubernetes does not tell Spark the addresses of the resources allocated to each container. For that reason, the user must specify a discovery script that gets run by the executor on startup to discover what resources are available to that executor. You can find an example scripts in examples/src/main/scripts/getGpusResources.sh. The script must have execute permissions set and the user should setup permissions to not allow malicious users to modify it. The script should write to STDOUT a JSON string in the format of the ResourceInformation class. This has the resource name and an array of resource addresses available to just that executor.
Resource Level Scheduling Overview
There are several resource level scheduling features supported by Spark on Kubernetes.
Priority Scheduling
Kubernetes supports Pod priority by default.
Spark on Kubernetes allows defining the priority of jobs by Pod template. The user can specify the priorityClassName in driver or executor Pod template spec section. Below is an example to show how to specify it:
apiVersion: v1
Kind: Pod
metadata:
  labels:
    template-label-key: driver-template-label-value
spec:
  # Specify the priority in here
  priorityClassName: system-node-critical
  containers:
  - name: test-driver-container
    image: will-be-overwritten

Customized Kubernetes Schedulers for Spark on Kubernetes
Spark allows users to specify a custom Kubernetes schedulers.


Specify a scheduler name.
Users can specify a custom scheduler using spark.kubernetes.scheduler.name or
spark.kubernetes.{driver/executor}.scheduler.name configuration.


Specify scheduler related configurations.
To configure the custom scheduler the user can use Pod templates, add labels (spark.kubernetes.{driver,executor}.label.*), annotations (spark.kubernetes.{driver/executor}.annotation.*) or scheduler specific configurations (such as spark.kubernetes.scheduler.volcano.podGroupTemplateFile).


Specify scheduler feature step.
Users may also consider to use spark.kubernetes.{driver/executor}.pod.featureSteps to support more complex requirements, including but not limited to:

Create additional Kubernetes custom resources for driver/executor scheduling.
Set scheduler hints according to configuration or existing Pod info dynamically.



Using Volcano as Customized Scheduler for Spark on Kubernetes
Prerequisites


Spark on Kubernetes with Volcano as a custom scheduler is supported since Spark v3.3.0 and Volcano v1.7.0. Below is an example to install Volcano 1.7.0:
kubectl apply -f https://raw.githubusercontent.com/volcano-sh/volcano/v1.7.0/installer/volcano-development.yaml
 


Build
To create a Spark distribution along with Volcano suppport like those distributed by the Spark Downloads page, also see more in “Building Spark”:
./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phive -Phive-thriftserver -Pkubernetes -Pvolcano

Usage
Spark on Kubernetes allows using Volcano as a custom scheduler. Users can use Volcano to
support more advanced resource scheduling: queue scheduling, resource reservation, priority scheduling, and more.
To use Volcano as a custom scheduler the user needs to specify the following configuration options:
# Specify volcano scheduler and PodGroup template
--conf spark.kubernetes.scheduler.name=volcano
--conf spark.kubernetes.scheduler.volcano.podGroupTemplateFile=/path/to/podgroup-template.yaml
# Specify driver/executor VolcanoFeatureStep
--conf spark.kubernetes.driver.pod.featureSteps=org.apache.spark.deploy.k8s.features.VolcanoFeatureStep
--conf spark.kubernetes.executor.pod.featureSteps=org.apache.spark.deploy.k8s.features.VolcanoFeatureStep

Volcano Feature Step
Volcano feature steps help users to create a Volcano PodGroup and set driver/executor pod annotation to link with this PodGroup.
Note that currently only driver/job level PodGroup is supported in Volcano Feature Step.
Volcano PodGroup Template
Volcano defines PodGroup spec using CRD yaml.
Similar to Pod template, Spark users can use Volcano PodGroup Template to define the PodGroup spec configurations.
To do so, specify the Spark property spark.kubernetes.scheduler.volcano.podGroupTemplateFile to point to files accessible to the spark-submit process.
Below is an example of PodGroup template:
apiVersion: scheduling.volcano.sh/v1beta1
kind: PodGroup
spec:
  # Specify minMember to 1 to make a driver pod
  minMember: 1
  # Specify minResources to support resource reservation (the driver pod resource and executors pod resource should be considered)
  # It is useful for ensource the available resources meet the minimum requirements of the Spark job and avoiding the
  # situation where drivers are scheduled, and then they are unable to schedule sufficient executors to progress.
  minResources:
    cpu: "2"
    memory: "3Gi"
  # Specify the priority, help users to specify job priority in the queue during scheduling.
  priorityClassName: system-node-critical
  # Specify the queue, indicates the resource queue which the job should be submitted to
  queue: default

Using Apache YuniKorn as Customized Scheduler for Spark on Kubernetes
Apache YuniKorn is a resource scheduler for Kubernetes that provides advanced batch scheduling
capabilities, such as job queuing, resource fairness, min/max queue capacity and flexible job ordering policies.
For available Apache YuniKorn features, please refer to core features.
Prerequisites
Install Apache YuniKorn:
helm repo add yunikorn https://apache.github.io/yunikorn-release
helm repo update
helm install yunikorn yunikorn/yunikorn --namespace yunikorn --version 1.3.0 --create-namespace --set embedAdmissionController=false

The above steps will install YuniKorn v1.3.0 on an existing Kubernetes cluster.
Get started
Submit Spark jobs with the following extra options:
--conf spark.kubernetes.scheduler.name=yunikorn
--conf spark.kubernetes.driver.label.queue=root.default
--conf spark.kubernetes.executor.label.queue=root.default
--conf spark.kubernetes.driver.annotation.yunikorn.apache.org/app-id={{APP_ID}}
--conf spark.kubernetes.executor.annotation.yunikorn.apache.org/app-id={{APP_ID}}

Note that {{APP_ID}} is the built-in variable that will be substituted with Spark job ID automatically.
With the above configuration, the job will be scheduled by YuniKorn scheduler instead of the default Kubernetes scheduler.
Stage Level Scheduling Overview
Stage level scheduling is supported on Kubernetes:

When dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.
When dynamic allocation is enabled: It allows users to specify task and executor resource requirements at the stage level and will request the extra executors. This also requires spark.dynamicAllocation.shuffleTracking.enabled to be enabled since Kubernetes doesn’t support an external shuffle service at this time. The order in which containers for different profiles is requested from Kubernetes is not guaranteed. Note that since dynamic allocation on Kubernetes requires the shuffle tracking feature, this means that executors from previous stages that used a different ResourceProfile may not idle timeout due to having shuffle data on them. This could result in using more cluster resources and in the worst case if there are no remaining resources on the Kubernetes cluster then Spark could potentially hang. You may consider looking at config spark.dynamicAllocation.shuffleTracking.timeout to set a timeout, but that could result in data having to be recomputed if the shuffle data is really needed.
Note, there is a difference in the way pod template resources are handled between the base default profile and custom ResourceProfiles. Any resources specified in the pod template file will only be used with the base default profile. If you create custom ResourceProfiles be sure to include all necessary resources there since the resources from the template file will not be propagated to custom ResourceProfiles.





















  




Running Spark on Mesos - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Running Spark on Mesos

Security
How it Works
Installing Mesos 
From Source
Third-Party Packages
Verification


Connecting Spark to Mesos 
Authenticating to Mesos 
Credential Specification Preference Order
Deploy to a Mesos running on Secure Sockets


Uploading Spark Package
Using a Mesos Master URL
Client Mode
Cluster mode


Mesos Run Modes 
Coarse-Grained
Fine-Grained (deprecated)


Mesos Docker Support
Running Alongside Hadoop
Dynamic Resource Allocation with Mesos
Configuration 
Spark Properties


Troubleshooting and Debugging

Note: Apache Mesos support is deprecated as of Apache Spark 3.2.0. It will be removed in a future version.
Spark can run on hardware clusters managed by Apache Mesos.
The advantages of deploying Spark with Mesos include:

dynamic partitioning between Spark and other
frameworks
scalable partitioning between multiple instances of Spark

Security
Security features like authentication are not enabled by default. When deploying a cluster that is open to the internet
or an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications
from running on the cluster.
Please see Spark Security and the specific security sections in this doc before running Spark.
How it Works
In a standalone cluster deployment, the cluster manager in the below diagram is a Spark master
instance.  When using Mesos, the Mesos master replaces the Spark master as the cluster manager.



Now when a driver creates a job and starts issuing tasks for scheduling, Mesos determines what
machines handle what tasks.  Because it takes into account other frameworks when scheduling these
many short-lived tasks, multiple frameworks can coexist on the same cluster without resorting to a
static partitioning of resources.
To get started, follow the steps below to install Mesos and deploy Spark jobs via Mesos.
Installing Mesos
Spark 3.5.5 is designed for use with Mesos 1.0.0 or newer and does not
require any special patches of Mesos. File and environment-based secrets support requires Mesos 1.3.0 or
newer.
If you already have a Mesos cluster running, you can skip this Mesos installation step.
Otherwise, installing Mesos for Spark is no different than installing Mesos for use by other
frameworks.  You can install Mesos either from source or using prebuilt packages.
From Source
To install Apache Mesos from source, follow these steps:

Download a Mesos release from a
mirror
Follow the Mesos Getting Started page for compiling and
installing Mesos

Note: If you want to run Mesos without installing it into the default paths on your system
(e.g., if you lack administrative privileges to install it), pass the
--prefix option to configure to tell it where to install. For example, pass
--prefix=/home/me/mesos. By default the prefix is /usr/local.
Third-Party Packages
The Apache Mesos project only publishes source releases, not binary packages.  But other
third party projects publish binary releases that may be helpful in setting Mesos up.
One of those is Mesosphere.  To install Mesos using the binary releases provided by Mesosphere:

Download Mesos installation package from downloads page
Follow their instructions for installation and configuration

The Mesosphere installation documents suggest setting up ZooKeeper to handle Mesos master failover,
but Mesos can be run without ZooKeeper using a single master as well.
Verification
To verify that the Mesos cluster is ready for Spark, navigate to the Mesos master webui at port
:5050  Confirm that all expected machines are present in the agents tab.
Connecting Spark to Mesos
To use Mesos from Spark, you need a Spark binary package available in a place accessible by Mesos, and
a Spark driver program configured to connect to Mesos.
Alternatively, you can also install Spark in the same location in all the Mesos agents, and configure
spark.mesos.executor.home (defaults to SPARK_HOME) to point to that location.
Authenticating to Mesos
When Mesos Framework authentication is enabled it is necessary to provide a principal and secret by which to authenticate Spark to Mesos.  Each Spark job will register with Mesos as a separate framework.
Depending on your deployment environment you may wish to create a single set of framework credentials that are shared across all users or create framework credentials for each user.  Creating and managing framework credentials should be done following the Mesos Authentication documentation.
Framework credentials may be specified in a variety of ways depending on your deployment environment and security requirements.  The most simple way is to specify the spark.mesos.principal and spark.mesos.secret values directly in your Spark configuration.  Alternatively you may specify these values indirectly by instead specifying spark.mesos.principal.file and spark.mesos.secret.file, these settings point to files containing the principal and secret.  These files must be plaintext files in UTF-8 encoding.  Combined with appropriate file ownership and mode/ACLs this provides a more secure way to specify these credentials.
Additionally, if you prefer to use environment variables you can specify all of the above via environment variables instead, the environment variable names are simply the configuration settings uppercased with . replaced with _ e.g. SPARK_MESOS_PRINCIPAL.
Credential Specification Preference Order
Please note that if you specify multiple ways to obtain the credentials then the following preference order applies.  Spark will use the first valid value found and any subsequent values are ignored:

spark.mesos.principal configuration setting
SPARK_MESOS_PRINCIPAL environment variable
spark.mesos.principal.file configuration setting
SPARK_MESOS_PRINCIPAL_FILE environment variable

An equivalent order applies for the secret.  Essentially we prefer the configuration to be specified directly rather than indirectly by files, and we prefer that configuration settings are used over environment variables.
Deploy to a Mesos running on Secure Sockets
If you want to deploy a Spark Application into a Mesos cluster that is running in a secure mode there are some environment variables that need to be set.

LIBPROCESS_SSL_ENABLED=true enables SSL communication
LIBPROCESS_SSL_VERIFY_CERT=false verifies the ssl certificate
LIBPROCESS_SSL_KEY_FILE=pathToKeyFile.key path to key
LIBPROCESS_SSL_CERT_FILE=pathToCRTFile.crt the certificate file to be used

All options can be found at http://mesos.apache.org/documentation/latest/ssl/
Then submit happens as described in Client mode or Cluster mode below
Uploading Spark Package
When Mesos runs a task on a Mesos agent for the first time, that agent must have a Spark binary
package for running the Spark Mesos executor backend.
The Spark package can be hosted at any Hadoop-accessible URI, including HTTP via http://,
Amazon Simple Storage Service via s3n://, or HDFS via hdfs://.
To use a precompiled package:

Download a Spark binary package from the Spark download page
Upload to hdfs/http/s3

To host on HDFS, use the Hadoop fs put command: hadoop fs -put spark-3.5.5.tar.gz
/path/to/spark-3.5.5.tar.gz
Or if you are using a custom-compiled version of Spark, you will need to create a package using
the dev/make-distribution.sh script included in a Spark source tarball/checkout.

Download and build Spark using the instructions here
Create a binary package using ./dev/make-distribution.sh --tgz.
Upload archive to http/s3/hdfs

Using a Mesos Master URL
The Master URLs for Mesos are in the form mesos://host:5050 for a single-master Mesos
cluster, or mesos://zk://host1:2181,host2:2181,host3:2181/mesos for a multi-master Mesos cluster using ZooKeeper.
Client Mode
In client mode, a Spark Mesos framework is launched directly on the client machine and waits for the driver output.
The driver needs some configuration in spark-env.sh to interact properly with Mesos:

In spark-env.sh set some environment variables:
    
export MESOS_NATIVE_JAVA_LIBRARY=<path to libmesos.so>. This path is typically
<prefix>/lib/libmesos.so where the prefix is /usr/local by default. See Mesos installation
instructions above. On Mac OS X, the library is called libmesos.dylib instead of
libmesos.so.
export SPARK_EXECUTOR_URI=<URL of spark-3.5.5.tar.gz uploaded above>.


Also set spark.executor.uri to <URL of spark-3.5.5.tar.gz>.

Now when starting a Spark application against the cluster, pass a mesos://
URL as the master when creating a SparkContext. For example:
val conf = new SparkConf()
  .setMaster("mesos://HOST:5050")
  .setAppName("My app")
  .set("spark.executor.uri", "<path to spark-3.5.5.tar.gz uploaded above>")
val sc = new SparkContext(conf)
(You can also use spark-submit and configure spark.executor.uri
in the conf/spark-defaults.conf file.)
When running a shell, the spark.executor.uri parameter is inherited from SPARK_EXECUTOR_URI, so
it does not need to be redundantly passed in as a system property.
./bin/spark-shell --master mesos://host:5050
Cluster mode
Spark on Mesos also supports cluster mode, where the driver is launched in the cluster and the client
can find the results of the driver from the Mesos Web UI.
To use cluster mode, you must start the MesosClusterDispatcher in your cluster via the sbin/start-mesos-dispatcher.sh script,
passing in the Mesos master URL (e.g: mesos://host:5050). This starts the MesosClusterDispatcher as a daemon running on the host.
Note that the MesosClusterDispatcher does not support authentication.  You should ensure that all network access to it is
protected (port 7077 by default).
By setting the Mesos proxy config property (requires mesos version >= 1.4), --conf spark.mesos.proxy.baseURL=http://localhost:5050 when launching the dispatcher, the mesos sandbox URI for each driver is added to the mesos dispatcher UI.
If you like to run the MesosClusterDispatcher with Marathon, you need to run the MesosClusterDispatcher in the foreground (i.e: ./bin/spark-class org.apache.spark.deploy.mesos.MesosClusterDispatcher). Note that the MesosClusterDispatcher not yet supports multiple instances for HA.
The MesosClusterDispatcher also supports writing recovery state into Zookeeper. This will allow the MesosClusterDispatcher to be able to recover all submitted and running containers on relaunch.   In order to enable this recovery mode, you can set SPARK_DAEMON_JAVA_OPTS in spark-env by configuring spark.deploy.recoveryMode and related spark.deploy.zookeeper.* configurations.
For more information about these configurations please refer to the configurations doc.
You can also specify any additional jars required by the MesosClusterDispatcher in the classpath by setting the environment variable SPARK_DAEMON_CLASSPATH in spark-env.
From the client, you can submit a job to Mesos cluster by running spark-submit and specifying the master URL
to the URL of the MesosClusterDispatcher (e.g: mesos://dispatcher:7077). You can view driver statuses on the
Spark cluster Web UI.
For example:
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master mesos://207.184.161.138:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000
Note that jars or python files that are passed to spark-submit should be URIs reachable by Mesos agents, as the Spark driver doesn’t automatically upload local jars.
Mesos Run Modes
Spark can run over Mesos in two modes: “coarse-grained” (default) and
“fine-grained” (deprecated).
Coarse-Grained
In “coarse-grained” mode, each Spark executor runs as a single Mesos
task.  Spark executors are sized according to the following
configuration variables:

Executor memory: spark.executor.memory
Executor cores: spark.executor.cores
Number of executors: spark.cores.max/spark.executor.cores

Please see the Spark Configuration page for
details and default values.
Executors are brought up eagerly when the application starts, until
spark.cores.max is reached.  If you don’t set spark.cores.max, the
Spark application will consume all resources offered to it by Mesos,
so we, of course, urge you to set this variable in any sort of
multi-tenant cluster, including one which runs multiple concurrent
Spark applications.
The scheduler will start executors round-robin on the offers Mesos
gives it, but there are no spread guarantees, as Mesos does not
provide such guarantees on the offer stream.
In this mode Spark executors will honor port allocation if such is
provided from the user. Specifically, if the user defines
spark.blockManager.port in Spark configuration,
the mesos scheduler will check the available offers for a valid port
range containing the port numbers. If no such range is available it will
not launch any task. If no restriction is imposed on port numbers by the
user, ephemeral ports are used as usual. This port honouring implementation
implies one task per host if the user defines a port. In the future network,
isolation shall be supported.
The benefit of coarse-grained mode is much lower startup overhead, but
at the cost of reserving Mesos resources for the complete duration of
the application.  To configure your job to dynamically adjust to its
resource requirements, look into
Dynamic Allocation.
Fine-Grained (deprecated)
NOTE: Fine-grained mode is deprecated as of Spark 2.0.0.  Consider
 using Dynamic Allocation
 for some of the benefits.  For a full explanation see
 SPARK-11857
In “fine-grained” mode, each Spark task inside the Spark executor runs
as a separate Mesos task. This allows multiple instances of Spark (and
other frameworks) to share cores at a very fine granularity, where
each application gets more or fewer cores as it ramps up and down, but
it comes with an additional overhead in launching each task. This mode
may be inappropriate for low-latency requirements like interactive
queries or serving web requests.
Note that while Spark tasks in fine-grained will relinquish cores as
they terminate, they will not relinquish memory, as the JVM does not
give memory back to the Operating System.  Neither will executors
terminate when they’re idle.
To run in fine-grained mode, set the spark.mesos.coarse property to false in your
SparkConf:
conf.set("spark.mesos.coarse", "false")
You may also make use of spark.mesos.constraints to set
attribute-based constraints on Mesos resource offers. By default, all
resource offers will be accepted.
conf.set("spark.mesos.constraints", "os:centos7;us-east-1:false")
For example, Let’s say spark.mesos.constraints is set to os:centos7;us-east-1:false, then the resource offers will
be checked to see if they meet both these constraints and only then will be accepted to start new executors.
To constrain where driver tasks are run, use spark.mesos.driver.constraints
Mesos Docker Support
Spark can make use of a Mesos Docker containerizer by setting the property spark.mesos.executor.docker.image
in your SparkConf.
The Docker image used must have an appropriate version of Spark already part of the image, or you can
have Mesos download Spark via the usual methods.
Requires Mesos version 0.20.1 or later.
Note that by default Mesos agents will not pull the image if it already exists on the agent. If you use mutable image
tags you can set spark.mesos.executor.docker.forcePullImage to true in order to force the agent to always pull the
image before running the executor. Force pulling images is only available in Mesos version 0.22 and above.
Running Alongside Hadoop
You can run Spark and Mesos alongside your existing Hadoop cluster by just launching them as a
separate service on the machines. To access Hadoop data from Spark, a full hdfs:// URL is required
(typically hdfs://<namenode>:9000/path, but you can find the right URL on your Hadoop Namenode web
UI).
In addition, it is possible to also run Hadoop MapReduce on Mesos for better resource isolation and
sharing between the two. In this case, Mesos will act as a unified scheduler that assigns cores to
either Hadoop or Spark, as opposed to having them share resources via the Linux scheduler on each
node. Please refer to Hadoop on Mesos.
In either case, HDFS runs separately from Hadoop MapReduce, without being scheduled through Mesos.
Dynamic Resource Allocation with Mesos
Mesos supports dynamic allocation only with coarse-grained mode, which can resize the number of
executors based on statistics of the application. For general information,
see Dynamic Resource Allocation.
The External Shuffle Service to use is the Mesos Shuffle Service. It provides shuffle data cleanup functionality
on top of the Shuffle Service since Mesos doesn’t yet support notifying another framework’s
termination. To launch it, run $SPARK_HOME/sbin/start-mesos-shuffle-service.sh on all agent nodes, with spark.shuffle.service.enabled set to true.
This can also be achieved through Marathon, using a unique host constraint, and the following command: ./bin/spark-class org.apache.spark.deploy.mesos.MesosExternalShuffleService.
Configuration
See the configuration page for information on Spark configurations.  The following configs are specific for Spark on Mesos.
Spark Properties

Property NameDefaultMeaningSince Version

spark.mesos.coarse
true

    If set to true, runs over Mesos clusters in "coarse-grained" sharing mode, where Spark acquires one long-lived Mesos task on each machine.
    If set to false, runs over Mesos cluster in "fine-grained" sharing mode, where one Mesos task is created per Spark task.
    Detailed information in 'Mesos Run Modes'.
  
0.6.0


spark.mesos.extra.cores
0

    Set the extra number of cores for an executor to advertise. This
    does not result in more cores allocated.  It instead means that an
    executor will "pretend" it has more cores, so that the driver will
    send it more tasks.  Use this to increase parallelism.  This
    setting is only used for Mesos coarse-grained mode.
  
0.6.0


spark.mesos.mesosExecutor.cores
1.0

    (Fine-grained mode only) Number of cores to give each Mesos executor. This does not
    include the cores used to run the Spark tasks. In other words, even if no Spark task
    is being run, each Mesos executor will occupy the number of cores configured here.
    The value can be a floating point number.
  
1.4.0


spark.mesos.executor.docker.image
(none)

    Set the name of the docker image that the Spark executors will run in. The selected
    image must have Spark installed, as well as a compatible version of the Mesos library.
    The installed path of Spark in the image can be specified with spark.mesos.executor.home;
    the installed path of the Mesos library can be specified with spark.executorEnv.MESOS_NATIVE_JAVA_LIBRARY.
  
1.4.0


spark.mesos.executor.docker.forcePullImage
false

    Force Mesos agents to pull the image specified in spark.mesos.executor.docker.image.
    By default Mesos agents will not pull images they already have cached.
  
2.1.0


spark.mesos.executor.docker.parameters
(none)

    Set the list of custom parameters which will be passed into the docker run command when launching the Spark executor on Mesos using the docker containerizer. The format of this property is a comma-separated list of
    key/value pairs. Example:

    key1=val1,key2=val2,key3=val3

2.2.0


spark.mesos.executor.docker.volumes
(none)

    Set the list of volumes which will be mounted into the Docker image, which was set using
    spark.mesos.executor.docker.image. The format of this property is a comma-separated list of
    mappings following the form passed to docker run -v. That is they take the form:

    [host_path:]container_path[:ro|:rw]

1.4.0


spark.mesos.task.labels
(none)

    Set the Mesos labels to add to each task. Labels are free-form key-value pairs.
    Key-value pairs should be separated by a colon, and commas used to
    list more than one.  If your label includes a colon or comma, you
    can escape it with a backslash.  Ex. key:value,key2:a\:b.
  
2.2.0


spark.mesos.executor.home
driver side SPARK_HOME

    Set the directory in which Spark is installed on the executors in Mesos. By default, the
    executors will simply use the driver's Spark home directory, which may not be visible to
    them. Note that this is only relevant if a Spark binary package is not specified through
    spark.executor.uri.
  
1.1.1


spark.mesos.executor.memoryOverhead
executor memory * 0.10, with minimum of 384

    The amount of additional memory, specified in MiB, to be allocated per executor. By default,
    the overhead will be larger of either 384 or 10% of spark.executor.memory. If set,
    the final overhead will be this value.
  
1.1.1


spark.mesos.driver.memoryOverhead
driver memory * 0.10, with minimum of 384

    The amount of additional memory, specified in MB, to be allocated to the driver. By default,
    the overhead will be larger of either 384 or 10% of spark.driver.memory. If set,
    the final overhead will be this value. Only applies to cluster mode.
  


spark.mesos.uris
(none)

    A comma-separated list of URIs to be downloaded to the sandbox
    when driver or executor is launched by Mesos.  This applies to
    both coarse-grained and fine-grained mode.
  
1.5.0


spark.mesos.principal
(none)

    Set the principal with which Spark framework will use to authenticate with Mesos.  You can also specify this via the environment variable `SPARK_MESOS_PRINCIPAL`.
  
1.5.0


spark.mesos.principal.file
(none)

    Set the file containing the principal with which Spark framework will use to authenticate with Mesos.  Allows specifying the principal indirectly in more security conscious deployments.  The file must be readable by the user launching the job and be UTF-8 encoded plaintext.  You can also specify this via the environment variable `SPARK_MESOS_PRINCIPAL_FILE`.
  
2.4.0


spark.mesos.secret
(none)

    Set the secret with which Spark framework will use to authenticate with Mesos. Used, for example, when
    authenticating with the registry.  You can also specify this via the environment variable `SPARK_MESOS_SECRET`.
  
1.5.0


spark.mesos.secret.file
(none)

    Set the file containing the secret with which Spark framework will use to authenticate with Mesos. Used, for example, when
    authenticating with the registry.  Allows for specifying the secret indirectly in more security conscious deployments.  The file must be readable by the user launching the job and be UTF-8 encoded plaintext.  You can also specify this via the environment variable `SPARK_MESOS_SECRET_FILE`.
  
2.4.0


spark.mesos.role
*

    Set the role of this Spark framework for Mesos. Roles are used in Mesos for reservations
    and resource weight sharing.
  
1.5.0


spark.mesos.constraints
(none)

    Attribute-based constraints on mesos resource offers. By default, all resource offers will be accepted. This setting
    applies only to executors. Refer to Mesos
    Attributes & Resources for more information on attributes.
    
Scalar constraints are matched with "less than equal" semantics i.e. value in the constraint must be less than or equal to the value in the resource offer.
Range constraints are matched with "contains" semantics i.e. value in the constraint must be within the resource offer's value.
Set constraints are matched with "subset of" semantics i.e. value in the constraint must be a subset of the resource offer's value.
Text constraints are matched with "equality" semantics i.e. value in the constraint must be exactly equal to the resource offer's value.
In case there is no value present as a part of the constraint any offer with the corresponding attribute will be accepted (without value check).


1.5.0


spark.mesos.driver.constraints
(none)

    Same as spark.mesos.constraints except applied to drivers when launched through the dispatcher. By default,
    all offers with sufficient resources will be accepted.
  
2.2.1


spark.mesos.containerizer
docker

    This only affects docker containers, and must be one of "docker"
    or "mesos".  Mesos supports two types of
    containerizers for docker: the "docker" containerizer, and the preferred
    "mesos" containerizer.  Read more here: http://mesos.apache.org/documentation/latest/container-image/
  
2.1.0


spark.mesos.driver.webui.url
(none)

    Set the Spark Mesos driver webui_url for interacting with the framework.
    If unset it will point to Spark's internal web UI.
  
2.0.0


spark.mesos.driver.labels
(none)

    Mesos labels to add to the driver.  See spark.mesos.task.labels
    for formatting information.
  
2.3.0



spark.mesos.driver.secret.values,
    spark.mesos.driver.secret.names,
    spark.mesos.executor.secret.values,
    spark.mesos.executor.secret.names,
  
(none)


      A secret is specified by its contents and destination. These properties
      specify a secret's contents. To specify a secret's destination, see the cell below.
    

      You can specify a secret's contents either (1) by value or (2) by reference.
    

      (1) To specify a secret by value, set the
      spark.mesos.[driver|executor].secret.values
      property, to make the secret available in the driver or executors.
      For example, to make a secret password "guessme" available to the driver process, set:

      spark.mesos.driver.secret.values=guessme


      (2) To specify a secret that has been placed in a secret store
      by reference, specify its name within the secret store
      by setting the spark.mesos.[driver|executor].secret.names
      property. For example, to make a secret password named "password" in a secret store
      available to the driver process, set:

      spark.mesos.driver.secret.names=password


      Note: To use a secret store, make sure one has been integrated with Mesos via a custom
      SecretResolver
      module.
    

      To specify multiple secrets, provide a comma-separated list:

      spark.mesos.driver.secret.values=guessme,passwd123

      or

      spark.mesos.driver.secret.names=password1,password2


2.3.0



spark.mesos.driver.secret.envkeys,
    spark.mesos.driver.secret.filenames,
    spark.mesos.executor.secret.envkeys,
    spark.mesos.executor.secret.filenames,
  
(none)


      A secret is specified by its contents and destination. These properties
      specify a secret's destination. To specify a secret's contents, see the cell above.
    

      You can specify a secret's destination in the driver or
      executors as either (1) an environment variable or (2) as a file.
    

      (1) To make an environment-based secret, set the
      spark.mesos.[driver|executor].secret.envkeys property.
      The secret will appear as an environment variable with the
      given name in the driver or executors. For example, to make a secret password available
      to the driver process as $PASSWORD, set:

      spark.mesos.driver.secret.envkeys=PASSWORD


      (2) To make a file-based secret, set the
      spark.mesos.[driver|executor].secret.filenames property.
      The secret will appear in the contents of a file with the given file name in
      the driver or executors. For example, to make a secret password available in a
      file named "pwdfile" in the driver process, set:

      spark.mesos.driver.secret.filenames=pwdfile


      Paths are relative to the container's work directory. Absolute paths must
      already exist. Note: File-based secrets require a custom
      SecretResolver
      module.
    

      To specify env vars or file names corresponding to multiple secrets,
      provide a comma-separated list:

      spark.mesos.driver.secret.envkeys=PASSWORD1,PASSWORD2

      or

      spark.mesos.driver.secret.filenames=pwdfile1,pwdfile2


2.3.0


spark.mesos.driverEnv.[EnvironmentVariableName]
(none)

    This only affects drivers submitted in cluster mode.  Add the
    environment variable specified by EnvironmentVariableName to the
    driver process. The user can specify multiple of these to set
    multiple environment variables.
  
2.1.0


spark.mesos.dispatcher.webui.url
(none)

    Set the Spark Mesos dispatcher webui_url for interacting with the framework.
    If unset it will point to Spark's internal web UI.
  
2.0.0


spark.mesos.dispatcher.driverDefault.[PropertyName]
(none)

    Set default properties for drivers submitted through the
    dispatcher.  For example,
    spark.mesos.dispatcher.driverProperty.spark.executor.memory=32g
    results in the executors for all drivers submitted in cluster mode
    to run in 32g containers.
  
2.1.0


spark.mesos.dispatcher.historyServer.url
(none)

    Set the URL of the history
    server.  The dispatcher will then link each driver to its entry
    in the history server.
  
2.1.0


spark.mesos.dispatcher.queue
(none)

    Set the name of the dispatcher queue to which the application is submitted.
    The specified queue must be added to the dispatcher with spark.mesos.dispatcher.queue.[QueueName].
    If no queue is specified, then the application is submitted to the "default" queue with 0.0 priority.
  
3.1.0


spark.mesos.dispatcher.queue.[QueueName]
0.0

    Add a new queue for submitted drivers with the specified priority.
    Higher numbers indicate higher priority.
    The user can specify multiple queues to define a workload management policy for queued drivers in the dispatcher.
    A driver can then be submitted to a specific queue with spark.mesos.dispatcher.queue.
    By default, the dispatcher has a single queue with 0.0 priority (cannot be overridden).
    It is possible to implement a consistent and overall workload management policy throughout the lifecycle of drivers
    by mapping priority queues to weighted Mesos roles, and by specifying a
    spark.mesos.role along with a spark.mesos.dispatcher.queue when submitting an application.
    For example, with the URGENT Mesos role:
    
    spark.mesos.dispatcher.queue.URGENT=1.0

    spark.mesos.dispatcher.queue=URGENT
    spark.mesos.role=URGENT
    

3.1.0


spark.mesos.gpus.max
0

    Set the maximum number GPU resources to acquire for this job. Note that executors will still launch when no GPU resources are found
    since this configuration is just an upper limit and not a guaranteed amount.
  
2.1.0


spark.mesos.network.name
(none)

    Attach containers to the given named network.  If this job is
    launched in cluster mode, also launch the driver in the given named
    network.  See
    the Mesos CNI docs
    for more details.
  
2.1.0


spark.mesos.network.labels
(none)

    Pass network labels to CNI plugins.  This is a comma-separated list
    of key-value pairs, where each key-value pair has the format key:value.
    Example:

    key1:val1,key2:val2
    See
    the Mesos CNI docs
    for more details.
  
2.3.0


spark.mesos.fetcherCache.enable
false

    If set to `true`, all URIs (example: `spark.executor.uri`,
    `spark.mesos.uris`) will be cached by the Mesos
    Fetcher Cache

2.1.0


spark.mesos.driver.failoverTimeout
0.0

    The amount of time (in seconds) that the master will wait for the
    driver to reconnect, after being temporarily disconnected, before
    it tears down the driver framework by killing all its
    executors. The default value is zero, meaning no timeout: if the
    driver disconnects, the master immediately tears down the framework.
  
2.3.0


spark.mesos.rejectOfferDuration
120s

    Time to consider unused resources refused, serves as a fallback of
    `spark.mesos.rejectOfferDurationForUnmetConstraints`,
    `spark.mesos.rejectOfferDurationForReachedMaxCores`
  
2.2.0


spark.mesos.rejectOfferDurationForUnmetConstraints
spark.mesos.rejectOfferDuration

    Time to consider unused resources refused with unmet constraints
  
1.6.0


spark.mesos.rejectOfferDurationForReachedMaxCores
spark.mesos.rejectOfferDuration

    Time to consider unused resources refused when maximum number of cores
    spark.cores.max is reached
  
2.0.0


spark.mesos.appJar.local.resolution.mode
host

    Provides support for the `local:///` scheme to reference the app jar resource in cluster mode.
    If user uses a local resource (`local:///path/to/jar`) and the config option is not used it defaults to `host` e.g.
    the mesos fetcher tries to get the resource from the host's file system.
    If the value is unknown it prints a warning msg in the dispatcher logs and defaults to `host`.
    If the value is `container` then spark submit in the container will use the jar in the container's path:
    `/path/to/jar`.
  
2.4.0


Troubleshooting and Debugging
A few places to look during debugging:

Mesos master on port :5050

Agents should appear in the agents tab
Spark applications should appear in the frameworks tab
Tasks should appear in the details of a framework
Check the stdout and stderr of the sandbox of failed tasks


Mesos logs
    
Master and agent logs are both in /var/log/mesos by default



And common pitfalls:

Spark assembly not reachable/accessible
    
Agents must be able to download the Spark binary package from the http://, hdfs:// or s3n:// URL you gave


Firewall blocking communications
    
Check for messages about failed connections
Temporarily disable firewalls for debugging and then poke appropriate holes























  




Running Spark on YARN - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Running Spark on YARN

Security
Launching Spark on YARN 
Adding Other JARs


Preparations
Configuration
Debugging your Application 
Spark Properties
Available patterns for SHS custom executor log URL


Resource Allocation and Configuration Overview
Stage Level Scheduling Overview
Important notes
Kerberos 
YARN-specific Kerberos Configuration
Troubleshooting Kerberos


Configuring the External Shuffle Service
Launching your application with Apache Oozie
Using the Spark History Server to replace the Spark Web UI
Running multiple versions of the Spark Shuffle Service

Support for running on YARN (Hadoop
NextGen)
was added to Spark in version 0.6.0, and improved in subsequent releases.
Security
Security features like authentication are not enabled by default. When deploying a cluster that is open to the internet
or an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications
from running on the cluster.
Please see Spark Security and the specific security sections in this doc before running Spark.
Launching Spark on YARN
Ensure that HADOOP_CONF_DIR or YARN_CONF_DIR points to the directory which contains the (client side) configuration files for the Hadoop cluster.
These configs are used to write to HDFS and connect to the YARN ResourceManager. The
configuration contained in this directory will be distributed to the YARN cluster so that all
containers used by the application use the same configuration. If the configuration references
Java system properties or environment variables not managed by YARN, they should also be set in the
Spark application’s configuration (driver, executors, and the AM when running in client mode).
There are two deploy modes that can be used to launch Spark applications on YARN. In cluster mode, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. In client mode, the driver runs in the client process, and the application master is only used for requesting resources from YARN.
Unlike other cluster managers supported by Spark in which the master’s address is specified in the --master
parameter, in YARN mode the ResourceManager’s address is picked up from the Hadoop configuration.
Thus, the --master parameter is yarn.
To launch a Spark application in cluster mode:
$ ./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]

For example:
$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    examples/jars/spark-examples*.jar \
    10

The above starts a YARN client program which starts the default Application Master. Then SparkPi will be run as a child thread of Application Master. The client will periodically poll the Application Master for status updates and display them in the console. The client will exit once your application has finished running.  Refer to the Debugging your Application section below for how to see driver and executor logs.
To launch a Spark application in client mode, do the same, but replace cluster with client. The following shows how you can run spark-shell in client mode:
$ ./bin/spark-shell --master yarn --deploy-mode client

Adding Other JARs
In cluster mode, the driver runs on a different machine than the client, so SparkContext.addJar won’t work out of the box with files that are local to the client. To make files on the client available to SparkContext.addJar, include them with the --jars option in the launch command.
$ ./bin/spark-submit --class my.main.Class \
    --master yarn \
    --deploy-mode cluster \
    --jars my-other-jar.jar,my-other-other-jar.jar \
    my-main-jar.jar \
    app_arg1 app_arg2

Preparations
Running Spark on YARN requires a binary distribution of Spark which is built with YARN support.
Binary distributions can be downloaded from the downloads page of the project website.
There are two variants of Spark binary distributions you can download. One is pre-built with a certain
version of Apache Hadoop; this Spark distribution contains built-in Hadoop runtime, so we call it with-hadoop Spark
distribution. The other one is pre-built with user-provided Hadoop; since this Spark distribution
doesn’t contain a built-in Hadoop runtime, it’s smaller, but users have to provide a Hadoop installation separately.
We call this variant no-hadoop Spark distribution. For with-hadoop Spark distribution, since
it contains a built-in Hadoop runtime already, by default, when a job is submitted to Hadoop Yarn cluster, to prevent jar conflict, it will not
populate Yarn’s classpath into Spark. To override this behavior, you can set spark.yarn.populateHadoopClasspath=true.
For no-hadoop Spark distribution, Spark will populate Yarn’s classpath by default in order to get Hadoop runtime. For with-hadoop Spark distribution,
if your application depends on certain library that is only available in the cluster, you can try to populate the Yarn classpath by setting
the property mentioned above. If you run into jar conflict issue by doing so, you will need to turn it off and include this library
in your application jar.
To build Spark yourself, refer to Building Spark.
To make Spark runtime jars accessible from YARN side, you can specify spark.yarn.archive or spark.yarn.jars. For details please refer to Spark Properties. If neither spark.yarn.archive nor spark.yarn.jars is specified, Spark will create a zip file with all jars under $SPARK_HOME/jars and upload it to the distributed cache.
Configuration
Most of the configs are the same for Spark on YARN as for other deployment modes. See the configuration page for more information on those.  These are configs that are specific to Spark on YARN.
Debugging your Application
In YARN terminology, executors and application masters run inside “containers”. YARN has two modes for handling container logs after an application has completed. If log aggregation is turned on (with the yarn.log-aggregation-enable config), container logs are copied to HDFS and deleted on the local machine. These logs can be viewed from anywhere on the cluster with the yarn logs command.
yarn logs -applicationId <app ID>

will print out the contents of all log files from all containers from the given application. You can also view the container log files directly in HDFS using the HDFS shell or API. The directory where they are located can be found by looking at your YARN configs (yarn.nodemanager.remote-app-log-dir and yarn.nodemanager.remote-app-log-dir-suffix). The logs are also available on the Spark Web UI under the Executors Tab. You need to have both the Spark history server and the MapReduce history server running and configure yarn.log.server.url in yarn-site.xml properly. The log URL on the Spark history server UI will redirect you to the MapReduce history server to show the aggregated logs.
When log aggregation isn’t turned on, logs are retained locally on each machine under YARN_APP_LOGS_DIR, which is usually configured to /tmp/logs or $HADOOP_HOME/logs/userlogs depending on the Hadoop version and installation. Viewing logs for a container requires going to the host that contains them and looking in this directory.  Subdirectories organize log files by application ID and container ID. The logs are also available on the Spark Web UI under the Executors Tab and doesn’t require running the MapReduce history server.
To review per-container launch environment, increase yarn.nodemanager.delete.debug-delay-sec to a
large value (e.g. 36000), and then access the application cache through yarn.nodemanager.local-dirs
on the nodes on which containers are launched. This directory contains the launch script, JARs, and
all environment variables used for launching each container. This process is useful for debugging
classpath problems in particular. (Note that enabling this requires admin privileges on cluster
settings and a restart of all node managers. Thus, this is not applicable to hosted clusters).
To use a custom log4j2 configuration for the application master or executors, here are the options:

upload a custom log4j2.properties using spark-submit, by adding it to the --files list of files
to be uploaded with the application.
add -Dlog4j.configurationFile=<location of configuration file> to spark.driver.extraJavaOptions
(for the driver) or spark.executor.extraJavaOptions (for executors). Note that if using a file,
the file: protocol should be explicitly provided, and the file needs to exist locally on all
the nodes.
update the $SPARK_CONF_DIR/log4j2.properties file and it will be automatically uploaded along
with the other configurations. Note that other 2 options has higher priority than this option if
multiple options are specified.

Note that for the first option, both executors and the application master will share the same
log4j configuration, which may cause issues when they run on the same node (e.g. trying to write
to the same log file).
If you need a reference to the proper location to put log files in the YARN so that YARN can properly display and aggregate them, use spark.yarn.app.container.log.dir in your log4j2.properties. For example, appender.file_appender.fileName=${sys:spark.yarn.app.container.log.dir}/spark.log. For streaming applications, configuring RollingFileAppender and setting file location to YARN’s log directory will avoid disk overflow caused by large log files, and logs can be accessed using YARN’s log utility.
To use a custom metrics.properties for the application master and executors, update the $SPARK_CONF_DIR/metrics.properties file. It will automatically be uploaded with other configurations, so you don’t need to specify it manually with --files.
Spark Properties

Property NameDefaultMeaningSince Version

spark.yarn.am.memory
512m

    Amount of memory to use for the YARN Application Master in client mode, in the same format as JVM memory strings (e.g. 512m, 2g).
    In cluster mode, use spark.driver.memory instead.
    
    Use lower-case suffixes, e.g. k, m, g, t, and p, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively.
  
1.3.0


spark.yarn.am.resource.{resource-type}.amount
(none)

    Amount of resource to use for the YARN Application Master in client mode.
    In cluster mode, use spark.yarn.driver.resource.<resource-type>.amount instead.
    Please note that this feature can be used only with YARN 3.0+
    For reference, see YARN Resource Model documentation: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceModel.html
    
    Example:
    To request GPU resources from YARN, use: spark.yarn.am.resource.yarn.io/gpu.amount

3.0.0


spark.yarn.applicationType
SPARK

    Defines more specific application types, e.g. SPARK, SPARK-SQL, SPARK-STREAMING,
    SPARK-MLLIB and SPARK-GRAPH. Please be careful not to exceed 20 characters.
  
3.1.0


spark.yarn.driver.resource.{resource-type}.amount
(none)

    Amount of resource to use for the YARN Application Master in cluster mode.
    Please note that this feature can be used only with YARN 3.0+
    For reference, see YARN Resource Model documentation: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceModel.html
    
    Example:
    To request GPU resources from YARN, use: spark.yarn.driver.resource.yarn.io/gpu.amount

3.0.0


spark.yarn.executor.resource.{resource-type}.amount
(none)

    Amount of resource to use per executor process.
    Please note that this feature can be used only with YARN 3.0+
    For reference, see YARN Resource Model documentation: https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceModel.html
    
    Example:
    To request GPU resources from YARN, use: spark.yarn.executor.resource.yarn.io/gpu.amount

3.0.0


spark.yarn.resourceGpuDeviceName
yarn.io/gpu

    Specify the mapping of the Spark resource type of gpu to the YARN resource
    representing a GPU. By default YARN uses yarn.io/gpu but if YARN has been
    configured with a custom resource type, this allows remapping it.
    Applies when using the spark.{driver/executor}.resource.gpu.* configs.
  
3.2.1


spark.yarn.resourceFpgaDeviceName
yarn.io/fpga

    Specify the mapping of the Spark resource type of fpga to the YARN resource
    representing a FPGA. By default YARN uses yarn.io/fpga but if YARN has been
    configured with a custom resource type, this allows remapping it.
    Applies when using the spark.{driver/executor}.resource.fpga.* configs.
  
3.2.1


spark.yarn.am.cores
1

    Number of cores to use for the YARN Application Master in client mode.
    In cluster mode, use spark.driver.cores instead.
  
1.3.0


spark.yarn.am.waitTime
100s

    Only used in cluster mode. Time for the YARN Application Master to wait for the
    SparkContext to be initialized.
  
1.3.0


spark.yarn.submit.file.replication
The default HDFS replication (usually 3)

    HDFS replication level for the files uploaded into HDFS for the application. These include things like the Spark jar, the app jar, and any distributed cache files/archives.
  
0.8.1


spark.yarn.stagingDir
Current user's home directory in the filesystem

    Staging directory used while submitting applications.
  
2.0.0


spark.yarn.preserve.staging.files
false

    Set to true to preserve the staged files (Spark jar, app jar, distributed cache files) at the end of the job rather than delete them.
  
1.1.0


spark.yarn.scheduler.heartbeat.interval-ms
3000

    The interval in ms in which the Spark application master heartbeats into the YARN ResourceManager.
    The value is capped at half the value of YARN's configuration for the expiry interval, i.e.
    yarn.am.liveness-monitor.expiry-interval-ms.
  
0.8.1


spark.yarn.scheduler.initial-allocation.interval
200ms

    The initial interval in which the Spark application master eagerly heartbeats to the YARN ResourceManager
    when there are pending container allocation requests. It should be no larger than
    spark.yarn.scheduler.heartbeat.interval-ms. The allocation interval will doubled on
    successive eager heartbeats if pending containers still exist, until
    spark.yarn.scheduler.heartbeat.interval-ms is reached.
  
1.4.0


spark.yarn.historyServer.address
(none)

    The address of the Spark history server, e.g. host.com:18080. The address should not contain a scheme (http://). Defaults to not being set since the history server is an optional service. This address is given to the YARN ResourceManager when the Spark application finishes to link the application from the ResourceManager UI to the Spark history server UI.
    For this property, YARN properties can be used as variables, and these are substituted by Spark at runtime. For example, if the Spark history server runs on the same node as the YARN ResourceManager, it can be set to ${hadoopconf-yarn.resourcemanager.hostname}:18080.
  
1.0.0


spark.yarn.dist.archives
(none)

    Comma separated list of archives to be extracted into the working directory of each executor.
  
1.0.0


spark.yarn.dist.files
(none)

    Comma-separated list of files to be placed in the working directory of each executor.
  
1.0.0


spark.yarn.dist.jars
(none)

    Comma-separated list of jars to be placed in the working directory of each executor.
  
2.0.0


spark.yarn.dist.forceDownloadSchemes
(none)

    Comma-separated list of schemes for which resources will be downloaded to the local disk prior to
    being added to YARN's distributed cache. For use in cases where the YARN service does not
    support schemes that are supported by Spark, like http, https and ftp, or jars required to be in the
    local YARN client's classpath. Wildcard '*' is denoted to download resources for all the schemes.
  
2.3.0


spark.executor.instances
2

    The number of executors for static allocation. With spark.dynamicAllocation.enabled, the initial set of executors will be at least this large.
  
1.0.0


spark.yarn.am.memoryOverhead
AM memory * 0.10, with minimum of 384 

    Same as spark.driver.memoryOverhead, but for the YARN Application Master in client mode.
  
1.3.0


spark.yarn.queue
default

    The name of the YARN queue to which the application is submitted.
  
1.0.0


spark.yarn.jars
(none)

    List of libraries containing Spark code to distribute to YARN containers.
    By default, Spark on YARN will use Spark jars installed locally, but the Spark jars can also be
    in a world-readable location on HDFS. This allows YARN to cache it on nodes so that it doesn't
    need to be distributed each time an application runs. To point to jars on HDFS, for example,
    set this configuration to hdfs:///some/path. Globs are allowed.
  
2.0.0


spark.yarn.archive
(none)

    An archive containing needed Spark jars for distribution to the YARN cache. If set, this
    configuration replaces spark.yarn.jars and the archive is used in all the
    application's containers. The archive should contain jar files in its root directory.
    Like with the previous option, the archive can also be hosted on HDFS to speed up file
    distribution.
  
2.0.0


spark.yarn.appMasterEnv.[EnvironmentVariableName]
(none)

     Add the environment variable specified by EnvironmentVariableName to the
     Application Master process launched on YARN. The user can specify multiple of
     these and to set multiple environment variables. In cluster mode this controls
     the environment of the Spark driver and in client mode it only controls
     the environment of the executor launcher.
  
1.1.0


spark.yarn.containerLauncherMaxThreads
25

    The maximum number of threads to use in the YARN Application Master for launching executor containers.
  
1.2.0


spark.yarn.am.extraJavaOptions
(none)

  A string of extra JVM options to pass to the YARN Application Master in client mode.
  In cluster mode, use spark.driver.extraJavaOptions instead. Note that it is illegal
  to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set
  with spark.yarn.am.memory

1.3.0


spark.yarn.am.extraLibraryPath
(none)

    Set a special library path to use when launching the YARN Application Master in client mode.
  
1.4.0


spark.yarn.populateHadoopClasspath

    For with-hadoop Spark distribution, this is set to false;
    for no-hadoop distribution, this is set to true.
  

    Whether to populate Hadoop classpath from yarn.application.classpath and
    mapreduce.application.classpath Note that if this is set to false,
    it requires a with-Hadoop Spark distribution that bundles Hadoop runtime or
    user has to provide a Hadoop installation separately.
  
2.4.6


spark.yarn.maxAppAttempts
yarn.resourcemanager.am.max-attempts in YARN

  The maximum number of attempts that will be made to submit the application.
  It should be no larger than the global number of max attempts in the YARN configuration.
  
1.3.0


spark.yarn.am.attemptFailuresValidityInterval
(none)

  Defines the validity interval for AM failure tracking.
  If the AM has been running for at least the defined interval, the AM failure count will be reset.
  This feature is not enabled if not configured.
  
1.6.0


spark.yarn.am.clientModeTreatDisconnectAsFailed
false

  Treat yarn-client unclean disconnects as failures. In yarn-client mode, normally the application will always finish
  with a final status of SUCCESS because in some cases, it is not possible to know if the Application was terminated
  intentionally by the user or if there was a real error. This config changes that behavior such that if the Application
  Master disconnects from the driver uncleanly (ie without the proper shutdown handshake) the application will
  terminate with a final status of FAILED. This will allow the caller to decide if it was truly a failure. Note that if
  this config is set and the user just terminate the client application badly it may show a status of FAILED when it wasn't really FAILED.
  
3.3.0


spark.yarn.am.clientModeExitOnError
false

  In yarn-client mode, when this is true, if driver got application report with final status of KILLED or FAILED,
  driver will stop corresponding SparkContext and exit program with code 1.
  Note, if this is true and called from another application, it will terminate the parent application as well.
  
3.3.0


spark.yarn.am.tokenConfRegex
(none)

    The value of this config is a regex expression used to grep a list of config entries from the job's configuration file (e.g., hdfs-site.xml)
    and send to RM, which uses them when renewing delegation tokens. A typical use case of this feature is to support delegation
    tokens in an environment where a YARN cluster needs to talk to multiple downstream HDFS clusters, where the YARN RM may not have configs
    (e.g., dfs.nameservices, dfs.ha.namenodes.*, dfs.namenode.rpc-address.*) to connect to these clusters.
    In this scenario, Spark users can specify the config value to be ^dfs.nameservices$|^dfs.namenode.rpc-address.*$|^dfs.ha.namenodes.*$ to parse
    these HDFS configs from the job's local configuration files. This config is very similar to mapreduce.job.send-token-conf. Please check YARN-5910 for more details.
  
3.3.0


spark.yarn.submit.waitAppCompletion
true

  In YARN cluster mode, controls whether the client waits to exit until the application completes.
  If set to true, the client process will stay alive reporting the application's status.
  Otherwise, the client process will exit after submission.
  
1.4.0


spark.yarn.am.nodeLabelExpression
(none)

  A YARN node label expression that restricts the set of nodes AM will be scheduled on.
  Only versions of YARN greater than or equal to 2.6 support node label expressions, so when
  running against earlier versions, this property will be ignored.
  
1.6.0


spark.yarn.executor.nodeLabelExpression
(none)

  A YARN node label expression that restricts the set of nodes executors will be scheduled on.
  Only versions of YARN greater than or equal to 2.6 support node label expressions, so when
  running against earlier versions, this property will be ignored.
  
1.4.0


spark.yarn.tags
(none)

  Comma-separated list of strings to pass through as YARN application tags appearing
  in YARN ApplicationReports, which can be used for filtering when querying YARN apps.
  
1.5.0


spark.yarn.priority
(none)

  Application priority for YARN to define pending applications ordering policy, those with higher
  integer value have a better opportunity to be activated. Currently, YARN only supports application
  priority when using FIFO ordering policy.
  
3.0.0


spark.yarn.config.gatewayPath
(none)

  A path that is valid on the gateway host (the host where a Spark application is started) but may
  differ for paths for the same resource in other nodes in the cluster. Coupled with
  spark.yarn.config.replacementPath, this is used to support clusters with
  heterogeneous configurations, so that Spark can correctly launch remote processes.
  
  The replacement path normally will contain a reference to some environment variable exported by
  YARN (and, thus, visible to Spark containers).
  
  For example, if the gateway node has Hadoop libraries installed on /disk1/hadoop, and
  the location of the Hadoop install is exported by YARN as the  HADOOP_HOME
  environment variable, setting this value to /disk1/hadoop and the replacement path to
  $HADOOP_HOME will make sure that paths used to launch remote processes properly
  reference the local YARN configuration.
  
1.5.0


spark.yarn.config.replacementPath
(none)

  See spark.yarn.config.gatewayPath.
  
1.5.0


spark.yarn.rolledLog.includePattern
(none)

  Java Regex to filter the log files which match the defined include pattern
  and those log files will be aggregated in a rolling fashion.
  This will be used with YARN's rolling log aggregation, to enable this feature in YARN side
  yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds should be
  configured in yarn-site.xml. The Spark log4j appender needs be changed to use
  FileAppender or another appender that can handle the files being removed while it is running. Based
  on the file name configured in the log4j configuration (like spark.log), the user should set the
  regex (spark*) to include all the log files that need to be aggregated.
  
2.0.0


spark.yarn.rolledLog.excludePattern
(none)

  Java Regex to filter the log files which match the defined exclude pattern
  and those log files will not be aggregated in a rolling fashion. If the log file
  name matches both the include and the exclude pattern, this file will be excluded eventually.
  
2.0.0


spark.yarn.executor.launch.excludeOnFailure.enabled
false

  Flag to enable exclusion of nodes having YARN resource allocation problems.
  The error limit for excluding can be configured by
  spark.excludeOnFailure.application.maxFailedExecutorsPerNode.
  
2.4.0


spark.yarn.exclude.nodes
(none)

  Comma-separated list of YARN node names which are excluded from resource allocation.
  
3.0.0


spark.yarn.metrics.namespace
(none)

  The root namespace for AM metrics reporting.
  If it is not set then the YARN application ID is used.
  
2.4.0


spark.yarn.report.interval
1s

    Interval between reports of the current Spark job status in cluster mode.
  
0.9.0


spark.yarn.report.loggingFrequency
30

    Maximum number of application reports processed until the next application status
    is logged. If there is a change of state, the application status will be logged regardless
    of the number of application reports processed.
  
3.5.0


spark.yarn.clientLaunchMonitorInterval
1s

    Interval between requests for status the client mode AM when starting the app.
  
2.3.0


spark.yarn.includeDriverLogsLink
false

    In cluster mode, whether the client application report includes links to the driver
    container's logs. This requires polling the ResourceManager's REST API, so it
    places some additional load on the RM.
  
3.1.0


spark.yarn.unmanagedAM.enabled
false

    In client mode, whether to launch the Application Master service as part of the client
    using unmanaged am.
  
3.0.0


spark.yarn.shuffle.server.recovery.disabled
false

    Set to true for applications that have higher security requirements and prefer that their
    secret is not saved in the db. The shuffle data of such applications wll not be recovered after
    the External Shuffle Service restarts.
  
3.5.0


Available patterns for SHS custom executor log URL

PatternMeaning

{{HTTP_SCHEME}}
http:// or https:// according to YARN HTTP policy. (Configured via yarn.http.policy)


{{NM_HOST}}
The "host" of node where container was run.


{{NM_PORT}}
The "port" of node manager where container was run.


{{NM_HTTP_PORT}}
The "port" of node manager's http server where container was run.


{{NM_HTTP_ADDRESS}}
Http URI of the node on which the container is allocated.


{{CLUSTER_ID}}
The cluster ID of Resource Manager. (Configured via yarn.resourcemanager.cluster-id)


{{CONTAINER_ID}}
The ID of container.


{{USER}}
SPARK_USER on system environment.


{{FILE_NAME}}
stdout, stderr.


For example, suppose you would like to point log url link to Job History Server directly instead of let NodeManager http server redirects it, you can configure spark.history.custom.executor.log.url as below:
{{HTTP_SCHEME}}<JHS_HOST>:<JHS_PORT>/jobhistory/logs/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}?start=-4096
NOTE: you need to replace <JHS_HOST> and <JHS_PORT> with actual value.
Resource Allocation and Configuration Overview
Please make sure to have read the Custom Resource Scheduling and Configuration Overview section on the configuration page. This section only talks about the YARN specific aspects of resource scheduling.
YARN needs to be configured to support any resources the user wants to use with Spark. Resource scheduling on YARN was added in YARN 3.1.0. See the YARN documentation for more information on configuring resources and properly setting up isolation. Ideally the resources are setup isolated so that an executor can only see the resources it was allocated. If you do not have isolation enabled, the user is responsible for creating a discovery script that ensures the resource is not shared between executors.
YARN supports user defined resource types but has built in types for GPU (yarn.io/gpu) and FPGA (yarn.io/fpga). For that reason, if you are using either of those resources, Spark can translate your request for spark resources into YARN resources and you only have to specify the spark.{driver/executor}.resource. configs. Note, if you are using a custom resource type for GPUs or FPGAs with YARN you can change the Spark mapping using spark.yarn.resourceGpuDeviceName and spark.yarn.resourceFpgaDeviceName.
 If you are using a resource other than FPGA or GPU, the user is responsible for specifying the configs for both YARN (spark.yarn.{driver/executor}.resource.) and Spark (spark.{driver/executor}.resource.).
For example, the user wants to request 2 GPUs for each executor. The user can just specify spark.executor.resource.gpu.amount=2 and Spark will handle requesting yarn.io/gpu resource type from YARN.
If the user has a user defined YARN resource, lets call it acceleratorX then the user must specify spark.yarn.executor.resource.acceleratorX.amount=2 and spark.executor.resource.acceleratorX.amount=2.
YARN does not tell Spark the addresses of the resources allocated to each container. For that reason, the user must specify a discovery script that gets run by the executor on startup to discover what resources are available to that executor. You can find an example scripts in examples/src/main/scripts/getGpusResources.sh. The script must have execute permissions set and the user should setup permissions to not allow malicious users to modify it. The script should write to STDOUT a JSON string in the format of the ResourceInformation class. This has the resource name and an array of resource addresses available to just that executor.
Stage Level Scheduling Overview
Stage level scheduling is supported on YARN:

When dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.
When dynamic allocation is enabled: It allows users to specify task and executor resource requirements at the stage level and will request the extra executors.

One thing to note that is YARN specific is that each ResourceProfile requires a different container priority on YARN. The mapping is simply the ResourceProfile id becomes the priority, on YARN lower numbers are higher priority. This means that profiles created earlier will have a higher priority in YARN. Normally this won’t matter as Spark finishes one stage before starting another one, the only case this might have an affect is in a job server type scenario, so its something to keep in mind.
Note there is a difference in the way custom resources are handled between the base default profile and custom ResourceProfiles. To allow for the user to request YARN containers with extra resources without Spark scheduling on them, the user can specify resources via the spark.yarn.executor.resource. config. Those configs are only used in the base default profile though and do not get propagated into any other custom ResourceProfiles. This is because there would be no way to remove them if you wanted a stage to not have them. This results in your default profile getting custom resources defined in spark.yarn.executor.resource. plus spark defined resources of GPU or FPGA. Spark converts GPU and FPGA resources into the YARN built in types yarn.io/gpu) and yarn.io/fpga, but does not know the mapping of any other resources. Any other Spark custom resources are not propagated to YARN for the default profile. So if you want Spark to schedule based off a custom resource and have it requested from YARN, you must specify it in both YARN (spark.yarn.{driver/executor}.resource.) and Spark (spark.{driver/executor}.resource.) configs. Leave the Spark config off if you only want YARN containers with the extra resources but Spark not to schedule using them. Now for custom ResourceProfiles, it doesn’t currently have a way to only specify YARN resources without Spark scheduling off of them. This means for custom ResourceProfiles we propagate all the resources defined in the ResourceProfile to YARN. We still convert GPU and FPGA to the YARN build in types as well. This requires that the name of any custom resources you specify match what they are defined as in YARN.
Important notes

Whether core requests are honored in scheduling decisions depends on which scheduler is in use and how it is configured.
In cluster mode, the local directories used by the Spark executors and the Spark driver will be the local directories configured for YARN (Hadoop YARN config yarn.nodemanager.local-dirs). If the user specifies spark.local.dir, it will be ignored. In client mode, the Spark executors will use the local directories configured for YARN while the Spark driver will use those defined in spark.local.dir. This is because the Spark driver does not run on the YARN cluster in client mode, only the Spark executors do.
The --files and --archives options support specifying file names with the # similar to Hadoop. For example, you can specify: --files localtest.txt#appSees.txt and this will upload the file you have locally named localtest.txt into HDFS but this will be linked to by the name appSees.txt, and your application should use the name as appSees.txt to reference it when running on YARN.
The --jars option allows the SparkContext.addJar function to work if you are using it with local files and running in cluster mode. It does not need to be used if you are using it with HDFS, HTTP, HTTPS, or FTP files.

Kerberos
Standard Kerberos support in Spark is covered in the Security page.
In YARN mode, when accessing Hadoop file systems, aside from the default file system in the hadoop
configuration, Spark will also automatically obtain delegation tokens for the service hosting the
staging directory of the Spark application.
YARN-specific Kerberos Configuration

Property NameDefaultMeaningSince Version

spark.kerberos.keytab
(none)

  The full path to the file that contains the keytab for the principal specified above. This keytab
  will be copied to the node running the YARN Application Master via the YARN Distributed Cache, and
  will be used for renewing the login tickets and the delegation tokens periodically. Equivalent to
  the --keytab command line argument.

   (Works also with the "local" master.)
  
3.0.0


spark.kerberos.principal
(none)

  Principal to be used to login to KDC, while running on secure clusters. Equivalent to the
  --principal command line argument.

   (Works also with the "local" master.)
  
3.0.0


spark.yarn.kerberos.relogin.period
1m

  How often to check whether the kerberos TGT should be renewed. This should be set to a value
  that is shorter than the TGT renewal period (or the TGT lifetime if TGT renewal is not enabled).
  The default value should be enough for most deployments.
  
2.3.0


spark.yarn.kerberos.renewal.excludeHadoopFileSystems
(none)

    A comma-separated list of Hadoop filesystems for whose hosts will be excluded from delegation
    token renewal at resource scheduler. For example, spark.yarn.kerberos.renewal.excludeHadoopFileSystems=hdfs://nn1.com:8032,
    hdfs://nn2.com:8032. This is known to work under YARN for now, so YARN Resource Manager won't renew tokens for the application.
    Note that as resource scheduler does not renew token, so any application running longer than the original token expiration that tries
    to use that token will likely fail.
  
3.2.0


Troubleshooting Kerberos
Debugging Hadoop/Kerberos problems can be “difficult”. One useful technique is to
enable extra logging of Kerberos operations in Hadoop by setting the HADOOP_JAAS_DEBUG
environment variable.
export HADOOP_JAAS_DEBUG=true

The JDK classes can be configured to enable extra logging of their Kerberos and
SPNEGO/REST authentication via the system properties sun.security.krb5.debug
and sun.security.spnego.debug=true
-Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true

All these options can be enabled in the Application Master:
spark.yarn.appMasterEnv.HADOOP_JAAS_DEBUG true
spark.yarn.am.extraJavaOptions -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true

Finally, if the log level for org.apache.spark.deploy.yarn.Client is set to DEBUG, the log
will include a list of all tokens obtained, and their expiry details
Configuring the External Shuffle Service
To start the Spark Shuffle Service on each NodeManager in your YARN cluster, follow these
instructions:

Build Spark with the YARN profile. Skip this step if you are using a
pre-packaged distribution.
Locate the spark-<version>-yarn-shuffle.jar. This should be under
$SPARK_HOME/common/network-yarn/target/scala-<version> if you are building Spark yourself, and under
yarn if you are using a distribution.
Add this jar to the classpath of all NodeManagers in your cluster.
In the yarn-site.xml on each node, add spark_shuffle to yarn.nodemanager.aux-services,
then set yarn.nodemanager.aux-services.spark_shuffle.class to
org.apache.spark.network.yarn.YarnShuffleService.
Increase NodeManager's heap size by setting YARN_HEAPSIZE (1000 by default) in etc/hadoop/yarn-env.sh
to avoid garbage collection issues during shuffle.
Restart all NodeManagers in your cluster.

The following extra configuration options are available when the shuffle service is running on YARN:

Property NameDefaultMeaningSince Version

spark.yarn.shuffle.stopOnFailure
false

    Whether to stop the NodeManager when there's a failure in the Spark Shuffle Service's
    initialization. This prevents application failures caused by running containers on
    NodeManagers where the Spark Shuffle Service is not running.
  
2.1.0


spark.yarn.shuffle.service.metrics.namespace
sparkShuffleService

    The namespace to use when emitting shuffle service metrics into Hadoop metrics2 system of the
    NodeManager.
  
3.2.0


spark.yarn.shuffle.service.logs.namespace
(not set)

    A namespace which will be appended to the class name when forming the logger name to use for
    emitting logs from the YARN shuffle service, like
    org.apache.spark.network.yarn.YarnShuffleService.logsNamespaceValue. Since some logging frameworks
    may expect the logger name to look like a class name, it's generally recommended to provide a value which
    would be a valid Java package or class name and not include spaces.
  
3.3.0


spark.shuffle.service.db.backend
LEVELDB

    When work-preserving restart is enabled in YARN, this is used to specify the disk-base store used
    in shuffle service state store, supports `LEVELDB` and `ROCKSDB` with `LEVELDB` as default value.
    The original data store in `LevelDB/RocksDB` will not be automatically converted to another kind
    of storage now. The original data store will be retained and the new type data store will be
    created when switching storage types.
  
3.4.0


Please note that the instructions above assume that the default shuffle service name,
spark_shuffle, has been used. It is possible to use any name here, but the values used in the
YARN NodeManager configurations must match the value of spark.shuffle.service.name in the
Spark application.
The shuffle service will, by default, take all of its configurations from the Hadoop Configuration
used by the NodeManager (e.g. yarn-site.xml). However, it is also possible to configure the
shuffle service independently using a file named spark-shuffle-site.xml which should be placed
onto the classpath of the shuffle service (which is, by default, shared with the classpath of the
NodeManager). The shuffle service will treat this as a standard Hadoop Configuration resource and
overlay it on top of the NodeManager’s configuration.
Launching your application with Apache Oozie
Apache Oozie can launch Spark applications as part of a workflow.
In a secure cluster, the launched application will need the relevant tokens to access the cluster’s
services. If Spark is launched with a keytab, this is automatic.
However, if Spark is to be launched without a keytab, the responsibility for setting up security
must be handed over to Oozie.
The details of configuring Oozie for secure clusters and obtaining
credentials for a job can be found on the Oozie web site
in the “Authentication” section of the specific release’s documentation.
For Spark applications, the Oozie workflow must be set up for Oozie to request all tokens which
the application needs, including:

The YARN resource manager.
The local Hadoop filesystem.
Any remote Hadoop filesystems used as a source or destination of I/O.
Hive —if used.
HBase —if used.
The YARN timeline server, if the application interacts with this.

To avoid Spark attempting —and then failing— to obtain Hive, HBase and remote HDFS tokens,
the Spark configuration must be set to disable token collection for the services.
The Spark configuration must include the lines:
spark.security.credentials.hive.enabled   false
spark.security.credentials.hbase.enabled  false

The configuration option spark.kerberos.access.hadoopFileSystems must be unset.
Using the Spark History Server to replace the Spark Web UI
It is possible to use the Spark History Server application page as the tracking URL for running
applications when the application UI is disabled. This may be desirable on secure clusters, or to
reduce the memory usage of the Spark driver. To set up tracking through the Spark History Server,
do the following:

On the application side, set spark.yarn.historyServer.allowTracking=true in Spark’s
configuration. This will tell Spark to use the history server’s URL as the tracking URL if
the application’s UI is disabled.
On the Spark History Server, add org.apache.spark.deploy.yarn.YarnProxyRedirectFilter
to the list of filters in the spark.ui.filters configuration.

Be aware that the history server information may not be up-to-date with the application’s state.
Running multiple versions of the Spark Shuffle Service
Please note that this section only applies when running on YARN versions >= 2.9.0.
In some cases it may be desirable to run multiple instances of the Spark Shuffle Service which are
using different versions of Spark. This can be helpful, for example, when running a YARN cluster
with a mixed workload of applications running multiple Spark versions, since a given version of
the shuffle service is not always compatible with other versions of Spark. YARN versions since 2.9.0
support the ability to run shuffle services within an isolated classloader
(see YARN-4577), meaning multiple Spark versions
can coexist within a single NodeManager. The
yarn.nodemanager.aux-services.<service-name>.classpath and, starting from YARN 2.10.2/3.1.1/3.2.0,
yarn.nodemanager.aux-services.<service-name>.remote-classpath options can be used to configure
this. Note that YARN 3.3.0/3.3.1 have an issue which requires setting
yarn.nodemanager.aux-services.<service-name>.system-classes as a workaround. See
YARN-11053 for details. In addition to setting
up separate classpaths, it’s necessary to ensure the two versions advertise to different ports.
This can be achieved using the spark-shuffle-site.xml file described above. For example, you may
have configuration like:
  yarn.nodemanager.aux-services = spark_shuffle_x,spark_shuffle_y
  yarn.nodemanager.aux-services.spark_shuffle_x.classpath = /path/to/spark-x-path/fat.jar:/path/to/spark-x-config
  yarn.nodemanager.aux-services.spark_shuffle_y.classpath = /path/to/spark-y-path/fat.jar:/path/to/spark-y-config

Or
  yarn.nodemanager.aux-services = spark_shuffle_x,spark_shuffle_y
  yarn.nodemanager.aux-services.spark_shuffle_x.classpath = /path/to/spark-x-path/*:/path/to/spark-x-config
  yarn.nodemanager.aux-services.spark_shuffle_y.classpath = /path/to/spark-y-path/*:/path/to/spark-y-config

The two spark-*-config directories each contain one file, spark-shuffle-site.xml. These are XML
files in the Hadoop Configuration format
which each contain a few configurations to adjust the port number and metrics name prefix used:
<configuration>
  <property>
    <name>spark.shuffle.service.port</name>
    <value>7001</value>
  </property>
  <property>
    <name>spark.yarn.shuffle.service.metrics.namespace</name>
    <value>sparkShuffleServiceX</value>
  </property>
</configuration>

The values should both be different for the two different services.
Then, in the configuration of the Spark applications, one should be configured with:
  spark.shuffle.service.name = spark_shuffle_x
  spark.shuffle.service.port = 7001

and one should be configured with:
  spark.shuffle.service.name = spark_shuffle_y
  spark.shuffle.service.port = <other value>





















  




Security - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Spark Security

Spark Security: Things You Need To Know
Spark RPC (Communication protocol between Spark processes) 
Authentication 
YARN
Kubernetes


Encryption


Local Storage Encryption
Web UI 
Authentication and Authorization
Spark History Server ACLs
SSL Configuration
Preparing the key stores 
YARN mode
Standalone mode
Mesos mode


HTTP Security Headers


Configuring Ports for Network Security 
Standalone mode only
All cluster managers


Kerberos 
Long-Running Applications 
Using a Keytab
Using a ticket cache


Secure Interaction with Kubernetes


Event Logging
Persisting driver logs in client mode

Spark Security: Things You Need To Know
Security features like authentication are not enabled by default. When deploying a cluster that is open to the internet
or an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications
from running on the cluster.
Spark supports multiple deployments types and each one supports different levels of security. Not
all deployment types will be secure in all environments and none are secure by default. Be
sure to evaluate your environment, what Spark supports, and take the appropriate measure to secure
your Spark deployment.
There are many different types of security concerns. Spark does not necessarily protect against
all things. Listed below are some of the things Spark supports. Also check the deployment
documentation for the type of deployment you are using for deployment specific settings. Anything
not documented, Spark does not support.
Spark RPC (Communication protocol between Spark processes)
Authentication
Spark currently supports authentication for RPC channels using a shared secret. Authentication can
be turned on by setting the spark.authenticate configuration parameter.
The exact mechanism used to generate and distribute the shared secret is deployment-specific. Unless
specified below, the secret must be defined by setting the spark.authenticate.secret config
option. The same secret is shared by all Spark applications and daemons in that case, which limits
the security of these deployments, especially on multi-tenant clusters.
The REST Submission Server and the MesosClusterDispatcher do not support authentication.  You should
ensure that all network access to the REST API & MesosClusterDispatcher (port 6066 and 7077
respectively by default) are restricted to hosts that are trusted to submit jobs.
YARN
For Spark on YARN, Spark will automatically handle generating and
distributing the shared secret. Each application will use a unique shared secret. In
the case of YARN, this feature relies on YARN RPC encryption being enabled for the distribution of
secrets to be secure.

Property NameDefaultMeaningSince Version

spark.yarn.shuffle.server.recovery.disabled
false

    Set to true for applications that have higher security requirements and prefer that their
    secret is not saved in the db. The shuffle data of such applications wll not be recovered after
    the External Shuffle Service restarts.
  
3.5.0


Kubernetes
On Kubernetes, Spark will also automatically generate an authentication secret unique to each
application. The secret is propagated to executor pods using environment variables. This means
that any user that can list pods in the namespace where the Spark application is running can
also see their authentication secret. Access control rules should be properly set up by the
Kubernetes admin to ensure that Spark authentication is secure.

Property NameDefaultMeaningSince Version

spark.authenticate
false
Whether Spark authenticates its internal connections.
1.0.0


spark.authenticate.secret
None

    The secret key used authentication. See above for when this configuration should be set.
  
1.0.0


Alternatively, one can mount authentication secrets using files and Kubernetes secrets that
the user mounts into their pods.

Property NameDefaultMeaningSince Version

spark.authenticate.secret.file
None

    Path pointing to the secret key to use for securing connections. Ensure that the
    contents of the file have been securely generated. This file is loaded on both the driver
    and the executors unless other settings override this (see below).
  
3.0.0


spark.authenticate.secret.driver.file
The value of spark.authenticate.secret.file

    When specified, overrides the location that the Spark driver reads to load the secret.
    Useful when in client mode, when the location of the secret file may differ in the pod versus
    the node the driver is running in. When this is specified,
    spark.authenticate.secret.executor.file must be specified so that the driver
    and the executors can both use files to load the secret key. Ensure that the contents of the file
    on the driver is identical to the contents of the file on the executors.
  
3.0.0


spark.authenticate.secret.executor.file
The value of spark.authenticate.secret.file

    When specified, overrides the location that the Spark executors read to load the secret.
    Useful in client mode, when the location of the secret file may differ in the pod versus
    the node the driver is running in. When this is specified,
    spark.authenticate.secret.driver.file must be specified so that the driver
    and the executors can both use files to load the secret key. Ensure that the contents of the file
    on the driver is identical to the contents of the file on the executors.
  
3.0.0


Note that when using files, Spark will not mount these files into the containers for you. It is up
you to ensure that the secret files are deployed securely into your containers and that the driver’s
secret file agrees with the executors’ secret file.
Encryption
Spark supports AES-based encryption for RPC connections. For encryption to be enabled, RPC
authentication must also be enabled and properly configured. AES encryption uses the
Apache Commons Crypto library, and Spark’s
configuration system allows access to that library’s configuration for advanced users.
This protocol has two mutually incompatible versions. Version 1 omits applying key derivation function
(KDF) to the key exchange protocol’s output, while version 2 applies a KDF to ensure that the derived
session key is uniformly distributed. Version 1 is default for backward compatibility. It is
recommended to use version 2 for better security properties. The version can be configured by setting
spark.network.crypto.authEngineVersion to 1 or 2 respectively.
There is also support for SASL-based encryption, although it should be considered deprecated. It
is still required when talking to shuffle services from Spark versions older than 2.2.0.
The following table describes the different options available for configuring this feature.

Property NameDefaultMeaningSince Version

spark.network.crypto.enabled
false

    Enable AES-based RPC encryption, including the new authentication protocol added in 2.2.0.
  
2.2.0


spark.network.crypto.cipher
AES/CTR/NoPadding

    Cipher mode to use. Defaults "AES/CTR/NoPadding" for backward compatibility, which is not authenticated. 
    Recommended to use "AES/GCM/NoPadding", which is an authenticated encryption mode.
  
4.0.0, 3.5.2, 3.4.4


spark.network.crypto.authEngineVersion
1
Version of AES-based RPC encryption to use. Valid versions are 1 or 2. Version 2 is recommended.
3.4.3, 3.5.2


spark.network.crypto.config.*
None

    Configuration values for the commons-crypto library, such as which cipher implementations to
    use. The config name should be the name of commons-crypto configuration without the
    commons.crypto prefix.
  
2.2.0


spark.network.crypto.saslFallback
true

    Whether to fall back to SASL authentication if authentication fails using Spark's internal
    mechanism. This is useful when the application is connecting to old shuffle services that
    do not support the internal Spark authentication protocol. On the shuffle service side,
    disabling this feature will block older clients from authenticating.
  
2.2.0


spark.authenticate.enableSaslEncryption
false

    Enable SASL-based encrypted communication.
  
2.2.0


spark.network.sasl.serverAlwaysEncrypt
false

    Disable unencrypted connections for ports using SASL authentication. This will deny connections
    from clients that have authentication enabled, but do not request SASL-based encryption.
  
1.4.0


Local Storage Encryption
Spark supports encrypting temporary data written to local disks. This covers shuffle files, shuffle
spills and data blocks stored on disk (for both caching and broadcast variables). It does not cover
encrypting output data generated by applications with APIs such as saveAsHadoopFile or
saveAsTable. It also may not cover temporary files created explicitly by the user.
The following settings cover enabling encryption for data written to disk:

Property NameDefaultMeaningSince Version

spark.io.encryption.enabled
false

    Enable local disk I/O encryption. Currently supported by all modes except Mesos. It's strongly
    recommended that RPC encryption be enabled when using this feature.
  
2.1.0


spark.io.encryption.keySizeBits
128

    IO encryption key size in bits. Supported values are 128, 192 and 256.
  
2.1.0


spark.io.encryption.keygen.algorithm
HmacSHA1

    The algorithm to use when generating the IO encryption key. The supported algorithms are
    described in the KeyGenerator section of the Java Cryptography Architecture Standard Algorithm
    Name Documentation.
  
2.1.0


spark.io.encryption.commons.config.*
None

    Configuration values for the commons-crypto library, such as which cipher implementations to
    use. The config name should be the name of commons-crypto configuration without the
    commons.crypto prefix.
  
2.1.0


Web UI
Authentication and Authorization
Enabling authentication for the Web UIs is done using javax servlet filters.
You will need a filter that implements the authentication method you want to deploy. Spark does not
provide any built-in authentication filters.
Spark also supports access control to the UI when an authentication filter is present. Each
application can be configured with its own separate access control lists (ACLs). Spark
differentiates between “view” permissions (who is allowed to see the application’s UI), and “modify”
permissions (who can do things like kill jobs in a running application).
ACLs can be configured for either users or groups. Configuration entries accept comma-separated
lists as input, meaning multiple users or groups can be given the desired privileges. This can be
used if you run on a shared cluster and have a set of administrators or developers who need to
monitor applications they may not have started themselves. A wildcard (*) added to specific ACL
means that all users will have the respective privilege. By default, only the user submitting the
application is added to the ACLs.
Group membership is established by using a configurable group mapping provider. The mapper is
configured using the spark.user.groups.mapping config option, described in the table
below.
The following options control the authentication of Web UIs:

Property NameDefaultMeaningSince Version

spark.ui.allowFramingFrom
SAMEORIGIN
Allow framing for a specific named URI via X-Frame-Options. By default, allow only from the same origin.
1.6.0


spark.ui.filters
None

    See the Spark UI configuration for how to configure
    filters.
  
1.0.0


spark.acls.enable
false

    Whether UI ACLs should be enabled. If enabled, this checks to see if the user has access
    permissions to view or modify the application. Note this requires the user to be authenticated,
    so if no authentication filter is installed, this option does not do anything.
  
1.1.0


spark.admin.acls
None

    Comma-separated list of users that have view and modify access to the Spark application.
  
1.1.0


spark.admin.acls.groups
None

    Comma-separated list of groups that have view and modify access to the Spark application.
  
2.0.0


spark.modify.acls
None

    Comma-separated list of users that have modify access to the Spark application.
  
1.1.0


spark.modify.acls.groups
None

    Comma-separated list of groups that have modify access to the Spark application.
  
2.0.0


spark.ui.view.acls
None

    Comma-separated list of users that have view access to the Spark application.
  
1.0.0


spark.ui.view.acls.groups
None

    Comma-separated list of groups that have view access to the Spark application.
  
2.0.0


spark.user.groups.mapping
org.apache.spark.security.ShellBasedGroupsMappingProvider

    The list of groups for a user is determined by a group mapping service defined by the trait
    org.apache.spark.security.GroupMappingServiceProvider, which can be configured by
    this property.

    By default, a Unix shell-based implementation is used, which collects this information
    from the host OS.

    Note: This implementation supports only Unix/Linux-based environments.
    Windows environment is currently not supported. However, a new platform/protocol can
    be supported by implementing the trait mentioned above.
  
2.0.0


On YARN, the view and modify ACLs are provided to the YARN service when submitting applications, and
control who has the respective privileges via YARN interfaces.
Spark History Server ACLs
Authentication for the SHS Web UI is enabled the same way as for regular applications, using
servlet filters.
To enable authorization in the SHS, a few extra options are used:

Property NameDefaultMeaningSince Version

spark.history.ui.acls.enable
false

    Specifies whether ACLs should be checked to authorize users viewing the applications in
    the history server. If enabled, access control checks are performed regardless of what the
    individual applications had set for spark.ui.acls.enable. The application owner
    will always have authorization to view their own application and any users specified via
    spark.ui.view.acls and groups specified via spark.ui.view.acls.groups
    when the application was run will also have authorization to view that application.
    If disabled, no access control checks are made for any application UIs available through
    the history server.
  
1.0.1


spark.history.ui.admin.acls
None

    Comma separated list of users that have view access to all the Spark applications in history
    server.
  
2.1.1


spark.history.ui.admin.acls.groups
None

    Comma separated list of groups that have view access to all the Spark applications in history
    server.
  
2.1.1


The SHS uses the same options to configure the group mapping provider as regular applications.
In this case, the group mapping provider will apply to all UIs server by the SHS, and individual
application configurations will be ignored.
SSL Configuration
Configuration for SSL is organized hierarchically. The user can configure the default SSL settings
which will be used for all the supported communication protocols unless they are overwritten by
protocol-specific settings. This way the user can easily provide the common settings for all the
protocols without disabling the ability to configure each one individually. The following table
describes the SSL configuration namespaces:



Config Namespace
Component



spark.ssl

      The default SSL configuration. These values will apply to all namespaces below, unless
      explicitly overridden at the namespace level.
    


spark.ssl.ui
Spark application Web UI


spark.ssl.standalone
Standalone Master / Worker Web UI


spark.ssl.historyServer
History Server Web UI


The full breakdown of available SSL options can be found below. The ${ns} placeholder should be
replaced with one of the above namespaces.

Property NameDefaultMeaning

${ns}.enabled
false
Enables SSL. When enabled, ${ns}.ssl.protocol is required.


${ns}.port
None

      The port where the SSL service will listen on.

      The port must be defined within a specific namespace configuration. The default
      namespace is ignored when reading this configuration.

      When not set, the SSL port will be derived from the non-SSL port for the
      same service. A value of "0" will make the service bind to an ephemeral port.
    


${ns}.enabledAlgorithms
None

      A comma-separated list of ciphers. The specified ciphers must be supported by JVM.

      The reference list of protocols can be found in the "JSSE Cipher Suite Names" section
      of the Java security guide. The list for Java 8 can be found at
      this
      page.

      Note: If not set, the default cipher suite for the JRE will be used.
    


${ns}.keyPassword
None

      The password to the private key in the key store.
    


${ns}.keyStore
None

      Path to the key store file. The path can be absolute or relative to the directory in which the
      process is started.
    


${ns}.keyStorePassword
None
Password to the key store.


${ns}.keyStoreType
JKS
The type of the key store.


${ns}.protocol
None

      TLS protocol to use. The protocol must be supported by JVM.

      The reference list of protocols can be found in the "Additional JSSE Standard Names"
      section of the Java security guide. For Java 8, the list can be found at
      this
      page.
    


${ns}.needClientAuth
false
Whether to require client authentication.


${ns}.trustStore
None

      Path to the trust store file. The path can be absolute or relative to the directory in which
      the process is started.
    


${ns}.trustStorePassword
None
Password for the trust store.


${ns}.trustStoreType
JKS
The type of the trust store.


Spark also supports retrieving ${ns}.keyPassword, ${ns}.keyStorePassword and ${ns}.trustStorePassword from
Hadoop Credential Providers.
User could store password into credential file and make it accessible by different components, like:
hadoop credential create spark.ssl.keyPassword -value password \
    -provider jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks

To configure the location of the credential provider, set the hadoop.security.credential.provider.path
config option in the Hadoop configuration used by Spark, like:
  <property>
    <name>hadoop.security.credential.provider.path</name>
    <value>jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks</value>
  </property>

Or via SparkConf “spark.hadoop.hadoop.security.credential.provider.path=jceks://hdfs@nn1.example.com:9001/user/backup/ssl.jceks”.
Preparing the key stores
Key stores can be generated by keytool program. The reference documentation for this tool for
Java 8 is here.
The most basic steps to configure the key stores and the trust store for a Spark Standalone
deployment mode is as follows:

Generate a key pair for each node
Export the public key of the key pair to a file on each node
Import all exported public keys into a single trust store
Distribute the trust store to the cluster nodes

YARN mode
To provide a local trust store or key store file to drivers running in cluster mode, they can be
distributed with the application using the --files command line argument (or the equivalent
spark.files configuration). The files will be placed on the driver’s working directory, so the TLS
configuration should just reference the file name with no absolute path.
Distributing local key stores this way may require the files to be staged in HDFS (or other similar
distributed file system used by the cluster), so it’s recommended that the underlying file system be
configured with security in mind (e.g. by enabling authentication and wire encryption).
Standalone mode
The user needs to provide key stores and configuration options for master and workers. They have to
be set by attaching appropriate Java system properties in SPARK_MASTER_OPTS and in
SPARK_WORKER_OPTS environment variables, or just in SPARK_DAEMON_JAVA_OPTS.
The user may allow the executors to use the SSL settings inherited from the worker process. That
can be accomplished by setting spark.ssl.useNodeLocalConf to true. In that case, the settings
provided by the user on the client side are not used.
Mesos mode
Mesos 1.3.0 and newer supports Secrets primitives as both file-based and environment based
secrets. Spark allows the specification of file-based and environment variable based secrets with
spark.mesos.driver.secret.filenames and spark.mesos.driver.secret.envkeys, respectively.
Depending on the secret store backend secrets can be passed by reference or by value with the
spark.mesos.driver.secret.names and spark.mesos.driver.secret.values configuration properties,
respectively.
Reference type secrets are served by the secret store and referred to by name, for example
/mysecret. Value type secrets are passed on the command line and translated into their
appropriate files or environment variables.
HTTP Security Headers
Apache Spark can be configured to include HTTP headers to aid in preventing Cross Site Scripting
(XSS), Cross-Frame Scripting (XFS), MIME-Sniffing, and also to enforce HTTP Strict Transport
Security.

Property NameDefaultMeaningSince Version

spark.ui.xXssProtection
1; mode=block

    Value for HTTP X-XSS-Protection response header. You can choose appropriate value
    from below:
    
0 (Disables XSS filtering)
1 (Enables XSS filtering. If a cross-site scripting attack is detected,
        the browser will sanitize the page.)
1; mode=block (Enables XSS filtering. The browser will prevent rendering
        of the page if an attack is detected.)


2.3.0


spark.ui.xContentTypeOptions.enabled
true

    When enabled, X-Content-Type-Options HTTP response header will be set to "nosniff".
  
2.3.0


spark.ui.strictTransportSecurity
None

    Value for HTTP Strict Transport Security (HSTS) Response Header. You can choose appropriate
    value from below and set expire-time accordingly. This option is only used when
    SSL/TLS is enabled.
    
max-age=<expire-time>
max-age=<expire-time>; includeSubDomains
max-age=<expire-time>; preload


2.3.0


Configuring Ports for Network Security
Generally speaking, a Spark cluster and its services are not deployed on the public internet.
They are generally private services, and should only be accessible within the network of the
organization that deploys Spark. Access to the hosts and ports used by Spark services should
be limited to origin hosts that need to access the services.
Below are the primary ports that Spark uses for its communication and how to
configure those ports.
Standalone mode only



FromToDefault PortPurposeConfiguration
    SettingNotes



Browser
Standalone Master
8080
Web UI
spark.master.ui.port / SPARK_MASTER_WEBUI_PORT
Jetty-based. Standalone mode only.


Browser
Standalone Worker
8081
Web UI
spark.worker.ui.port / SPARK_WORKER_WEBUI_PORT
Jetty-based. Standalone mode only.


Driver / Standalone Worker
Standalone Master
7077
Submit job to cluster / Join cluster
SPARK_MASTER_PORT
Set to "0" to choose a port randomly. Standalone mode only.


External Service
Standalone Master
6066
Submit job to cluster via REST API
spark.master.rest.port
Use spark.master.rest.enabled to enable/disable this service. Standalone mode only.


Standalone Master
Standalone Worker
(random)
Schedule executors
SPARK_WORKER_PORT
Set to "0" to choose a port randomly. Standalone mode only.


All cluster managers



FromToDefault PortPurposeConfiguration
    SettingNotes



Browser
Application
4040
Web UI
spark.ui.port
Jetty-based


Browser
History Server
18080
Web UI
spark.history.ui.port
Jetty-based


Executor / Standalone Master
Driver
(random)
Connect to application / Notify executor state changes
spark.driver.port
Set to "0" to choose a port randomly.


Executor / Driver
Executor / Driver
(random)
Block Manager port
spark.blockManager.port
Raw socket via ServerSocketChannel


Kerberos
Spark supports submitting applications in environments that use Kerberos for authentication.
In most cases, Spark relies on the credentials of the current logged in user when authenticating
to Kerberos-aware services. Such credentials can be obtained by logging in to the configured KDC
with tools like kinit.
When talking to Hadoop-based services, Spark needs to obtain delegation tokens so that non-local
processes can authenticate. Spark ships with support for HDFS and other Hadoop file systems, Hive
and HBase.
When using a Hadoop filesystem (such HDFS or WebHDFS), Spark will acquire the relevant tokens
for the service hosting the user’s home directory.
An HBase token will be obtained if HBase is in the application’s classpath, and the HBase
configuration has Kerberos authentication turned (hbase.security.authentication=kerberos).
Similarly, a Hive token will be obtained if Hive is in the classpath, and the configuration includes
URIs for remote metastore services (hive.metastore.uris is not empty).
If an application needs to interact with other secure Hadoop filesystems, their URIs need to be
explicitly provided to Spark at launch time. This is done by listing them in the
spark.kerberos.access.hadoopFileSystems property, described in the configuration section below.
Spark also supports custom delegation token providers using the Java Services
mechanism (see java.util.ServiceLoader). Implementations of
org.apache.spark.security.HadoopDelegationTokenProvider can be made available to Spark
by listing their names in the corresponding file in the jar’s META-INF/services directory.
Delegation token support is currently only supported in YARN, Kubernetes and Mesos modes. Consult the
deployment-specific page for more information.
The following options provides finer-grained control for this feature:

Property NameDefaultMeaningSince Version

spark.security.credentials.${service}.enabled
true

    Controls whether to obtain credentials for services when security is enabled.
    By default, credentials for all supported services are retrieved when those services are
    configured, but it's possible to disable that behavior if it somehow conflicts with the
    application being run.
  
2.3.0


spark.kerberos.access.hadoopFileSystems
(none)

    A comma-separated list of secure Hadoop filesystems your Spark application is going to access. For
    example, spark.kerberos.access.hadoopFileSystems=hdfs://nn1.com:8032,hdfs://nn2.com:8032,
    webhdfs://nn3.com:50070. The Spark application must have access to the filesystems listed
    and Kerberos must be properly configured to be able to access them (either in the same realm
    or in a trusted realm). Spark acquires security tokens for each of the filesystems so that
    the Spark application can access those remote Hadoop filesystems.
  
3.0.0


Users can exclude Kerberos delegation token renewal at resource scheduler. Currently it is only supported
on YARN. The configuration is covered in the Running Spark on YARN page.
Long-Running Applications
Long-running applications may run into issues if their run time exceeds the maximum delegation
token lifetime configured in services it needs to access.
This feature is not available everywhere. In particular, it’s only implemented
on YARN and Kubernetes (both client and cluster modes), and on Mesos when using client mode.
Spark supports automatically creating new tokens for these applications. There are two ways to
enable this functionality.
Using a Keytab
By providing Spark with a principal and keytab (e.g. using spark-submit with --principal
and --keytab parameters), the application will maintain a valid Kerberos login that can be
used to retrieve delegation tokens indefinitely.
Note that when using a keytab in cluster mode, it will be copied over to the machine running the
Spark driver. In the case of YARN, this means using HDFS as a staging area for the keytab, so it’s
strongly recommended that both YARN and HDFS be secured with encryption, at least.
Using a ticket cache
By setting spark.kerberos.renewal.credentials to ccache in Spark’s configuration, the local
Kerberos ticket cache will be used for authentication. Spark will keep the ticket renewed during its
renewable life, but after it expires a new ticket needs to be acquired (e.g. by running kinit).
It’s up to the user to maintain an updated ticket cache that Spark can use.
The location of the ticket cache can be customized by setting the KRB5CCNAME environment
variable.
Secure Interaction with Kubernetes
When talking to Hadoop-based services behind Kerberos, it was noted that Spark needs to obtain delegation tokens
so that non-local processes can authenticate. These delegation tokens in Kubernetes are stored in Secrets that are
shared by the Driver and its Executors. As such, there are three ways of submitting a Kerberos job:
In all cases you must define the environment variable: HADOOP_CONF_DIR or
spark.kubernetes.hadoop.configMapName.
It also important to note that the KDC needs to be visible from inside the containers.
If a user wishes to use a remote HADOOP_CONF directory, that contains the Hadoop configuration files, this could be
achieved by setting spark.kubernetes.hadoop.configMapName to a pre-existing ConfigMap.

Submitting with a $kinit that stores a TGT in the Local Ticket Cache:
    /usr/bin/kinit -kt <keytab_file> <username>/<krb5 realm>
/opt/spark/bin/spark-submit \
 --deploy-mode cluster \
 --class org.apache.spark.examples.HdfsTest \
 --master k8s://<KUBERNETES_MASTER_ENDPOINT> \
 --conf spark.executor.instances=1 \
 --conf spark.app.name=spark-hdfs \
 --conf spark.kubernetes.container.image=spark:latest \
 --conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \
 local:///opt/spark/examples/jars/spark-examples_<VERSION>.jar \
 <HDFS_FILE_LOCATION>
 

Submitting with a local Keytab and Principal
    /opt/spark/bin/spark-submit \
 --deploy-mode cluster \
 --class org.apache.spark.examples.HdfsTest \
 --master k8s://<KUBERNETES_MASTER_ENDPOINT> \
 --conf spark.executor.instances=1 \
 --conf spark.app.name=spark-hdfs \
 --conf spark.kubernetes.container.image=spark:latest \
 --conf spark.kerberos.keytab=<KEYTAB_FILE> \
 --conf spark.kerberos.principal=<PRINCIPAL> \
 --conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \
 local:///opt/spark/examples/jars/spark-examples_<VERSION>.jar \
 <HDFS_FILE_LOCATION>
 

Submitting with pre-populated secrets, that contain the Delegation Token, already existing within the namespace
    /opt/spark/bin/spark-submit \
 --deploy-mode cluster \
 --class org.apache.spark.examples.HdfsTest \
 --master k8s://<KUBERNETES_MASTER_ENDPOINT> \
 --conf spark.executor.instances=1 \
 --conf spark.app.name=spark-hdfs \
 --conf spark.kubernetes.container.image=spark:latest \
 --conf spark.kubernetes.kerberos.tokenSecret.name=<SECRET_TOKEN_NAME> \
 --conf spark.kubernetes.kerberos.tokenSecret.itemKey=<SECRET_ITEM_KEY> \
 --conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \
 local:///opt/spark/examples/jars/spark-examples_<VERSION>.jar \
 <HDFS_FILE_LOCATION>
 


3b. Submitting like in (3) however specifying a pre-created krb5 ConfigMap and pre-created HADOOP_CONF_DIR ConfigMap
/opt/spark/bin/spark-submit \
    --deploy-mode cluster \
    --class org.apache.spark.examples.HdfsTest \
    --master k8s://<KUBERNETES_MASTER_ENDPOINT> \
    --conf spark.executor.instances=1 \
    --conf spark.app.name=spark-hdfs \
    --conf spark.kubernetes.container.image=spark:latest \
    --conf spark.kubernetes.kerberos.tokenSecret.name=<SECRET_TOKEN_NAME> \
    --conf spark.kubernetes.kerberos.tokenSecret.itemKey=<SECRET_ITEM_KEY> \
    --conf spark.kubernetes.hadoop.configMapName=<HCONF_CONFIG_MAP_NAME> \
    --conf spark.kubernetes.kerberos.krb5.configMapName=<KRB_CONFIG_MAP_NAME> \
    local:///opt/spark/examples/jars/spark-examples_<VERSION>.jar \
    <HDFS_FILE_LOCATION>

Event Logging
If your applications are using event logging, the directory where the event logs go
(spark.eventLog.dir) should be manually created with proper permissions. To secure the log files,
the directory permissions should be set to drwxrwxrwxt. The owner and group of the directory
should correspond to the super user who is running the Spark History Server.
This will allow all users to write to the directory but will prevent unprivileged users from
reading, removing or renaming a file unless they own it. The event log files will be created by
Spark with permissions such that only the user and group have read and write access.
Persisting driver logs in client mode
If your applications persist driver logs in client mode by enabling spark.driver.log.persistToDfs.enabled,
the directory where the driver logs go (spark.driver.log.dfsDir) should be manually created with proper
permissions. To secure the log files, the directory permissions should be set to drwxrwxrwxt. The owner
and group of the directory should correspond to the super user who is running the Spark History Server.
This will allow all users to write to the directory but will prevent unprivileged users from
reading, removing or renaming a file unless they own it. The driver log files will be created by
Spark with permissions such that only the user and group have read and write access.




















  




Spark Standalone Mode - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Spark Standalone Mode

Security
Installing Spark Standalone to a Cluster
Starting a Cluster Manually
Cluster Launch Scripts
Resource Allocation and Configuration Overview
Connecting an Application to the Cluster
Client Properties
Launching Spark Applications 
Spark Protocol
REST API


Resource Scheduling
Executors Scheduling
Stage Level Scheduling Overview 
Caveats


Monitoring and Logging
Running Alongside Hadoop
Configuring Ports for Network Security
High Availability 
Standby Masters with ZooKeeper
Single-Node Recovery with Local File System



In addition to running on the Mesos or YARN cluster managers, Spark also provides a simple standalone deploy mode. You can launch a standalone cluster either manually, by starting a master and workers by hand, or use our provided launch scripts. It is also possible to run these daemons on a single machine for testing.
Security
Security features like authentication are not enabled by default. When deploying a cluster that is open to the internet
or an untrusted network, it’s important to secure access to the cluster to prevent unauthorized applications
from running on the cluster.
Please see Spark Security and the specific security sections in this doc before running Spark.
Installing Spark Standalone to a Cluster
To install Spark Standalone mode, you simply place a compiled version of Spark on each node on the cluster. You can obtain pre-built versions of Spark with each release or build it yourself.
Starting a Cluster Manually
You can start a standalone master server by executing:
./sbin/start-master.sh

Once started, the master will print out a spark://HOST:PORT URL for itself, which you can use to connect workers to it,
or pass as the “master” argument to SparkContext. You can also find this URL on
the master’s web UI, which is http://localhost:8080 by default.
Similarly, you can start one or more workers and connect them to the master via:
./sbin/start-worker.sh <master-spark-URL>

Once you have started a worker, look at the master’s web UI (http://localhost:8080 by default).
You should see the new node listed there, along with its number of CPUs and memory (minus one gigabyte left for the OS).
Finally, the following configuration options can be passed to the master and worker:

ArgumentMeaning

-h HOST, --host HOST
Hostname to listen on


-i HOST, --ip HOST
Hostname to listen on (deprecated, use -h or --host)


-p PORT, --port PORT
Port for service to listen on (default: 7077 for master, random for worker)


--webui-port PORT
Port for web UI (default: 8080 for master, 8081 for worker)


-c CORES, --cores CORES
Total CPU cores to allow Spark applications to use on the machine (default: all available); only on worker


-m MEM, --memory MEM
Total amount of memory to allow Spark applications to use on the machine, in a format like 1000M or 2G (default: your machine's total RAM minus 1 GiB); only on worker


-d DIR, --work-dir DIR
Directory to use for scratch space and job output logs (default: SPARK_HOME/work); only on worker


--properties-file FILE
Path to a custom Spark properties file to load (default: conf/spark-defaults.conf)


Cluster Launch Scripts
To launch a Spark standalone cluster with the launch scripts, you should create a file called conf/workers in your Spark directory,
which must contain the hostnames of all the machines where you intend to start Spark workers, one per line.
If conf/workers does not exist, the launch scripts defaults to a single machine (localhost), which is useful for testing.
Note, the master machine accesses each of the worker machines via ssh. By default, ssh is run in parallel and requires password-less (using a private key) access to be setup.
If you do not have a password-less setup, you can set the environment variable SPARK_SSH_FOREGROUND and serially provide a password for each worker.
Once you’ve set up this file, you can launch or stop your cluster with the following shell scripts, based on Hadoop’s deploy scripts, and available in SPARK_HOME/sbin:

sbin/start-master.sh - Starts a master instance on the machine the script is executed on.
sbin/start-workers.sh - Starts a worker instance on each machine specified in the conf/workers file.
sbin/start-worker.sh - Starts a worker instance on the machine the script is executed on.
sbin/start-connect-server.sh - Starts a Spark Connect server on the machine the script is executed on.
sbin/start-all.sh - Starts both a master and a number of workers as described above.
sbin/stop-master.sh - Stops the master that was started via the sbin/start-master.sh script.
sbin/stop-worker.sh - Stops all worker instances on the machine the script is executed on.
sbin/stop-workers.sh - Stops all worker instances on the machines specified in the conf/workers file.
sbin/stop-connect-server.sh - Stops all Spark Connect server instances on the machine the script is executed on.
sbin/stop-all.sh - Stops both the master and the workers as described above.

Note that these scripts must be executed on the machine you want to run the Spark master on, not your local machine.
You can optionally configure the cluster further by setting environment variables in conf/spark-env.sh. Create this file by starting with the conf/spark-env.sh.template, and copy it to all your worker machines for the settings to take effect. The following settings are available:

Environment VariableMeaning

SPARK_MASTER_HOST
Bind the master to a specific hostname or IP address, for example a public one.


SPARK_MASTER_PORT
Start the master on a different port (default: 7077).


SPARK_MASTER_WEBUI_PORT
Port for the master web UI (default: 8080).


SPARK_MASTER_OPTS
Configuration properties that apply only to the master in the form "-Dx=y" (default: none). See below for a list of possible options.


SPARK_LOCAL_DIRS

    Directory to use for "scratch" space in Spark, including map output files and RDDs that get
    stored on disk. This should be on a fast, local disk in your system. It can also be a
    comma-separated list of multiple directories on different disks.
    


SPARK_WORKER_CORES
Total number of cores to allow Spark applications to use on the machine (default: all available cores).


SPARK_WORKER_MEMORY
Total amount of memory to allow Spark applications to use on the machine, e.g. 1000m, 2g (default: total memory minus 1 GiB); note that each application's individual memory is configured using its spark.executor.memory property.


SPARK_WORKER_PORT
Start the Spark worker on a specific port (default: random).


SPARK_WORKER_WEBUI_PORT
Port for the worker web UI (default: 8081).


SPARK_WORKER_DIR
Directory to run applications in, which will include both logs and scratch space (default: SPARK_HOME/work).


SPARK_WORKER_OPTS
Configuration properties that apply only to the worker in the form "-Dx=y" (default: none). See below for a list of possible options.


SPARK_DAEMON_MEMORY
Memory to allocate to the Spark master and worker daemons themselves (default: 1g).


SPARK_DAEMON_JAVA_OPTS
JVM options for the Spark master and worker daemons themselves in the form "-Dx=y" (default: none).


SPARK_DAEMON_CLASSPATH
Classpath for the Spark master and worker daemons themselves (default: none).


SPARK_PUBLIC_DNS
The public DNS name of the Spark master and workers (default: none).


Note: The launch scripts do not currently support Windows. To run a Spark cluster on Windows, start the master and workers by hand.
SPARK_MASTER_OPTS supports the following system properties:

Property NameDefaultMeaningSince Version

spark.master.ui.port
8080

    Specifies the port number of the Master Web UI endpoint.
  
1.1.0


spark.master.ui.decommission.allow.mode
LOCAL

    Specifies the behavior of the Master Web UI's /workers/kill endpoint. Possible choices
    are: LOCAL means allow this endpoint from IP's that are local to the machine running
    the Master, DENY means to completely disable this endpoint, ALLOW means to allow
    calling this endpoint from any IP.
  
3.1.0


spark.master.rest.enabled
false

    Whether to use the Master REST API endpoint or not.
  
1.3.0


spark.master.rest.port
6066

    Specifies the port number of the Master REST API endpoint.
  
1.3.0


spark.deploy.retainedApplications
200

    The maximum number of completed applications to display. Older applications will be dropped from the UI to maintain this limit.

0.8.0


spark.deploy.retainedDrivers
200

   The maximum number of completed drivers to display. Older drivers will be dropped from the UI to maintain this limit.

1.1.0


spark.deploy.spreadOut
true

    Whether the standalone cluster manager should spread applications out across nodes or try
    to consolidate them onto as few nodes as possible. Spreading out is usually better for
    data locality in HDFS, but consolidating is more efficient for compute-intensive workloads. 

0.6.1


spark.deploy.defaultCores
(infinite)

    Default number of cores to give to applications in Spark's standalone mode if they don't
    set spark.cores.max. If not set, applications always get all available
    cores unless they configure spark.cores.max themselves.
    Set this lower on a shared cluster to prevent users from grabbing
    the whole cluster by default. 

0.9.0


spark.deploy.maxExecutorRetries
10

    Limit on the maximum number of back-to-back executor failures that can occur before the
    standalone cluster manager removes a faulty application. An application will never be removed
    if it has any running executors. If an application experiences more than
    spark.deploy.maxExecutorRetries failures in a row, no executors
    successfully start running in between those failures, and the application has no running
    executors then the standalone cluster manager will remove the application and mark it as failed.
    To disable this automatic removal, set spark.deploy.maxExecutorRetries to
    -1.
    

1.6.3


spark.worker.timeout
60

    Number of seconds after which the standalone deploy master considers a worker lost if it
    receives no heartbeats.
  
0.6.2


spark.worker.resource.{name}.amount
(none)

    Amount of a particular resource to use on the worker.
  
3.0.0


spark.worker.resource.{name}.discoveryScript
(none)

    Path to resource discovery script, which is used to find a particular resource while worker starting up.
    And the output of the script should be formatted like the ResourceInformation class.
  
3.0.0


spark.worker.resourcesFile
(none)

    Path to resources file which is used to find various resources while worker starting up.
    The content of resources file should be formatted like
    [{"id":{"componentName":
"spark.worker", "resourceName":"gpu"},
"addresses":["0","1","2"]}].
    If a particular resource is not found in the resources file, the discovery script would be used to
    find that resource. If the discovery script also does not find the resources, the worker will fail
    to start up.
  
3.0.0


SPARK_WORKER_OPTS supports the following system properties:

Property NameDefaultMeaningSince Version

spark.worker.cleanup.enabled
false

    Enable periodic cleanup of worker / application directories.  Note that this only affects standalone
    mode, as YARN works differently. Only the directories of stopped applications are cleaned up.
    This should be enabled if spark.shuffle.service.db.enabled is "true"
  
1.0.0


spark.worker.cleanup.interval
1800 (30 minutes)

    Controls the interval, in seconds, at which the worker cleans up old application work dirs
    on the local machine.
  
1.0.0


spark.worker.cleanup.appDataTtl
604800 (7 days, 7 * 24 * 3600)

    The number of seconds to retain application work directories on each worker.  This is a Time To Live
    and should depend on the amount of available disk space you have.  Application logs and jars are
    downloaded to each application work dir.  Over time, the work dirs can quickly fill up disk space,
    especially if you run jobs very frequently.
  
1.0.0


spark.shuffle.service.db.enabled
true

    Store External Shuffle service state on local disk so that when the external shuffle service is restarted, it will
    automatically reload info on current executors.  This only affects standalone mode (yarn always has this behavior
    enabled).  You should also enable spark.worker.cleanup.enabled, to ensure that the state
    eventually gets cleaned up.  This config may be removed in the future.
  
3.0.0


spark.shuffle.service.db.backend
LEVELDB

    When spark.shuffle.service.db.enabled is true, user can use this to specify the kind of disk-based
    store used in shuffle service state store. This supports `LEVELDB` and `ROCKSDB` now and `LEVELDB` as default value.
    The original data store in `LevelDB/RocksDB` will not be automatically convert to another kind of storage now.
  
3.4.0


spark.storage.cleanupFilesAfterExecutorExit
true

    Enable cleanup non-shuffle files(such as temp. shuffle blocks, cached RDD/broadcast blocks,
    spill files, etc) of worker directories following executor exits. Note that this doesn't
    overlap with `spark.worker.cleanup.enabled`, as this enables cleanup of non-shuffle files in
    local directories of a dead executor, while `spark.worker.cleanup.enabled` enables cleanup of
    all files/subdirectories of a stopped and timeout application.
    This only affects Standalone mode, support of other cluster managers can be added in the future.
  
2.4.0


spark.worker.ui.compressedLogFileLengthCacheSize
100

    For compressed log files, the uncompressed file can only be computed by uncompressing the files.
    Spark caches the uncompressed file size of compressed log files. This property controls the cache
    size.
  
2.0.2


Resource Allocation and Configuration Overview
Please make sure to have read the Custom Resource Scheduling and Configuration Overview section on the configuration page. This section only talks about the Spark Standalone specific aspects of resource scheduling.
Spark Standalone has 2 parts, the first is configuring the resources for the Worker, the second is the resource allocation for a specific application.
The user must configure the Workers to have a set of resources available so that it can assign them out to Executors. The spark.worker.resource.{resourceName}.amount is used to control the amount of each resource the worker has allocated. The user must also specify either spark.worker.resourcesFile or spark.worker.resource.{resourceName}.discoveryScript to specify how the Worker discovers the resources its assigned. See the descriptions above for each of those to see which method works best for your setup.
The second part is running an application on Spark Standalone. The only special case from the standard Spark resource configs is when you are running the Driver in client mode. For a Driver in client mode, the user can specify the resources it uses via spark.driver.resourcesFile or spark.driver.resource.{resourceName}.discoveryScript. If the Driver is running on the same host as other Drivers, please make sure the resources file or discovery script only returns resources that do not conflict with other Drivers running on the same node.
Note, the user does not need to specify a discovery script when submitting an application as the Worker will start each Executor with the resources it allocates to it.
Connecting an Application to the Cluster
To run an application on the Spark cluster, simply pass the spark://IP:PORT URL of the master as to the SparkContext
constructor.
To run an interactive Spark shell against the cluster, run the following command:
./bin/spark-shell --master spark://IP:PORT

You can also pass an option --total-executor-cores <numCores> to control the number of cores that spark-shell uses on the cluster.
Client Properties
Spark applications supports the following configuration properties specific to standalone mode:

Property NameDefault ValueMeaningSince Version

spark.standalone.submit.waitAppCompletion
false

  In standalone cluster mode, controls whether the client waits to exit until the application completes.
  If set to true, the client process will stay alive polling the driver's status.
  Otherwise, the client process will exit after submission.
  
3.1.0


Launching Spark Applications
Spark Protocol
The spark-submit script provides the most straightforward way to
submit a compiled Spark application to the cluster. For standalone clusters, Spark currently
supports two deploy modes. In client mode, the driver is launched in the same process as the
client that submits the application. In cluster mode, however, the driver is launched from one
of the Worker processes inside the cluster, and the client process exits as soon as it fulfills
its responsibility of submitting the application without waiting for the application to finish.
If your application is launched through Spark submit, then the application jar is automatically
distributed to all worker nodes. For any additional jars that your application depends on, you
should specify them through the --jars flag using comma as a delimiter (e.g. --jars jar1,jar2).
To control the application’s configuration or execution environment, see
Spark Configuration.
Additionally, standalone cluster mode supports restarting your application automatically if it
exited with non-zero exit code. To use this feature, you may pass in the --supervise flag to
spark-submit when launching your application. Then, if you wish to kill an application that is
failing repeatedly, you may do so through:
./bin/spark-class org.apache.spark.deploy.Client kill <master url> <driver ID>

You can find the driver ID through the standalone Master web UI at http://<master url>:8080.
REST API
If spark.master.rest.enabled is enabled, Spark master provides additional REST API
via http://[host:port]/[version]/submissions/[action] where
host is the master host, and
port is the port number specified by spark.master.rest.port (default: 6066), and 
version is a protocol version, v1 as of today, and
action is one of the following supported actions.

CommandDescriptionHTTP METHODSince Version

create
Create a Spark driver via cluster mode.
POST
1.3.0


kill
Kill a single Spark driver.
POST
1.3.0


status
Check the status of a Spark job.
GET
1.3.0


The following is a curl CLI command example with the pi.py and REST API.
$ curl -XPOST http://IP:PORT/v1/submissions/create \
--header "Content-Type:application/json;charset=UTF-8" \
--data '{
  "appResource": "",
  "sparkProperties": {
    "spark.master": "spark://master:7077",
    "spark.app.name": "Spark Pi",
    "spark.driver.memory": "1g",
    "spark.driver.cores": "1",
    "spark.jars": ""
  },
  "clientSparkVersion": "",
  "mainClass": "org.apache.spark.deploy.SparkSubmit",
  "environmentVariables": { },
  "action": "CreateSubmissionRequest",
  "appArgs": [ "/opt/spark/examples/src/main/python/pi.py", "10" ]
}'

The following is the response from the REST API for the above create request.
{
  "action" : "CreateSubmissionResponse",
  "message" : "Driver successfully submitted as driver-20231124153531-0000",
  "serverSparkVersion" : "3.5.1",
  "submissionId" : "driver-20231124153531-0000",
  "success" : true
}

Resource Scheduling
The standalone cluster mode currently only supports a simple FIFO scheduler across applications.
However, to allow multiple concurrent users, you can control the maximum number of resources each
application will use.
By default, it will acquire all cores in the cluster, which only makes sense if you just run one
application at a time. You can cap the number of cores by setting spark.cores.max in your
SparkConf. For example:
val conf = new SparkConf()
  .setMaster(...)
  .setAppName(...)
  .set("spark.cores.max", "10")
val sc = new SparkContext(conf)
In addition, you can configure spark.deploy.defaultCores on the cluster master process to change the
default for applications that don’t set spark.cores.max to something less than infinite.
Do this by adding the following to conf/spark-env.sh:
export SPARK_MASTER_OPTS="-Dspark.deploy.defaultCores=<value>"
This is useful on shared clusters where users might not have configured a maximum number of cores
individually.
Executors Scheduling
The number of cores assigned to each executor is configurable. When spark.executor.cores is
explicitly set, multiple executors from the same application may be launched on the same worker
if the worker has enough cores and memory. Otherwise, each executor grabs all the cores available
on the worker by default, in which case only one executor per application may be launched on each
worker during one single schedule iteration.
Stage Level Scheduling Overview
Stage level scheduling is supported on Standalone:

When dynamic allocation is disabled: It allows users to specify different task resource requirements at the stage level and will use the same executors requested at startup.
When dynamic allocation is enabled: Currently, when the Master allocates executors for one application, it will schedule based on the order of the ResourceProfile ids for multiple ResourceProfiles. The ResourceProfile with smaller id will be scheduled firstly. Normally this won’t matter as Spark finishes one stage before starting another one, the only case this might have an affect is in a job server type scenario, so its something to keep in mind. For scheduling, we will only take executor memory and executor cores from built-in executor resources and all other custom resources from a ResourceProfile, other built-in executor resources such as offHeap and memoryOverhead won’t take any effect. The base default profile will be created based on the spark configs when you submit an application. Executor memory and executor cores from the base default profile can be propagated to custom ResourceProfiles, but all other custom resources can not be propagated.

Caveats
As mentioned in Dynamic Resource Allocation, if cores for each executor is not explicitly specified with dynamic allocation enabled, spark will possibly acquire much more executors than expected. So you are recommended to explicitly set executor cores for each resource profile when using stage level scheduling.
Monitoring and Logging
Spark’s standalone mode offers a web-based user interface to monitor the cluster. The master and each worker has its own web UI that shows cluster and job statistics. By default, you can access the web UI for the master at port 8080. The port can be changed either in the configuration file or via command-line options.
In addition, detailed log output for each job is also written to the work directory of each worker node (SPARK_HOME/work by default). You will see two files for each job, stdout and stderr, with all output it wrote to its console.
Running Alongside Hadoop
You can run Spark alongside your existing Hadoop cluster by just launching it as a separate service on the same machines. To access Hadoop data from Spark, just use an hdfs:// URL (typically hdfs://<namenode>:9000/path, but you can find the right URL on your Hadoop Namenode’s web UI). Alternatively, you can set up a separate cluster for Spark, and still have it access HDFS over the network; this will be slower than disk-local access, but may not be a concern if you are still running in the same local area network (e.g. you place a few Spark machines on each rack that you have Hadoop on).
Configuring Ports for Network Security
Generally speaking, a Spark cluster and its services are not deployed on the public internet.
They are generally private services, and should only be accessible within the network of the
organization that deploys Spark. Access to the hosts and ports used by Spark services should
be limited to origin hosts that need to access the services.
This is particularly important for clusters using the standalone resource manager, as they do
not support fine-grained access control in a way that other resource managers do.
For a complete list of ports to configure, see the
security page.
High Availability
By default, standalone scheduling clusters are resilient to Worker failures (insofar as Spark itself is resilient to losing work by moving it to other workers). However, the scheduler uses a Master to make scheduling decisions, and this (by default) creates a single point of failure: if the Master crashes, no new applications can be created. In order to circumvent this, we have two high availability schemes, detailed below.
Standby Masters with ZooKeeper
Overview
Utilizing ZooKeeper to provide leader election and some state storage, you can launch multiple Masters in your cluster connected to the same ZooKeeper instance. One will be elected “leader” and the others will remain in standby mode. If the current leader dies, another Master will be elected, recover the old Master’s state, and then resume scheduling. The entire recovery process (from the time the first leader goes down) should take between 1 and 2 minutes. Note that this delay only affects scheduling new applications – applications that were already running during Master failover are unaffected.
Learn more about getting started with ZooKeeper here.
Configuration
In order to enable this recovery mode, you can set SPARK_DAEMON_JAVA_OPTS in spark-env by configuring spark.deploy.recoveryMode and related spark.deploy.zookeeper.* configurations.
For more information about these configurations please refer to the configuration doc
Possible gotcha: If you have multiple Masters in your cluster but fail to correctly configure the Masters to use ZooKeeper, the Masters will fail to discover each other and think they’re all leaders. This will not lead to a healthy cluster state (as all Masters will schedule independently).
Details
After you have a ZooKeeper cluster set up, enabling high availability is straightforward. Simply start multiple Master processes on different nodes with the same ZooKeeper configuration (ZooKeeper URL and directory). Masters can be added and removed at any time.
In order to schedule new applications or add Workers to the cluster, they need to know the IP address of the current leader. This can be accomplished by simply passing in a list of Masters where you used to pass in a single one. For example, you might start your SparkContext pointing to spark://host1:port1,host2:port2. This would cause your SparkContext to try registering with both Masters – if host1 goes down, this configuration would still be correct as we’d find the new leader, host2.
There’s an important distinction to be made between “registering with a Master” and normal operation. When starting up, an application or Worker needs to be able to find and register with the current lead Master. Once it successfully registers, though, it is “in the system” (i.e., stored in ZooKeeper). If failover occurs, the new leader will contact all previously registered applications and Workers to inform them of the change in leadership, so they need not even have known of the existence of the new Master at startup.
Due to this property, new Masters can be created at any time, and the only thing you need to worry about is that new applications and Workers can find it to register with in case it becomes the leader. Once registered, you’re taken care of.
Single-Node Recovery with Local File System
Overview
ZooKeeper is the best way to go for production-level high availability, but if you just want to be able to restart the Master if it goes down, FILESYSTEM mode can take care of it. When applications and Workers register, they have enough state written to the provided directory so that they can be recovered upon a restart of the Master process.
Configuration
In order to enable this recovery mode, you can set SPARK_DAEMON_JAVA_OPTS in spark-env using this configuration:

System propertyMeaningSince Version

spark.deploy.recoveryMode
Set to FILESYSTEM to enable single-node recovery mode (default: NONE).
0.8.1


spark.deploy.recoveryDirectory
The directory in which Spark will store recovery state, accessible from the Master's perspective.
0.8.1


Details

This solution can be used in tandem with a process monitor/manager like monit, or just to enable manual recovery via restart.
While filesystem recovery seems straightforwardly better than not doing any recovery at all, this mode may be suboptimal for certain development or experimental purposes. In particular, killing a master via stop-master.sh does not clean up its recovery state, so whenever you start a new Master, it will enter recovery mode. This could increase the startup time by up to 1 minute if it needs to wait for all previously-registered Workers/clients to timeout.
While it’s not officially supported, you could mount an NFS directory as the recovery directory. If the original Master node dies completely, you could then start a Master on a different node, which would correctly recover all previously registered Workers/applications (equivalent to ZooKeeper recovery). Future applications will have to be able to find the new Master, however, in order to register.





















  




SparkR (R on Spark) - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











SparkR (R on Spark)

Overview
SparkDataFrame 
Starting Up: SparkSession
Starting Up from RStudio
Creating SparkDataFrames 
From local data frames
From Data Sources
From Hive tables


SparkDataFrame Operations 
Selecting rows, columns
Grouping, Aggregation
Operating on Columns
Applying User-Defined Function 
Run a given function on a large dataset using dapply or dapplyCollect 
dapply
dapplyCollect


Run a given function on a large dataset grouping by input column(s) and using gapply or gapplyCollect 
gapply
gapplyCollect


Run local R functions distributed using spark.lapply 
spark.lapply




Eager execution


Running SQL Queries from SparkR


Machine Learning 
Algorithms 
Classification
Regression
Tree
Clustering
Collaborative Filtering
Frequent Pattern Mining
Statistics


Model persistence


Data type mapping between R and Spark
Structured Streaming
Apache Arrow in SparkR 
Ensure Arrow Installed
Enabling for Conversion to/from R DataFrame, dapply and gapply
Supported SQL Types


R Function Name Conflicts
Migration Guide

Overview
SparkR is an R package that provides a light-weight frontend to use Apache Spark from R.
In Spark 3.5.5, SparkR provides a distributed data frame implementation that
supports operations like selection, filtering, aggregation etc. (similar to R data frames,
dplyr) but on large datasets. SparkR also supports distributed
machine learning using MLlib.
SparkDataFrame
A SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually
equivalent to a table in a relational database or a data frame in R, but with richer
optimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as:
structured data files, tables in Hive, external databases, or existing local R data frames.
All of the examples on this page use sample data included in R or the Spark distribution and can be run using the ./bin/sparkR shell.
Starting Up: SparkSession

The entry point into SparkR is the SparkSession which connects your R program to a Spark cluster.
You can create a SparkSession using sparkR.session and pass in options such as the application name, any spark packages depended on, etc. Further, you can also work with SparkDataFrames via SparkSession. If you are working from the sparkR shell, the SparkSession should already be created for you, and you would not need to call sparkR.session.

sparkR.session()

Starting Up from RStudio
You can also start SparkR from RStudio. You can connect your R program to a Spark cluster from
RStudio, R shell, Rscript or other R IDEs. To start, make sure SPARK_HOME is set in environment
(you can check Sys.getenv),
load the SparkR package, and call sparkR.session as below. It will check for the Spark installation, and, if not found, it will be downloaded and cached automatically. Alternatively, you can also run install.spark manually.
In addition to calling sparkR.session,
 you could also specify certain Spark driver properties. Normally these
Application properties and
Runtime Environment cannot be set programmatically, as the
driver JVM process would have been started, in this case SparkR takes care of this for you. To set
them, pass them as you would other configuration properties in the sparkConfig argument to
sparkR.session().

if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
  Sys.setenv(SPARK_HOME = "/home/spark")
}
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))

The following Spark driver properties can be set in sparkConfig with sparkR.session from RStudio:

Property NameProperty groupspark-submit equivalent

spark.master
Application Properties
--master


spark.kerberos.keytab
Application Properties
--keytab


spark.kerberos.principal
Application Properties
--principal


spark.driver.memory
Application Properties
--driver-memory


spark.driver.extraClassPath
Runtime Environment
--driver-class-path


spark.driver.extraJavaOptions
Runtime Environment
--driver-java-options


spark.driver.extraLibraryPath
Runtime Environment
--driver-library-path



Creating SparkDataFrames
With a SparkSession, applications can create SparkDataFrames from a local R data frame, from a Hive table, or from other data sources.
From local data frames
The simplest way to create a data frame is to convert a local R data frame into a SparkDataFrame. Specifically, we can use as.DataFrame or createDataFrame and pass in the local R data frame to create a SparkDataFrame. As an example, the following creates a SparkDataFrame based using the faithful dataset from R.

df <- as.DataFrame(faithful)

# Displays the first part of the SparkDataFrame
head(df)
##  eruptions waiting
##1     3.600      79
##2     1.800      54
##3     3.333      74

From Data Sources
SparkR supports operating on a variety of data sources through the SparkDataFrame interface. This section describes the general methods for loading and saving data using Data Sources. You can check the Spark SQL programming guide for more specific options that are available for the built-in data sources.
The general method for creating SparkDataFrames from data sources is read.df. This method takes in the path for the file to load and the type of data source, and the currently active SparkSession will be used automatically.
SparkR supports reading JSON, CSV and Parquet files natively, and through packages available from sources like Third Party Projects, you can find data source connectors for popular file formats like Avro. These packages can either be added by
specifying --packages with spark-submit or sparkR commands, or if initializing SparkSession with sparkPackages parameter when in an interactive R shell or from RStudio.

sparkR.session(sparkPackages = "org.apache.spark:spark-avro_2.12:3.5.5")

We can see how to use data sources using an example JSON input file. Note that the file that is used here is not a typical JSON file. Each line in the file must contain a separate, self-contained valid JSON object. For more information, please see JSON Lines text format, also called newline-delimited JSON. As a consequence, a regular multi-line JSON file will most often fail.

people <- read.df("./examples/src/main/resources/people.json", "json")
head(people)
##  age    name
##1  NA Michael
##2  30    Andy
##3  19  Justin

# SparkR automatically infers the schema from the JSON file
printSchema(people)
# root
#  |-- age: long (nullable = true)
#  |-- name: string (nullable = true)

# Similarly, multiple files can be read with read.json
people <- read.json(c("./examples/src/main/resources/people.json", "./examples/src/main/resources/people2.json"))

The data sources API natively supports CSV formatted input files. For more information please refer to SparkR read.df API documentation.

df <- read.df(csvPath, "csv", header = "true", inferSchema = "true", na.strings = "NA")

The data sources API can also be used to save out SparkDataFrames into multiple file formats. For example, we can save the SparkDataFrame from the previous example
to a Parquet file using write.df.

write.df(people, path = "people.parquet", source = "parquet", mode = "overwrite")

From Hive tables
You can also create SparkDataFrames from Hive tables. To do this we will need to create a SparkSession with Hive support which can access tables in the Hive MetaStore. Note that Spark should have been built with Hive support and more details can be found in the SQL programming guide. In SparkR, by default it will attempt to create a SparkSession with Hive support enabled (enableHiveSupport = TRUE).

sparkR.session()

sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")

# Queries can be expressed in HiveQL.
results <- sql("FROM src SELECT key, value")

# results is now a SparkDataFrame
head(results)
##  key   value
## 1 238 val_238
## 2  86  val_86
## 3 311 val_311

SparkDataFrame Operations
SparkDataFrames support a number of functions to do structured data processing.
Here we include some basic examples and a complete list can be found in the API docs:
Selecting rows, columns

# Create the SparkDataFrame
df <- as.DataFrame(faithful)

# Get basic information about the SparkDataFrame
df
## SparkDataFrame[eruptions:double, waiting:double]

# Select only the "eruptions" column
head(select(df, df$eruptions))
##  eruptions
##1     3.600
##2     1.800
##3     3.333

# You can also pass in column name as strings
head(select(df, "eruptions"))

# Filter the SparkDataFrame to only retain rows with wait times shorter than 50 mins
head(filter(df, df$waiting < 50))
##  eruptions waiting
##1     1.750      47
##2     1.750      47
##3     1.867      48

Grouping, Aggregation
SparkR data frames support a number of commonly used functions to aggregate data after grouping. For example, we can compute a histogram of the waiting time in the faithful dataset as shown below

# We use the `n` operator to count the number of times each waiting time appears
head(summarize(groupBy(df, df$waiting), count = n(df$waiting)))
##  waiting count
##1      70     4
##2      67     1
##3      69     2

# We can also sort the output from the aggregation to get the most common waiting times
waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting))
head(arrange(waiting_counts, desc(waiting_counts$count)))
##   waiting count
##1      78    15
##2      83    14
##3      81    13

In addition to standard aggregations, SparkR supports OLAP cube operators cube:

head(agg(cube(df, "cyl", "disp", "gear"), avg(df$mpg)))
##  cyl  disp gear avg(mpg)
##1  NA 140.8    4     22.8
##2   4  75.7    4     30.4
##3   8 400.0    3     19.2
##4   8 318.0    3     15.5
##5  NA 351.0   NA     15.8
##6  NA 275.8   NA     16.3

and rollup:

head(agg(rollup(df, "cyl", "disp", "gear"), avg(df$mpg)))
##  cyl  disp gear avg(mpg)
##1   4  75.7    4     30.4
##2   8 400.0    3     19.2
##3   8 318.0    3     15.5
##4   4  78.7   NA     32.4
##5   8 304.0    3     15.2
##6   4  79.0   NA     27.3

Operating on Columns
SparkR also provides a number of functions that can be directly applied to columns for data processing and during aggregation. The example below shows the use of basic arithmetic functions.

# Convert waiting time from hours to seconds.
# Note that we can assign this to a new column in the same SparkDataFrame
df$waiting_secs <- df$waiting * 60
head(df)
##  eruptions waiting waiting_secs
##1     3.600      79         4740
##2     1.800      54         3240
##3     3.333      74         4440

Applying User-Defined Function
In SparkR, we support several kinds of User-Defined Functions:
Run a given function on a large dataset using dapply or dapplyCollect
dapply
Apply a function to each partition of a SparkDataFrame. The function to be applied to each partition of the SparkDataFrame
and should have only one parameter, to which a data.frame corresponds to each partition will be passed. The output of function should be a data.frame. Schema specifies the row format of the resulting a SparkDataFrame. It must match to data types of returned value.

# Convert waiting time from hours to seconds.
# Note that we can apply UDF to DataFrame.
schema <- structType(structField("eruptions", "double"), structField("waiting", "double"),
                     structField("waiting_secs", "double"))
df1 <- dapply(df, function(x) { x <- cbind(x, x$waiting * 60) }, schema)
head(collect(df1))
##  eruptions waiting waiting_secs
##1     3.600      79         4740
##2     1.800      54         3240
##3     3.333      74         4440
##4     2.283      62         3720
##5     4.533      85         5100
##6     2.883      55         3300

dapplyCollect
Like dapply, apply a function to each partition of a SparkDataFrame and collect the result back. The output of function
should be a data.frame. But, Schema is not required to be passed. Note that dapplyCollect can fail if the output of UDF run on all the partition cannot be pulled to the driver and fit in driver memory.

# Convert waiting time from hours to seconds.
# Note that we can apply UDF to DataFrame and return a R's data.frame
ldf <- dapplyCollect(
         df,
         function(x) {
           x <- cbind(x, "waiting_secs" = x$waiting * 60)
         })
head(ldf, 3)
##  eruptions waiting waiting_secs
##1     3.600      79         4740
##2     1.800      54         3240
##3     3.333      74         4440

Run a given function on a large dataset grouping by input column(s) and using gapply or gapplyCollect
gapply
Apply a function to each group of a SparkDataFrame. The function is to be applied to each group of the SparkDataFrame and should have only two parameters: grouping key and R data.frame corresponding to
that key. The groups are chosen from SparkDataFrames column(s).
The output of function should be a data.frame. Schema specifies the row format of the resulting
SparkDataFrame. It must represent R function’s output schema on the basis of Spark data types. The column names of the returned data.frame are set by user.

# Determine six waiting times with the largest eruption time in minutes.
schema <- structType(structField("waiting", "double"), structField("max_eruption", "double"))
result <- gapply(
    df,
    "waiting",
    function(key, x) {
        y <- data.frame(key, max(x$eruptions))
    },
    schema)
head(collect(arrange(result, "max_eruption", decreasing = TRUE)))

##    waiting   max_eruption
##1      64       5.100
##2      69       5.067
##3      71       5.033
##4      87       5.000
##5      63       4.933
##6      89       4.900

gapplyCollect
Like gapply, applies a function to each partition of a SparkDataFrame and collect the result back to R data.frame. The output of the function should be a data.frame. But, the schema is not required to be passed. Note that gapplyCollect can fail if the output of UDF run on all the partition cannot be pulled to the driver and fit in driver memory.

# Determine six waiting times with the largest eruption time in minutes.
result <- gapplyCollect(
    df,
    "waiting",
    function(key, x) {
        y <- data.frame(key, max(x$eruptions))
        colnames(y) <- c("waiting", "max_eruption")
        y
    })
head(result[order(result$max_eruption, decreasing = TRUE), ])

##    waiting   max_eruption
##1      64       5.100
##2      69       5.067
##3      71       5.033
##4      87       5.000
##5      63       4.933
##6      89       4.900

Run local R functions distributed using spark.lapply
spark.lapply
Similar to lapply in native R, spark.lapply runs a function over a list of elements and distributes the computations with Spark.
Applies a function in a manner that is similar to doParallel or lapply to elements of a list. The results of all the computations
should fit in a single machine. If that is not the case they can do something like df <- createDataFrame(list) and then use
dapply

# Perform distributed training of multiple models with spark.lapply. Here, we pass
# a read-only list of arguments which specifies family the generalized linear model should be.
families <- c("gaussian", "poisson")
train <- function(family) {
  model <- glm(Sepal.Length ~ Sepal.Width + Species, iris, family = family)
  summary(model)
}
# Return a list of model's summaries
model.summaries <- spark.lapply(families, train)

# Print the summary of each model
print(model.summaries)

Eager execution
If eager execution is enabled, the data will be returned to R client immediately when the SparkDataFrame is created. By default, eager execution is not enabled and can be enabled by setting the configuration property spark.sql.repl.eagerEval.enabled to true when the SparkSession is started up.
Maximum number of rows and maximum number of characters per column of data to display can be controlled by spark.sql.repl.eagerEval.maxNumRows and spark.sql.repl.eagerEval.truncate configuration properties, respectively. These properties are only effective when eager execution is enabled. If these properties are not set explicitly, by default, data up to 20 rows and up to 20 characters per column will be showed.

# Start up spark session with eager execution enabled
sparkR.session(master = "local[*]",
               sparkConfig = list(spark.sql.repl.eagerEval.enabled = "true",
                                  spark.sql.repl.eagerEval.maxNumRows = as.integer(10)))

# Create a grouped and sorted SparkDataFrame
df <- createDataFrame(faithful)
df2 <- arrange(summarize(groupBy(df, df$waiting), count = n(df$waiting)), "waiting")

# Similar to R data.frame, displays the data returned, instead of SparkDataFrame class string
df2

##+-------+-----+
##|waiting|count|
##+-------+-----+
##|   43.0|    1|
##|   45.0|    3|
##|   46.0|    5|
##|   47.0|    4|
##|   48.0|    3|
##|   49.0|    5|
##|   50.0|    5|
##|   51.0|    6|
##|   52.0|    5|
##|   53.0|    7|
##+-------+-----+
##only showing top 10 rows

Note that to enable eager execution in sparkR shell, add spark.sql.repl.eagerEval.enabled=true configuration property to the --conf option.
Running SQL Queries from SparkR
A SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data.
The sql function enables applications to run SQL queries programmatically and returns the result as a SparkDataFrame.

# Load a JSON file
people <- read.df("./examples/src/main/resources/people.json", "json")

# Register this SparkDataFrame as a temporary view.
createOrReplaceTempView(people, "people")

# SQL statements can be run by using the sql method
teenagers <- sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")
head(teenagers)
##    name
##1 Justin

Machine Learning
Algorithms
SparkR supports the following machine learning algorithms currently:
Classification

spark.logit: Logistic Regression
spark.mlp: Multilayer Perceptron (MLP)
spark.naiveBayes: Naive Bayes
spark.svmLinear: Linear Support Vector Machine
spark.fmClassifier: Factorization Machines classifier

Regression

spark.survreg: Accelerated Failure Time (AFT) Survival  Model
spark.glm or glm: Generalized Linear Model (GLM)
spark.isoreg: Isotonic Regression
spark.lm: Linear Regression
spark.fmRegressor: Factorization Machines regressor

Tree

spark.decisionTree: Decision Tree for Regression and Classification
spark.gbt: Gradient Boosted Trees for Regression and Classification
spark.randomForest: Random Forest for Regression and Classification

Clustering

spark.bisectingKmeans: Bisecting k-means
spark.gaussianMixture: Gaussian Mixture Model (GMM)
spark.kmeans: K-Means
spark.lda: Latent Dirichlet Allocation (LDA)
spark.powerIterationClustering (PIC): Power Iteration Clustering (PIC)

Collaborative Filtering

spark.als: Alternating Least Squares (ALS)

Frequent Pattern Mining

spark.fpGrowth : FP-growth
spark.prefixSpan : PrefixSpan

Statistics

spark.kstest: Kolmogorov-Smirnov Test

Under the hood, SparkR uses MLlib to train the model. Please refer to the corresponding section of MLlib user guide for example code.
Users can call summary to print a summary of the fitted model, predict to make predictions on new data, and write.ml/read.ml to save/load fitted models.
SparkR supports a subset of the available R formula operators for model fitting, including ‘~’, ‘.’, ‘:’, ‘+’, and ‘-‘.
Model persistence
The following example shows how to save/load a MLlib model by SparkR.
training <- read.df("data/mllib/sample_multiclass_classification_data.txt", source = "libsvm")
# Fit a generalized linear model of family "gaussian" with spark.glm
df_list <- randomSplit(training, c(7,3), 2)
gaussianDF <- df_list[[1]]
gaussianTestDF <- df_list[[2]]
gaussianGLM <- spark.glm(gaussianDF, label ~ features, family = "gaussian")

# Save and then load a fitted MLlib model
modelPath <- tempfile(pattern = "ml", fileext = ".tmp")
write.ml(gaussianGLM, modelPath)
gaussianGLM2 <- read.ml(modelPath)

# Check model summary
summary(gaussianGLM2)

# Check model prediction
gaussianPredictions <- predict(gaussianGLM2, gaussianTestDF)
head(gaussianPredictions)

unlink(modelPath)
Find full example code at "examples/src/main/r/ml/ml.R" in the Spark repo.
Data type mapping between R and Spark

RSpark

byte
byte


integer
integer


float
float


double
double


numeric
double


character
string


string
string


binary
binary


raw
binary


logical
boolean


POSIXct
timestamp


POSIXlt
timestamp


Date
date


array
array


list
array


env
map


Structured Streaming
SparkR supports the Structured Streaming API. Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. For more information see the R API on the Structured Streaming Programming Guide
Apache Arrow in SparkR
Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and R processes. See also PySpark optimization done, PySpark Usage Guide for Pandas with Apache Arrow. This guide targets to explain how to use Arrow optimization in SparkR with some key points.
Ensure Arrow Installed
Arrow R library is available on CRAN and it can be installed as below.
Rscript -e 'install.packages("arrow", repos="https://cloud.r-project.org/")'

Please refer the official documentation of Apache Arrow for more details.
Note that you must ensure that Arrow R package is installed and available on all cluster nodes.
The current supported minimum version is 1.0.0; however, this might change between the minor releases since Arrow optimization in SparkR is experimental.
Enabling for Conversion to/from R DataFrame, dapply and gapply
Arrow optimization is available when converting a Spark DataFrame to an R DataFrame using the call collect(spark_df),
when creating a Spark DataFrame from an R DataFrame with createDataFrame(r_df), when applying an R native function to each partition
via dapply(...) and when applying an R native function to grouped data via gapply(...).
To use Arrow when executing these, users need to set the Spark configuration ‘spark.sql.execution.arrow.sparkr.enabled’
to ‘true’ first. This is disabled by default.
Whether the optimization is enabled or not, SparkR produces the same results. In addition, the conversion
between Spark DataFrame and R DataFrame falls back automatically to non-Arrow optimization implementation
when the optimization fails for any reasons before the actual computation.

# Start up spark session with Arrow optimization enabled
sparkR.session(master = "local[*]",
               sparkConfig = list(spark.sql.execution.arrow.sparkr.enabled = "true"))

# Converts Spark DataFrame from an R DataFrame
spark_df <- createDataFrame(mtcars)

# Converts Spark DataFrame to an R DataFrame
collect(spark_df)

# Apply an R native function to each partition.
collect(dapply(spark_df, function(rdf) { data.frame(rdf$gear + 1) }, structType("gear double")))

# Apply an R native function to grouped data.
collect(gapply(spark_df,
               "gear",
               function(key, group) {
                 data.frame(gear = key[[1]], disp = mean(group$disp) > group$disp)
               },
               structType("gear double, disp boolean")))

Note that even with Arrow, collect(spark_df) results in the collection of all records in the DataFrame to
the driver program and should be done on a small subset of the data. In addition, the specified output schema
in gapply(...) and dapply(...) should be matched to the R DataFrame’s returned by the given function.
Supported SQL Types
Currently, all Spark SQL data types are supported by Arrow-based conversion except FloatType, BinaryType, ArrayType, StructType and MapType.
R Function Name Conflicts
When loading and attaching a new package in R, it is possible to have a name conflict, where a
function is masking another function.
The following functions are masked by the SparkR package:

Masked functionHow to Access

cov in package:stats
stats::cov(x, y = NULL, use = "everything",
           method = c("pearson", "kendall", "spearman"))


filter in package:stats
stats::filter(x, filter, method = c("convolution", "recursive"),
              sides = 2, circular = FALSE, init)


sample in package:base
base::sample(x, size, replace = FALSE, prob = NULL)


Since part of SparkR is modeled on the dplyr package, certain functions in SparkR share the same names with those in dplyr. Depending on the load order of the two packages, some functions from the package loaded first are masked by those in the package loaded after. In such case, prefix such calls with the package name, for instance, SparkR::cume_dist(x) or dplyr::cume_dist(x).
You can inspect the search path in R with search()
Migration Guide
The migration guide is now archived on this page.




















  




Data Sources - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        




            
                Generic Load/Save Functions
            
        



            
                Generic File Source Options
            
        



            
                Parquet Files
            
        



            
                ORC Files
            
        



            
                JSON Files
            
        



            
                CSV Files
            
        



            
                Text Files
            
        



            
                Hive Tables
            
        



            
                JDBC To Other Databases
            
        



            
                Avro Files
            
        



            
                Protobuf data
            
        



            
                Whole Binary Files
            
        



            
                Troubleshooting
            
        




            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        







Data Sources
Spark SQL supports operating on a variety of data sources through the DataFrame interface.
A DataFrame can be operated on using relational transformations and can also be used to create a temporary view.
Registering a DataFrame as a temporary view allows you to run SQL queries over its data. This section
describes the general methods for loading and saving data using the Spark Data Sources and then
goes into specific options that are available for the built-in data sources.

Generic Load/Save Functions

Manually Specifying Options
Run SQL on files directly
Save Modes
Saving to Persistent Tables
Bucketing, Sorting and Partitioning


Generic File Source Options

Ignore Corrupt Files
Ignore Missing Files
Path Global Filter
Recursive File Lookup


Parquet Files

Loading Data Programmatically
Partition Discovery
Schema Merging
Hive metastore Parquet table conversion
Configuration


ORC Files
JSON Files
CSV Files
Text Files
Hive Tables

Specifying storage format for Hive tables
Interacting with Different Versions of Hive Metastore


JDBC To Other Databases
Avro Files

Deploying
Load and Save Functions
to_avro() and from_avro()
Data Source Option
Configuration
Compatibility with Databricks spark-avro
Supported types for Avro -> Spark SQL conversion
Supported types for Spark SQL -> Avro conversion


Protobuf data

Deploying
to_protobuf() and from_protobuf()
Supported types for Protobuf -> Spark SQL conversion
Supported types for Spark SQL -> Protobuf conversion
Handling circular references protobuf fields


Whole Binary Files
Troubleshooting





















  




Distributed SQL Engine - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        




            
                Running the Thrift JDBC/ODBC server
            
        



            
                Running the Spark SQL CLI
            
        




            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        







Distributed SQL Engine

Running the Thrift JDBC/ODBC server
Running the Spark SQL CLI

Spark SQL can also act as a distributed query engine using its JDBC/ODBC or command-line interface.
In this mode, end-users or applications can interact with Spark SQL directly to run SQL queries,
without the need to write any code.
Running the Thrift JDBC/ODBC server
The Thrift JDBC/ODBC server implemented here corresponds to the HiveServer2
in built-in Hive. You can test the JDBC server with the beeline script that comes with either Spark or compatible Hive.
To start the JDBC/ODBC server, run the following in the Spark directory:
./sbin/start-thriftserver.sh

This script accepts all bin/spark-submit command line options, plus a --hiveconf option to
specify Hive properties. You may run ./sbin/start-thriftserver.sh --help for a complete list of
all available options. By default, the server listens on localhost:10000. You may override this
behaviour via either environment variables, i.e.:
export HIVE_SERVER2_THRIFT_PORT=<listening-port>
export HIVE_SERVER2_THRIFT_BIND_HOST=<listening-host>
./sbin/start-thriftserver.sh \
  --master <master-uri> \
  ...
or system properties:
./sbin/start-thriftserver.sh \
  --hiveconf hive.server2.thrift.port=<listening-port> \
  --hiveconf hive.server2.thrift.bind.host=<listening-host> \
  --master <master-uri>
  ...
Now you can use beeline to test the Thrift JDBC/ODBC server:
./bin/beeline

Connect to the JDBC/ODBC server in beeline with:
beeline> !connect jdbc:hive2://localhost:10000

Beeline will ask you for a username and password. In non-secure mode, simply enter the username on
your machine and a blank password. For secure mode, please follow the instructions given in the
beeline documentation.
Configuration of Hive is done by placing your hive-site.xml, core-site.xml and hdfs-site.xml files in conf/.
You may also use the beeline script that comes with Hive.
Thrift JDBC server also supports sending thrift RPC messages over HTTP transport.
Use the following setting to enable HTTP mode as system property or in hive-site.xml file in conf/:
hive.server2.transport.mode - Set this to value: http
hive.server2.thrift.http.port - HTTP port number to listen on; default is 10001
hive.server2.http.endpoint - HTTP endpoint; default is cliservice

To test, use beeline to connect to the JDBC/ODBC server in http mode with:
beeline> !connect jdbc:hive2://<host>:<port>/<database>?hive.server2.transport.mode=http;hive.server2.thrift.http.path=<http_endpoint>

If you closed a session and do CTAS, you must set fs.%s.impl.disable.cache to true in hive-site.xml.
See more details in [SPARK-21067].
Running the Spark SQL CLI
To use the Spark SQL command line interface (CLI) from the shell:
./bin/spark-sql

For details, please refer to Spark SQL CLI




















  




Error Conditions - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        




            
                SQLSTATE Codes
            
        



            
                CONNECT error class
            
        



            
                DATATYPE_MISMATCH error class
            
        



            
                INCOMPATIBLE_DATA_FOR_TABLE error class
            
        



            
                INCOMPLETE_TYPE_DEFINITION error class
            
        



            
                INCONSISTENT_BEHAVIOR_CROSS_VERSION error class
            
        



            
                INVALID_FORMAT error class
            
        



            
                INVALID_OPTIONS error class
            
        



            
                INVALID_PARAMETER_VALUE error class
            
        



            
                INVALID_SCHEMA error class
            
        



            
                INVALID_SUBQUERY_EXPRESSION error class
            
        



            
                NOT_NULL_CONSTRAINT_VIOLATION error class
            
        



            
                UNRESOLVED_COLUMN error class
            
        



            
                UNRESOLVED_FIELD error class
            
        



            
                UNRESOLVED_MAP_KEY error class
            
        



            
                UNSUPPORTED_DESERIALIZER error class
            
        



            
                UNSUPPORTED_FEATURE error class
            
        



            
                UNSUPPORTED_GENERATOR error class
            
        



            
                UNSUPPORTED_SAVE_MODE error class
            
        



            
                UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY error class
            
        



            
                WRONG_NUM_ARGS error class
            
        








Error Conditions
This is a list of common, named error conditions returned by Spark SQL.
Also see SQLSTATE Codes.
AGGREGATE_FUNCTION_WITH_NONDETERMINISTIC_EXPRESSION
SQLSTATE: none assigned
Non-deterministic expression <sqlExpr> should not appear in the arguments of an aggregate function.
ALL_PARTITION_COLUMNS_NOT_ALLOWED
SQLSTATE: none assigned
Cannot use all columns for partition columns.
ALTER_TABLE_COLUMN_DESCRIPTOR_DUPLICATE
SQLSTATE: 42710
ALTER TABLE <type> column <columnName> specifies descriptor “<optionName>” more than once, which is invalid.
AMBIGUOUS_ALIAS_IN_NESTED_CTE
SQLSTATE: none assigned
Name <name> is ambiguous in nested CTE.
Please set <config> to “CORRECTED” so that name defined in inner CTE takes precedence. If set it to “LEGACY”, outer CTE definitions will take precedence.
See ‘<docroot>/sql-migration-guide.html#query-engine’.
AMBIGUOUS_COLUMN_OR_FIELD
SQLSTATE: 42702
Column or field <name> is ambiguous and has <n> matches.
AMBIGUOUS_COLUMN_REFERENCE
SQLSTATE: 42702
Column <name> is ambiguous. It’s because you joined several DataFrame together, and some of these DataFrames are the same.
This column points to one of the DataFrame but Spark is unable to figure out which one.
Please alias the DataFrames with different names via DataFrame.alias before joining them,
and specify the column using qualified name, e.g. df.alias("a").join(df.alias("b"), col("a.id") > col("b.id")).
AMBIGUOUS_LATERAL_COLUMN_ALIAS
SQLSTATE: 42702
Lateral column alias <name> is ambiguous and has <n> matches.
AMBIGUOUS_REFERENCE
SQLSTATE: 42704
Reference <name> is ambiguous, could be: <referenceNames>.
AMBIGUOUS_REFERENCE_TO_FIELDS
SQLSTATE: 42000
Ambiguous reference to the field <field>. It appears <count> times in the schema.
ARITHMETIC_OVERFLOW
SQLSTATE: 22003
<message>.<alternative> If necessary set <config> to “false” to bypass this error.
AS_OF_JOIN
SQLSTATE: none assigned
Invalid as-of join.
For more details see AS_OF_JOIN
AVRO_INCOMPATIBLE_READ_TYPE
SQLSTATE: none assigned
Cannot convert Avro <avroPath> to SQL <sqlPath> because the original encoded data type is <avroType>, however you’re trying to read the field as <sqlType>, which would lead to an incorrect answer. To allow reading this field, enable the SQL configuration: “spark.sql.legacy.avro.allowIncompatibleSchema”.
BATCH_METADATA_NOT_FOUND
SQLSTATE: 42K03
Unable to find batch <batchMetadataFile>.
BINARY_ARITHMETIC_OVERFLOW
SQLSTATE: 22003
<value1> <symbol> <value2> caused overflow.
CALL_ON_STREAMING_DATASET_UNSUPPORTED
SQLSTATE: none assigned
The method <methodName> can not be called on streaming Dataset/DataFrame.
CANNOT_CAST_DATATYPE
SQLSTATE: 42846
Cannot cast <sourceType> to <targetType>.
CANNOT_CONVERT_PROTOBUF_FIELD_TYPE_TO_SQL_TYPE
SQLSTATE: none assigned
Cannot convert Protobuf <protobufColumn> to SQL <sqlColumn> because schema is incompatible (protobufType = <protobufType>, sqlType = <sqlType>).
CANNOT_CONVERT_PROTOBUF_MESSAGE_TYPE_TO_SQL_TYPE
SQLSTATE: none assigned
Unable to convert <protobufType> of Protobuf to SQL type <toType>.
CANNOT_CONVERT_SQL_TYPE_TO_PROTOBUF_FIELD_TYPE
SQLSTATE: none assigned
Cannot convert SQL <sqlColumn> to Protobuf <protobufColumn> because schema is incompatible (protobufType = <protobufType>, sqlType = <sqlType>).
CANNOT_CONVERT_SQL_VALUE_TO_PROTOBUF_ENUM_TYPE
SQLSTATE: none assigned
Cannot convert SQL <sqlColumn> to Protobuf <protobufColumn> because <data> is not in defined values for enum: <enumString>.
CANNOT_DECODE_URL
SQLSTATE: 22546
The provided URL cannot be decoded: <url>. Please ensure that the URL is properly formatted and try again.
CANNOT_INVOKE_IN_TRANSFORMATIONS
SQLSTATE: none assigned
Dataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.
CANNOT_LOAD_FUNCTION_CLASS
SQLSTATE: none assigned
Cannot load class <className> when registering the function <functionName>, please make sure it is on the classpath.
CANNOT_LOAD_PROTOBUF_CLASS
SQLSTATE: none assigned
Could not load Protobuf class with name <protobufClassName>. <explanation>.
CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE
SQLSTATE: 42825
Failed to merge incompatible data types <left> and <right>. Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.
CANNOT_MERGE_SCHEMAS
SQLSTATE: 42KD9
Failed merging schemas:
Initial schema:
<left>
Schema that cannot be merged with the initial schema:
<right>.
CANNOT_MODIFY_CONFIG
SQLSTATE: 46110
Cannot modify the value of the Spark config: <key>.
See also ‘<docroot>/sql-migration-guide.html#ddl-statements’.
CANNOT_PARSE_DECIMAL
SQLSTATE: 22018
Cannot parse decimal. Please ensure that the input is a valid number with optional decimal point or comma separators.
CANNOT_PARSE_INTERVAL
SQLSTATE: none assigned
Unable to parse <intervalString>. Please ensure that the value provided is in a valid format for defining an interval. You can reference the documentation for the correct format. If the issue persists, please double check that the input value is not null or empty and try again.
CANNOT_PARSE_JSON_FIELD
SQLSTATE: 2203G
Cannot parse the field name <fieldName> and the value <fieldValue> of the JSON token type <jsonType> to target Spark data type <dataType>.
CANNOT_PARSE_PROTOBUF_DESCRIPTOR
SQLSTATE: none assigned
Error parsing descriptor bytes into Protobuf FileDescriptorSet.
CANNOT_PARSE_TIMESTAMP
SQLSTATE: 22007
<message>. If necessary set <ansiConfig> to “false” to bypass this error.
CANNOT_READ_FILE_FOOTER
SQLSTATE: none assigned
Could not read footer for file: <file>. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.
CANNOT_RECOGNIZE_HIVE_TYPE
SQLSTATE: 429BB
Cannot recognize hive type string: <fieldType>, column: <fieldName>. The specified data type for the field cannot be recognized by Spark SQL. Please check the data type of the specified field and ensure that it is a valid Spark SQL data type. Refer to the Spark SQL documentation for a list of valid data types and their format. If the data type is correct, please ensure that you are using a supported version of Spark SQL.
CANNOT_RENAME_ACROSS_SCHEMA
SQLSTATE: 0AKD0
Renaming a <type> across schemas is not allowed.
CANNOT_RESOLVE_STAR_EXPAND
SQLSTATE: none assigned
Cannot resolve <targetString>.* given input columns <columns>. Please check that the specified table or struct exists and is accessible in the input columns.
CANNOT_RESTORE_PERMISSIONS_FOR_PATH
SQLSTATE: none assigned
Failed to set permissions on created path <path> back to <permission>.
CANNOT_UPDATE_FIELD
SQLSTATE: none assigned
Cannot update <table> field <fieldName> type:
For more details see CANNOT_UPDATE_FIELD
CANNOT_UP_CAST_DATATYPE
SQLSTATE: none assigned
Cannot up cast <expression> from <sourceType> to <targetType>.
<details>
CAST_INVALID_INPUT
SQLSTATE: 22018
The value <expression> of the type <sourceType> cannot be cast to <targetType> because it is malformed. Correct the value as per the syntax, or change its target type. Use try_cast to tolerate malformed input and return NULL instead. If necessary set <ansiConfig> to “false” to bypass this error.
CAST_OVERFLOW
SQLSTATE: 22003
The value <value> of the type <sourceType> cannot be cast to <targetType> due to an overflow. Use try_cast to tolerate overflow and return NULL instead. If necessary set <ansiConfig> to “false” to bypass this error.
CAST_OVERFLOW_IN_TABLE_INSERT
SQLSTATE: 22003
Fail to insert a value of <sourceType> type into the <targetType> type column <columnName> due to an overflow. Use try_cast on the input value to tolerate overflow and return NULL instead.
CODEC_NOT_AVAILABLE
SQLSTATE: none assigned
The codec <codecName> is not available. Consider to set the config <configKey> to <configVal>.
CODEC_SHORT_NAME_NOT_FOUND
SQLSTATE: none assigned
Cannot find a short name for the codec <codecName>.
COLUMN_ALIASES_IS_NOT_ALLOWED
SQLSTATE: none assigned
Columns aliases are not allowed in <op>.
COLUMN_ALREADY_EXISTS
SQLSTATE: 42711
The column <columnName> already exists. Consider to choose another name or rename the existing column.
COLUMN_NOT_DEFINED_IN_TABLE
SQLSTATE: none assigned
<colType> column <colName> is not defined in table <tableName>, defined table columns are: <tableCols>.
COLUMN_NOT_FOUND
SQLSTATE: 42703
The column <colName> cannot be found. Verify the spelling and correctness of the column name according to the SQL config <caseSensitiveConfig>.
COMPARATOR_RETURNS_NULL
SQLSTATE: none assigned
The comparator has returned a NULL for a comparison between <firstValue> and <secondValue>. It should return a positive integer for “greater than”, 0 for “equal” and a negative integer for “less than”. To revert to deprecated behavior where NULL is treated as 0 (equal), you must set “spark.sql.legacy.allowNullComparisonResultInArraySort” to “true”.
CONCURRENT_QUERY
SQLSTATE: none assigned
Another instance of this query was just started by a concurrent session.
CONCURRENT_STREAM_LOG_UPDATE
SQLSTATE: 40000
Concurrent update to the log. Multiple streaming jobs detected for <batchId>.
Please make sure only one streaming job runs on a specific checkpoint location at a time.
CONNECT
SQLSTATE: none assigned
Generic Spark Connect error.
For more details see CONNECT
CONVERSION_INVALID_INPUT
SQLSTATE: 22018
The value <str> (<fmt>) cannot be converted to <targetType> because it is malformed. Correct the value as per the syntax, or change its format. Use <suggestion> to tolerate malformed input and return NULL instead.
CREATE_PERMANENT_VIEW_WITHOUT_ALIAS
SQLSTATE: none assigned
Not allowed to create the permanent view <name> without explicitly assigning an alias for the expression <attr>.
CREATE_TABLE_COLUMN_DESCRIPTOR_DUPLICATE
SQLSTATE: 42710
CREATE TABLE column <columnName> specifies descriptor “<optionName>” more than once, which is invalid.
CREATE_VIEW_COLUMN_ARITY_MISMATCH
SQLSTATE: 21S01
Cannot create view <viewName>, the reason is
For more details see CREATE_VIEW_COLUMN_ARITY_MISMATCH
DATATYPE_MISMATCH
SQLSTATE: 42K09
Cannot resolve <sqlExpr> due to data type mismatch:
For more details see DATATYPE_MISMATCH
DATATYPE_MISSING_SIZE
SQLSTATE: 42K01
DataType <type> requires a length parameter, for example <type>(10). Please specify the length.
DATA_SOURCE_NOT_FOUND
SQLSTATE: 42K02
Failed to find the data source: <provider>. Please find packages at https://spark.apache.org/third-party-projects.html.
DATETIME_OVERFLOW
SQLSTATE: 22008
Datetime operation overflow: <operation>.
DECIMAL_PRECISION_EXCEEDS_MAX_PRECISION
SQLSTATE: 22003
Decimal precision <precision> exceeds max precision <maxPrecision>.
DEFAULT_DATABASE_NOT_EXISTS
SQLSTATE: 42704
Default database <defaultDatabase> does not exist, please create it first or change default database to <defaultDatabase>.
DISTINCT_WINDOW_FUNCTION_UNSUPPORTED
SQLSTATE: none assigned
Distinct window functions are not supported: <windowExpr>.
DIVIDE_BY_ZERO
SQLSTATE: 22012
Division by zero. Use try_divide to tolerate divisor being 0 and return NULL instead. If necessary set <config> to “false” to bypass this error.
DUPLICATED_FIELD_NAME_IN_ARROW_STRUCT
SQLSTATE: none assigned
Duplicated field names in Arrow Struct are not allowed, got <fieldNames>.
DUPLICATED_MAP_KEY
SQLSTATE: 23505
Duplicate map key <key> was found, please check the input data. If you want to remove the duplicated keys, you can set <mapKeyDedupPolicy> to “LAST_WIN” so that the key inserted at last takes precedence.
DUPLICATED_METRICS_NAME
SQLSTATE: none assigned
The metric name is not unique: <metricName>. The same name cannot be used for metrics with different results. However multiple instances of metrics with with same result and name are allowed (e.g. self-joins).
DUPLICATE_CLAUSES
SQLSTATE: none assigned
Found duplicate clauses: <clauseName>. Please, remove one of them.
DUPLICATE_KEY
SQLSTATE: 23505
Found duplicate keys <keyColumn>.
DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT
SQLSTATE: 4274K
Call to function <functionName> is invalid because it includes multiple argument assignments to the same parameter name <parameterName>.
For more details see DUPLICATE_ROUTINE_PARAMETER_ASSIGNMENT
EMPTY_JSON_FIELD_VALUE
SQLSTATE: 42604
Failed to parse an empty string for data type <dataType>.
ENCODER_NOT_FOUND
SQLSTATE: none assigned
Not found an encoder of the type <typeName> to Spark SQL internal representation. Consider to change the input type to one of supported at ‘<docroot>/sql-ref-datatypes.html’.
EVENT_TIME_IS_NOT_ON_TIMESTAMP_TYPE
SQLSTATE: none assigned
The event time <eventName> has the invalid type <eventType>, but expected “TIMESTAMP”.
EXCEED_LIMIT_LENGTH
SQLSTATE: none assigned
Exceeds char/varchar type length limitation: <limit>.
EXPRESSION_TYPE_IS_NOT_ORDERABLE
SQLSTATE: none assigned
Column expression <expr> cannot be sorted because its type <exprType> is not orderable.
FAILED_EXECUTE_UDF
SQLSTATE: 39000
Failed to execute user defined function (<functionName>: (<signature>) => <result>).
FAILED_FUNCTION_CALL
SQLSTATE: 38000
Failed preparing of the function <funcName> for call. Please, double check function’s arguments.
FAILED_PARSE_STRUCT_TYPE
SQLSTATE: 22018
Failed parsing struct: <raw>.
FAILED_RENAME_PATH
SQLSTATE: 42K04
Failed to rename <sourcePath> to <targetPath> as destination already exists.
FAILED_RENAME_TEMP_FILE
SQLSTATE: none assigned
Failed to rename temp file <srcPath> to <dstPath> as FileSystem.rename returned false.
FIELDS_ALREADY_EXISTS
SQLSTATE: none assigned
Cannot <op> column, because <fieldNames> already exists in <struct>.
FIELD_NOT_FOUND
SQLSTATE: 42704
No such struct field <fieldName> in <fields>.
FORBIDDEN_OPERATION
SQLSTATE: 42809
The operation <statement> is not allowed on the <objectType>: <objectName>.
GENERATED_COLUMN_WITH_DEFAULT_VALUE
SQLSTATE: none assigned
A column cannot have both a default value and a generation expression but column <colName> has default value: (<defaultValue>) and generation expression: (<genExpr>).
GRAPHITE_SINK_INVALID_PROTOCOL
SQLSTATE: none assigned
Invalid Graphite protocol: <protocol>.
GRAPHITE_SINK_PROPERTY_MISSING
SQLSTATE: none assigned
Graphite sink requires ‘<property>’ property.
GROUPING_COLUMN_MISMATCH
SQLSTATE: 42803
Column of grouping (<grouping>) can’t be found in grouping columns <groupingColumns>.
GROUPING_ID_COLUMN_MISMATCH
SQLSTATE: 42803
Columns of grouping_id (<groupingIdColumn>) does not match grouping columns (<groupByColumns>).
GROUPING_SIZE_LIMIT_EXCEEDED
SQLSTATE: 54000
Grouping sets size cannot be greater than <maxSize>.
GROUP_BY_AGGREGATE
SQLSTATE: 42903
Aggregate functions are not allowed in GROUP BY, but found <sqlExpr>.
GROUP_BY_POS_AGGREGATE
SQLSTATE: 42903
GROUP BY <index> refers to an expression <aggExpr> that contains an aggregate function. Aggregate functions are not allowed in GROUP BY.
GROUP_BY_POS_OUT_OF_RANGE
SQLSTATE: 42805
GROUP BY position <index> is not in select list (valid range is [1, <size>]).
GROUP_EXPRESSION_TYPE_IS_NOT_ORDERABLE
SQLSTATE: none assigned
The expression <sqlExpr> cannot be used as a grouping expression because its data type <dataType> is not an orderable data type.
HLL_INVALID_INPUT_SKETCH_BUFFER
SQLSTATE: none assigned
Invalid call to <function>; only valid HLL sketch buffers are supported as inputs (such as those produced by the hll_sketch_agg function).
HLL_INVALID_LG_K
SQLSTATE: none assigned
Invalid call to <function>; the lgConfigK value must be between <min> and <max>, inclusive: <value>.
HLL_UNION_DIFFERENT_LG_K
SQLSTATE: none assigned
Sketches have different lgConfigK values: <left> and <right>. Set the allowDifferentLgConfigK parameter to true to call <function> with different lgConfigK values.
IDENTIFIER_TOO_MANY_NAME_PARTS
SQLSTATE: 42601
<identifier> is not a valid identifier as it has more than 2 name parts.
INCOMPARABLE_PIVOT_COLUMN
SQLSTATE: 42818
Invalid pivot column <columnName>. Pivot columns must be comparable.
INCOMPATIBLE_COLUMN_TYPE
SQLSTATE: 42825
<operator> can only be performed on tables with compatible column types. The <columnOrdinalNumber> column of the <tableOrdinalNumber> table is <dataType1> type which is not compatible with <dataType2> at the same column of the first table.<hint>.
INCOMPATIBLE_DATASOURCE_REGISTER
SQLSTATE: none assigned
Detected an incompatible DataSourceRegister. Please remove the incompatible library from classpath or upgrade it. Error: <message>
INCOMPATIBLE_DATA_FOR_TABLE
SQLSTATE: KD000
Cannot write incompatible data for the table <tableName>:
For more details see INCOMPATIBLE_DATA_FOR_TABLE
INCOMPATIBLE_JOIN_TYPES
SQLSTATE: 42613
The join types <joinType1> and <joinType2> are incompatible.
INCOMPATIBLE_VIEW_SCHEMA_CHANGE
SQLSTATE: none assigned
The SQL query of view <viewName> has an incompatible schema change and column <colName> cannot be resolved. Expected <expectedNum> columns named <colName> but got <actualCols>.
Please try to re-create the view by running: <suggestion>.
INCOMPLETE_TYPE_DEFINITION
SQLSTATE: 42K01
Incomplete complex type:
For more details see INCOMPLETE_TYPE_DEFINITION
INCONSISTENT_BEHAVIOR_CROSS_VERSION
SQLSTATE: 42K0B
You may get a different result due to the upgrading to
For more details see INCONSISTENT_BEHAVIOR_CROSS_VERSION
INCORRECT_END_OFFSET
SQLSTATE: 22003
Max offset with <rowsPerSecond> rowsPerSecond is <maxSeconds>, but it’s <endSeconds> now.
INCORRECT_RAMP_UP_RATE
SQLSTATE: 22003
Max offset with <rowsPerSecond> rowsPerSecond is <maxSeconds>, but ‘rampUpTimeSeconds’ is <rampUpTimeSeconds>.
INDEX_ALREADY_EXISTS
SQLSTATE: 42710
Cannot create the index <indexName> on table <tableName> because it already exists.
INDEX_NOT_FOUND
SQLSTATE: 42704
Cannot find the index <indexName> on table <tableName>.
INSERT_COLUMN_ARITY_MISMATCH
SQLSTATE: 21S01
Cannot write to <tableName>, the reason is
For more details see INSERT_COLUMN_ARITY_MISMATCH
INSERT_PARTITION_COLUMN_ARITY_MISMATCH
SQLSTATE: 21S01
Cannot write to ‘<tableName>’, <reason>:
Table columns: <tableColumns>.
Partition columns with static values: <staticPartCols>.
Data columns: <dataColumns>.
INSUFFICIENT_TABLE_PROPERTY
SQLSTATE: none assigned
Can’t find table property:
For more details see INSUFFICIENT_TABLE_PROPERTY
INTERNAL_ERROR
SQLSTATE: XX000
<message>
INTERNAL_ERROR_BROADCAST
SQLSTATE: XX000
<message>
INTERNAL_ERROR_EXECUTOR
SQLSTATE: XX000
<message>
INTERNAL_ERROR_MEMORY
SQLSTATE: XX000
<message>
INTERNAL_ERROR_NETWORK
SQLSTATE: XX000
<message>
INTERNAL_ERROR_SHUFFLE
SQLSTATE: XX000
<message>
INTERNAL_ERROR_STORAGE
SQLSTATE: XX000
<message>
INTERVAL_ARITHMETIC_OVERFLOW
SQLSTATE: 22015
<message>.<alternative>
INTERVAL_DIVIDED_BY_ZERO
SQLSTATE: 22012
Division by zero. Use try_divide to tolerate divisor being 0 and return NULL instead.
INVALID_ARRAY_INDEX
SQLSTATE: 22003
The index <indexValue> is out of bounds. The array has <arraySize> elements. Use the SQL function get() to tolerate accessing element at invalid index and return NULL instead. If necessary set <ansiConfig> to “false” to bypass this error.
INVALID_ARRAY_INDEX_IN_ELEMENT_AT
SQLSTATE: 22003
The index <indexValue> is out of bounds. The array has <arraySize> elements. Use try_element_at to tolerate accessing element at invalid index and return NULL instead. If necessary set <ansiConfig> to “false” to bypass this error.
INVALID_BITMAP_POSITION
SQLSTATE: 22003
The 0-indexed bitmap position <bitPosition> is out of bounds. The bitmap has <bitmapNumBits> bits (<bitmapNumBytes> bytes).
INVALID_BOUNDARY
SQLSTATE: none assigned
The boundary <boundary> is invalid: <invalidValue>.
For more details see INVALID_BOUNDARY
INVALID_BUCKET_FILE
SQLSTATE: none assigned
Invalid bucket file: <path>.
INVALID_BYTE_STRING
SQLSTATE: none assigned
The expected format is ByteString, but was <unsupported> (<class>).
INVALID_COLUMN_NAME_AS_PATH
SQLSTATE: 46121
The datasource <datasource> cannot save the column <columnName> because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
INVALID_COLUMN_OR_FIELD_DATA_TYPE
SQLSTATE: 42000
Column or field <name> is of type <type> while it’s required to be <expectedType>.
INVALID_CURSOR
SQLSTATE: HY109
The cursor is invalid.
For more details see INVALID_CURSOR
INVALID_DEFAULT_VALUE
SQLSTATE: none assigned
Failed to execute <statement> command because the destination table column <colName> has a DEFAULT value <defaultValue>,
For more details see INVALID_DEFAULT_VALUE
INVALID_DRIVER_MEMORY
SQLSTATE: F0000
System memory <systemMemory> must be at least <minSystemMemory>. Please increase heap size using the –driver-memory option or “<config>” in Spark configuration.
INVALID_EMPTY_LOCATION
SQLSTATE: 42K05
The location name cannot be empty string, but <location> was given.
INVALID_ESC
SQLSTATE: none assigned
Found an invalid escape string: <invalidEscape>. The escape string must contain only one character.
INVALID_ESCAPE_CHAR
SQLSTATE: none assigned
EscapeChar should be a string literal of length one, but got <sqlExpr>.
INVALID_EXECUTOR_MEMORY
SQLSTATE: F0000
Executor memory <executorMemory> must be at least <minSystemMemory>. Please increase executor memory using the –executor-memory option or “<config>” in Spark configuration.
INVALID_EXTRACT_BASE_FIELD_TYPE
SQLSTATE: 42000
Can’t extract a value from <base>. Need a complex type [STRUCT, ARRAY, MAP] but got <other>.
INVALID_EXTRACT_FIELD
SQLSTATE: 42601
Cannot extract <field> from <expr>.
INVALID_EXTRACT_FIELD_TYPE
SQLSTATE: 42000
Field name should be a non-null string literal, but it’s <extraction>.
INVALID_FIELD_NAME
SQLSTATE: 42000
Field name <fieldName> is invalid: <path> is not a struct.
INVALID_FORMAT
SQLSTATE: 42601
The format is invalid: <format>.
For more details see INVALID_FORMAT
INVALID_FRACTION_OF_SECOND
SQLSTATE: 22023
The fraction of sec must be zero. Valid range is [0, 60]. If necessary set <ansiConfig> to “false” to bypass this error.
INVALID_HANDLE
SQLSTATE: HY000
The handle <handle> is invalid.
For more details see INVALID_HANDLE
INVALID_HIVE_COLUMN_NAME
SQLSTATE: none assigned
Cannot create the table <tableName> having the nested column <columnName> whose name contains invalid characters <invalidChars> in Hive metastore.
INVALID_IDENTIFIER
SQLSTATE: 42602
The identifier <ident> is invalid. Please, consider quoting it with back-quotes as <ident>.
INVALID_INDEX_OF_ZERO
SQLSTATE: 22003
The index 0 is invalid. An index shall be either < 0 or > 0 (the first element has index 1).
INVALID_INLINE_TABLE
SQLSTATE: none assigned
Invalid inline table.
For more details see INVALID_INLINE_TABLE
INVALID_JSON_ROOT_FIELD
SQLSTATE: 22032
Cannot convert JSON root field to target Spark type.
INVALID_JSON_SCHEMA_MAP_TYPE
SQLSTATE: 22032
Input schema <jsonSchema> can only contain STRING as a key type for a MAP.
INVALID_LAMBDA_FUNCTION_CALL
SQLSTATE: none assigned
Invalid lambda function call.
For more details see INVALID_LAMBDA_FUNCTION_CALL
INVALID_LATERAL_JOIN_TYPE
SQLSTATE: 42613
The <joinType> JOIN with LATERAL correlation is not allowed because an OUTER subquery cannot correlate to its join partner. Remove the LATERAL correlation or use an INNER JOIN, or LEFT OUTER JOIN instead.
INVALID_LIMIT_LIKE_EXPRESSION
SQLSTATE: none assigned
The limit like expression <expr> is invalid.
For more details see INVALID_LIMIT_LIKE_EXPRESSION
INVALID_NON_DETERMINISTIC_EXPRESSIONS
SQLSTATE: none assigned
The operator expects a deterministic expression, but the actual expression is <sqlExprs>.
INVALID_NUMERIC_LITERAL_RANGE
SQLSTATE: none assigned
Numeric literal <rawStrippedQualifier> is outside the valid range for <typeName> with minimum value of <minValue> and maximum value of <maxValue>. Please adjust the value accordingly.
INVALID_OBSERVED_METRICS
SQLSTATE: none assigned
Invalid observed metrics.
For more details see INVALID_OBSERVED_METRICS
INVALID_OPTIONS
SQLSTATE: 42K06
Invalid options:
For more details see INVALID_OPTIONS
INVALID_PANDAS_UDF_PLACEMENT
SQLSTATE: 0A000
The group aggregate pandas UDF <functionList> cannot be invoked together with as other, non-pandas aggregate functions.
INVALID_PARAMETER_VALUE
SQLSTATE: 22023
The value of parameter(s) <parameter> in <functionName> is invalid:
For more details see INVALID_PARAMETER_VALUE
INVALID_PARTITION_OPERATION
SQLSTATE: none assigned
The partition command is invalid.
For more details see INVALID_PARTITION_OPERATION
INVALID_PROPERTY_KEY
SQLSTATE: 42602
<key> is an invalid property key, please use quotes, e.g. SET <key>=<value>.
INVALID_PROPERTY_VALUE
SQLSTATE: 42602
<value> is an invalid property value, please use quotes, e.g. SET <key>=<value>
INVALID_SCHEMA
SQLSTATE: 42K07
The input schema <inputSchema> is not a valid schema string.
For more details see INVALID_SCHEMA
INVALID_SET_SYNTAX
SQLSTATE: 42000
Expected format is ‘SET’, ‘SET key’, or ‘SET key=value’. If you want to include special characters in key, or include semicolon in value, please use backquotes, e.g., SET key=value.
INVALID_SQL_ARG
SQLSTATE: none assigned
The argument <name> of sql() is invalid. Consider to replace it by a SQL literal.
INVALID_SQL_SYNTAX
SQLSTATE: 42000
Invalid SQL syntax:
For more details see INVALID_SQL_SYNTAX
INVALID_SUBQUERY_EXPRESSION
SQLSTATE: 42823
Invalid subquery:
For more details see INVALID_SUBQUERY_EXPRESSION
INVALID_TEMP_OBJ_REFERENCE
SQLSTATE: none assigned
Cannot create the persistent object <objName> of the type <obj> because it references to the temporary object <tempObjName> of the type <tempObj>. Please make the temporary object <tempObjName> persistent, or make the persistent object <objName> temporary.
INVALID_TIME_TRAVEL_TIMESTAMP_EXPR
SQLSTATE: none assigned
The time travel timestamp expression <expr> is invalid.
For more details see INVALID_TIME_TRAVEL_TIMESTAMP_EXPR
INVALID_TYPED_LITERAL
SQLSTATE: 42604
The value of the typed literal <valueType> is invalid: <value>.
INVALID_UDF_IMPLEMENTATION
SQLSTATE: none assigned
Function <funcName> does not implement ScalarFunction or AggregateFunction.
INVALID_URL
SQLSTATE: none assigned
The url is invalid: <url>. If necessary set <ansiConfig> to “false” to bypass this error.
INVALID_USAGE_OF_STAR_OR_REGEX
SQLSTATE: 42000
Invalid usage of <elem> in <prettyName>.
INVALID_VIEW_TEXT
SQLSTATE: none assigned
The view <viewName> cannot be displayed due to invalid view text: <viewText>. This may be caused by an unauthorized modification of the view or an incorrect query syntax. Please check your query syntax and verify that the view has not been tampered with.
INVALID_WHERE_CONDITION
SQLSTATE: 42903
The WHERE condition <condition> contains invalid expressions: <expressionList>.
Rewrite the query to avoid window functions, aggregate functions, and generator functions in the WHERE clause.
INVALID_WINDOW_SPEC_FOR_AGGREGATION_FUNC
SQLSTATE: none assigned
Cannot specify ORDER BY or a window frame for <aggFunc>.
INVALID_WRITE_DISTRIBUTION
SQLSTATE: none assigned
The requested write distribution is invalid.
For more details see INVALID_WRITE_DISTRIBUTION
JOIN_CONDITION_IS_NOT_BOOLEAN_TYPE
SQLSTATE: none assigned
The join condition <joinCondition> has the invalid type <conditionType>, expected “BOOLEAN”.
LOAD_DATA_PATH_NOT_EXISTS
SQLSTATE: none assigned
LOAD DATA input path does not exist: <path>.
LOCAL_MUST_WITH_SCHEMA_FILE
SQLSTATE: none assigned
LOCAL must be used together with the schema of file, but got: <actualSchema>.
LOCATION_ALREADY_EXISTS
SQLSTATE: 42710
Cannot name the managed table as <identifier>, as its associated location <location> already exists. Please pick a different table name, or remove the existing location first.
MALFORMED_CSV_RECORD
SQLSTATE: none assigned
Malformed CSV record: <badRecord>
MALFORMED_PROTOBUF_MESSAGE
SQLSTATE: none assigned
Malformed Protobuf messages are detected in message deserialization. Parse Mode: <failFastMode>. To process malformed protobuf message as null result, try setting the option ‘mode’ as ‘PERMISSIVE’.
MALFORMED_RECORD_IN_PARSING
SQLSTATE: 22023
Malformed records are detected in record parsing: <badRecord>.
Parse Mode: <failFastMode>. To process malformed records as null result, try setting the option ‘mode’ as ‘PERMISSIVE’.
For more details see MALFORMED_RECORD_IN_PARSING
MERGE_CARDINALITY_VIOLATION
SQLSTATE: 23K01
The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table.
This could result in the target row being operated on more than once with an update or delete operation and is not allowed.
MISSING_AGGREGATION
SQLSTATE: 42803
The non-aggregating expression <expression> is based on columns which are not participating in the GROUP BY clause.
Add the columns or the expression to the GROUP BY, aggregate the expression, or use <expressionAnyValue> if you do not care which of the values within a group is returned.
MISSING_ATTRIBUTES
SQLSTATE: none assigned
Resolved attribute(s) <missingAttributes> missing from <input> in operator <operator>.
For more details see MISSING_ATTRIBUTES
MISSING_GROUP_BY
SQLSTATE: 42803
The query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.
MULTI_SOURCES_UNSUPPORTED_FOR_EXPRESSION
SQLSTATE: none assigned
The expression <expr> does not support more than one source.
MULTI_UDF_INTERFACE_ERROR
SQLSTATE: none assigned
Not allowed to implement multiple UDF interfaces, UDF class <className>.
NAMED_PARAMETERS_NOT_SUPPORTED
SQLSTATE: 4274K
Named parameters are not supported for function <functionName>; please retry the query with positional arguments to the function call instead.
NAMED_PARAMETER_SUPPORT_DISABLED
SQLSTATE: none assigned
Cannot call function <functionName> because named argument references are not enabled here. In this case, the named argument reference was <argument>. Set “spark.sql.allowNamedFunctionArguments” to “true” to turn on feature.
NESTED_AGGREGATE_FUNCTION
SQLSTATE: 42607
It is not allowed to use an aggregate function in the argument of another aggregate function. Please use the inner aggregate function in a sub-query.
NON_LAST_MATCHED_CLAUSE_OMIT_CONDITION
SQLSTATE: 42613
When there are more than one MATCHED clauses in a MERGE statement, only the last MATCHED clause can omit the condition.
NON_LAST_NOT_MATCHED_BY_SOURCE_CLAUSE_OMIT_CONDITION
SQLSTATE: 42613
When there are more than one NOT MATCHED BY SOURCE clauses in a MERGE statement, only the last NOT MATCHED BY SOURCE clause can omit the condition.
NON_LAST_NOT_MATCHED_BY_TARGET_CLAUSE_OMIT_CONDITION
SQLSTATE: 42613
When there are more than one NOT MATCHED [BY TARGET] clauses in a MERGE statement, only the last NOT MATCHED [BY TARGET] clause can omit the condition.
NON_LITERAL_PIVOT_VALUES
SQLSTATE: 42K08
Literal expressions required for pivot values, found <expression>.
NON_PARTITION_COLUMN
SQLSTATE: 42000
PARTITION clause cannot contain the non-partition column: <columnName>.
NON_TIME_WINDOW_NOT_SUPPORTED_IN_STREAMING
SQLSTATE: none assigned
Window function is not supported in <windowFunc> (as column <columnName>) on streaming DataFrames/Datasets. Structured Streaming only supports time-window aggregation using the WINDOW function. (window specification: <windowSpec>)
NOT_ALLOWED_IN_FROM
SQLSTATE: none assigned
Not allowed in the FROM clause:
For more details see NOT_ALLOWED_IN_FROM
NOT_A_CONSTANT_STRING
SQLSTATE: 42601
The expression <expr> used for the routine or clause <name> must be a constant STRING which is NOT NULL.
For more details see NOT_A_CONSTANT_STRING
NOT_A_PARTITIONED_TABLE
SQLSTATE: none assigned
Operation <operation> is not allowed for <tableIdentWithDB> because it is not a partitioned table.
NOT_NULL_CONSTRAINT_VIOLATION
SQLSTATE: 42000
Assigning a NULL is not allowed here.
For more details see NOT_NULL_CONSTRAINT_VIOLATION
NOT_SUPPORTED_CHANGE_COLUMN
SQLSTATE: none assigned
ALTER TABLE ALTER/CHANGE COLUMN is not supported for changing <table>’s column <originName> with type <originType> to <newName> with type <newType>.
NOT_SUPPORTED_COMMAND_FOR_V2_TABLE
SQLSTATE: 46110
<cmd> is not supported for v2 tables.
NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT
SQLSTATE: none assigned
<cmd> is not supported, if you want to enable it, please set “spark.sql.catalogImplementation” to “hive”.
NOT_SUPPORTED_IN_JDBC_CATALOG
SQLSTATE: 46110
Not supported command in JDBC catalog:
For more details see NOT_SUPPORTED_IN_JDBC_CATALOG
NO_DEFAULT_COLUMN_VALUE_AVAILABLE
SQLSTATE: 42608
Can’t determine the default value for <colName> since it is not nullable and it has no default value.
NO_HANDLER_FOR_UDAF
SQLSTATE: none assigned
No handler for UDAF ‘<functionName>’. Use sparkSession.udf.register(…) instead.
NO_SQL_TYPE_IN_PROTOBUF_SCHEMA
SQLSTATE: none assigned
Cannot find <catalystFieldPath> in Protobuf schema.
NO_UDF_INTERFACE
SQLSTATE: none assigned
UDF class <className> doesn’t implement any UDF interface.
NULLABLE_COLUMN_OR_FIELD
SQLSTATE: 42000
Column or field <name> is nullable while it’s required to be non-nullable.
NULLABLE_ROW_ID_ATTRIBUTES
SQLSTATE: 42000
Row ID attributes cannot be nullable: <nullableRowIdAttrs>.
NULL_MAP_KEY
SQLSTATE: 2200E
Cannot use null as map key.
NUMERIC_OUT_OF_SUPPORTED_RANGE
SQLSTATE: 22003
The value <value> cannot be interpreted as a numeric since it has more than 38 digits.
NUMERIC_VALUE_OUT_OF_RANGE
SQLSTATE: 22003
<value> cannot be represented as Decimal(<precision>, <scale>). If necessary set <config> to “false” to bypass this error, and return NULL instead.
NUM_COLUMNS_MISMATCH
SQLSTATE: 42826
<operator> can only be performed on inputs with the same number of columns, but the first input has <firstNumColumns> columns and the <invalidOrdinalNum> input has <invalidNumColumns> columns.
NUM_TABLE_VALUE_ALIASES_MISMATCH
SQLSTATE: none assigned
Number of given aliases does not match number of output columns. Function name: <funcName>; number of aliases: <aliasesNum>; number of output columns: <outColsNum>.
OPERATION_CANCELED
SQLSTATE: HY008
Operation has been canceled.
ORDER_BY_POS_OUT_OF_RANGE
SQLSTATE: 42805
ORDER BY position <index> is not in select list (valid range is [1, <size>]).
PARSE_EMPTY_STATEMENT
SQLSTATE: 42617
Syntax error, unexpected empty statement.
PARSE_SYNTAX_ERROR
SQLSTATE: 42601
Syntax error at or near <error><hint>.
PARTITIONS_ALREADY_EXIST
SQLSTATE: 428FT
Cannot ADD or RENAME TO partition(s) <partitionList> in table <tableName> because they already exist.
Choose a different name, drop the existing partition, or add the IF NOT EXISTS clause to tolerate a pre-existing partition.
PARTITIONS_NOT_FOUND
SQLSTATE: 428FT
The partition(s) <partitionList> cannot be found in table <tableName>.
Verify the partition specification and table name.
To tolerate the error on drop use ALTER TABLE … DROP IF EXISTS PARTITION.
PATH_ALREADY_EXISTS
SQLSTATE: 42K04
Path <outputPath> already exists. Set mode as “overwrite” to overwrite the existing path.
PATH_NOT_FOUND
SQLSTATE: 42K03
Path does not exist: <path>.
PIVOT_VALUE_DATA_TYPE_MISMATCH
SQLSTATE: 42K09
Invalid pivot value ‘<value>’: value data type <valueType> does not match pivot column data type <pivotType>.
PLAN_VALIDATION_FAILED_RULE_EXECUTOR
SQLSTATE: none assigned
The input plan of <ruleExecutor> is invalid: <reason>
PLAN_VALIDATION_FAILED_RULE_IN_BATCH
SQLSTATE: none assigned
Rule <rule> in batch <batch> generated an invalid plan: <reason>
PROTOBUF_DEPENDENCY_NOT_FOUND
SQLSTATE: none assigned
Could not find dependency: <dependencyName>.
PROTOBUF_DESCRIPTOR_FILE_NOT_FOUND
SQLSTATE: none assigned
Error reading Protobuf descriptor file at path: <filePath>.
PROTOBUF_FIELD_MISSING
SQLSTATE: none assigned
Searching for <field> in Protobuf schema at <protobufSchema> gave <matchSize> matches. Candidates: <matches>.
PROTOBUF_FIELD_MISSING_IN_SQL_SCHEMA
SQLSTATE: none assigned
Found <field> in Protobuf schema but there is no match in the SQL schema.
PROTOBUF_FIELD_TYPE_MISMATCH
SQLSTATE: none assigned
Type mismatch encountered for field: <field>.
PROTOBUF_MESSAGE_NOT_FOUND
SQLSTATE: none assigned
Unable to locate Message <messageName> in Descriptor.
PROTOBUF_TYPE_NOT_SUPPORT
SQLSTATE: none assigned
Protobuf type not yet supported: <protobufType>.
RECURSIVE_PROTOBUF_SCHEMA
SQLSTATE: none assigned
Found recursive reference in Protobuf schema, which can not be processed by Spark by default: <fieldDescriptor>. try setting the option recursive.fields.max.depth 0 to 10. Going beyond 10 levels of recursion is not allowed.
RECURSIVE_VIEW
SQLSTATE: none assigned
Recursive view <viewIdent> detected (cycle: <newPath>).
REF_DEFAULT_VALUE_IS_NOT_ALLOWED_IN_PARTITION
SQLSTATE: none assigned
References to DEFAULT column values are not allowed within the PARTITION clause.
RENAME_SRC_PATH_NOT_FOUND
SQLSTATE: 42K03
Failed to rename as <sourcePath> was not found.
REPEATED_CLAUSE
SQLSTATE: 42614
The <clause> clause may be used at most once per <operation> operation.
REQUIRED_PARAMETER_NOT_FOUND
SQLSTATE: 4274K
Cannot invoke function <functionName> because the parameter named <parameterName> is required, but the function call did not supply a value. Please update the function call to supply an argument value (either positionally at index <index> or by name) and retry the query again.
REQUIRES_SINGLE_PART_NAMESPACE
SQLSTATE: 42K05
<sessionCatalog> requires a single-part namespace, but got <namespace>.
ROUTINE_ALREADY_EXISTS
SQLSTATE: 42723
Cannot create the function <routineName> because it already exists.
Choose a different name, drop or replace the existing function, or add the IF NOT EXISTS clause to tolerate a pre-existing function.
ROUTINE_NOT_FOUND
SQLSTATE: 42883
The function <routineName> cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema and catalog, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP FUNCTION IF EXISTS.
RULE_ID_NOT_FOUND
SQLSTATE: 22023
Not found an id for the rule name “<ruleName>”. Please modify RuleIdCollection.scala if you are adding a new rule.
SCALAR_SUBQUERY_IS_IN_GROUP_BY_OR_AGGREGATE_FUNCTION
SQLSTATE: none assigned
The correlated scalar subquery ‘<sqlExpr>’ is neither present in GROUP BY, nor in an aggregate function. Add it to GROUP BY using ordinal position or wrap it in first() (or first_value) if you don’t care which value you get.
SCALAR_SUBQUERY_TOO_MANY_ROWS
SQLSTATE: 21000
More than one row returned by a subquery used as an expression.
SCHEMA_ALREADY_EXISTS
SQLSTATE: 42P06
Cannot create schema <schemaName> because it already exists.
Choose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.
SCHEMA_NOT_EMPTY
SQLSTATE: 2BP01
Cannot drop a schema <schemaName> because it contains objects.
Use DROP SCHEMA … CASCADE to drop the schema and all its objects.
SCHEMA_NOT_FOUND
SQLSTATE: 42704
The schema <schemaName> cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
SECOND_FUNCTION_ARGUMENT_NOT_INTEGER
SQLSTATE: 22023
The second argument of <functionName> function needs to be an integer.
SEED_EXPRESSION_IS_UNFOLDABLE
SQLSTATE: none assigned
The seed expression <seedExpr> of the expression <exprWithSeed> must be foldable.
SORT_BY_WITHOUT_BUCKETING
SQLSTATE: none assigned
sortBy must be used together with bucketBy.
SPECIFY_BUCKETING_IS_NOT_ALLOWED
SQLSTATE: none assigned
Cannot specify bucketing information if the table schema is not specified when creating and will be inferred at runtime.
SPECIFY_PARTITION_IS_NOT_ALLOWED
SQLSTATE: none assigned
It is not allowed to specify partition columns when the table schema is not defined. When the table schema is not provided, schema and partition columns will be inferred.
SQL_CONF_NOT_FOUND
SQLSTATE: none assigned
The SQL config <sqlConf> cannot be found. Please verify that the config exists.
STAR_GROUP_BY_POS
SQLSTATE: 0A000
Star (*) is not allowed in a select list when GROUP BY an ordinal position is used.
STATIC_PARTITION_COLUMN_IN_INSERT_COLUMN_LIST
SQLSTATE: none assigned
Static partition column <staticName> is also specified in the column list.
STREAM_FAILED
SQLSTATE: none assigned
Query [id = <id>, runId = <runId>] terminated with exception: <message>
SUM_OF_LIMIT_AND_OFFSET_EXCEEDS_MAX_INT
SQLSTATE: none assigned
The sum of the LIMIT clause and the OFFSET clause must not be greater than the maximum 32-bit integer value (2,147,483,647) but found limit = <limit>, offset = <offset>.
TABLE_OR_VIEW_ALREADY_EXISTS
SQLSTATE: 42P07
Cannot create table or view <relationName> because it already exists.
Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.
TABLE_OR_VIEW_NOT_FOUND
SQLSTATE: 42P01
The table or view <relationName> cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.
TABLE_VALUED_FUNCTION_TOO_MANY_TABLE_ARGUMENTS
SQLSTATE: none assigned
There are too many table arguments for table-valued function. It allows one table argument, but got: <num>. If you want to allow it, please set “spark.sql.allowMultipleTableArguments.enabled” to “true”
TASK_WRITE_FAILED
SQLSTATE: none assigned
Task failed while writing rows to <path>.
TEMP_TABLE_OR_VIEW_ALREADY_EXISTS
SQLSTATE: 42P07
Cannot create the temporary view <relationName> because it already exists.
Choose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views.
TEMP_VIEW_NAME_TOO_MANY_NAME_PARTS
SQLSTATE: 428EK
CREATE TEMPORARY VIEW or the corresponding Dataset APIs only accept single-part view names, but got: <actualName>.
TOO_MANY_ARRAY_ELEMENTS
SQLSTATE: 54000
Cannot initialize array with <numElements> elements of size <size>.
UDTF_ALIAS_NUMBER_MISMATCH
SQLSTATE: none assigned
The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF. Expected <aliasesSize> aliases, but got <aliasesNames>. Please ensure that the number of aliases provided matches the number of columns output by the UDTF.
UNABLE_TO_ACQUIRE_MEMORY
SQLSTATE: 53200
Unable to acquire <requestedBytes> bytes of memory, got <receivedBytes>.
UNABLE_TO_CONVERT_TO_PROTOBUF_MESSAGE_TYPE
SQLSTATE: none assigned
Unable to convert SQL type <toType> to Protobuf type <protobufType>.
UNABLE_TO_INFER_SCHEMA
SQLSTATE: 42KD9
Unable to infer schema for <format>. It must be specified manually.
UNBOUND_SQL_PARAMETER
SQLSTATE: 42P02
Found the unbound parameter: <name>. Please, fix args and provide a mapping of the parameter to a SQL literal.
UNCLOSED_BRACKETED_COMMENT
SQLSTATE: 42601
Found an unclosed bracketed comment. Please, append */ at the end of the comment.
UNEXPECTED_INPUT_TYPE
SQLSTATE: 42K09
Parameter <paramIndex> of function <functionName> requires the <requiredType> type, however <inputSql> has the type <inputType>.
UNEXPECTED_POSITIONAL_ARGUMENT
SQLSTATE: 4274K
Cannot invoke function <functionName> because it contains positional argument(s) following the named argument assigned to <parameterName>; please rearrange them so the positional arguments come first and then retry the query again.
UNKNOWN_PROTOBUF_MESSAGE_TYPE
SQLSTATE: none assigned
Attempting to treat <descriptorName> as a Message, but it was <containingType>.
UNPIVOT_REQUIRES_ATTRIBUTES
SQLSTATE: 42K0A
UNPIVOT requires all given <given> expressions to be columns when no <empty> expressions are given. These are not columns: [<expressions>].
UNPIVOT_REQUIRES_VALUE_COLUMNS
SQLSTATE: 42K0A
At least one value column needs to be specified for UNPIVOT, all columns specified as ids.
UNPIVOT_VALUE_DATA_TYPE_MISMATCH
SQLSTATE: 42K09
Unpivot value columns must share a least common type, some types do not: [<types>].
UNPIVOT_VALUE_SIZE_MISMATCH
SQLSTATE: 428C4
All unpivot value columns must have the same size as there are value column names (<names>).
UNRECOGNIZED_PARAMETER_NAME
SQLSTATE: 4274K
Cannot invoke function <functionName> because the function call included a named argument reference for the argument named <argumentName>, but this function does not include any signature containing an argument with this name. Did you mean one of the following? [<proposal>].
UNRECOGNIZED_SQL_TYPE
SQLSTATE: 42704
Unrecognized SQL type - name: <typeName>, id: <jdbcType>.
UNRESOLVABLE_TABLE_VALUED_FUNCTION
SQLSTATE: none assigned
Could not resolve <name> to a table-valued function. Please make sure that <name> is defined as a table-valued function and that all required parameters are provided correctly. If <name> is not defined, please create the table-valued function before using it. For more information about defining table-valued functions, please refer to the Apache Spark documentation.
UNRESOLVED_ALL_IN_GROUP_BY
SQLSTATE: 42803
Cannot infer grouping columns for GROUP BY ALL based on the select clause. Please explicitly specify the grouping columns.
UNRESOLVED_COLUMN
SQLSTATE: 42703
A column or function parameter with name <objectName> cannot be resolved.
For more details see UNRESOLVED_COLUMN
UNRESOLVED_FIELD
SQLSTATE: 42703
A field with name <fieldName> cannot be resolved with the struct-type column <columnPath>.
For more details see UNRESOLVED_FIELD
UNRESOLVED_MAP_KEY
SQLSTATE: 42703
Cannot resolve column <objectName> as a map key. If the key is a string literal, add the single quotes ‘’ around it.
For more details see UNRESOLVED_MAP_KEY
UNRESOLVED_ROUTINE
SQLSTATE: 42883
Cannot resolve function <routineName> on search path <searchPath>.
UNRESOLVED_USING_COLUMN_FOR_JOIN
SQLSTATE: 42703
USING column <colName> cannot be resolved on the <side> side of the join. The <side>-side columns: [<suggestion>].
UNSET_NONEXISTENT_PROPERTIES
SQLSTATE: none assigned
Attempted to unset non-existent properties [<properties>] in table <table>.
UNSUPPORTED_ADD_FILE
SQLSTATE: none assigned
Don’t support add file.
For more details see UNSUPPORTED_ADD_FILE
UNSUPPORTED_ARROWTYPE
SQLSTATE: 0A000
Unsupported arrow type <typeName>.
UNSUPPORTED_CHAR_OR_VARCHAR_AS_STRING
SQLSTATE: none assigned
The char/varchar type can’t be used in the table schema. If you want Spark treat them as string type as same as Spark 3.0 and earlier, please set “spark.sql.legacy.charVarcharAsString” to “true”.
UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY
SQLSTATE: none assigned
Unsupported data source type for direct query on files: <dataSourceType>
UNSUPPORTED_DATATYPE
SQLSTATE: 0A000
Unsupported data type <typeName>.
UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE
SQLSTATE: none assigned
The <format> datasource doesn’t support the column <columnName> of the type <columnType>.
UNSUPPORTED_DEFAULT_VALUE
SQLSTATE: none assigned
DEFAULT column values is not supported.
For more details see UNSUPPORTED_DEFAULT_VALUE
UNSUPPORTED_DESERIALIZER
SQLSTATE: 0A000
The deserializer is not supported:
For more details see UNSUPPORTED_DESERIALIZER
UNSUPPORTED_EXPRESSION_GENERATED_COLUMN
SQLSTATE: none assigned
Cannot create generated column <fieldName> with generation expression <expressionStr> because <reason>.
UNSUPPORTED_EXPR_FOR_OPERATOR
SQLSTATE: none assigned
A query operator contains one or more unsupported expressions. Consider to rewrite it to avoid window functions, aggregate functions, and generator functions in the WHERE clause.
Invalid expressions: [<invalidExprSqls>]
UNSUPPORTED_EXPR_FOR_WINDOW
SQLSTATE: 42P20
Expression <sqlExpr> not supported within a window function.
UNSUPPORTED_FEATURE
SQLSTATE: 0A000
The feature is not supported:
For more details see UNSUPPORTED_FEATURE
UNSUPPORTED_GENERATOR
SQLSTATE: 0A000
The generator is not supported:
For more details see UNSUPPORTED_GENERATOR
UNSUPPORTED_GROUPING_EXPRESSION
SQLSTATE: none assigned
grouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup.
UNSUPPORTED_INSERT
SQLSTATE: none assigned
Can’t insert into the target.
For more details see UNSUPPORTED_INSERT
UNSUPPORTED_MERGE_CONDITION
SQLSTATE: none assigned
MERGE operation contains unsupported <condName> condition.
For more details see UNSUPPORTED_MERGE_CONDITION
UNSUPPORTED_OVERWRITE
SQLSTATE: none assigned
Can’t overwrite the target that is also being read from.
For more details see UNSUPPORTED_OVERWRITE
UNSUPPORTED_SAVE_MODE
SQLSTATE: none assigned
The save mode <saveMode> is not supported for:
For more details see UNSUPPORTED_SAVE_MODE
UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY
SQLSTATE: 0A000
Unsupported subquery expression:
For more details see UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY
UNSUPPORTED_TYPED_LITERAL
SQLSTATE: 0A000
Literals of the type <unsupportedType> are not supported. Supported types are <supportedTypes>.
UNTYPED_SCALA_UDF
SQLSTATE: none assigned
You’re using untyped Scala UDF, which does not have the input type information. Spark may blindly pass null to the Scala closure with primitive-type argument, and the closure will see the default value of the Java type for the null argument, e.g. udf((x: Int) => x, IntegerType), the result is 0 for null input. To get rid of this error, you could:

use typed Scala UDF APIs(without return type parameter), e.g. udf((x: Int) => x).
use Java UDF APIs, e.g. udf(new UDF1[String, Integer] { override def call(s: String): Integer = s.length() }, IntegerType), if input types are all non primitive.
set “spark.sql.legacy.allowUntypedScalaUDF” to “true” and use this API with caution.

VIEW_ALREADY_EXISTS
SQLSTATE: 42P07
Cannot create view <relationName> because it already exists.
Choose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.
VIEW_NOT_FOUND
SQLSTATE: 42P01
The view <relationName> cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS.
WINDOW_FUNCTION_AND_FRAME_MISMATCH
SQLSTATE: none assigned
<funcName> function can only be evaluated in an ordered row-based window frame with a single offset: <windowExpr>.
WINDOW_FUNCTION_WITHOUT_OVER_CLAUSE
SQLSTATE: none assigned
Window function <funcName> requires an OVER clause.
WRITE_STREAM_NOT_ALLOWED
SQLSTATE: none assigned
writeStream can be called only on streaming Dataset/DataFrame.
WRONG_COMMAND_FOR_OBJECT_TYPE
SQLSTATE: none assigned
The operation <operation> requires a <requiredType>. But <objectName> is a <foundType>. Use <alternative> instead.
WRONG_NUM_ARGS
SQLSTATE: 42605
The <functionName> requires <expectedNum> parameters but the actual number is <actualNum>.
For more details see WRONG_NUM_ARGS




















  




Getting Started - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        




            
                Starting Point: SparkSession
            
        



            
                Creating DataFrames
            
        



            
                Untyped Dataset Operations (DataFrame operations)
            
        



            
                Running SQL Queries Programmatically
            
        



            
                Global Temporary View
            
        



            
                Creating Datasets
            
        



            
                Interoperating with RDDs
            
        



            
                Scalar Functions
            
        



            
                Aggregate Functions
            
        




            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        







Getting Started

Starting Point: SparkSession
Creating DataFrames
Untyped Dataset Operations (aka DataFrame Operations)
Running SQL Queries Programmatically
Global Temporary View
Creating Datasets
Interoperating with RDDs 
Inferring the Schema Using Reflection
Programmatically Specifying the Schema


Scalar Functions
Aggregate Functions

Starting Point: SparkSession


The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder:
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
Find full example code at "examples/src/main/python/sql/basic.py" in the Spark repo.


The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder():
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.


The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder():
import org.apache.spark.sql.SparkSession;

SparkSession spark = SparkSession
  .builder()
  .appName("Java Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate();
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.


The entry point into all functionality in Spark is the SparkSession class. To initialize a basic SparkSession, just call sparkR.session():
sparkR.session(appName = "R Spark SQL basic example", sparkConfig = list(spark.some.config.option = "some-value"))
Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.
Note that when invoked for the first time, sparkR.session() initializes a global SparkSession singleton instance, and always returns a reference to this instance for successive invocations. In this way, users only need to initialize the SparkSession once, then SparkR functions like read.df will be able to access this global instance implicitly, and users don’t need to pass the SparkSession instance around.


SparkSession in Spark 2.0 provides builtin support for Hive features including the ability to
write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables.
To use these features, you do not need to have an existing Hive setup.
Creating DataFrames


With a SparkSession, applications can create DataFrames from an existing RDD,
from a Hive table, or from Spark data sources.
As an example, the following creates a DataFrame based on the content of a JSON file:
# spark is an existing SparkSession
df = spark.read.json("examples/src/main/resources/people.json")
# Displays the content of the DataFrame to stdout
df.show()
# +----+-------+
# | age|   name|
# +----+-------+
# |null|Michael|
# |  30|   Andy|
# |  19| Justin|
# +----+-------+
Find full example code at "examples/src/main/python/sql/basic.py" in the Spark repo.


With a SparkSession, applications can create DataFrames from an existing RDD,
from a Hive table, or from Spark data sources.
As an example, the following creates a DataFrame based on the content of a JSON file:
val df = spark.read.json("examples/src/main/resources/people.json")

// Displays the content of the DataFrame to stdout
df.show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.


With a SparkSession, applications can create DataFrames from an existing RDD,
from a Hive table, or from Spark data sources.
As an example, the following creates a DataFrame based on the content of a JSON file:
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

Dataset<Row> df = spark.read().json("examples/src/main/resources/people.json");

// Displays the content of the DataFrame to stdout
df.show();
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.


With a SparkSession, applications can create DataFrames from a local R data.frame,
from a Hive table, or from Spark data sources.
As an example, the following creates a DataFrame based on the content of a JSON file:
df <- read.json("examples/src/main/resources/people.json")

# Displays the content of the DataFrame
head(df)
##   age    name
## 1  NA Michael
## 2  30    Andy
## 3  19  Justin

# Another method to print the first few rows and optionally truncate the printing of long values
showDF(df)
## +----+-------+
## | age|   name|
## +----+-------+
## |null|Michael|
## |  30|   Andy|
## |  19| Justin|
## +----+-------+
Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.


Untyped Dataset Operations (aka DataFrame Operations)
DataFrames provide a domain-specific language for structured data manipulation in Scala, Java, Python and R.
As mentioned above, in Spark 2.0, DataFrames are just Dataset of Rows in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.
Here we include some basic examples of structured data processing using Datasets:


In Python, it’s possible to access a DataFrame’s columns either by attribute
(df.age) or by indexing (df['age']). While the former is convenient for
interactive data exploration, users are highly encouraged to use the
latter form, which is future proof and won’t break with column names that
are also attributes on the DataFrame class.
# spark, df are from the previous example
# Print the schema in a tree format
df.printSchema()
# root
# |-- age: long (nullable = true)
# |-- name: string (nullable = true)

# Select only the "name" column
df.select("name").show()
# +-------+
# |   name|
# +-------+
# |Michael|
# |   Andy|
# | Justin|
# +-------+

# Select everybody, but increment the age by 1
df.select(df['name'], df['age'] + 1).show()
# +-------+---------+
# |   name|(age + 1)|
# +-------+---------+
# |Michael|     null|
# |   Andy|       31|
# | Justin|       20|
# +-------+---------+

# Select people older than 21
df.filter(df['age'] > 21).show()
# +---+----+
# |age|name|
# +---+----+
# | 30|Andy|
# +---+----+

# Count people by age
df.groupBy("age").count().show()
# +----+-----+
# | age|count|
# +----+-----+
# |  19|    1|
# |null|    1|
# |  30|    1|
# +----+-----+
Find full example code at "examples/src/main/python/sql/basic.py" in the Spark repo.
For a complete list of the types of operations that can be performed on a DataFrame refer to the API Documentation.
In addition to simple column references and expressions, DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the DataFrame Function Reference.


// This import is needed to use the $-notation
import spark.implicits._
// Print the schema in a tree format
df.printSchema()
// root
// |-- age: long (nullable = true)
// |-- name: string (nullable = true)

// Select only the "name" column
df.select("name").show()
// +-------+
// |   name|
// +-------+
// |Michael|
// |   Andy|
// | Justin|
// +-------+

// Select everybody, but increment the age by 1
df.select($"name", $"age" + 1).show()
// +-------+---------+
// |   name|(age + 1)|
// +-------+---------+
// |Michael|     null|
// |   Andy|       31|
// | Justin|       20|
// +-------+---------+

// Select people older than 21
df.filter($"age" > 21).show()
// +---+----+
// |age|name|
// +---+----+
// | 30|Andy|
// +---+----+

// Count people by age
df.groupBy("age").count().show()
// +----+-----+
// | age|count|
// +----+-----+
// |  19|    1|
// |null|    1|
// |  30|    1|
// +----+-----+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.
For a complete list of the types of operations that can be performed on a Dataset, refer to the API Documentation.
In addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the DataFrame Function Reference.


// col("...") is preferable to df.col("...")
import static org.apache.spark.sql.functions.col;

// Print the schema in a tree format
df.printSchema();
// root
// |-- age: long (nullable = true)
// |-- name: string (nullable = true)

// Select only the "name" column
df.select("name").show();
// +-------+
// |   name|
// +-------+
// |Michael|
// |   Andy|
// | Justin|
// +-------+

// Select everybody, but increment the age by 1
df.select(col("name"), col("age").plus(1)).show();
// +-------+---------+
// |   name|(age + 1)|
// +-------+---------+
// |Michael|     null|
// |   Andy|       31|
// | Justin|       20|
// +-------+---------+

// Select people older than 21
df.filter(col("age").gt(21)).show();
// +---+----+
// |age|name|
// +---+----+
// | 30|Andy|
// +---+----+

// Count people by age
df.groupBy("age").count().show();
// +----+-----+
// | age|count|
// +----+-----+
// |  19|    1|
// |null|    1|
// |  30|    1|
// +----+-----+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.
For a complete list of the types of operations that can be performed on a Dataset refer to the API Documentation.
In addition to simple column references and expressions, Datasets also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the DataFrame Function Reference.


# Create the DataFrame
df <- read.json("examples/src/main/resources/people.json")

# Show the content of the DataFrame
head(df)
##   age    name
## 1  NA Michael
## 2  30    Andy
## 3  19  Justin


# Print the schema in a tree format
printSchema(df)
## root
## |-- age: long (nullable = true)
## |-- name: string (nullable = true)

# Select only the "name" column
head(select(df, "name"))
##      name
## 1 Michael
## 2    Andy
## 3  Justin

# Select everybody, but increment the age by 1
head(select(df, df$name, df$age + 1))
##      name (age + 1.0)
## 1 Michael          NA
## 2    Andy          31
## 3  Justin          20

# Select people older than 21
head(where(df, df$age > 21))
##   age name
## 1  30 Andy

# Count people by age
head(count(groupBy(df, "age")))
##   age count
## 1  19     1
## 2  NA     1
## 3  30     1
Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.
For a complete list of the types of operations that can be performed on a DataFrame refer to the API Documentation.
In addition to simple column references and expressions, DataFrames also have a rich library of functions including string manipulation, date arithmetic, common math operations and more. The complete list is available in the DataFrame Function Reference.


Running SQL Queries Programmatically


The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame.
# Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("people")

sqlDF = spark.sql("SELECT * FROM people")
sqlDF.show()
# +----+-------+
# | age|   name|
# +----+-------+
# |null|Michael|
# |  30|   Andy|
# |  19| Justin|
# +----+-------+
Find full example code at "examples/src/main/python/sql/basic.py" in the Spark repo.


The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame.
// Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("people")

val sqlDF = spark.sql("SELECT * FROM people")
sqlDF.show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.


The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a Dataset<Row>.
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

// Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("people");

Dataset<Row> sqlDF = spark.sql("SELECT * FROM people");
sqlDF.show();
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.


The sql function enables applications to run SQL queries programmatically and returns the result as a SparkDataFrame.
df <- sql("SELECT * FROM table")
Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.


Global Temporary View
Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it
terminates. If you want to have a temporary view that is shared among all sessions and keep alive
until the Spark application terminates, you can create a global temporary view. Global temporary
view is tied to a system preserved database global_temp, and we must use the qualified name to
refer it, e.g. SELECT * FROM global_temp.view1.


# Register the DataFrame as a global temporary view
df.createGlobalTempView("people")

# Global temporary view is tied to a system preserved database `global_temp`
spark.sql("SELECT * FROM global_temp.people").show()
# +----+-------+
# | age|   name|
# +----+-------+
# |null|Michael|
# |  30|   Andy|
# |  19| Justin|
# +----+-------+

# Global temporary view is cross-session
spark.newSession().sql("SELECT * FROM global_temp.people").show()
# +----+-------+
# | age|   name|
# +----+-------+
# |null|Michael|
# |  30|   Andy|
# |  19| Justin|
# +----+-------+
Find full example code at "examples/src/main/python/sql/basic.py" in the Spark repo.


// Register the DataFrame as a global temporary view
df.createGlobalTempView("people")

// Global temporary view is tied to a system preserved database `global_temp`
spark.sql("SELECT * FROM global_temp.people").show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+

// Global temporary view is cross-session
spark.newSession().sql("SELECT * FROM global_temp.people").show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.


// Register the DataFrame as a global temporary view
df.createGlobalTempView("people");

// Global temporary view is tied to a system preserved database `global_temp`
spark.sql("SELECT * FROM global_temp.people").show();
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+

// Global temporary view is cross-session
spark.newSession().sql("SELECT * FROM global_temp.people").show();
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.


CREATE GLOBAL TEMPORARY VIEW temp_view AS SELECT a + 1, b * 2 FROM tbl

SELECT * FROM global_temp.temp_view


Creating Datasets
Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use
a specialized Encoder to serialize the objects
for processing or transmitting over the network. While both encoders and standard serialization are
responsible for turning an object into bytes, encoders are code generated dynamically and use a format
that allows Spark to perform many operations like filtering, sorting and hashing without deserializing
the bytes back into an object.


case class Person(name: String, age: Long)

// Encoders are created for case classes
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()
// +----+---+
// |name|age|
// +----+---+
// |Andy| 32|
// +----+---+

// Encoders for most common types are automatically provided by importing spark.implicits._
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)

// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name
val path = "examples/src/main/resources/people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.


import java.util.Arrays;
import java.util.Collections;
import java.io.Serializable;

import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.Encoder;
import org.apache.spark.sql.Encoders;

public static class Person implements Serializable {
  private String name;
  private long age;

  public String getName() {
    return name;
  }

  public void setName(String name) {
    this.name = name;
  }

  public long getAge() {
    return age;
  }

  public void setAge(long age) {
    this.age = age;
  }
}

// Create an instance of a Bean class
Person person = new Person();
person.setName("Andy");
person.setAge(32);

// Encoders are created for Java beans
Encoder<Person> personEncoder = Encoders.bean(Person.class);
Dataset<Person> javaBeanDS = spark.createDataset(
  Collections.singletonList(person),
  personEncoder
);
javaBeanDS.show();
// +---+----+
// |age|name|
// +---+----+
// | 32|Andy|
// +---+----+

// Encoders for most common types are provided in class Encoders
Encoder<Long> longEncoder = Encoders.LONG();
Dataset<Long> primitiveDS = spark.createDataset(Arrays.asList(1L, 2L, 3L), longEncoder);
Dataset<Long> transformedDS = primitiveDS.map(
    (MapFunction<Long, Long>) value -> value + 1L,
    longEncoder);
transformedDS.collect(); // Returns [2, 3, 4]

// DataFrames can be converted to a Dataset by providing a class. Mapping based on name
String path = "examples/src/main/resources/people.json";
Dataset<Person> peopleDS = spark.read().json(path).as(personEncoder);
peopleDS.show();
// +----+-------+
// | age|   name|
// +----+-------+
// |null|Michael|
// |  30|   Andy|
// |  19| Justin|
// +----+-------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.


Interoperating with RDDs
Spark SQL supports two different methods for converting existing RDDs into Datasets. The first
method uses reflection to infer the schema of an RDD that contains specific types of objects. This
reflection-based approach leads to more concise code and works well when you already know the schema
while writing your Spark application.
The second method for creating Datasets is through a programmatic interface that allows you to
construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows
you to construct Datasets when the columns and their types are not known until runtime.
Inferring the Schema Using Reflection


Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of
key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table,
and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files.
from pyspark.sql import Row

sc = spark.sparkContext

# Load a text file and convert each line to a Row.
lines = sc.textFile("examples/src/main/resources/people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))

# Infer the schema, and register the DataFrame as a table.
schemaPeople = spark.createDataFrame(people)
schemaPeople.createOrReplaceTempView("people")

# SQL can be run over DataFrames that have been registered as a table.
teenagers = spark.sql("SELECT name FROM people WHERE age >= 13 AND age <= 19")

# The results of SQL queries are Dataframe objects.
# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.
teenNames = teenagers.rdd.map(lambda p: "Name: " + p.name).collect()
for name in teenNames:
    print(name)
# Name: Justin
Find full example code at "examples/src/main/python/sql/basic.py" in the Spark repo.


The Scala interface for Spark SQL supports automatically converting an RDD containing case classes
to a DataFrame. The case class
defines the schema of the table. The names of the arguments to the case class are read using
reflection and become the names of the columns. Case classes can also be nested or contain complex
types such as Seqs or Arrays. This RDD can be implicitly converted to a DataFrame and then be
registered as a table. Tables can be used in subsequent SQL statements.
// For implicit conversions from RDDs to DataFrames
import spark.implicits._

// Create an RDD of Person objects from a text file, convert it to a Dataframe
val peopleDF = spark.sparkContext
  .textFile("examples/src/main/resources/people.txt")
  .map(_.split(","))
  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))
  .toDF()
// Register the DataFrame as a temporary view
peopleDF.createOrReplaceTempView("people")

// SQL statements can be run by using the sql methods provided by Spark
val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")

// The columns of a row in the result can be accessed by field index
teenagersDF.map(teenager => "Name: " + teenager(0)).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// or by field name
teenagersDF.map(teenager => "Name: " + teenager.getAs[String]("name")).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// No pre-defined encoders for Dataset[Map[K,V]], define explicitly
implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
// Primitive types and case classes can be also defined as
// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()

// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]
teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
// Array(Map("name" -> "Justin", "age" -> 19))
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.


Spark SQL supports automatically converting an RDD of
JavaBeans into a DataFrame.
The BeanInfo, obtained using reflection, defines the schema of the table. Currently, Spark SQL
does not support JavaBeans that contain Map field(s). Nested JavaBeans and List or Array
fields are supported though. You can create a JavaBean by creating a class that implements
Serializable and has getters and setters for all of its fields.
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.Encoder;
import org.apache.spark.sql.Encoders;

// Create an RDD of Person objects from a text file
JavaRDD<Person> peopleRDD = spark.read()
  .textFile("examples/src/main/resources/people.txt")
  .javaRDD()
  .map(line -> {
    String[] parts = line.split(",");
    Person person = new Person();
    person.setName(parts[0]);
    person.setAge(Integer.parseInt(parts[1].trim()));
    return person;
  });

// Apply a schema to an RDD of JavaBeans to get a DataFrame
Dataset<Row> peopleDF = spark.createDataFrame(peopleRDD, Person.class);
// Register the DataFrame as a temporary view
peopleDF.createOrReplaceTempView("people");

// SQL statements can be run by using the sql methods provided by spark
Dataset<Row> teenagersDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19");

// The columns of a row in the result can be accessed by field index
Encoder<String> stringEncoder = Encoders.STRING();
Dataset<String> teenagerNamesByIndexDF = teenagersDF.map(
    (MapFunction<Row, String>) row -> "Name: " + row.getString(0),
    stringEncoder);
teenagerNamesByIndexDF.show();
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// or by field name
Dataset<String> teenagerNamesByFieldDF = teenagersDF.map(
    (MapFunction<Row, String>) row -> "Name: " + row.<String>getAs("name"),
    stringEncoder);
teenagerNamesByFieldDF.show();
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.


Programmatically Specifying the Schema


When a dictionary of kwargs cannot be defined ahead of time (for example,
the structure of records is encoded in a string, or a text dataset will be parsed and
fields will be projected differently for different users),
a DataFrame can be created programmatically with three steps.

Create an RDD of tuples or lists from the original RDD;
Create the schema represented by a StructType matching the structure of
tuples or lists in the RDD created in the step 1.
Apply the schema to the RDD via createDataFrame method provided by SparkSession.

For example:
# Import data types
from pyspark.sql.types import StringType, StructType, StructField

sc = spark.sparkContext

# Load a text file and convert each line to a Row.
lines = sc.textFile("examples/src/main/resources/people.txt")
parts = lines.map(lambda l: l.split(","))
# Each line is converted to a tuple.
people = parts.map(lambda p: (p[0], p[1].strip()))

# The schema is encoded in a string.
schemaString = "name age"

fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)

# Apply the schema to the RDD.
schemaPeople = spark.createDataFrame(people, schema)

# Creates a temporary view using the DataFrame
schemaPeople.createOrReplaceTempView("people")

# SQL can be run over DataFrames that have been registered as a table.
results = spark.sql("SELECT name FROM people")

results.show()
# +-------+
# |   name|
# +-------+
# |Michael|
# |   Andy|
# | Justin|
# +-------+
Find full example code at "examples/src/main/python/sql/basic.py" in the Spark repo.


When case classes cannot be defined ahead of time (for example,
the structure of records is encoded in a string, or a text dataset will be parsed
and fields will be projected differently for different users),
a DataFrame can be created programmatically with three steps.

Create an RDD of Rows from the original RDD;
Create the schema represented by a StructType matching the structure of
Rows in the RDD created in Step 1.
Apply the schema to the RDD of Rows via createDataFrame method provided
by SparkSession.

For example:
import org.apache.spark.sql.Row

import org.apache.spark.sql.types._

// Create an RDD
val peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")

// The schema is encoded in a string
val schemaString = "name age"

// Generate the schema based on the string of schema
val fields = schemaString.split(" ")
  .map(fieldName => StructField(fieldName, StringType, nullable = true))
val schema = StructType(fields)

// Convert records of the RDD (people) to Rows
val rowRDD = peopleRDD
  .map(_.split(","))
  .map(attributes => Row(attributes(0), attributes(1).trim))

// Apply the schema to the RDD
val peopleDF = spark.createDataFrame(rowRDD, schema)

// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView("people")

// SQL can be run over a temporary view created using DataFrames
val results = spark.sql("SELECT name FROM people")

// The results of SQL queries are DataFrames and support all the normal RDD operations
// The columns of a row in the result can be accessed by field index or by field name
results.map(attributes => "Name: " + attributes(0)).show()
// +-------------+
// |        value|
// +-------------+
// |Name: Michael|
// |   Name: Andy|
// | Name: Justin|
// +-------------+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.


When JavaBean classes cannot be defined ahead of time (for example,
the structure of records is encoded in a string, or a text dataset will be parsed and
fields will be projected differently for different users),
a Dataset<Row> can be created programmatically with three steps.

Create an RDD of Rows from the original RDD;
Create the schema represented by a StructType matching the structure of
Rows in the RDD created in Step 1.
Apply the schema to the RDD of Rows via createDataFrame method provided
by SparkSession.

For example:
import java.util.ArrayList;
import java.util.List;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

// Create an RDD
JavaRDD<String> peopleRDD = spark.sparkContext()
  .textFile("examples/src/main/resources/people.txt", 1)
  .toJavaRDD();

// The schema is encoded in a string
String schemaString = "name age";

// Generate the schema based on the string of schema
List<StructField> fields = new ArrayList<>();
for (String fieldName : schemaString.split(" ")) {
  StructField field = DataTypes.createStructField(fieldName, DataTypes.StringType, true);
  fields.add(field);
}
StructType schema = DataTypes.createStructType(fields);

// Convert records of the RDD (people) to Rows
JavaRDD<Row> rowRDD = peopleRDD.map((Function<String, Row>) record -> {
  String[] attributes = record.split(",");
  return RowFactory.create(attributes[0], attributes[1].trim());
});

// Apply the schema to the RDD
Dataset<Row> peopleDataFrame = spark.createDataFrame(rowRDD, schema);

// Creates a temporary view using the DataFrame
peopleDataFrame.createOrReplaceTempView("people");

// SQL can be run over a temporary view created using DataFrames
Dataset<Row> results = spark.sql("SELECT name FROM people");

// The results of SQL queries are DataFrames and support all the normal RDD operations
// The columns of a row in the result can be accessed by field index or by field name
Dataset<String> namesDS = results.map(
    (MapFunction<Row, String>) row -> "Name: " + row.getString(0),
    Encoders.STRING());
namesDS.show();
// +-------------+
// |        value|
// +-------------+
// |Name: Michael|
// |   Name: Andy|
// | Name: Justin|
// +-------------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java" in the Spark repo.


Scalar Functions
Scalar functions are functions that return a single value per row, as opposed to aggregation functions, which return a value for a group of rows. Spark SQL supports a variety of Built-in Scalar Functions. It also supports User Defined Scalar Functions.
Aggregate Functions
Aggregate functions are functions that return a single value on a group of rows. The Built-in Aggregate Functions provide common aggregations such as count(), count_distinct(), avg(), max(), min(), etc.
Users are not limited to the predefined aggregate functions and can create their own. For more details
about user defined aggregate functions, please refer to the documentation of
User Defined Aggregate Functions.




















  




Migration Guide: SQL, Datasets and DataFrame - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        







Migration Guide: SQL, Datasets and DataFrame

Upgrading from Spark SQL 3.5.3 to 3.5.4
Upgrading from Spark SQL 3.5.1 to 3.5.2
Upgrading from Spark SQL 3.5.0 to 3.5.1
Upgrading from Spark SQL 3.4 to 3.5
Upgrading from Spark SQL 3.3 to 3.4
Upgrading from Spark SQL 3.2 to 3.3
Upgrading from Spark SQL 3.1 to 3.2
Upgrading from Spark SQL 3.0 to 3.1
Upgrading from Spark SQL 3.0.1 to 3.0.2
Upgrading from Spark SQL 3.0 to 3.0.1
Upgrading from Spark SQL 2.4 to 3.0 
Dataset/DataFrame APIs
DDL Statements
UDFs and Built-in Functions
Query Engine
Data Sources
Others


Upgrading from Spark SQL 2.4.7 to 2.4.8
Upgrading from Spark SQL 2.4.5 to 2.4.6
Upgrading from Spark SQL 2.4.4 to 2.4.5
Upgrading from Spark SQL 2.4.3 to 2.4.4
Upgrading from Spark SQL 2.4 to 2.4.1
Upgrading from Spark SQL 2.3 to 2.4
Upgrading from Spark SQL 2.2 to 2.3
Upgrading from Spark SQL 2.1 to 2.2
Upgrading from Spark SQL 2.0 to 2.1
Upgrading from Spark SQL 1.6 to 2.0
Upgrading from Spark SQL 1.5 to 1.6
Upgrading from Spark SQL 1.4 to 1.5
Upgrading from Spark SQL 1.3 to 1.4
Upgrading from Spark SQL 1.0-1.2 to 1.3
Compatibility with Apache Hive

Upgrading from Spark SQL 3.5.3 to 3.5.4

Since Spark 3.5.4, when reading SQL tables hits org.apache.hadoop.security.AccessControlException and org.apache.hadoop.hdfs.BlockMissingException, the exception will be thrown and fail the task, even if spark.sql.files.ignoreCorruptFiles is set to true.

Upgrading from Spark SQL 3.5.1 to 3.5.2

Since 3.5.2, MySQL JDBC datasource will read TINYINT UNSIGNED as ShortType, while in 3.5.1, it was wrongly read as ByteType.

Upgrading from Spark SQL 3.5.0 to 3.5.1

Since Spark 3.5.1, MySQL JDBC datasource will read TINYINT(n > 1) and TINYINT UNSIGNED as ByteType, while in Spark 3.5.0 and below, they were read as IntegerType. To restore the previous behavior, you can cast the column to the old type.

Upgrading from Spark SQL 3.4 to 3.5

Since Spark 3.5, the JDBC options related to DS V2 pushdown are true by default. These options include: pushDownAggregate, pushDownLimit, pushDownOffset and pushDownTableSample. To restore the legacy behavior, please set them to false. e.g. set spark.sql.catalog.your_catalog_name.pushDownAggregate to false.
Since Spark 3.5, Spark thrift server will interrupt task when canceling a running statement. To restore the previous behavior, set spark.sql.thriftServer.interruptOnCancel to false.
Since Spark 3.5, Row’s json and prettyJson methods are moved to ToJsonUtil.
Since Spark 3.5, the plan field is moved from AnalysisException to EnhancedAnalysisException.
Since Spark 3.5, spark.sql.optimizer.canChangeCachedPlanOutputPartitioning is enabled by default. To restore the previous behavior, set spark.sql.optimizer.canChangeCachedPlanOutputPartitioning to false.
Since Spark 3.5, the array_insert function is 1-based for negative indexes. It inserts new element at the end of input arrays for the index -1. To restore the previous behavior, set spark.sql.legacy.negativeIndexInArrayInsert to true.
Since Spark 3.5, the Avro will throw AnalysisException when reading Interval types as Date or Timestamp types, or reading Decimal types with lower precision. To restore the legacy behavior, set spark.sql.legacy.avro.allowIncompatibleSchema to true

Upgrading from Spark SQL 3.3 to 3.4

Since Spark 3.4, INSERT INTO commands with explicit column lists comprising fewer columns than the target table will automatically add the corresponding default values for the remaining columns (or NULL for any column lacking an explicitly-assigned default value). In Spark 3.3 or earlier, these commands would have failed returning errors reporting that the number of provided columns does not match the number of columns in the target table. Note that disabling spark.sql.defaultColumn.useNullsForMissingDefaultValues will restore the previous behavior.
Since Spark 3.4, Number or Number(*) from Teradata will be treated as Decimal(38,18). In Spark 3.3 or earlier, Number or Number(*) from Teradata will be treated as Decimal(38, 0), in which case the fractional part will be removed.
Since Spark 3.4, v1 database, table, permanent view and function identifier will include ‘spark_catalog’ as the catalog name if database is defined, e.g. a table identifier will be: spark_catalog.default.t. To restore the legacy behavior, set spark.sql.legacy.v1IdentifierNoCatalog to true.
Since Spark 3.4, when ANSI SQL mode(configuration spark.sql.ansi.enabled) is on, Spark SQL always returns NULL result on getting a map value with a non-existing key. In Spark 3.3 or earlier, there will be an error.
Since Spark 3.4, the SQL CLI spark-sql does not print the prefix Error in query: before the error message of AnalysisException.
Since Spark 3.4, split function ignores trailing empty strings when regex parameter is empty.
Since Spark 3.4, the to_binary function throws error for a malformed str input. Use try_to_binary to tolerate malformed input and return NULL instead.
    
Valid Base64 string should include symbols from in base64 alphabet (A-Za-z0-9+/), optional padding (=), and optional whitespaces. Whitespaces are skipped in conversion except when they are preceded by padding symbol(s). If padding is present it should conclude the string and follow rules described in RFC 4648 § 4.
Valid hexadecimal strings should include only allowed symbols (0-9A-Fa-f).
Valid values for fmt are case-insensitive hex, base64, utf-8, utf8.


Since Spark 3.4, Spark throws only PartitionsAlreadyExistException when it creates partitions but some of them exist already. In Spark 3.3 or earlier, Spark can throw either PartitionsAlreadyExistException or PartitionAlreadyExistsException.
Since Spark 3.4, Spark will do validation for partition spec in ALTER PARTITION to follow the behavior of spark.sql.storeAssignmentPolicy which may cause an exception if type conversion fails, e.g. ALTER TABLE .. ADD PARTITION(p='a') if column p is int type. To restore the legacy behavior, set spark.sql.legacy.skipTypeValidationOnAlterPartition to true.
Since Spark 3.4, vectorized readers are enabled by default for the nested data types (array, map and struct). To restore the legacy behavior, set spark.sql.orc.enableNestedColumnVectorizedReader and spark.sql.parquet.enableNestedColumnVectorizedReader to false.
Since Spark 3.4, BinaryType is not supported in CSV datasource. In Spark 3.3 or earlier, users can write binary columns in CSV datasource, but the output content in CSV files is Object.toString() which is meaningless; meanwhile, if users read CSV tables with binary columns, Spark will throw an Unsupported type: binary exception.
Since Spark 3.4, bloom filter joins are enabled by default. To restore the legacy behavior, set spark.sql.optimizer.runtime.bloomFilter.enabled to false.
Since Spark 3.4, when schema inference on external Parquet files, INT64 timestamps with annotation isAdjustedToUTC=false will be inferred as TimestampNTZ type instead of Timestamp type. To restore the legacy behavior, set spark.sql.parquet.inferTimestampNTZ.enabled to false.
Since Spark 3.4, the behavior for CREATE TABLE AS SELECT ... is changed from OVERWRITE to APPEND when spark.sql.legacy.allowNonEmptyLocationInCTAS is set to true. Users are recommended to avoid CTAS with a non-empty table location.

Upgrading from Spark SQL 3.2 to 3.3


Since Spark 3.3, the histogram_numeric function in Spark SQL returns an output type of an array of structs (x, y), where the type of the ‘x’ field in the return value is propagated from the input values consumed in the aggregate function. In Spark 3.2 or earlier, ‘x’ always had double type. Optionally, use the configuration spark.sql.legacy.histogramNumericPropagateInputType since Spark 3.3 to revert back to the previous behavior.


Since Spark 3.3, DayTimeIntervalType in Spark SQL is mapped to Arrow’s Duration type in ArrowWriter and ArrowColumnVector developer APIs. Previously, DayTimeIntervalType was mapped to Arrow’s Interval type which does not match with the types of other languages Spark SQL maps. For example, DayTimeIntervalType is mapped to java.time.Duration in Java.


Since Spark 3.3, the functions lpad and rpad have been overloaded to support byte sequences. When the first argument is a byte sequence, the optional padding pattern must also be a byte sequence and the result is a BINARY value. The default padding pattern in this case is the zero byte. To restore the legacy behavior of always returning string types, set spark.sql.legacy.lpadRpadAlwaysReturnString to true.


Since Spark 3.3, Spark turns a non-nullable schema into nullable for API DataFrameReader.schema(schema: StructType).json(jsonDataset: Dataset[String]) and DataFrameReader.schema(schema: StructType).csv(csvDataset: Dataset[String]) when the schema is specified by the user and contains non-nullable fields. To restore the legacy behavior of respecting the nullability, set spark.sql.legacy.respectNullabilityInTextDatasetConversion to true.


Since Spark 3.3, when the date or timestamp pattern is not specified, Spark converts an input string to a date/timestamp using the CAST expression approach. The changes affect CSV/JSON datasources and parsing of partition values. In Spark 3.2 or earlier, when the date or timestamp pattern is not set, Spark uses the default patterns: yyyy-MM-dd for dates and yyyy-MM-dd HH:mm:ss for timestamps. After the changes, Spark still recognizes the pattern together with
Date patterns:

[+-]yyyy*
[+-]yyyy*-[m]m
[+-]yyyy*-[m]m-[d]d
[+-]yyyy*-[m]m-[d]d 
[+-]yyyy*-[m]m-[d]d *
[+-]yyyy*-[m]m-[d]dT*

Timestamp patterns:

[+-]yyyy*
[+-]yyyy*-[m]m
[+-]yyyy*-[m]m-[d]d
[+-]yyyy*-[m]m-[d]d 
[+-]yyyy*-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]
[+-]yyyy*-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]
[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]
T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]



Since Spark 3.3, the strfmt in format_string(strfmt, obj, ...) and printf(strfmt, obj, ...) will no longer support to use “0$” to specify the first argument, the first argument should always reference by “1$” when use argument index to indicating the position of the argument in the argument list.


Since Spark 3.3, nulls are written as empty strings in CSV data source by default. In Spark 3.2 or earlier, nulls were written as empty strings as quoted empty strings, "". To restore the previous behavior, set nullValue to "", or set the configuration spark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv to true.


Since Spark 3.3, DESCRIBE FUNCTION fails if the function does not exist. In Spark 3.2 or earlier, DESCRIBE FUNCTION can still run and print “Function: func_name not found”.


Since Spark 3.3, the table property external becomes reserved. Certain commands will fail if you specify the external property, such as CREATE TABLE ... TBLPROPERTIES and ALTER TABLE ... SET TBLPROPERTIES. In Spark 3.2 and earlier, the table property external is silently ignored. You can set spark.sql.legacy.notReserveProperties to true to restore the old behavior.


Since Spark 3.3, DROP FUNCTION fails if the function name matches one of the built-in functions’ name and is not qualified. In Spark 3.2 or earlier, DROP FUNCTION can still drop a persistent function even if the name is not qualified and is the same as a built-in function’s name.


Since Spark 3.3, when reading values from a JSON attribute defined as FloatType or DoubleType, the strings "+Infinity", "+INF", and "-INF" are now parsed to the appropriate values, in addition to the already supported "Infinity" and "-Infinity" variations. This change was made to improve consistency with Jackson’s parsing of the unquoted versions of these values. Also, the allowNonNumericNumbers option is now respected so these strings will now be considered invalid if this option is disabled.


Since Spark 3.3, Spark will try to use built-in data source writer instead of Hive serde in INSERT OVERWRITE DIRECTORY. This behavior is effective only if spark.sql.hive.convertMetastoreParquet or spark.sql.hive.convertMetastoreOrc is enabled respectively for Parquet and ORC formats. To restore the behavior before Spark 3.3, you can set spark.sql.hive.convertMetastoreInsertDir to false.


Since Spark 3.3, the precision of the return type of round-like functions has been fixed. This may cause Spark throw AnalysisException of the CANNOT_UP_CAST_DATATYPE error class when using views created by prior versions. In such cases, you need to recreate the views using ALTER VIEW AS or CREATE OR REPLACE VIEW AS with newer Spark versions.


Since Spark 3.3, the unbase64 function throws error for a malformed str input. Use try_to_binary(<str>, 'base64') to tolerate malformed input and return NULL instead. In Spark 3.2 and earlier, the unbase64 function returns a best-efforts result for a malformed str input.


Since Spark 3.3, when reading Parquet files that were not produced by Spark, Parquet timestamp columns with annotation isAdjustedToUTC = false are inferred as TIMESTAMP_NTZ type during schema inference. In Spark 3.2 and earlier, these columns are inferred as TIMESTAMP type. To restore the behavior before Spark 3.3, you can set spark.sql.parquet.inferTimestampNTZ.enabled to false.


Since Spark 3.3.1 and 3.2.3, for SELECT ... GROUP BY a GROUPING SETS (b)-style SQL statements, grouping__id returns different values from Apache Spark 3.2.0, 3.2.1, 3.2.2, and 3.3.0. It computes based on user-given group-by expressions plus grouping set columns. To restore the behavior before 3.3.1 and 3.2.3, you can set spark.sql.legacy.groupingIdWithAppendedUserGroupBy. For details, see SPARK-40218 and SPARK-40562.


Upgrading from Spark SQL 3.1 to 3.2


Since Spark 3.2, ADD FILE/JAR/ARCHIVE commands require each path to be enclosed by " or ' if the path contains whitespaces.


Since Spark 3.2, all the supported JDBC dialects use StringType for ROWID. In Spark 3.1 or earlier, Oracle dialect uses StringType and the other dialects use LongType.


In Spark 3.2, PostgreSQL JDBC dialect uses StringType for MONEY and MONEY[] is not supported due to the JDBC driver for PostgreSQL can’t handle those types properly. In Spark 3.1 or earlier, DoubleType and ArrayType of DoubleType are used respectively.


In Spark 3.2, spark.sql.adaptive.enabled is enabled by default. To restore the behavior before Spark 3.2, you can set spark.sql.adaptive.enabled to false.

In Spark 3.2, the following meta-characters are escaped in the show() action. In Spark 3.1 or earlier, the following metacharacters are output as it is.
    
\n (new line)
\r (carriage ret)
\t (horizontal tab)
\f (form feed)
\b (backspace)
\u000B (vertical tab)
\u0007 (bell)



In Spark 3.2, ALTER TABLE .. RENAME TO PARTITION throws PartitionAlreadyExistsException instead of AnalysisException for tables from Hive external when the target partition already exists.


In Spark 3.2, script transform default FIELD DELIMIT is \u0001 for no serde mode, serde property field.delim is \t for Hive serde mode when user specifies serde. In Spark 3.1 or earlier, the default FIELD DELIMIT is \t, serde property field.delim is \u0001 for Hive serde mode when user specifies serde.


In Spark 3.2, the auto-generated Cast (such as those added by type coercion rules) will be stripped when generating column alias names. E.g., sql("SELECT floor(1)").columns will be FLOOR(1) instead of FLOOR(CAST(1 AS DOUBLE)).


In Spark 3.2, the output schema of SHOW TABLES becomes namespace: string, tableName: string, isTemporary: boolean. In Spark 3.1 or earlier, the namespace field was named database for the builtin catalog, and there is no isTemporary field for v2 catalogs. To restore the old schema with the builtin catalog, you can set spark.sql.legacy.keepCommandOutputSchema to true.


In Spark 3.2, the output schema of SHOW TABLE EXTENDED becomes namespace: string, tableName: string, isTemporary: boolean, information: string. In Spark 3.1 or earlier, the namespace field was named database for the builtin catalog, and no change for the v2 catalogs. To restore the old schema with the builtin catalog, you can set spark.sql.legacy.keepCommandOutputSchema to true.


In Spark 3.2, the output schema of SHOW TBLPROPERTIES becomes key: string, value: string whether you specify the table property key or not. In Spark 3.1 and earlier, the output schema of SHOW TBLPROPERTIES is value: string when you specify the table property key. To restore the old schema with the builtin catalog, you can set spark.sql.legacy.keepCommandOutputSchema to true.


In Spark 3.2, the output schema of DESCRIBE NAMESPACE becomes info_name: string, info_value: string. In Spark 3.1 or earlier, the info_name field was named database_description_item and the info_value field was named database_description_value for the builtin catalog. To restore the old schema with the builtin catalog, you can set spark.sql.legacy.keepCommandOutputSchema to true.

In Spark 3.2, table refreshing clears cached data of the table as well as of all its dependents such as views while keeping the dependents cached. The following commands perform table refreshing:
    
ALTER TABLE .. ADD PARTITION
ALTER TABLE .. RENAME PARTITION
ALTER TABLE .. DROP PARTITION
ALTER TABLE .. RECOVER PARTITIONS
MSCK REPAIR TABLE
LOAD DATA
REFRESH TABLE
TRUNCATE TABLE
and the method spark.catalog.refreshTable
In Spark 3.1 and earlier, table refreshing leaves dependents uncached.



In Spark 3.2, the usage of count(tblName.*) is blocked to avoid producing ambiguous results. Because count(*) and count(tblName.*) will output differently if there is any null values. To restore the behavior before Spark 3.2, you can set spark.sql.legacy.allowStarWithSingleTableIdentifierInCount to true.


In Spark 3.2, we support typed literals in the partition spec of INSERT and ADD/DROP/RENAME PARTITION. For example, ADD PARTITION(dt = date'2020-01-01') adds a partition with date value 2020-01-01. In Spark 3.1 and earlier, the partition value will be parsed as string value date '2020-01-01', which is an illegal date value, and we add a partition with null value at the end.


In Spark 3.2, DataFrameNaFunctions.replace() no longer uses exact string match for the input column names, to match the SQL syntax and support qualified column names. Input column name having a dot in the name (not nested) needs to be escaped with backtick `. Now, it throws AnalysisException if the column is not found in the data frame schema. It also throws IllegalArgumentException if the input column name is a nested column. In Spark 3.1 and earlier, it used to ignore invalid input column name and nested column name.


In Spark 3.2, the dates subtraction expression such as date1 - date2 returns values of DayTimeIntervalType. In Spark 3.1 and earlier, the returned type is CalendarIntervalType. To restore the behavior before Spark 3.2, you can set spark.sql.legacy.interval.enabled to true.


In Spark 3.2, the timestamps subtraction expression such as timestamp '2021-03-31 23:48:00' - timestamp '2021-01-01 00:00:00' returns values of DayTimeIntervalType. In Spark 3.1 and earlier, the type of the same expression is CalendarIntervalType. To restore the behavior before Spark 3.2, you can set spark.sql.legacy.interval.enabled to true.


In Spark 3.2, CREATE TABLE .. LIKE .. command can not use reserved properties. You need their specific clauses to specify them, for example, CREATE TABLE test1 LIKE test LOCATION 'some path'. You can set spark.sql.legacy.notReserveProperties to true to ignore the ParseException, in this case, these properties will be silently removed, for example: TBLPROPERTIES('owner'='yao') will have no effect. In Spark version 3.1 and below, the reserved properties can be used in CREATE TABLE .. LIKE .. command but have no side effects, for example, TBLPROPERTIES('location'='/tmp') does not change the location of the table but only create a headless property just like 'a'='b'.


In Spark 3.2, TRANSFORM operator can’t support alias in inputs. In Spark 3.1 and earlier, we can write script transform like SELECT TRANSFORM(a AS c1, b AS c2) USING 'cat' FROM TBL.


In Spark 3.2, TRANSFORM operator can support ArrayType/MapType/StructType without Hive SerDe, in this mode, we use StructsToJson to convert ArrayType/MapType/StructType column to STRING and use JsonToStructs to parse STRING to ArrayType/MapType/StructType. In Spark 3.1, Spark just support case ArrayType/MapType/StructType column as STRING but can’t support parse STRING to ArrayType/MapType/StructType output columns.


In Spark 3.2, the unit-to-unit interval literals like INTERVAL '1-1' YEAR TO MONTH and the unit list interval literals like INTERVAL '3' DAYS '1' HOUR are converted to ANSI interval types: YearMonthIntervalType or DayTimeIntervalType. In Spark 3.1 and earlier, such interval literals are converted to CalendarIntervalType. To restore the behavior before Spark 3.2, you can set spark.sql.legacy.interval.enabled to true.


In Spark 3.2, the unit list interval literals can not mix year-month fields (YEAR and MONTH) and day-time fields (WEEK, DAY, …, MICROSECOND). For example, INTERVAL 1 month 1 hour is invalid in Spark 3.2. In Spark 3.1 and earlier, there is no such limitation and the literal returns value of CalendarIntervalType. To restore the behavior before Spark 3.2, you can set spark.sql.legacy.interval.enabled to true.


In Spark 3.2, Spark supports DayTimeIntervalType and YearMonthIntervalType as inputs and outputs of TRANSFORM clause in Hive SERDE mode, the behavior is different between Hive SERDE mode and ROW FORMAT DELIMITED mode when these two types are used as inputs. In Hive SERDE mode, DayTimeIntervalType column is converted to HiveIntervalDayTime, its string format is [-]?d h:m:s.n, but in ROW FORMAT DELIMITED mode the format is INTERVAL '[-]?d h:m:s.n' DAY TO TIME. In Hive SERDE mode, YearMonthIntervalType column is converted to HiveIntervalYearMonth, its string format is [-]?y-m, but in ROW FORMAT DELIMITED mode the format is INTERVAL '[-]?y-m' YEAR TO MONTH.


In Spark 3.2, hash(0) == hash(-0) for floating point types. Previously, different values were generated.


In Spark 3.2, CREATE TABLE AS SELECT with non-empty LOCATION will throw AnalysisException. To restore the behavior before Spark 3.2, you can set spark.sql.legacy.allowNonEmptyLocationInCTAS to true.


In Spark 3.2, special datetime values such as epoch, today, yesterday, tomorrow, and now are supported in typed literals or in cast of foldable strings only, for instance, select timestamp'now' or select cast('today' as date). In Spark 3.1 and 3.0, such special values are supported in any casts of strings to dates/timestamps. To keep these special values as dates/timestamps in Spark 3.1 and 3.0, you should replace them manually, e.g. if (c in ('now', 'today'), current_date(), cast(c as date)).


In Spark 3.2, FloatType is mapped to FLOAT in MySQL. Prior to this, it used to be mapped to REAL, which is by default a synonym to DOUBLE PRECISION in MySQL.


In Spark 3.2, the query executions triggered by DataFrameWriter are always named command when being sent to QueryExecutionListener. In Spark 3.1 and earlier, the name is one of save, insertInto, saveAsTable.


In Spark 3.2, Dataset.unionByName with allowMissingColumns set to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.


In Spark 3.2, create/alter view will fail if the input query output columns contain auto-generated alias. This is necessary to make sure the query output column names are stable across different spark versions. To restore the behavior before Spark 3.2, set spark.sql.legacy.allowAutoGeneratedAliasForView to true.

In Spark 3.2, date +/- interval with only day-time fields such as date '2011-11-11' + interval 12 hours returns timestamp. In Spark 3.1 and earlier, the same expression returns date. To restore the behavior before Spark 3.2, you can use cast to convert timestamp as date.

Upgrading from Spark SQL 3.0 to 3.1


In Spark 3.1, statistical aggregation function includes std, stddev, stddev_samp, variance, var_samp, skewness, kurtosis, covar_samp, corr will return NULL instead of Double.NaN when DivideByZero occurs during expression evaluation, for example, when stddev_samp applied on a single element set. In Spark version 3.0 and earlier, it will return Double.NaN in such case. To restore the behavior before Spark 3.1, you can set spark.sql.legacy.statisticalAggregate to true.


In Spark 3.1, grouping_id() returns long values. In Spark version 3.0 and earlier, this function returns int values. To restore the behavior before Spark 3.1, you can set spark.sql.legacy.integerGroupingId to true.


In Spark 3.1, SQL UI data adopts the formatted mode for the query plan explain results. To restore the behavior before Spark 3.1, you can set spark.sql.ui.explainMode to extended.


In Spark 3.1, from_unixtime, unix_timestamp,to_unix_timestamp, to_timestamp and to_date will fail if the specified datetime pattern is invalid. In Spark 3.0 or earlier, they result NULL.


In Spark 3.1, the Parquet, ORC, Avro and JSON datasources throw the exception org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema in read if they detect duplicate names in top-level columns as well in nested structures. The datasources take into account the SQL config spark.sql.caseSensitive while detecting column name duplicates.


In Spark 3.1, structs and maps are wrapped by the {} brackets in casting them to strings. For instance, the show() action and the CAST expression use such brackets. In Spark 3.0 and earlier, the [] brackets are used for the same purpose. To restore the behavior before Spark 3.1, you can set spark.sql.legacy.castComplexTypesToString.enabled to true.


In Spark 3.1, NULL elements of structures, arrays and maps are converted to “null” in casting them to strings. In Spark 3.0 or earlier, NULL elements are converted to empty strings. To restore the behavior before Spark 3.1, you can set spark.sql.legacy.castComplexTypesToString.enabled to true.


In Spark 3.1, when spark.sql.ansi.enabled is false, Spark always returns null if the sum of decimal type column overflows. In Spark 3.0 or earlier, in the case, the sum of decimal type column may return null or incorrect result, or even fails at runtime (depending on the actual query plan execution).


In Spark 3.1, path option cannot coexist when the following methods are called with path parameter(s): DataFrameReader.load(), DataFrameWriter.save(), DataStreamReader.load(), or DataStreamWriter.start(). In addition, paths option cannot coexist for DataFrameReader.load(). For example, spark.read.format("csv").option("path", "/tmp").load("/tmp2") or spark.read.option("path", "/tmp").csv("/tmp2") will throw org.apache.spark.sql.AnalysisException. In Spark version 3.0 and below, path option is overwritten if one path parameter is passed to above methods; path option is added to the overall paths if multiple path parameters are passed to DataFrameReader.load(). To restore the behavior before Spark 3.1, you can set spark.sql.legacy.pathOptionBehavior.enabled to true.


In Spark 3.1, IllegalArgumentException is returned for the incomplete interval literals, e.g. INTERVAL '1', INTERVAL '1 DAY 2', which are invalid. In Spark 3.0, these literals result in NULLs.


In Spark 3.1, we remove the built-in Hive 1.2. You need to migrate your custom SerDes to Hive 2.3. See HIVE-15167 for more details.


In Spark 3.1, loading and saving of timestamps from/to parquet files fails if the timestamps are before 1900-01-01 00:00:00Z, and loaded (saved) as the INT96 type. In Spark 3.0, the actions don’t fail but might lead to shifting of the input timestamps due to rebasing from/to Julian to/from Proleptic Gregorian calendar. To restore the behavior before Spark 3.1, you can set spark.sql.legacy.parquet.int96RebaseModeInRead or/and spark.sql.legacy.parquet.int96RebaseModeInWrite to LEGACY.


In Spark 3.1, the schema_of_json and schema_of_csv functions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns a catalog string without field quoting and in lower case.


In Spark 3.1, refreshing a table will trigger an uncache operation for all other caches that reference the table, even if the table itself is not cached. In Spark 3.0 the operation will only be triggered if the table itself is cached.


In Spark 3.1, creating or altering a permanent view will capture runtime SQL configs and store them as view properties. These configs will be applied during the parsing and analysis phases of the view resolution. To restore the behavior before Spark 3.1, you can set spark.sql.legacy.useCurrentConfigsForView to true.


In Spark 3.1, the temporary view will have same behaviors with the permanent view, i.e. capture and store runtime SQL configs, SQL text, catalog and namespace. The captured view properties will be applied during the parsing and analysis phases of the view resolution. To restore the behavior before Spark 3.1, you can set spark.sql.legacy.storeAnalyzedPlanForView to true.


In Spark 3.1, temporary view created via CACHE TABLE ... AS SELECT will also have the same behavior with permanent view. In particular, when the temporary view is dropped, Spark will invalidate all its cache dependents, as well as the cache for the temporary view itself. This is different from Spark 3.0 and below, which only does the latter. To restore the previous behavior, you can set spark.sql.legacy.storeAnalyzedPlanForView to true.


Since Spark 3.1, CHAR/CHARACTER and VARCHAR types are supported in the table schema. Table scan/insertion will respect the char/varchar semantic. If char/varchar is used in places other than table schema, an exception will be thrown (CAST is an exception that simply treats char/varchar as string like before). To restore the behavior before Spark 3.1, which treats them as STRING types and ignores a length parameter, e.g. CHAR(4), you can set spark.sql.legacy.charVarcharAsString to true.


In Spark 3.1, AnalysisException is replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:

ALTER TABLE .. ADD PARTITION throws PartitionsAlreadyExistException if new partition exists already
ALTER TABLE .. DROP PARTITION throws NoSuchPartitionsException for not existing partitions



Upgrading from Spark SQL 3.0.1 to 3.0.2

In Spark 3.0.2, AnalysisException is replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:
    
ALTER TABLE .. ADD PARTITION throws PartitionsAlreadyExistException if new partition exists already
ALTER TABLE .. DROP PARTITION throws NoSuchPartitionsException for not existing partitions



In Spark 3.0.2, PARTITION(col=null) is always parsed as a null literal in the partition spec. In Spark 3.0.1 or earlier, it is parsed as a string literal of its text representation, e.g., string “null”, if the partition column is string type. To restore the legacy behavior, you can set spark.sql.legacy.parseNullPartitionSpecAsStringLiteral as true.

In Spark 3.0.2, the output schema of SHOW DATABASES becomes namespace: string. In Spark version 3.0.1 and earlier, the schema was databaseName: string. Since Spark 3.0.2, you can restore the old schema by setting spark.sql.legacy.keepCommandOutputSchema to true.

Upgrading from Spark SQL 3.0 to 3.0.1


In Spark 3.0, JSON datasource and JSON function schema_of_json infer TimestampType from string values if they match to the pattern defined by the JSON option timestampFormat. Since version 3.0.1, the timestamp type inference is disabled by default. Set the JSON option inferTimestamp to true to enable such type inference.


In Spark 3.0, when casting string to integral types(tinyint, smallint, int and bigint), datetime types(date, timestamp and interval) and boolean type, the leading and trailing characters (<= ASCII 32) will be trimmed. For example, cast('\b1\b' as int) results 1. Since Spark 3.0.1, only the leading and trailing whitespace ASCII characters will be trimmed. For example, cast('\t1\t' as int) results 1 but cast('\b1\b' as int) results NULL.


Upgrading from Spark SQL 2.4 to 3.0
Dataset/DataFrame APIs


In Spark 3.0, the Dataset and DataFrame API unionAll is no longer deprecated. It is an alias for union.


In Spark 2.4 and below, Dataset.groupByKey results to a grouped dataset with key attribute is wrongly named as “value”, if the key is non-struct type, for example, int, string, array, etc. This is counterintuitive and makes the schema of aggregation queries unexpected. For example, the schema of ds.groupByKey(...).count() is (value, count). Since Spark 3.0, we name the grouping attribute to “key”. The old behavior is preserved under a newly added configuration spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue with a default value of false.


In Spark 3.0, the column metadata will always be propagated in the API Column.name and Column.as. In Spark version 2.4 and earlier, the metadata of NamedExpression is set as the explicitMetadata for the new column at the time the API is called, it won’t change even if the underlying NamedExpression changes metadata. To restore the behavior before Spark 3.0, you can use the API as(alias: String, metadata: Metadata) with explicit metadata.


When turning a Dataset to another Dataset, Spark will up cast the fields in the original Dataset to the type of corresponding fields in the target DataSet. In version 2.4 and earlier, this up cast is not very strict, e.g. Seq("str").toDS.as[Int] fails, but Seq("str").toDS.as[Boolean] works and throw NPE during execution. In Spark 3.0, the up cast is stricter and turning String into something else is not allowed, i.e. Seq("str").toDS.as[Boolean] will fail during analysis. To restore the behavior before Spark 3.0, set spark.sql.legacy.doLooseUpcast to true.


DDL Statements


In Spark 3.0, when inserting a value into a table column with a different data type, the type coercion is performed as per ANSI SQL standard. Certain unreasonable type conversions such as converting string to int and double to boolean are disallowed. A runtime exception is thrown if the value is out-of-range for the data type of the column. In Spark version 2.4 and below, type conversions during table insertion are allowed as long as they are valid Cast. When inserting an out-of-range value to an integral field, the low-order bits of the value is inserted(the same as Java/Scala numeric type casting). For example, if 257 is inserted to a field of byte type, the result is 1. The behavior is controlled by the option spark.sql.storeAssignmentPolicy, with a default value as “ANSI”. Setting the option as “Legacy” restores the previous behavior.


The ADD JAR command previously returned a result set with the single value 0. It now returns an empty result set.


Spark 2.4 and below: the SET command works without any warnings even if the specified key is for SparkConf entries and it has no effect because the command does not update SparkConf, but the behavior might confuse users. In 3.0, the command fails if a SparkConf key is used. You can disable such a check by setting spark.sql.legacy.setCommandRejectsSparkCoreConfs to false.


Refreshing a cached table would trigger a table uncache operation and then a table cache (lazily) operation. In Spark version 2.4 and below, the cache name and storage level are not preserved before the uncache operation. Therefore, the cache name and storage level could be changed unexpectedly. In Spark 3.0, cache name and storage level are first preserved for cache recreation. It helps to maintain a consistent cache behavior upon table refreshing.


In Spark 3.0, the properties listing below become reserved; commands fail if you specify reserved properties in places like CREATE DATABASE ... WITH DBPROPERTIES and ALTER TABLE ... SET TBLPROPERTIES. You need their specific clauses to specify them, for example, CREATE DATABASE test COMMENT 'any comment' LOCATION 'some path'. You can set spark.sql.legacy.notReserveProperties to true to ignore the ParseException, in this case, these properties will be silently removed, for example: SET DBPROPERTIES('location'='/tmp') will have no effect. In Spark version 2.4 and below, these properties are neither reserved nor have side effects, for example, SET DBPROPERTIES('location'='/tmp') do not change the location of the database but only create a headless property just like 'a'='b'.



Property (case sensitive)
Database Reserved
Table Reserved
Remarks




provider
no
yes
For tables, use the USING clause to specify it. Once set, it can’t be changed.


location
yes
yes
For databases and tables, use the LOCATION clause to specify it.


owner
yes
yes
For databases and tables, it is determined by the user who runs spark and create the table.





In Spark 3.0, you can use ADD FILE to add file directories as well. Earlier you could add only single files using this command. To restore the behavior of earlier versions, set spark.sql.legacy.addSingleFileInAddFile to true.


In Spark 3.0, SHOW TBLPROPERTIES throws AnalysisException if the table does not exist. In Spark version 2.4 and below, this scenario caused NoSuchTableException.


In Spark 3.0, SHOW CREATE TABLE table_identifier always returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use SHOW CREATE TABLE table_identifier AS SERDE command instead.


In Spark 3.0, column of CHAR type is not allowed in non-Hive-Serde tables, and CREATE/ALTER TABLE commands will fail if CHAR type is detected. Please use STRING type instead. In Spark version 2.4 and below, CHAR type is treated as STRING type and the length parameter is simply ignored.


UDFs and Built-in Functions


In Spark 3.0, the date_add and date_sub functions accepts only int, smallint, tinyint as the 2nd argument; fractional and non-literal strings are not valid anymore, for example: date_add(cast('1964-05-23' as date), '12.34') causes AnalysisException. Note that, string literals are still allowed, but Spark will throw AnalysisException if the string content is not a valid integer. In Spark version 2.4 and below, if the 2nd argument is fractional or string value, it is coerced to int value, and the result is a date value of 1964-06-04.


In Spark 3.0, the function percentile_approx and its alias approx_percentile only accept integral value with range in [1, 2147483647] as its 3rd argument accuracy, fractional and string types are disallowed, for example, percentile_approx(10.0, 0.2, 1.8D) causes AnalysisException. In Spark version 2.4 and below, if accuracy is fractional or string value, it is coerced to an int value, percentile_approx(10.0, 0.2, 1.8D) is operated as percentile_approx(10.0, 0.2, 1) which results in 10.0.


In Spark 3.0, an analysis exception is thrown when hash expressions are applied on elements of MapType. To restore the behavior before Spark 3.0, set spark.sql.legacy.allowHashOnMapType to true.


In Spark 3.0, when the array/map function is called without any parameters, it returns an empty collection with NullType as element type. In Spark version 2.4 and below, it returns an empty collection with StringType as element type. To restore the behavior before Spark 3.0, you can set spark.sql.legacy.createEmptyCollectionUsingStringType to true.


In Spark 3.0, the from_json functions supports two modes - PERMISSIVE and FAILFAST. The modes can be set via the mode option. The default mode became PERMISSIVE. In previous versions, behavior of from_json did not conform to either PERMISSIVE nor FAILFAST, especially in processing of malformed JSON records. For example, the JSON string {"a" 1} with the schema a INT is converted to null by previous versions but Spark 3.0 converts it to Row(null).


In Spark version 2.4 and below, you can create map values with map type key via built-in function such as CreateMap, MapFromArrays, etc. In Spark 3.0, it’s not allowed to create map values with map type key with these built-in functions. Users can use map_entries function to convert map to array<struct<key, value» as a workaround. In addition, users can still read map values with map type key from data source or Java/Scala collections, though it is discouraged.


In Spark version 2.4 and below, you can create a map with duplicated keys via built-in functions like CreateMap, StringToMap, etc. The behavior of map with duplicated keys is undefined, for example, map look up respects the duplicated key appears first, Dataset.collect only keeps the duplicated key appears last, MapKeys returns duplicated keys, etc. In Spark 3.0, Spark throws RuntimeException when duplicated keys are found. You can set spark.sql.mapKeyDedupPolicy to LAST_WIN to deduplicate map keys with last wins policy. Users may still read map values with duplicated keys from data sources which do not enforce it (for example, Parquet), the behavior is undefined.


In Spark 3.0, using org.apache.spark.sql.functions.udf(AnyRef, DataType) is not allowed by default. Remove the return type parameter to automatically switch to typed Scala udf is recommended, or set spark.sql.legacy.allowUntypedScalaUDF to true to keep using it. In Spark version 2.4 and below, if org.apache.spark.sql.functions.udf(AnyRef, DataType) gets a Scala closure with primitive-type argument, the returned UDF returns null if the input values is null. However, in Spark 3.0, the UDF returns the default value of the Java type if the input value is null. For example, val f = udf((x: Int) => x, IntegerType), f($"x") returns null in Spark 2.4 and below if column x is null, and return 0 in Spark 3.0. This behavior change is introduced because Spark 3.0 is built with Scala 2.12 by default.


In Spark 3.0, a higher-order function exists follows the three-valued boolean logic, that is, if the predicate returns any nulls and no true is obtained, then exists returns null instead of false. For example, exists(array(1, null, 3), x -> x % 2 == 0) is null. The previous behavior can be restored by setting spark.sql.legacy.followThreeValuedLogicInArrayExists to false.


In Spark 3.0, the add_months function does not adjust the resulting date to a last day of month if the original date is a last day of months. For example, select add_months(DATE'2019-02-28', 1) results 2019-03-28. In Spark version 2.4 and below, the resulting date is adjusted when the original date is a last day of months. For example, adding a month to 2019-02-28 results in 2019-03-31.


In Spark version 2.4 and below, the current_timestamp function returns a timestamp with millisecond resolution only. In Spark 3.0, the function can return the result with microsecond resolution if the underlying clock available on the system offers such resolution.


In Spark 3.0, a 0-argument Java UDF is executed in the executor side identically with other UDFs. In Spark version 2.4 and below, the 0-argument Java UDF alone was executed in the driver side, and the result was propagated to executors, which might be more performant in some cases but caused inconsistency with a correctness issue in some cases.


The result of java.lang.Math’s log, log1p, exp, expm1, and pow may vary across platforms. In Spark 3.0, the result of the equivalent SQL functions (including related SQL functions like LOG10) return values consistent with java.lang.StrictMath. In virtually all cases this makes no difference in the return value, and the difference is very small, but may not exactly match java.lang.Math on x86 platforms in cases like, for example, log(3.0), whose value varies between Math.log() and StrictMath.log().


In Spark 3.0, the cast function processes string literals such as ‘Infinity’, ‘+Infinity’, ‘-Infinity’, ‘NaN’, ‘Inf’, ‘+Inf’, ‘-Inf’ in a case-insensitive manner when casting the literals to Double or Float type to ensure greater compatibility with other database systems. This behavior change is illustrated in the table below:



Operation
Result before Spark 3.0
Result in Spark 3.0




CAST(‘infinity’ AS DOUBLE)
NULL
Double.PositiveInfinity


CAST(‘+infinity’ AS DOUBLE)
NULL
Double.PositiveInfinity


CAST(‘inf’ AS DOUBLE)
NULL
Double.PositiveInfinity


CAST(‘inf’ AS DOUBLE)
NULL
Double.PositiveInfinity


CAST(‘-infinity’ AS DOUBLE)
NULL
Double.NegativeInfinity


CAST(‘-inf’ AS DOUBLE)
NULL
Double.NegativeInfinity


CAST(‘infinity’ AS FLOAT)
NULL
Float.PositiveInfinity


CAST(‘+infinity’ AS FLOAT)
NULL
Float.PositiveInfinity


CAST(‘inf’ AS FLOAT)
NULL
Float.PositiveInfinity


CAST(‘+inf’ AS FLOAT)
NULL
Float.PositiveInfinity


CAST(‘-infinity’ AS FLOAT)
NULL
Float.NegativeInfinity


CAST(‘-inf’ AS FLOAT)
NULL
Float.NegativeInfinity


CAST(‘nan’ AS DOUBLE)
NULL
Double.NaN


CAST(‘nan’ AS FLOAT)
NULL
Float.NaN





In Spark 3.0, when casting interval values to string type, there is no “interval” prefix, for example, 1 days 2 hours. In Spark version 2.4 and below, the string contains the “interval” prefix like interval 1 days 2 hours.


In Spark 3.0, when casting string value to integral types(tinyint, smallint, int and bigint), datetime types(date, timestamp and interval) and boolean type, the leading and trailing whitespaces (<= ASCII 32) will be trimmed before converted to these type values, for example, cast(' 1\t' as int) results 1, cast(' 1\t' as boolean) results true, cast('2019-10-10\t as date) results the date value 2019-10-10. In Spark version 2.4 and below, when casting string to integrals and booleans, it does not trim the whitespaces from both ends; the foregoing results is null, while to datetimes, only the trailing spaces (= ASCII 32) are removed.


Query Engine


In Spark version 2.4 and below, SQL queries such as FROM <table> or FROM <table> UNION ALL FROM <table> are supported by accident. In hive-style FROM <table> SELECT <expr>, the SELECT clause is not negligible. Neither Hive nor Presto support this syntax. These queries are treated as invalid in Spark 3.0.


In Spark 3.0, the interval literal syntax does not allow multiple from-to units anymore. For example, SELECT INTERVAL '1-1' YEAR TO MONTH '2-2' YEAR TO MONTH' throws parser exception.


In Spark 3.0, numbers written in scientific notation(for example, 1E2) would be parsed as Double. In Spark version 2.4 and below, they’re parsed as Decimal. To restore the behavior before Spark 3.0, you can set spark.sql.legacy.exponentLiteralAsDecimal.enabled to true.


In Spark 3.0, day-time interval strings are converted to intervals with respect to the from and to bounds. If an input string does not match to the pattern defined by specified bounds, the ParseException exception is thrown. For example, interval '2 10:20' hour to minute raises the exception because the expected format is [+|-]h[h]:[m]m. In Spark version 2.4, the from bound was not taken into account, and the to bound was used to truncate the resulted interval. For instance, the day-time interval string from the showed example is converted to interval 10 hours 20 minutes. To restore the behavior before Spark 3.0, you can set spark.sql.legacy.fromDayTimeString.enabled to true.


In Spark 3.0, negative scale of decimal is not allowed by default, for example, data type of literal like 1E10BD is DecimalType(11, 0). In Spark version 2.4 and below, it was DecimalType(2, -9). To restore the behavior before Spark 3.0, you can set spark.sql.legacy.allowNegativeScaleOfDecimal to true.


In Spark 3.0, the unary arithmetic operator plus(+) only accepts string, numeric and interval type values as inputs. Besides, + with an integral string representation is coerced to a double value, for example, +'1' returns 1.0. In Spark version 2.4 and below, this operator is ignored. There is no type checking for it, thus, all type values with a + prefix are valid, for example, + array(1, 2) is valid and results [1, 2]. Besides, there is no type coercion for it at all, for example, in Spark 2.4, the result of +'1' is string 1.


In Spark 3.0, Dataset query fails if it contains ambiguous column reference that is caused by self join. A typical example: val df1 = ...; val df2 = df1.filter(...);, then df1.join(df2, df1("a") > df2("a")) returns an empty result which is quite confusing. This is because Spark cannot resolve Dataset column references that point to tables being self joined, and df1("a") is exactly the same as df2("a") in Spark. To restore the behavior before Spark 3.0, you can set spark.sql.analyzer.failAmbiguousSelfJoin to false.


In Spark 3.0, spark.sql.legacy.ctePrecedencePolicy is introduced to control the behavior for name conflicting in the nested WITH clause. By default value EXCEPTION, Spark throws an AnalysisException, it forces users to choose the specific substitution order they wanted. If set to CORRECTED (which is recommended), inner CTE definitions take precedence over outer definitions. For example, set the config to false, WITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2 returns 2, while setting it to LEGACY, the result is 1 which is the behavior in version 2.4 and below.


In Spark 3.0, configuration spark.sql.crossJoin.enabled become internal configuration, and is true by default, so by default spark won’t raise exception on sql with implicit cross join.


In Spark version 2.4 and below, float/double -0.0 is semantically equal to 0.0, but -0.0 and 0.0 are considered as different values when used in aggregate grouping keys, window partition keys, and join keys. In Spark 3.0, this bug is fixed. For example, Seq(-0.0, 0.0).toDF("d").groupBy("d").count() returns [(0.0, 2)] in Spark 3.0, and [(0.0, 1), (-0.0, 1)] in Spark 2.4 and below.


In Spark version 2.4 and below, invalid time zone ids are silently ignored and replaced by GMT time zone, for example, in the from_utc_timestamp function. In Spark 3.0, such time zone ids are rejected, and Spark throws java.time.DateTimeException.


In Spark 3.0, Proleptic Gregorian calendar is used in parsing, formatting, and converting dates and timestamps as well as in extracting sub-components like years, days and so on. Spark 3.0 uses Java 8 API classes from the java.time packages that are based on ISO chronology. In Spark version 2.4 and below, those operations are performed using the hybrid calendar (Julian + Gregorian. The changes impact on the results for dates before October 15, 1582 (Gregorian) and affect on the following Spark 3.0 API:


Parsing/formatting of timestamp/date strings. This effects on CSV/JSON datasources and on the unix_timestamp, date_format, to_unix_timestamp, from_unixtime, to_date, to_timestamp functions when patterns specified by users is used for parsing and formatting. In Spark 3.0, we define our own pattern strings in Datetime Patterns for Formatting and Parsing,
 which is implemented via DateTimeFormatter under the hood. New implementation performs strict checking of its input. For example, the 2015-07-22 10:00:00 timestamp cannot be parse if pattern is yyyy-MM-dd because the parser does not consume whole input. Another example is the 31/01/2015 00:00 input cannot be parsed by the dd/MM/yyyy hh:mm pattern because hh supposes hours in the range 1-12. In Spark version 2.4 and below, java.text.SimpleDateFormat is used for timestamp/date string conversions, and the supported patterns are described in SimpleDateFormat. The old behavior can be restored by setting spark.sql.legacy.timeParserPolicy to LEGACY.


The weekofyear, weekday, dayofweek, date_trunc, from_utc_timestamp, to_utc_timestamp, and unix_timestamp functions use java.time API for calculation week number of year, day number of week as well for conversion from/to TimestampType values in UTC time zone.


The JDBC options lowerBound and upperBound are converted to TimestampType/DateType values in the same way as casting strings to TimestampType/DateType values. The conversion is based on Proleptic Gregorian calendar, and time zone defined by the SQL config spark.sql.session.timeZone. In Spark version 2.4 and below, the conversion is based on the hybrid calendar (Julian + Gregorian) and on default system time zone.


Formatting TIMESTAMP and DATE literals.


Creating typed TIMESTAMP and DATE literals from strings. In Spark 3.0, string conversion to typed TIMESTAMP/DATE literals is performed via casting to TIMESTAMP/DATE values. For example, TIMESTAMP '2019-12-23 12:59:30' is semantically equal to CAST('2019-12-23 12:59:30' AS TIMESTAMP). When the input string does not contain information about time zone, the time zone from the SQL config spark.sql.session.timeZone is used in that case. In Spark version 2.4 and below, the conversion is based on JVM system time zone. The different sources of the default time zone may change the behavior of typed TIMESTAMP and DATE literals.




In Spark 3.0, TIMESTAMP literals are converted to strings using the SQL config spark.sql.session.timeZone. In Spark version 2.4 and below, the conversion uses the default time zone of the Java virtual machine.


In Spark 3.0, Spark casts String to Date/Timestamp in binary comparisons with dates/timestamps. The previous behavior of casting Date/Timestamp to String can be restored by setting spark.sql.legacy.typeCoercion.datetimeToString.enabled to true.

In Spark 3.0, special values are supported in conversion from strings to dates and timestamps. Those values are simply notational shorthands that are converted to ordinary date or timestamp values when read. The following string values are supported for dates:
    
epoch [zoneId] - 1970-01-01
today [zoneId] - the current date in the time zone specified by spark.sql.session.timeZone
yesterday [zoneId] - the current date - 1
tomorrow [zoneId] - the current date + 1
now - the date of running the current query. It has the same notion as today

For example SELECT date 'tomorrow' - date 'yesterday'; should output 2. Here are special timestamp values:

epoch [zoneId] - 1970-01-01 00:00:00+00 (Unix system time zero)
today [zoneId] - midnight today
yesterday [zoneId] - midnight yesterday
tomorrow [zoneId] - midnight tomorrow
now - current query start time

For example SELECT timestamp 'tomorrow';.


Since Spark 3.0, when using EXTRACT expression to extract the second field from date/timestamp values, the result will be a DecimalType(8, 6) value with 2 digits for second part, and 6 digits for the fractional part with microsecond precision. e.g. extract(second from to_timestamp('2019-09-20 10:10:10.1')) results 10.100000.  In Spark version 2.4 and earlier, it returns an IntegerType value and the result for the former example is 10.


In Spark 3.0, datetime pattern letter F is aligned day of week in month that represents the concept of the count of days within the period of a week where the weeks are aligned to the start of the month. In Spark version 2.4 and earlier, it is week of month that represents the concept of the count of weeks within the month where weeks start on a fixed day-of-week, e.g. 2020-07-30 is 30 days (4 weeks and 2 days) after the first day of the month, so date_format(date '2020-07-30', 'F') returns 2 in Spark 3.0, but as a week count in Spark 2.x, it returns 5 because it locates in the 5th week of July 2020, where week one is 2020-07-01 to 07-04.


In Spark 3.0, Spark will try to use built-in data source writer instead of Hive serde in CTAS. This behavior is effective only if spark.sql.hive.convertMetastoreParquet or spark.sql.hive.convertMetastoreOrc is enabled respectively for Parquet and ORC formats. To restore the behavior before Spark 3.0, you can set spark.sql.hive.convertMetastoreCtas to false.

In Spark 3.0, Spark will try to use built-in data source writer instead of Hive serde to process inserting into partitioned ORC/Parquet tables created by using the HiveSQL syntax. This behavior is effective only if spark.sql.hive.convertMetastoreParquet or spark.sql.hive.convertMetastoreOrc is enabled respectively for Parquet and ORC formats. To restore the behavior before Spark 3.0, you can set spark.sql.hive.convertInsertingPartitionedTable to false.

Data Sources


In Spark version 2.4 and below, when reading a Hive SerDe table with Spark native data sources(parquet/orc), Spark infers the actual file schema and update the table schema in metastore. In Spark 3.0, Spark doesn’t infer the schema anymore. This should not cause any problems to end users, but if it does, set spark.sql.hive.caseSensitiveInferenceMode to INFER_AND_SAVE.


In Spark version 2.4 and below, partition column value is converted as null if it can’t be casted to corresponding user provided schema. In 3.0, partition column value is validated with user provided schema. An exception is thrown if the validation fails. You can disable such validation by setting  spark.sql.sources.validatePartitionColumns to false.


In Spark 3.0, if files or subdirectories disappear during recursive directory listing (that is, they appear in an intermediate listing but then cannot be read or listed during later phases of the recursive directory listing, due to either concurrent file deletions or object store consistency issues) then the listing will fail with an exception unless spark.sql.files.ignoreMissingFiles is true (default false). In previous versions, these missing files or subdirectories would be ignored. Note that this change of behavior only applies during initial table file listing (or during REFRESH TABLE), not during query execution: the net change is that spark.sql.files.ignoreMissingFiles is now obeyed during table file listing / query planning, not only at query execution time.


In Spark version 2.4 and below, the parser of JSON data source treats empty strings as null for some data types such as IntegerType. For FloatType, DoubleType, DateType and TimestampType, it fails on empty strings and throws exceptions. Spark 3.0 disallows empty strings and will throw an exception for data types except for StringType and BinaryType. The previous behavior of allowing an empty string can be restored by setting spark.sql.legacy.json.allowEmptyString.enabled to true.


In Spark version 2.4 and below, JSON datasource and JSON functions like from_json convert a bad JSON record to a row with all nulls in the PERMISSIVE mode when specified schema is StructType. In Spark 3.0, the returned row can contain non-null fields if some of JSON column values were parsed and converted to desired types successfully.


In Spark 3.0, JSON datasource and JSON function schema_of_json infer TimestampType from string values if they match to the pattern defined by the JSON option timestampFormat. Set JSON option inferTimestamp to false to disable such type inference.


In Spark version 2.4 and below, CSV datasource converts a malformed CSV string to a row with all nulls in the PERMISSIVE mode. In Spark 3.0, the returned row can contain non-null fields if some of CSV column values were parsed and converted to desired types successfully.


In Spark 3.0, when Avro files are written with user provided schema, the fields are matched by field names between catalyst schema and Avro schema instead of positions.


In Spark 3.0, when Avro files are written with user provided non-nullable schema, even the catalyst schema is nullable, Spark is still able to write the files. However, Spark throws runtime NullPointerException if any of the records contains null.


In Spark version 2.4 and below, CSV datasource can detect encoding of input files automatically when the files have BOM at the beginning. For instance, CSV datasource can recognize UTF-8, UTF-16BE, UTF-16LE, UTF-32BE and UTF-32LE in the multi-line mode (the CSV option multiLine is set to true). In Spark 3.0, CSV datasource reads input files in encoding specified via the CSV option encoding which has the default value of UTF-8. In this way, if file encoding doesn’t match to the encoding specified via the CSV option, Spark loads the file incorrectly. To solve the issue, users should either set correct encoding via the CSV option encoding or set the option to null which fallbacks to encoding auto-detection as in Spark versions before 3.0.


Others


In Spark version 2.4, when a Spark session is created via cloneSession(), the newly created Spark session inherits its configuration from its parent SparkContext even though the same configuration may exist with a different value in its parent Spark session. In Spark 3.0, the configurations of a parent SparkSession have a higher precedence over the parent SparkContext. You can restore the old behavior by setting spark.sql.legacy.sessionInitWithConfigDefaults to true.


In Spark 3.0, if hive.default.fileformat is not found in Spark SQL configuration then it falls back to the hive-site.xml file present in the Hadoop configuration of SparkContext.


In Spark 3.0, we pad decimal numbers with trailing zeros to the scale of the column for spark-sql interface, for example:



Query
Spark 2.4
Spark 3.0




SELECT CAST(1 AS decimal(38, 18));
1
1.000000000000000000





In Spark 3.0, we upgraded the built-in Hive from 1.2 to 2.3 and it brings following impacts:


You may need to set spark.sql.hive.metastore.version and spark.sql.hive.metastore.jars according to the version of the Hive metastore you want to connect to. For example: set spark.sql.hive.metastore.version to 1.2.1 and spark.sql.hive.metastore.jars to maven if your Hive metastore version is 1.2.1.


You need to migrate your custom SerDes to Hive 2.3 or build your own Spark with hive-1.2 profile. See HIVE-15167 for more details.


The decimal string representation can be different between Hive 1.2 and Hive 2.3 when using TRANSFORM operator in SQL for script transformation, which depends on hive’s behavior. In Hive 1.2, the string representation omits trailing zeroes. But in Hive 2.3, it is always padded to 18 digits with trailing zeroes if necessary.




Upgrading from Spark SQL 2.4.7 to 2.4.8

In Spark 2.4.8, AnalysisException is replaced by its sub-classes that are thrown for tables from Hive external catalog in the following situations:
    
ALTER TABLE .. ADD PARTITION throws PartitionsAlreadyExistException if new partition exists already
ALTER TABLE .. DROP PARTITION throws NoSuchPartitionsException for not existing partitions



Upgrading from Spark SQL 2.4.5 to 2.4.6

In Spark 2.4.6, the RESET command does not reset the static SQL configuration values to the default. It only clears the runtime SQL configuration values.

Upgrading from Spark SQL 2.4.4 to 2.4.5


Since Spark 2.4.5, TRUNCATE TABLE command tries to set back original permission and ACLs during re-creating the table/partition paths. To restore the behaviour of earlier versions, set spark.sql.truncateTable.ignorePermissionAcl.enabled to true.


Since Spark 2.4.5, spark.sql.legacy.mssqlserver.numericMapping.enabled configuration is added in order to support the legacy MsSQLServer dialect mapping behavior using IntegerType and DoubleType for SMALLINT and REAL JDBC types, respectively. To restore the behaviour of 2.4.3 and earlier versions, set spark.sql.legacy.mssqlserver.numericMapping.enabled to true.


Upgrading from Spark SQL 2.4.3 to 2.4.4

Since Spark 2.4.4, according to MsSqlServer Guide, MsSQLServer JDBC Dialect uses ShortType and FloatType for SMALLINT and REAL, respectively. Previously, IntegerType and DoubleType is used.

Upgrading from Spark SQL 2.4 to 2.4.1

The value of spark.executor.heartbeatInterval, when specified without units like “30” rather than “30s”, was
inconsistently interpreted as both seconds and milliseconds in Spark 2.4.0 in different parts of the code.
Unitless values are now consistently interpreted as milliseconds. Applications that set values like “30”
need to specify a value with units like “30s” now, to avoid being interpreted as milliseconds; otherwise,
the extremely short interval that results will likely cause applications to fail.

Upgrading from Spark SQL 2.3 to 2.4

In Spark version 2.3 and earlier, the second parameter to array_contains function is implicitly promoted to the element type of first array type parameter. This type promotion can be lossy and may cause array_contains function to return wrong result. This problem has been addressed in 2.4 by employing a safer type promotion mechanism. This can cause some change in behavior and are illustrated in the table below.
    



Query


Spark 2.3 or Prior


Spark 2.4


Remarks





SELECT array_contains(array(1), 1.34D);


true


false


        In Spark 2.4, left and right parameters are promoted to array type of double type and double type respectively.
      



SELECT array_contains(array(1), '1');


true


AnalysisException is thrown.
      

        Explicit cast can be used in arguments to avoid the exception. In Spark 2.4, AnalysisException is thrown since integer type can not be promoted to string type in a loss-less manner.
      



SELECT array_contains(array(1), 'anystring');


null


AnalysisException is thrown.
      

        Explicit cast can be used in arguments to avoid the exception. In Spark 2.4, AnalysisException is thrown since integer type can not be promoted to string type in a loss-less manner.
      




Since Spark 2.4, when there is a struct field in front of the IN operator before a subquery, the inner query must contain a struct field as well. In previous versions, instead, the fields of the struct were compared to the output of the inner query. For example, if a is a struct(a string, b int), in Spark 2.4 a in (select (1 as a, 'a' as b) from range(1)) is a valid query, while a in (select 1, 'a' from range(1)) is not. In previous version it was the opposite.


In versions 2.2.1+ and 2.3, if spark.sql.caseSensitive is set to true, then the CURRENT_DATE and CURRENT_TIMESTAMP functions incorrectly became case-sensitive and would resolve to columns (unless typed in lower case). In Spark 2.4 this has been fixed and the functions are no longer case-sensitive.


Since Spark 2.4, Spark will evaluate the set operations referenced in a query by following a precedence rule as per the SQL standard. If the order is not specified by parentheses, set operations are performed from left to right with the exception that all INTERSECT operations are performed before any UNION, EXCEPT or MINUS operations. The old behaviour of giving equal precedence to all the set operations are preserved under a newly added configuration spark.sql.legacy.setopsPrecedence.enabled with a default value of false. When this property is set to true, spark will evaluate the set operators from left to right as they appear in the query given no explicit ordering is enforced by usage of parenthesis.


Since Spark 2.4, Spark will display table description column Last Access value as UNKNOWN when the value was Jan 01 1970.


Since Spark 2.4, Spark maximizes the usage of a vectorized ORC reader for ORC files by default. To do that, spark.sql.orc.impl and spark.sql.orc.filterPushdown change their default values to native and true respectively. ORC files created by native ORC writer cannot be read by some old Apache Hive releases. Use spark.sql.orc.impl=hive to create the files shared with Hive 2.1.1 and older.


Since Spark 2.4, writing an empty dataframe to a directory launches at least one write task, even if physically the dataframe has no partition. This introduces a small behavior change that for self-describing file formats like Parquet and Orc, Spark creates a metadata-only file in the target directory when writing a 0-partition dataframe, so that schema inference can still work if users read that directory later. The new behavior is more reasonable and more consistent regarding writing empty dataframe.


Since Spark 2.4, expression IDs in UDF arguments do not appear in column names. For example, a column name in Spark 2.4 is not UDF:f(col0 AS colA#28) but UDF:f(col0 AS `colA`).


Since Spark 2.4, writing a dataframe with an empty or nested empty schema using any file formats (parquet, orc, json, text, csv etc.) is not allowed. An exception is thrown when attempting to write dataframes with empty schema.


Since Spark 2.4, Spark compares a DATE type with a TIMESTAMP type after promotes both sides to TIMESTAMP. To set false to spark.sql.legacy.compareDateTimestampInTimestamp restores the previous behavior. This option will be removed in Spark 3.0.


Since Spark 2.4, creating a managed table with nonempty location is not allowed. An exception is thrown when attempting to create a managed table with nonempty location. To set true to spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation restores the previous behavior. This option will be removed in Spark 3.0.


Since Spark 2.4, renaming a managed table to existing location is not allowed. An exception is thrown when attempting to rename a managed table to existing location.


Since Spark 2.4, the type coercion rules can automatically promote the argument types of the variadic SQL functions (e.g., IN/COALESCE) to the widest common type, no matter how the input arguments order. In prior Spark versions, the promotion could fail in some specific orders (e.g., TimestampType, IntegerType and StringType) and throw an exception.


Since Spark 2.4, Spark has enabled non-cascading SQL cache invalidation in addition to the traditional cache invalidation mechanism. The non-cascading cache invalidation mechanism allows users to remove a cache without impacting its dependent caches. This new cache invalidation mechanism is used in scenarios where the data of the cache to be removed is still valid, e.g., calling unpersist() on a Dataset, or dropping a temporary view. This allows users to free up memory and keep the desired caches valid at the same time.


In version 2.3 and earlier, Spark converts Parquet Hive tables by default but ignores table properties like TBLPROPERTIES (parquet.compression 'NONE'). This happens for ORC Hive table properties like TBLPROPERTIES (orc.compress 'NONE') in case of spark.sql.hive.convertMetastoreOrc=true, too. Since Spark 2.4, Spark respects Parquet/ORC specific table properties while converting Parquet/ORC Hive tables. As an example, CREATE TABLE t(id int) STORED AS PARQUET TBLPROPERTIES (parquet.compression 'NONE') would generate Snappy parquet files during insertion in Spark 2.3, and in Spark 2.4, the result would be uncompressed parquet files.


Since Spark 2.0, Spark converts Parquet Hive tables by default for better performance. Since Spark 2.4, Spark converts ORC Hive tables by default, too. It means Spark uses its own ORC support by default instead of Hive SerDe. As an example, CREATE TABLE t(id int) STORED AS ORC would be handled with Hive SerDe in Spark 2.3, and in Spark 2.4, it would be converted into Spark’s ORC data source table and ORC vectorization would be applied. To set false to spark.sql.hive.convertMetastoreOrc restores the previous behavior.


In version 2.3 and earlier, CSV rows are considered as malformed if at least one column value in the row is malformed. CSV parser dropped such rows in the DROPMALFORMED mode or outputs an error in the FAILFAST mode. Since Spark 2.4, CSV row is considered as malformed only when it contains malformed column values requested from CSV datasource, other values can be ignored. As an example, CSV file contains the “id,name” header and one row “1234”. In Spark 2.4, selection of the id column consists of a row with one column value 1234 but in Spark 2.3 and earlier it is empty in the DROPMALFORMED mode. To restore the previous behavior, set spark.sql.csv.parser.columnPruning.enabled to false.


Since Spark 2.4, File listing for compute statistics is done in parallel by default. This can be disabled by setting spark.sql.statistics.parallelFileListingInStatsComputation.enabled to False.


Since Spark 2.4, Metadata files (e.g. Parquet summary files) and temporary files are not counted as data files when calculating table size during Statistics computation.


Since Spark 2.4, empty strings are saved as quoted empty strings "". In version 2.3 and earlier, empty strings are equal to null values and do not reflect to any characters in saved CSV files. For example, the row of "a", null, "", 1 was written as a,,,1. Since Spark 2.4, the same row is saved as a,,"",1. To restore the previous behavior, set the CSV option emptyValue to empty (not quoted) string.


Since Spark 2.4, The LOAD DATA command supports wildcard ? and *, which match any one character, and zero or more characters, respectively. Example: LOAD DATA INPATH '/tmp/folder*/' or LOAD DATA INPATH '/tmp/part-?'. Special Characters like space also now work in paths. Example: LOAD DATA INPATH '/tmp/folder name/'.


In Spark version 2.3 and earlier, HAVING without GROUP BY is treated as WHERE. This means, SELECT 1 FROM range(10) HAVING true is executed as SELECT 1 FROM range(10) WHERE true  and returns 10 rows. This violates SQL standard, and has been fixed in Spark 2.4. Since Spark 2.4, HAVING without GROUP BY is treated as a global aggregate, which means SELECT 1 FROM range(10) HAVING true will return only one row. To restore the previous behavior, set spark.sql.legacy.parser.havingWithoutGroupByAsWhere to true.

In version 2.3 and earlier, when reading from a Parquet data source table, Spark always returns null for any column whose column names in Hive metastore schema and Parquet schema are in different letter cases, no matter whether spark.sql.caseSensitive is set to true or false. Since 2.4, when spark.sql.caseSensitive is set to false, Spark does case insensitive column name resolution between Hive metastore schema and Parquet schema, so even column names are in different letter cases, Spark returns corresponding column values. An exception is thrown if there is ambiguity, i.e. more than one Parquet column is matched. This change also applies to Parquet Hive tables when spark.sql.hive.convertMetastoreParquet is set to true.

Upgrading from Spark SQL 2.2 to 2.3


Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record column (named _corrupt_record by default). For example, spark.read.schema(schema).json(file).filter($"_corrupt_record".isNotNull).count() and spark.read.schema(schema).json(file).select("_corrupt_record").show(). Instead, you can cache or save the parsed results and then send the same query. For example, val df = spark.read.schema(schema).json(file).cache() and then df.filter($"_corrupt_record".isNotNull).count().


The percentile_approx function previously accepted numeric type input and output double type results. Now it supports date type, timestamp type and numeric types as input types. The result type is also changed to be the same as the input type, which is more reasonable for percentiles.


Since Spark 2.3, the Join/Filter’s deterministic predicates that are after the first non-deterministic predicates are also pushed down/through the child operators, if possible. In prior Spark versions, these filters are not eligible for predicate pushdown.


Partition column inference previously found incorrect common type for different inferred types, for example, previously it ended up with double type as the common type for double type and date type. Now it finds the correct common type for such conflicts. The conflict resolution follows the table below:



InputA \ InputB
NullType
IntegerType
LongType
DecimalType(38,0)*
DoubleType
DateType
TimestampType
StringType




NullType
NullType
IntegerType
LongType
DecimalType(38,0)
DoubleType
DateType
TimestampType
StringType


IntegerType
IntegerType
IntegerType
LongType
DecimalType(38,0)
DoubleType
StringType
StringType
StringType


LongType
LongType
LongType
LongType
DecimalType(38,0)
StringType
StringType
StringType
StringType


DecimalType(38,0)*
DecimalType(38,0)
DecimalType(38,0)
DecimalType(38,0)
DecimalType(38,0)
StringType
StringType
StringType
StringType


DoubleType
DoubleType
DoubleType
StringType
StringType
DoubleType
StringType
StringType
StringType


DateType
DateType
StringType
StringType
StringType
StringType
DateType
TimestampType
StringType


TimestampType
TimestampType
StringType
StringType
StringType
StringType
TimestampType
TimestampType
StringType


StringType
StringType
StringType
StringType
StringType
StringType
StringType
StringType
StringType



Note that, for DecimalType(38,0)*, the table above intentionally does not cover all other combinations of scales and precisions because currently we only infer decimal type like BigInteger/BigInt. For example, 1.1 is inferred as double type.


Since Spark 2.3, when either broadcast hash join or broadcast nested loop join is applicable, we prefer to broadcasting the table that is explicitly specified in a broadcast hint. For details, see the section Join Strategy Hints for SQL Queries and SPARK-22489.


Since Spark 2.3, when all inputs are binary, functions.concat() returns an output as binary. Otherwise, it returns as a string. Until Spark 2.3, it always returns as a string despite of input types. To keep the old behavior, set spark.sql.function.concatBinaryAsString to true.


Since Spark 2.3, when all inputs are binary, SQL elt() returns an output as binary. Otherwise, it returns as a string. Until Spark 2.3, it always returns as a string despite of input types. To keep the old behavior, set spark.sql.function.eltOutputAsString to true.


Since Spark 2.3, by default arithmetic operations between decimals return a rounded value if an exact representation is not possible (instead of returning NULL). This is compliant with SQL ANSI 2011 specification and Hive’s new behavior introduced in Hive 2.2 (HIVE-15331). This involves the following changes


The rules to determine the result type of an arithmetic operation have been updated. In particular, if the precision / scale needed are out of the range of available values, the scale is reduced up to 6, in order to prevent the truncation of the integer part of the decimals. All the arithmetic operations are affected by the change, i.e. addition (+), subtraction (-), multiplication (*), division (/), remainder (%) and positive modulus (pmod).


Literal values used in SQL operations are converted to DECIMAL with the exact precision and scale needed by them.


The configuration spark.sql.decimalOperations.allowPrecisionLoss has been introduced. It defaults to true, which means the new behavior described here; if set to false, Spark uses previous rules, i.e. it doesn’t adjust the needed scale to represent the values and it returns NULL if an exact representation of the value is not possible.




Un-aliased subquery’s semantic has not been well defined with confusing behaviors. Since Spark 2.3, we invalidate such confusing cases, for example: SELECT v.i from (SELECT i FROM v), Spark will throw an analysis exception in this case because users should not be able to use the qualifier inside a subquery. See SPARK-20690 and SPARK-21335 for more details.


When creating a SparkSession with SparkSession.builder.getOrCreate(), if there is an existing SparkContext, the builder was trying to update the SparkConf of the existing SparkContext with configurations specified to the builder, but the SparkContext is shared by all SparkSessions, so we should not update them. Since 2.3, the builder comes to not update the configurations. If you want to update them, you need to update them prior to creating a SparkSession.


Upgrading from Spark SQL 2.1 to 2.2


Spark 2.1.1 introduced a new configuration key: spark.sql.hive.caseSensitiveInferenceMode. It had a default setting of NEVER_INFER, which kept behavior identical to 2.1.0. However, Spark 2.2.0 changes this setting’s default value to INFER_AND_SAVE to restore compatibility with reading Hive metastore tables whose underlying file schema have mixed-case column names. With the INFER_AND_SAVE configuration value, on first access Spark will perform schema inference on any Hive metastore table for which it has not already saved an inferred schema. Note that schema inference can be a very time-consuming operation for tables with thousands of partitions. If compatibility with mixed-case column names is not a concern, you can safely set spark.sql.hive.caseSensitiveInferenceMode to NEVER_INFER to avoid the initial overhead of schema inference. Note that with the new default INFER_AND_SAVE setting, the results of the schema inference are saved as a metastore key for future use. Therefore, the initial schema inference occurs only at a table’s first access.


Since Spark 2.2.1 and 2.3.0, the schema is always inferred at runtime when the data source tables have the columns that exist in both partition schema and data schema. The inferred schema does not have the partitioned columns. When reading the table, Spark respects the partition values of these overlapping columns instead of the values stored in the data source files. In 2.2.0 and 2.1.x release, the inferred schema is partitioned but the data of the table is invisible to users (i.e., the result set is empty).


Since Spark 2.2, view definitions are stored in a different way from prior versions. This may cause Spark unable to read views created by prior versions. In such cases, you need to recreate the views using ALTER VIEW AS or CREATE OR REPLACE VIEW AS with newer Spark versions.


Upgrading from Spark SQL 2.0 to 2.1


Datasource tables now store partition metadata in the Hive metastore. This means that Hive DDLs such as ALTER TABLE PARTITION ... SET LOCATION are now available for tables created with the Datasource API.


Legacy datasource tables can be migrated to this format via the MSCK REPAIR TABLE command. Migrating legacy tables is recommended to take advantage of Hive DDL support and improved planning performance.


To determine if a table has been migrated, look for the PartitionProvider: Catalog attribute when issuing DESCRIBE FORMATTED on the table.




Changes to INSERT OVERWRITE TABLE ... PARTITION ... behavior for Datasource tables.


In prior Spark versions INSERT OVERWRITE overwrote the entire Datasource table, even when given a partition specification. Now only partitions matching the specification are overwritten.


Note that this still differs from the behavior of Hive tables, which is to overwrite only partitions overlapping with newly inserted data.




Upgrading from Spark SQL 1.6 to 2.0


SparkSession is now the new entry point of Spark that replaces the old SQLContext and
HiveContext. Note that the old SQLContext and HiveContext are kept for backward compatibility. A new catalog interface is accessible from SparkSession - existing API on databases and tables access such as listTables, createExternalTable, dropTempView, cacheTable are moved here.


Dataset API and DataFrame API are unified. In Scala, DataFrame becomes a type alias for
Dataset[Row], while Java API users must replace DataFrame with Dataset<Row>. Both the typed
transformations (e.g., map, filter, and groupByKey) and untyped transformations (e.g.,
select and groupBy) are available on the Dataset class. Since compile-time type-safety in
Python and R is not a language feature, the concept of Dataset does not apply to these languages’
APIs. Instead, DataFrame remains the primary programming abstraction, which is analogous to the
single-node data frame notion in these languages.


Dataset and DataFrame API unionAll has been deprecated and replaced by union


Dataset and DataFrame API explode has been deprecated, alternatively, use functions.explode() with select or flatMap


Dataset and DataFrame API registerTempTable has been deprecated and replaced by createOrReplaceTempView


Changes to CREATE TABLE ... LOCATION behavior for Hive tables.


From Spark 2.0, CREATE TABLE ... LOCATION is equivalent to CREATE EXTERNAL TABLE ... LOCATION
in order to prevent accidental dropping the existing data in the user-provided locations.
That means, a Hive table created in Spark SQL with the user-specified location is always a Hive external table.
Dropping external tables will not remove the data. Users are not allowed to specify the location for Hive managed tables.
Note that this is different from the Hive behavior.


As a result, DROP TABLE statements on those tables will not remove the data.




spark.sql.parquet.cacheMetadata is no longer used.
See SPARK-13664 for details.


Upgrading from Spark SQL 1.5 to 1.6

From Spark 1.6, by default, the Thrift server runs in multi-session mode. Which means each JDBC/ODBC
connection owns a copy of their own SQL configuration and temporary function registry. Cached
tables are still shared though. If you prefer to run the Thrift server in the old single-session
mode, please set option spark.sql.hive.thriftServer.singleSession to true. You may either add
this option to spark-defaults.conf, or pass it to start-thriftserver.sh via --conf:

   ./sbin/start-thriftserver.sh \
     --conf spark.sql.hive.thriftServer.singleSession=true \
     ...
   

From Spark 1.6, LongType casts to TimestampType expect seconds instead of microseconds. This
change was made to match the behavior of Hive 1.2 for more consistent type casting to TimestampType
from numeric types. See SPARK-11724 for
details.

Upgrading from Spark SQL 1.4 to 1.5


Optimized execution using manually managed memory (Tungsten) is now enabled by default, along with
code generation for expression evaluation. These features can both be disabled by setting
spark.sql.tungsten.enabled to false.


Parquet schema merging is no longer enabled by default. It can be re-enabled by setting
spark.sql.parquet.mergeSchema to true.


In-memory columnar storage partition pruning is on by default. It can be disabled by setting
spark.sql.inMemoryColumnarStorage.partitionPruning to false.


Unlimited precision decimal columns are no longer supported, instead Spark SQL enforces a maximum
precision of 38. When inferring schema from BigDecimal objects, a precision of (38, 18) is now
used. When no precision is specified in DDL then the default remains Decimal(10, 0).


Timestamps are now stored at a precision of 1us, rather than 1ns


In the sql dialect, floating point numbers are now parsed as decimal. HiveQL parsing remains
unchanged.


The canonical name of SQL/DataFrame functions are now lower case (e.g., sum vs SUM).


JSON data source will not automatically load new files that are created by other applications
(i.e. files that are not inserted to the dataset through Spark SQL).
For a JSON persistent table (i.e. the metadata of the table is stored in Hive Metastore),
users can use REFRESH TABLE SQL command or HiveContext’s refreshTable method
to include those new files to the table. For a DataFrame representing a JSON dataset, users need to recreate
the DataFrame and the new DataFrame will include new files.


Upgrading from Spark SQL 1.3 to 1.4
DataFrame data reader/writer interface
Based on user feedback, we created a new, more fluid API for reading data in (SQLContext.read)
and writing data out (DataFrame.write),
and deprecated the old APIs (e.g., SQLContext.parquetFile, SQLContext.jsonFile).
See the API docs for SQLContext.read (
  Scala,
  Java,
  Python
) and DataFrame.write (
  Scala,
  Java,
  Python
) more information.
DataFrame.groupBy retains grouping columns
Based on user feedback, we changed the default behavior of DataFrame.groupBy().agg() to retain the
grouping columns in the resulting DataFrame. To keep the behavior in 1.3, set spark.sql.retainGroupColumns to false.


import pyspark.sql.functions as func

# In 1.3.x, in order for the grouping column "department" to show up,
# it must be included explicitly as part of the agg function call.
df.groupBy("department").agg(df["department"], func.max("age"), func.sum("expense"))

# In 1.4+, grouping column "department" is included automatically.
df.groupBy("department").agg(func.max("age"), func.sum("expense"))

# Revert to 1.3.x behavior (not retaining grouping column) by:
sqlContext.setConf("spark.sql.retainGroupColumns", "false")


// In 1.3.x, in order for the grouping column "department" to show up,
// it must be included explicitly as part of the agg function call.
df.groupBy("department").agg($"department", max("age"), sum("expense"))

// In 1.4+, grouping column "department" is included automatically.
df.groupBy("department").agg(max("age"), sum("expense"))

// Revert to 1.3 behavior (not retaining grouping column) by:
sqlContext.setConf("spark.sql.retainGroupColumns", "false")


// In 1.3.x, in order for the grouping column "department" to show up,
// it must be included explicitly as part of the agg function call.
df.groupBy("department").agg(col("department"), max("age"), sum("expense"));

// In 1.4+, grouping column "department" is included automatically.
df.groupBy("department").agg(max("age"), sum("expense"));

// Revert to 1.3 behavior (not retaining grouping column) by:
sqlContext.setConf("spark.sql.retainGroupColumns", "false");


Behavior change on DataFrame.withColumn
Prior to 1.4, DataFrame.withColumn() supports adding a column only. The column will always be added
as a new column with its specified name in the result DataFrame even if there may be any existing
columns of the same name. Since 1.4, DataFrame.withColumn() supports adding a column of a different
name from names of all existing columns or replacing existing columns of the same name.
Note that this change is only for Scala API, not for PySpark and SparkR.
Upgrading from Spark SQL 1.0-1.2 to 1.3
In Spark 1.3 we removed the “Alpha” label from Spark SQL and as part of this did a cleanup of the
available APIs. From Spark 1.3 onwards, Spark SQL will provide binary compatibility with other
releases in the 1.X series. This compatibility guarantee excludes APIs that are explicitly marked
as unstable (i.e., DeveloperAPI or Experimental).
Rename of SchemaRDD to DataFrame
The largest change that users will notice when upgrading to Spark SQL 1.3 is that SchemaRDD has
been renamed to DataFrame. This is primarily because DataFrames no longer inherit from RDD
directly, but instead provide most of the functionality that RDDs provide though their own
implementation. DataFrames can still be converted to RDDs by calling the .rdd method.
In Scala, there is a type alias from SchemaRDD to DataFrame to provide source compatibility for
some use cases. It is still recommended that users update their code to use DataFrame instead.
Java and Python users will need to update their code.
Unification of the Java and Scala APIs
Prior to Spark 1.3 there were separate Java compatible classes (JavaSQLContext and JavaSchemaRDD)
that mirrored the Scala API. In Spark 1.3 the Java API and Scala API have been unified. Users
of either language should use SQLContext and DataFrame. In general these classes try to
use types that are usable from both languages (i.e. Array instead of language-specific collections).
In some cases where no common type exists (e.g., for passing in closures or Maps) function overloading
is used instead.
Additionally, the Java specific types API has been removed. Users of both Scala and Java should
use the classes present in org.apache.spark.sql.types to describe schema programmatically.
Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)
Many of the code examples prior to Spark 1.3 started with import sqlContext._, which brought
all of the functions from sqlContext into scope. In Spark 1.3 we have isolated the implicit
conversions for converting RDDs into DataFrames into an object inside of the SQLContext.
Users should now write import sqlContext.implicits._.
Additionally, the implicit conversions now only augment RDDs that are composed of Products (i.e.,
case classes or tuples) with a method toDF, instead of applying automatically.
When using function inside of the DSL (now replaced with the DataFrame API) users used to import
org.apache.spark.sql.catalyst.dsl. Instead the public dataframe functions API should be used:
import org.apache.spark.sql.functions._.
Removal of the type aliases in org.apache.spark.sql for DataType (Scala-only)
Spark 1.3 removes the type aliases that were present in the base sql package for DataType. Users
should instead import the classes in org.apache.spark.sql.types
UDF Registration Moved to sqlContext.udf (Java & Scala)
Functions that are used to register UDFs, either for use in the DataFrame DSL or SQL, have been
moved into the udf object in SQLContext.


sqlContext.udf.register("strLen", (s: String) => s.length())


sqlContext.udf().register("strLen", (String s) -> s.length(), DataTypes.IntegerType);


Python UDF registration is unchanged.
Compatibility with Apache Hive
Spark SQL is designed to be compatible with the Hive Metastore, SerDes and UDFs.
Currently, Hive SerDes and UDFs are based on built-in Hive,
and Spark SQL can be connected to different versions of Hive Metastore
(from 0.12.0 to 2.3.9 and 3.0.0 to 3.1.3. Also see Interacting with Different Versions of Hive Metastore).
Deploying in Existing Hive Warehouses
The Spark SQL Thrift JDBC server is designed to be “out of the box” compatible with existing Hive
installations. You do not need to modify your existing Hive Metastore or change the data placement
or partitioning of your tables.
Supported Hive Features
Spark SQL supports the vast majority of Hive features, such as:

Hive query statements, including:
    
SELECT
GROUP BY
ORDER BY
DISTRIBUTE BY
CLUSTER BY
SORT BY


All Hive operators, including:
    
Relational operators (=, <=>, ==, <>, <, >, >=, <=, etc)
Arithmetic operators (+, -, *, /, %, etc)
Logical operators (AND, OR, etc)
Complex type constructors
Mathematical functions (sign, ln, cos, etc)
String functions (instr, length, printf, etc)


User defined functions (UDF)
User defined aggregation functions (UDAF)
User defined serialization formats (SerDes)
Window functions
Joins
    
JOIN
{LEFT|RIGHT|FULL} OUTER JOIN
LEFT SEMI JOIN
LEFT ANTI JOIN
CROSS JOIN


Unions
Sub-queries
    

Sub-queries in the FROM Clause
SELECT col FROM (SELECT a + b AS col FROM t1) t2


Sub-queries in WHERE Clause


Correlated or non-correlated IN and NOT IN statement in WHERE Clause
SELECT col FROM t1 WHERE col IN (SELECT a FROM t2 WHERE t1.a = t2.a)
SELECT col FROM t1 WHERE col IN (SELECT a FROM t2)
 


Correlated or non-correlated EXISTS and NOT EXISTS statement in WHERE Clause
SELECT col FROM t1 WHERE EXISTS (SELECT t2.a FROM t2 WHERE t1.a = t2.a AND t2.a > 10)
SELECT col FROM t1 WHERE EXISTS (SELECT t2.a FROM t2 WHERE t2.a > 10)
 


Non-correlated IN and NOT IN statement in JOIN Condition
SELECT t1.col FROM t1 JOIN t2 ON t1.a = t2.a AND t1.a IN (SELECT a FROM t3)


Non-correlated EXISTS and NOT EXISTS statement in JOIN Condition
SELECT t1.col FROM t1 JOIN t2 ON t1.a = t2.a AND EXISTS (SELECT * FROM t3 WHERE t3.a > 10)





Sampling
Explain
Partitioned tables including dynamic partition insertion
View
    

If column aliases are not specified in view definition queries, both Spark and Hive will
generate alias names, but in different ways. In order for Spark to be able to read views created
by Hive, users should explicitly specify column aliases in view definition queries. As an
example, Spark cannot read v1 created as below by Hive.
CREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 FROM (SELECT 1 c) t1) t2;
 
Instead, you should create v1 as below with column aliases explicitly specified.
CREATE VIEW v1 AS SELECT * FROM (SELECT c + 1 AS inc_c FROM (SELECT 1 c) t1) t2;
 



All Hive DDL Functions, including:
    
CREATE TABLE
CREATE TABLE AS SELECT
CREATE TABLE LIKE
ALTER TABLE


Most Hive Data types, including:
    
TINYINT
SMALLINT
INT
BIGINT
BOOLEAN
FLOAT
DOUBLE
STRING
BINARY
TIMESTAMP
DATE
ARRAY<>
MAP<>
STRUCT<>



Unsupported Hive Functionality
Below is a list of Hive features that we don’t support yet. Most of these features are rarely used
in Hive deployments.
Esoteric Hive Features

UNION type
Unique join
Column statistics collecting: Spark SQL does not piggyback scans to collect column statistics at
the moment and only supports populating the sizeInBytes field of the hive metastore.

Hive Input/Output Formats

File format for CLI: For results showing back to the CLI, Spark SQL only supports TextOutputFormat.
Hadoop archive

Hive Optimizations
A handful of Hive optimizations are not yet included in Spark. Some of these (such as indexes) are
less important due to Spark SQL’s in-memory computational model. Others are slotted for future
releases of Spark SQL.

Block-level bitmap indexes and virtual columns (used to build indexes)
Automatically determine the number of reducers for joins and groupbys: Currently, in Spark SQL, you
need to control the degree of parallelism post-shuffle using “SET spark.sql.shuffle.partitions=[num_tasks];”.
Meta-data only query: For queries that can be answered by using only metadata, Spark SQL still
launches tasks to compute the result.
Skew data flag: Spark SQL does not follow the skew data flags in Hive.
STREAMTABLE hint in join: Spark SQL does not follow the STREAMTABLE hint.
Merge multiple small files for query results: if the result output contains multiple small files,
Hive can optionally merge the small files into fewer large files to avoid overflowing the HDFS
metadata. Spark SQL does not support that.

Hive UDF/UDTF/UDAF
Not all the APIs of the Hive UDF/UDTF/UDAF are supported by Spark SQL. Below are the unsupported APIs:

getRequiredJars and getRequiredFiles (UDF and GenericUDF) are functions to automatically
include additional resources required by this UDF.
initialize(StructObjectInspector) in GenericUDTF is not supported yet. Spark SQL currently uses
a deprecated interface initialize(ObjectInspector[]) only.
configure (GenericUDF, GenericUDTF, and GenericUDAFEvaluator) is a function to initialize
functions with MapredContext, which is inapplicable to Spark.
close (GenericUDF and GenericUDAFEvaluator) is a function to release associated resources.
Spark SQL does not call this function when tasks finish.
reset (GenericUDAFEvaluator) is a function to re-initialize aggregation for reusing the same aggregation.
Spark SQL currently does not support the reuse of aggregation.
getWindowingEvaluator (GenericUDAFEvaluator) is a function to optimize aggregation by evaluating
an aggregate over a fixed window.

Incompatible Hive UDF
Below are the scenarios in which Hive and Spark generate different results:

SQRT(n) If n < 0, Hive returns null, Spark SQL returns NaN.
ACOS(n) If n < -1 or n > 1, Hive returns null, Spark SQL returns NaN.
ASIN(n) If n < -1 or n > 1, Hive returns null, Spark SQL returns NaN.
CAST(n AS TIMESTAMP) If n is integral numbers, Hive treats n as milliseconds, Spark SQL treats n as seconds.





















  




Performance Tuning - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        




            
                Caching Data In Memory
            
        



            
                Other Configuration Options
            
        



            
                Join Strategy Hints for SQL Queries
            
        



            
                Coalesce Hints for SQL Queries
            
        



            
                Adaptive Query Execution
            
        




            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        







Performance Tuning

Caching Data In Memory
Other Configuration Options
Join Strategy Hints for SQL Queries
Coalesce Hints for SQL Queries
Adaptive Query Execution 
Coalescing Post Shuffle Partitions
Spliting skewed shuffle partitions
Converting sort-merge join to broadcast join
Converting sort-merge join to shuffled hash join
Optimizing Skew Join
Misc



For some workloads, it is possible to improve performance by either caching data in memory, or by
turning on some experimental options.
Caching Data In Memory
Spark SQL can cache tables using an in-memory columnar format by calling spark.catalog.cacheTable("tableName") or dataFrame.cache().
Then Spark SQL will scan only required columns and will automatically tune compression to minimize
memory usage and GC pressure. You can call spark.catalog.uncacheTable("tableName") or dataFrame.unpersist() to remove the table from memory.
Configuration of in-memory caching can be done using the setConf method on SparkSession or by running
SET key=value commands using SQL.

Property NameDefaultMeaningSince Version

spark.sql.inMemoryColumnarStorage.compressed
true

    When set to true, Spark SQL will automatically select a compression codec for each column based
    on statistics of the data.
  
1.0.1


spark.sql.inMemoryColumnarStorage.batchSize
10000

    Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization
    and compression, but risk OOMs when caching data.
  
1.1.1


Other Configuration Options
The following options can also be used to tune the performance of query execution. It is possible
that these options will be deprecated in future release as more optimizations are performed automatically.

Property NameDefaultMeaningSince Version

spark.sql.files.maxPartitionBytes
134217728 (128 MB)

      The maximum number of bytes to pack into a single partition when reading files.
      This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
    
2.0.0


spark.sql.files.openCostInBytes
4194304 (4 MB)

      The estimated cost to open a file, measured by the number of bytes that could be scanned in the same
      time. This is used when putting multiple files into a partition. It is better to over-estimate,
      then the partitions with small files will be faster than partitions with bigger files (which is
      scheduled first). This configuration is effective only when using file-based sources such as Parquet,
      JSON and ORC.
    
2.0.0


spark.sql.files.minPartitionNum
Default Parallelism

      The suggested (not guaranteed) minimum number of split file partitions. If not set, the default
      value is `spark.sql.leafNodeDefaultParallelism`. This configuration is effective only when using file-based
      sources such as Parquet, JSON and ORC.
    
3.1.0


spark.sql.files.maxPartitionNum
None

      The suggested (not guaranteed) maximum number of split file partitions. If it is set,
      Spark will rescale each partition to make the number of partitions is close to this
      value if the initial number of partitions exceeds this value. This configuration is
      effective only when using file-based sources such as Parquet, JSON and ORC.
    
3.5.0


spark.sql.broadcastTimeout
300


        Timeout in seconds for the broadcast wait time in broadcast joins
      

1.3.0


spark.sql.autoBroadcastJoinThreshold
10485760 (10 MB)

      Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when
      performing a join. By setting this value to -1, broadcasting can be disabled. Note that currently
      statistics are only supported for Hive Metastore tables where the command
      ANALYZE TABLE <tableName> COMPUTE STATISTICS noscan has been run.
    
1.1.0


spark.sql.shuffle.partitions
200

      Configures the number of partitions to use when shuffling data for joins or aggregations.
    
1.1.0


spark.sql.sources.parallelPartitionDiscovery.threshold
32

      Configures the threshold to enable parallel listing for job input paths. If the number of
      input paths is larger than this threshold, Spark will list the files by using Spark distributed job.
      Otherwise, it will fallback to sequential listing. This configuration is only effective when
      using file-based data sources such as Parquet, ORC and JSON.
    
1.5.0


spark.sql.sources.parallelPartitionDiscovery.parallelism
10000

      Configures the maximum listing parallelism for job input paths. In case the number of input
      paths is larger than this value, it will be throttled down to use this value. This configuration is only effective when using file-based data sources such as Parquet, ORC
      and JSON.
    
2.1.1


Join Strategy Hints for SQL Queries
The join strategy hints, namely BROADCAST, MERGE, SHUFFLE_HASH and SHUFFLE_REPLICATE_NL,
instruct Spark to use the hinted strategy on each specified relation when joining them with another
relation. For example, when the BROADCAST hint is used on table ‘t1’, broadcast join (either
broadcast hash join or broadcast nested loop join depending on whether there is any equi-join key)
with ‘t1’ as the build side will be prioritized by Spark even if the size of table ‘t1’ suggested
by the statistics is above the configuration spark.sql.autoBroadcastJoinThreshold.
When different join strategy hints are specified on both sides of a join, Spark prioritizes the
BROADCAST hint over the MERGE hint over the SHUFFLE_HASH hint over the SHUFFLE_REPLICATE_NL
hint. When both sides are specified with the BROADCAST hint or the SHUFFLE_HASH hint, Spark will
pick the build side based on the join type and the sizes of the relations.
Note that there is no guarantee that Spark will choose the join strategy specified in the hint since
a specific strategy may not support all join types.


spark.table("src").join(spark.table("records").hint("broadcast"), "key").show()


spark.table("src").join(spark.table("records").hint("broadcast"), "key").show()


spark.table("src").join(spark.table("records").hint("broadcast"), "key").show();


src <- sql("SELECT * FROM src")
records <- sql("SELECT * FROM records")
head(join(src, hint(records, "broadcast"), src$key == records$key))


-- We accept BROADCAST, BROADCASTJOIN and MAPJOIN for broadcast hint
SELECT /*+ BROADCAST(r) */ * FROM records r JOIN src s ON r.key = s.key


For more details please refer to the documentation of Join Hints.
Coalesce Hints for SQL Queries
Coalesce hints allow Spark SQL users to control the number of output files just like
coalesce, repartition and repartitionByRange in the Dataset API, they can be used for performance
tuning and reducing the number of output files. The “COALESCE” hint only has a partition number as a
parameter. The “REPARTITION” hint has a partition number, columns, or both/neither of them as parameters.
The “REPARTITION_BY_RANGE” hint must have column names and a partition number is optional. The “REBALANCE”
hint has an initial partition number, columns, or both/neither of them as parameters.
SELECT /*+ COALESCE(3) */ * FROM t;
SELECT /*+ REPARTITION(3) */ * FROM t;
SELECT /*+ REPARTITION(c) */ * FROM t;
SELECT /*+ REPARTITION(3, c) */ * FROM t;
SELECT /*+ REPARTITION */ * FROM t;
SELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t;
SELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t;
SELECT /*+ REBALANCE */ * FROM t;
SELECT /*+ REBALANCE(3) */ * FROM t;
SELECT /*+ REBALANCE(c) */ * FROM t;
SELECT /*+ REBALANCE(3, c) */ * FROM t;

For more details please refer to the documentation of Partitioning Hints.
Adaptive Query Execution
Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0. Spark SQL can turn on and off AQE by spark.sql.adaptive.enabled as an umbrella configuration. As of Spark 3.0, there are three major features in AQE: including coalescing post-shuffle partitions, converting sort-merge join to broadcast join, and skew join optimization.
Coalescing Post Shuffle Partitions
This feature coalesces the post shuffle partitions based on the map output statistics when both spark.sql.adaptive.enabled and spark.sql.adaptive.coalescePartitions.enabled configurations are true. This feature simplifies the tuning of shuffle partition number when running queries. You do not need to set a proper shuffle partition number to fit your dataset. Spark can pick the proper shuffle partition number at runtime once you set a large enough initial number of shuffle partitions via spark.sql.adaptive.coalescePartitions.initialPartitionNum configuration.

Property NameDefaultMeaningSince Version

spark.sql.adaptive.coalescePartitions.enabled
true

       When true and spark.sql.adaptive.enabled is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid too many small tasks.
     
3.0.0


spark.sql.adaptive.coalescePartitions.parallelismFirst
true

       When true, Spark ignores the target size specified by spark.sql.adaptive.advisoryPartitionSizeInBytes (default 64MB) when coalescing contiguous shuffle partitions, and only respect the minimum partition size specified by spark.sql.adaptive.coalescePartitions.minPartitionSize (default 1MB), to maximize the parallelism. This is to avoid performance regression when enabling adaptive query execution. It's recommended to set this config to false and respect the target size specified by spark.sql.adaptive.advisoryPartitionSizeInBytes.
     
3.2.0


spark.sql.adaptive.coalescePartitions.minPartitionSize
1MB

       The minimum size of shuffle partitions after coalescing. This is useful when the target size is ignored during partition coalescing, which is the default case.
     
3.2.0


spark.sql.adaptive.coalescePartitions.initialPartitionNum
(none)

       The initial number of shuffle partitions before coalescing. If not set, it equals to spark.sql.shuffle.partitions. This configuration only has an effect when spark.sql.adaptive.enabled and spark.sql.adaptive.coalescePartitions.enabled are both enabled.
     
3.0.0


spark.sql.adaptive.advisoryPartitionSizeInBytes
64 MB

       The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.
     
3.0.0


Spliting skewed shuffle partitions

Property NameDefaultMeaningSince Version

spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled
true

       When true and spark.sql.adaptive.enabled is true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid data skew.
     
3.2.0


spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor
0.2

       A partition will be merged during splitting if its size is small than this factor multiply spark.sql.adaptive.advisoryPartitionSizeInBytes.
     
3.3.0


Converting sort-merge join to broadcast join
AQE converts sort-merge join to broadcast hash join when the runtime statistics of any join side is smaller than the adaptive broadcast hash join threshold. This is not as efficient as planning a broadcast hash join in the first place, but it’s better than keep doing the sort-merge join, as we can save the sorting of both the join sides, and read shuffle files locally to save network traffic(if spark.sql.adaptive.localShuffleReader.enabled is true)

Property NameDefaultMeaningSince Version

spark.sql.adaptive.autoBroadcastJoinThreshold
(none)

         Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1, broadcasting can be disabled. The default value is the same as spark.sql.autoBroadcastJoinThreshold. Note that, this config is used only in adaptive framework.
       
3.2.0


spark.sql.adaptive.localShuffleReader.enabled
true

         When true and spark.sql.adaptive.enabled is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.
       
3.0.0


Converting sort-merge join to shuffled hash join
AQE converts sort-merge join to shuffled hash join when all post shuffle partitions are smaller than a threshold, the max threshold can see the config spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold.

Property NameDefaultMeaningSince Version

spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold
0

         Configures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than spark.sql.adaptive.advisoryPartitionSizeInBytes and all the partition sizes are not larger than this config, join selection prefers to use shuffled hash join instead of sort merge join regardless of the value of spark.sql.join.preferSortMergeJoin.
       
3.2.0


Optimizing Skew Join
Data skew can severely downgrade the performance of join queries. This feature dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed tasks into roughly evenly sized tasks. It takes effect when both spark.sql.adaptive.enabled and spark.sql.adaptive.skewJoin.enabled configurations are enabled.

Property NameDefaultMeaningSince Version

spark.sql.adaptive.skewJoin.enabled
true

         When true and spark.sql.adaptive.enabled is true, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.
       
3.0.0


spark.sql.adaptive.skewJoin.skewedPartitionFactor
5.0

         A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes.
       
3.0.0


spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes
256MB

         A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than spark.sql.adaptive.skewJoin.skewedPartitionFactor multiplying the median partition size. Ideally, this config should be set larger than spark.sql.adaptive.advisoryPartitionSizeInBytes.
       
3.0.0


spark.sql.adaptive.forceOptimizeSkewedJoin
false

         When true, force enable OptimizeSkewedJoin, which is an adaptive rule to optimize skewed joins to avoid straggler tasks, even if it introduces extra shuffle.
       
3.3.0


Misc

Property NameDefaultMeaningSince Version

spark.sql.adaptive.optimizer.excludedRules
(none)

        Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.
      
3.1.0


spark.sql.adaptive.customCostEvaluatorClass
(none)

        The custom cost evaluator class to be used for adaptive execution. If not being set, Spark will use its own SimpleCostEvaluator by default.
      
3.2.0






















  




Spark SQL and DataFrames - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        







Spark SQL, DataFrames and Datasets Guide
Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided
by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally,
Spark SQL uses this extra information to perform extra optimizations. There are several ways to
interact with Spark SQL including SQL and the Dataset API. When computing a result,
the same execution engine is used, independent of which API/language you are using to express the
computation. This unification means that developers can easily switch back and forth between
different APIs based on which provides the most natural way to express a given transformation.
All of the examples on this page use sample data included in the Spark distribution and can be run in
the spark-shell, pyspark shell, or sparkR shell.
SQL
One use of Spark SQL is to execute SQL queries.
Spark SQL can also be used to read data from an existing Hive installation. For more on how to
configure this feature, please refer to the Hive Tables section. When running
SQL from within another programming language the results will be returned as a Dataset/DataFrame.
You can also interact with the SQL interface using the command-line
or over JDBC/ODBC.
Datasets and DataFrames
A Dataset is a distributed collection of data.
Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong
typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized
execution engine. A Dataset can be constructed from JVM objects and then
manipulated using functional transformations (map, flatMap, filter, etc.).
The Dataset API is available in Scala and
Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature,
many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally
row.columnName). The case for R is similar.
A DataFrame is a Dataset organized into named columns. It is conceptually
equivalent to a table in a relational database or a data frame in R/Python, but with richer
optimizations under the hood. DataFrames can be constructed from a wide array of sources such
as: structured data files, tables in Hive, external databases, or existing RDDs.
The DataFrame API is available in Scala,
Java, Python, and R.
In Scala and Java, a DataFrame is represented by a Dataset of Rows.
In the Scala API, DataFrame is simply a type alias of Dataset[Row].
While, in Java API, users need to use Dataset<Row> to represent a DataFrame.
Throughout this document, we will often refer to Scala/Java Datasets of Rows as DataFrames.




















  




PySpark Usage Guide for Pandas with Apache Arrow - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        



            
                Error Conditions
            
        







PySpark Usage Guide for Pandas with Apache Arrow
The Arrow usage guide is now archived on this page.




















  




ANSI Compliance - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







ANSI Compliance
In Spark SQL, there are two options to comply with the SQL standard: spark.sql.ansi.enabled and spark.sql.storeAssignmentPolicy (See a table below for details).
When spark.sql.ansi.enabled is set to true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results if the inputs to a SQL operator/function are invalid. Some ANSI dialect features may be not from the ANSI SQL standard directly, but their behaviors align with ANSI SQL’s style.
Moreover, Spark SQL has an independent option to control implicit casting behaviours when inserting rows in a table.
The casting behaviours are defined as store assignment rules in the standard.
When spark.sql.storeAssignmentPolicy is set to ANSI, Spark SQL complies with the ANSI store assignment rules. This is a separate configuration because its default value is ANSI, while the configuration spark.sql.ansi.enabled is disabled by default.



Property Name
Default
Meaning
Since Version




spark.sql.ansi.enabled
false
When true, Spark tries to conform to the ANSI SQL specification:  1. Spark SQL will throw runtime exceptions on invalid operations, including integer overflow errors, string parsing errors, etc.  2. Spark will use different type coercion rules for resolving conflicts among data types. The rules are consistently based on data type precedence.
3.0.0


spark.sql.storeAssignmentPolicy
ANSI
When inserting a value into a column with different data type, Spark will perform type conversion.  Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. 1. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL.  It disallows certain unreasonable type conversions such as converting string to int or double to boolean. On inserting a numeric type column, an overflow error will be thrown if the value is out of the target data type’s range.2. With legacy policy, Spark allows the type coercion as long as it is a valid Cast, which is very loose.  e.g. converting string to int or double to boolean is allowed.  It is also the only behavior in Spark 2.x and it is compatible with Hive.3. With strict policy, Spark doesn’t allow any possible precision loss or data truncation in type coercion, e.g. converting double to int or decimal to double is not allowed.
3.0.0



The following subsections present behaviour changes in arithmetic operations, type conversions, and SQL parsing when the ANSI mode enabled. For type conversions in Spark SQL, there are three kinds of them and this article will introduce them one by one: cast, store assignment and type coercion.
Arithmetic Operations
In Spark SQL, arithmetic operations performed on numeric types (with the exception of decimal) are not checked for overflows by default.
This means that in case an operation causes overflows, the result is the same with the corresponding operation in a Java/Scala program (e.g., if the sum of 2 integers is higher than the maximum value representable, the result is a negative number).
On the other hand, Spark SQL returns null for decimal overflows.
When spark.sql.ansi.enabled is set to true and an overflow occurs in numeric and interval arithmetic operations, it throws an arithmetic exception at runtime.
-- `spark.sql.ansi.enabled=true`
SELECT 2147483647 + 1;
org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] integer overflow. Use 'try_add' to tolerate overflow and return NULL instead. If necessary set spark.sql.ansi.enabled to "false" to bypass this error.
== SQL(line 1, position 8) ==
SELECT 2147483647 + 1
       ^^^^^^^^^^^^^^

SELECT abs(-2147483648);
org.apache.spark.SparkArithmeticException: [ARITHMETIC_OVERFLOW] integer overflow. If necessary set spark.sql.ansi.enabled to "false" to bypass this error.

-- `spark.sql.ansi.enabled=false`
SELECT 2147483647 + 1;
+----------------+
|(2147483647 + 1)|
+----------------+
|     -2147483648|
+----------------+

SELECT abs(-2147483648);
+----------------+
|abs(-2147483648)|
+----------------+
|     -2147483648|
+----------------+

Cast
When spark.sql.ansi.enabled is set to true, explicit casting by CAST syntax throws a runtime exception for illegal cast patterns defined in the standard, e.g. casts from a string to an integer.
Besides, the ANSI SQL mode disallows the following type conversions which are allowed when ANSI mode is off:

Numeric <=> Binary
Date <=> Boolean
Timestamp <=> Boolean
Date => Numeric

The valid combinations of source and target data type in a CAST expression are given by the following table.
“Y” indicates that the combination is syntactically valid without restriction and “N” indicates that the combination is not valid.



Source\Target
Numeric
String
Date
Timestamp
Timestamp_NTZ
Interval
Boolean
Binary
Array
Map
Struct




Numeric
Y
Y
N
Y
N
Y
Y
N
N
N
N


String
Y
Y
Y
Y
Y
Y
Y
Y
N
N
N


Date
N
Y
Y
Y
Y
N
N
N
N
N
N


Timestamp
Y
Y
Y
Y
Y
N
N
N
N
N
N


Timestamp_NTZ
N
Y
Y
Y
Y
N
N
N
N
N
N


Interval
Y
Y
N
N
N
Y
N
N
N
N
N


Boolean
Y
Y
N
N
N
N
Y
N
N
N
N


Binary
N
Y
N
N
N
N
N
Y
N
N
N


Array
N
Y
N
N
N
N
N
N
Y
N
N


Map
N
Y
N
N
N
N
N
N
N
Y
N


Struct
N
Y
N
N
N
N
N
N
N
N
Y



In the table above, all the CASTs with new syntax are marked as red Y:

CAST(Numeric AS Numeric): raise an overflow exception if the value is out of the target data type’s range.
CAST(String AS (Numeric/Date/Timestamp/Timestamp_NTZ/Interval/Boolean)): raise a runtime exception if the value can’t be parsed as the target data type.
CAST(Timestamp AS Numeric): raise an overflow exception if the number of seconds since epoch is out of the target data type’s range.
CAST(Numeric AS Timestamp): raise an overflow exception if numeric value times 1000000(microseconds per second) is out of the range of Long type.
CAST(Array AS Array): raise an exception if there is any on the conversion of the elements.
CAST(Map AS Map): raise an exception if there is any on the conversion of the keys and the values.
CAST(Struct AS Struct): raise an exception if there is any on the conversion of the struct fields.
CAST(Numeric AS String): Always use plain string representation on casting decimal values to strings, instead of using scientific notation if an exponent is needed
CAST(Interval AS Numeric): raise an overflow exception if the number of microseconds of the day-time interval or months of year-month interval is out of the target data type’s range.
CAST(Numeric AS Interval): raise an overflow exception if numeric value times by the target interval’s end-unit is out of the range of the Int type for year-month intervals or the Long type for day-time intervals.

-- Examples of explicit casting

-- `spark.sql.ansi.enabled=true`
SELECT CAST('a' AS INT);
org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'a' of the type "STRING" cannot be cast to "INT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 8) ==
SELECT CAST('a' AS INT)
       ^^^^^^^^^^^^^^^^

SELECT CAST(2147483648L AS INT);
org.apache.spark.SparkArithmeticException: [CAST_OVERFLOW] The value 2147483648L of the type "BIGINT" cannot be cast to "INT" due to an overflow. Use `try_cast` to tolerate overflow and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.

SELECT CAST(DATE'2020-01-01' AS INT);
org.apache.spark.sql.AnalysisException: cannot resolve 'CAST(DATE '2020-01-01' AS INT)' due to data type mismatch: cannot cast date to int.
To convert values from date to int, you can use function UNIX_DATE instead.

-- `spark.sql.ansi.enabled=false` (This is a default behaviour)
SELECT CAST('a' AS INT);
+--------------+
|CAST(a AS INT)|
+--------------+
|          null|
+--------------+

SELECT CAST(2147483648L AS INT);
+-----------------------+
|CAST(2147483648 AS INT)|
+-----------------------+
|            -2147483648|
+-----------------------+

SELECT CAST(DATE'2020-01-01' AS INT)
+------------------------------+
|CAST(DATE '2020-01-01' AS INT)|
+------------------------------+
|                          null|
+------------------------------+

-- Examples of store assignment rules
CREATE TABLE t (v INT);

-- `spark.sql.storeAssignmentPolicy=ANSI`
INSERT INTO t VALUES ('1');
org.apache.spark.sql.AnalysisException: [INCOMPATIBLE_DATA_FOR_TABLE.CANNOT_SAFELY_CAST] Cannot write incompatible data for table `spark_catalog`.`default`.`t`: Cannot safely cast `v`: "STRING" to "INT".

-- `spark.sql.storeAssignmentPolicy=LEGACY` (This is a legacy behaviour until Spark 2.x)
INSERT INTO t VALUES ('1');
SELECT * FROM t;
+---+
|  v|
+---+
|  1|
+---+

Rounding in cast
While casting of a decimal with a fraction to an interval type with SECOND as the end-unit like INTERVAL HOUR TO SECOND, Spark rounds the fractional part towards “nearest neighbor” unless both neighbors are equidistant, in which case round up.
Store assignment
As mentioned at the beginning, when spark.sql.storeAssignmentPolicy is set to ANSI(which is the default value), Spark SQL complies with the ANSI store assignment rules on table insertions. The valid combinations of source and target data type in table insertions are given by the following table.



Source\Target
Numeric
String
Date
Timestamp
Timestamp_NTZ
Interval
Boolean
Binary
Array
Map
Struct




Numeric
Y
Y
N
N
N
N
N
N
N
N
N


String
N
Y
N
N
N
N
N
N
N
N
N


Date
N
Y
Y
Y
Y
N
N
N
N
N
N


Timestamp
N
Y
Y
Y
Y
N
N
N
N
N
N


Timestamp_NTZ
N
Y
Y
Y
Y
N
N
N
N
N
N


Interval
N
Y
N
N
N
N*
N
N
N
N
N


Boolean
N
Y
N
N
N
N
Y
N
N
N
N


Binary
N
Y
N
N
N
N
N
Y
N
N
N


Array
N
N
N
N
N
N
N
N
Y**
N
N


Map
N
N
N
N
N
N
N
N
N
Y**
N


Struct
N
N
N
N
N
N
N
N
N
N
Y**



* Spark doesn’t support interval type table column.
** For Array/Map/Struct types, the data type check rule applies recursively to its component elements.
During table insertion, Spark will throw exception on numeric value overflow.
CREATE TABLE test(i INT);

INSERT INTO test VALUES (2147483648L);
org.apache.spark.SparkArithmeticException: [CAST_OVERFLOW_IN_TABLE_INSERT] Fail to insert a value of "BIGINT" type into the "INT" type column `i` due to an overflow. Use `try_cast` on the input value to tolerate overflow and return NULL instead.

Type coercion
Type Promotion and Precedence
When spark.sql.ansi.enabled is set to true, Spark SQL uses several rules that govern how conflicts between data types are resolved.
At the heart of this conflict resolution is the Type Precedence List which defines whether values of a given data type can be promoted to another data type implicitly.



Data type
precedence list(from narrowest to widest)




Byte
Byte -> Short -> Int -> Long -> Decimal -> Float* -> Double


Short
Short -> Int -> Long -> Decimal-> Float* -> Double


Int
Int -> Long -> Decimal -> Float* -> Double


Long
Long -> Decimal -> Float* -> Double


Decimal
Decimal -> Float* -> Double


Float
Float -> Double


Double
Double


Date
Date -> Timestamp_NTZ -> Timestamp


Timestamp
Timestamp


String
String, Long -> Double, Date -> Timestamp_NTZ -> Timestamp , Boolean, Binary **


Binary
Binary


Boolean
Boolean


Interval
Interval


Map
Map***


Array
Array***


Struct
Struct***



* For least common type resolution float is skipped to avoid loss of precision.
** String can be promoted to multiple kinds of data types. Note that Byte/Short/Int/Decimal/Float is not on this precedent list. The least common type between Byte/Short/Int and String is Long, while the least common type between Decimal/Float is Double.
*** For a complex type, the precedence rule applies recursively to its component elements.
Special rules apply for untyped NULL. A NULL can be promoted to any other type.
This is a graphical depiction of the precedence list as a directed tree:

Least Common Type Resolution
The least common type from a set of types is the narrowest type reachable from the precedence list by all elements of the set of types.
The least common type resolution is used to:

Derive the argument type for functions which expect a shared argument type for multiple parameters, such as coalesce, least, or greatest.
Derive the operand types for operators such as arithmetic operations or comparisons.
Derive the result type for expressions such as the case expression.
Derive the element, key, or value types for array and map constructors.
Special rules are applied if the least common type resolves to FLOAT. With float type values, if any of the types is INT, BIGINT, or DECIMAL the least common type is pushed to DOUBLE to avoid potential loss of digits.

-- The coalesce function accepts any set of argument types as long as they share a least common type. 
-- The result type is the least common type of the arguments.
> SET spark.sql.ansi.enabled=true;
> SELECT typeof(coalesce(1Y, 1L, NULL));
BIGINT
> SELECT typeof(coalesce(1, DATE'2020-01-01'));
Error: Incompatible types [INT, DATE]

> SELECT typeof(coalesce(ARRAY(1Y), ARRAY(1L)));
ARRAY<BIGINT>
> SELECT typeof(coalesce(1, 1F));
DOUBLE
> SELECT typeof(coalesce(1L, 1F));
DOUBLE
> SELECT (typeof(coalesce(1BD, 1F)));
DOUBLE

> SELECT typeof(coalesce(1, '2147483648'))
BIGINT
> SELECT typeof(coalesce(1.0, '2147483648'))
DOUBLE
> SELECT typeof(coalesce(DATE'2021-01-01', '2022-01-01'))
DATE

SQL Functions
Function invocation
Under ANSI mode(spark.sql.ansi.enabled=true), the function invocation of Spark SQL:

In general, it follows the Store assignment rules as storing the input values as the declared parameter type of the SQL functions
Special rules apply for untyped NULL. A NULL can be promoted to any other type.

> SET spark.sql.ansi.enabled=true;
-- implicitly cast Int to String type
> SELECT concat('total number: ', 1);
total number: 1
-- implicitly cast Timestamp to Date type
> select datediff(now(), current_date);
0

-- implicitly cast String to Double type
> SELECT ceil('0.1');
1
-- special rule: implicitly cast NULL to Date type
> SELECT year(null);
NULL

> CREATE TABLE t(s string);
-- Can't store String column as Numeric types.
> SELECT ceil(s) from t;
Error in query: cannot resolve 'CEIL(spark_catalog.default.t.s)' due to data type mismatch
-- Can't store String column as Date type.
> select year(s) from t;
Error in query: cannot resolve 'year(spark_catalog.default.t.s)' due to data type mismatch

Functions with different behaviors
The behavior of some SQL functions can be different under ANSI mode (spark.sql.ansi.enabled=true).

size: This function returns null for null input.
element_at:
    
This function throws ArrayIndexOutOfBoundsException if using invalid indices.


elt: This function throws ArrayIndexOutOfBoundsException if using invalid indices.
parse_url: This function throws IllegalArgumentException if an input string is not a valid url.
to_date: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.
to_timestamp: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.
unix_timestamp: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.
to_unix_timestamp: This function should fail with an exception if the input string can’t be parsed, or the pattern string is invalid.
make_date: This function should fail with an exception if the result date is invalid.
make_timestamp: This function should fail with an exception if the result timestamp is invalid.
make_interval:  This function should fail with an exception if the result interval is invalid.
next_day: This function throws IllegalArgumentException if input is not a valid day of week.

SQL Operators
The behavior of some SQL operators can be different under ANSI mode (spark.sql.ansi.enabled=true).

array_col[index]: This operator throws ArrayIndexOutOfBoundsException if using invalid indices.

Useful Functions for ANSI Mode
When ANSI mode is on, it throws exceptions for invalid operations. You can use the following SQL functions to suppress such exceptions.

try_cast: identical to CAST, except that it returns NULL result instead of throwing an exception on runtime error.
try_add: identical to the add operator +, except that it returns NULL result instead of throwing an exception on integral value overflow.
try_subtract: identical to the add operator -, except that it returns NULL result instead of throwing an exception on integral value overflow.
try_multiply: identical to the add operator *, except that it returns NULL result instead of throwing an exception on integral value overflow.
try_divide: identical to the division operator /, except that it returns NULL result instead of throwing an exception on dividing 0.
try_sum: identical to the function sum, except that it returns NULL result instead of throwing an exception on integral/decimal/interval value overflow.
try_avg: identical to the function avg, except that it returns NULL result instead of throwing an exception on decimal/interval value overflow.
try_element_at: identical to the function element_at, except that it returns NULL result instead of throwing an exception on array’s index out of bound.
try_to_timestamp: identical to the function to_timestamp, except that it returns NULL result instead of throwing an exception on string parsing error.

SQL Keywords (optional, disabled by default)
When both spark.sql.ansi.enabled and spark.sql.ansi.enforceReservedKeywords are true, Spark SQL will use the ANSI mode parser.
With the ANSI mode parser, Spark SQL has two kinds of keywords:

Non-reserved keywords: Keywords that have a special meaning only in particular contexts and can be used as identifiers in other contexts. For example, EXPLAIN SELECT ... is a command, but EXPLAIN can be used as identifiers in other places.
Reserved keywords: Keywords that are reserved and can’t be used as identifiers for table, view, column, function, alias, etc.

With the default parser, Spark SQL has two kinds of keywords:

Non-reserved keywords: Same definition as the one when the ANSI mode enabled.
Strict-non-reserved keywords: A strict version of non-reserved keywords, which can not be used as table alias.

By default, both spark.sql.ansi.enabled and spark.sql.ansi.enforceReservedKeywords are false.
Below is a list of all the keywords in Spark SQL.



Keyword
Spark SQLANSI Mode
Spark SQLDefault Mode
SQL-2016




ADD
non-reserved
non-reserved
non-reserved


AFTER
non-reserved
non-reserved
non-reserved


ALL
reserved
non-reserved
reserved


ALTER
non-reserved
non-reserved
reserved


ALWAYS
non-reserved
non-reserved
non-reserved


ANALYZE
non-reserved
non-reserved
non-reserved


AND
reserved
non-reserved
reserved


ANTI
non-reserved
strict-non-reserved
non-reserved


ANY
reserved
non-reserved
reserved


ANY_VALUE
non-reserved
non-reserved
non-reserved


ARCHIVE
non-reserved
non-reserved
non-reserved


ARRAY
non-reserved
non-reserved
reserved


AS
reserved
non-reserved
reserved


ASC
non-reserved
non-reserved
non-reserved


AT
non-reserved
non-reserved
reserved


AUTHORIZATION
reserved
non-reserved
reserved


BETWEEN
non-reserved
non-reserved
reserved


BIGINT
non-reserved
non-reserved
reserved


BINARY
non-reserved
non-reserved
reserved


BOOLEAN
non-reserved
non-reserved
reserved


BOTH
reserved
non-reserved
reserved


BUCKET
non-reserved
non-reserved
non-reserved


BUCKETS
non-reserved
non-reserved
non-reserved


BY
non-reserved
non-reserved
reserved


BYTE
non-reserved
non-reserved
non-reserved


CACHE
non-reserved
non-reserved
non-reserved


CASCADE
non-reserved
non-reserved
non-reserved


CASE
reserved
non-reserved
reserved


CAST
reserved
non-reserved
reserved


CATALOG
non-reserved
non-reserved
non-reserved


CATALOGS
non-reserved
non-reserved
non-reserved


CHANGE
non-reserved
non-reserved
non-reserved


CHAR
non-reserved
non-reserved
reserved


CHARACTER
non-reserved
non-reserved
reserved


CHECK
reserved
non-reserved
reserved


CLEAR
non-reserved
non-reserved
non-reserved


CLUSTER
non-reserved
non-reserved
non-reserved


CLUSTERED
non-reserved
non-reserved
non-reserved


CODEGEN
non-reserved
non-reserved
non-reserved


COLLATE
reserved
non-reserved
reserved


COLLECTION
non-reserved
non-reserved
non-reserved


COLUMN
reserved
non-reserved
reserved


COLUMNS
non-reserved
non-reserved
non-reserved


COMMENT
non-reserved
non-reserved
non-reserved


COMMIT
non-reserved
non-reserved
reserved


COMPACT
non-reserved
non-reserved
non-reserved


COMPACTIONS
non-reserved
non-reserved
non-reserved


COMPUTE
non-reserved
non-reserved
non-reserved


CONCATENATE
non-reserved
non-reserved
non-reserved


CONSTRAINT
reserved
non-reserved
reserved


COST
non-reserved
non-reserved
non-reserved


CREATE
reserved
non-reserved
reserved


CROSS
reserved
strict-non-reserved
reserved


CUBE
non-reserved
non-reserved
reserved


CURRENT
non-reserved
non-reserved
reserved


CURRENT_DATE
reserved
non-reserved
reserved


CURRENT_TIME
reserved
non-reserved
reserved


CURRENT_TIMESTAMP
reserved
non-reserved
reserved


CURRENT_USER
reserved
non-reserved
reserved


DATA
non-reserved
non-reserved
non-reserved


DATE
non-reserved
non-reserved
reserved


DATABASE
non-reserved
non-reserved
non-reserved


DATABASES
non-reserved
non-reserved
non-reserved


DATEADD
non-reserved
non-reserved
non-reserved


DATE_ADD
non-reserved
non-reserved
non-reserved


DATEDIFF
non-reserved
non-reserved
non-reserved


DATE_DIFF
non-reserved
non-reserved
non-reserved


DAY
non-reserved
non-reserved
non-reserved


DAYS
non-reserved
non-reserved
non-reserved


DAYOFYEAR
non-reserved
non-reserved
non-reserved


DBPROPERTIES
non-reserved
non-reserved
non-reserved


DEC
non-reserved
non-reserved
reserved


DECIMAL
non-reserved
non-reserved
reserved


DEFAULT
non-reserved
non-reserved
non-reserved


DEFINED
non-reserved
non-reserved
non-reserved


DELETE
non-reserved
non-reserved
reserved


DELIMITED
non-reserved
non-reserved
non-reserved


DESC
non-reserved
non-reserved
non-reserved


DESCRIBE
non-reserved
non-reserved
reserved


DFS
non-reserved
non-reserved
non-reserved


DIRECTORIES
non-reserved
non-reserved
non-reserved


DIRECTORY
non-reserved
non-reserved
non-reserved


DISTINCT
reserved
non-reserved
reserved


DISTRIBUTE
non-reserved
non-reserved
non-reserved


DIV
non-reserved
non-reserved
not a keyword


DOUBLE
non-reserved
non-reserved
reserved


DROP
non-reserved
non-reserved
reserved


ELSE
reserved
non-reserved
reserved


END
reserved
non-reserved
reserved


ESCAPE
reserved
non-reserved
reserved


ESCAPED
non-reserved
non-reserved
non-reserved


EXCEPT
reserved
strict-non-reserved
reserved


EXCHANGE
non-reserved
non-reserved
non-reserved


EXCLUDE
non-reserved
non-reserved
non-reserved


EXISTS
non-reserved
non-reserved
reserved


EXPLAIN
non-reserved
non-reserved
non-reserved


EXPORT
non-reserved
non-reserved
non-reserved


EXTENDED
non-reserved
non-reserved
non-reserved


EXTERNAL
non-reserved
non-reserved
reserved


EXTRACT
non-reserved
non-reserved
reserved


FALSE
reserved
non-reserved
reserved


FETCH
reserved
non-reserved
reserved


FIELDS
non-reserved
non-reserved
non-reserved


FILTER
reserved
non-reserved
reserved


FILEFORMAT
non-reserved
non-reserved
non-reserved


FIRST
non-reserved
non-reserved
non-reserved


FLOAT
non-reserved
non-reserved
reserved


FOLLOWING
non-reserved
non-reserved
non-reserved


FOR
reserved
non-reserved
reserved


FOREIGN
reserved
non-reserved
reserved


FORMAT
non-reserved
non-reserved
non-reserved


FORMATTED
non-reserved
non-reserved
non-reserved


FROM
reserved
non-reserved
reserved


FULL
reserved
strict-non-reserved
reserved


FUNCTION
non-reserved
non-reserved
reserved


FUNCTIONS
non-reserved
non-reserved
non-reserved


GENERATED
non-reserved
non-reserved
non-reserved


GLOBAL
non-reserved
non-reserved
reserved


GRANT
reserved
non-reserved
reserved


GROUP
reserved
non-reserved
reserved


GROUPING
non-reserved
non-reserved
reserved


HAVING
reserved
non-reserved
reserved


HOUR
non-reserved
non-reserved
non-reserved


HOURS
non-reserved
non-reserved
non-reserved


IDENTIFIER
non-reserved
non-reserved
non-reserved


IF
non-reserved
non-reserved
not a keyword


IGNORE
non-reserved
non-reserved
non-reserved


IMPORT
non-reserved
non-reserved
non-reserved


IN
reserved
non-reserved
reserved


INCLUDE
non-reserved
non-reserved
non-reserved


INDEX
non-reserved
non-reserved
non-reserved


INDEXES
non-reserved
non-reserved
non-reserved


INNER
reserved
strict-non-reserved
reserved


INPATH
non-reserved
non-reserved
non-reserved


INPUTFORMAT
non-reserved
non-reserved
non-reserved


INSERT
non-reserved
non-reserved
reserved


INT
non-reserved
non-reserved
reserved


INTEGER
non-reserved
non-reserved
reserved


INTERSECT
reserved
strict-non-reserved
reserved


INTERVAL
non-reserved
non-reserved
reserved


INTO
reserved
non-reserved
reserved


IS
reserved
non-reserved
reserved


ITEMS
non-reserved
non-reserved
non-reserved


JOIN
reserved
strict-non-reserved
reserved


KEYS
non-reserved
non-reserved
non-reserved


LAST
non-reserved
non-reserved
non-reserved


LATERAL
reserved
strict-non-reserved
reserved


LAZY
non-reserved
non-reserved
non-reserved


LEADING
reserved
non-reserved
reserved


LEFT
reserved
strict-non-reserved
reserved


LIKE
non-reserved
non-reserved
reserved


ILIKE
non-reserved
non-reserved
non-reserved


LIMIT
non-reserved
non-reserved
non-reserved


LINES
non-reserved
non-reserved
non-reserved


LIST
non-reserved
non-reserved
non-reserved


LOAD
non-reserved
non-reserved
non-reserved


LOCAL
non-reserved
non-reserved
reserved


LOCATION
non-reserved
non-reserved
non-reserved


LOCK
non-reserved
non-reserved
non-reserved


LOCKS
non-reserved
non-reserved
non-reserved


LOGICAL
non-reserved
non-reserved
non-reserved


LONG
non-reserved
non-reserved
non-reserved


MACRO
non-reserved
non-reserved
non-reserved


MAP
non-reserved
non-reserved
non-reserved


MATCHED
non-reserved
non-reserved
non-reserved


MERGE
non-reserved
non-reserved
non-reserved


MICROSECOND
non-reserved
non-reserved
non-reserved


MICROSECONDS
non-reserved
non-reserved
non-reserved


MILLISECOND
non-reserved
non-reserved
non-reserved


MILLISECONDS
non-reserved
non-reserved
non-reserved


MINUTE
non-reserved
non-reserved
non-reserved


MINUTES
non-reserved
non-reserved
non-reserved


MINUS
non-reserved
strict-non-reserved
non-reserved


MONTH
non-reserved
non-reserved
non-reserved


MONTHS
non-reserved
non-reserved
non-reserved


MSCK
non-reserved
non-reserved
non-reserved


NAME
non-reserved
non-reserved
non-reserved


NAMESPACE
non-reserved
non-reserved
non-reserved


NAMESPACES
non-reserved
non-reserved
non-reserved


NANOSECOND
non-reserved
non-reserved
non-reserved


NANOSECONDS
non-reserved
non-reserved
non-reserved


NATURAL
reserved
strict-non-reserved
reserved


NO
non-reserved
non-reserved
reserved


NOT
reserved
non-reserved
reserved


NULL
reserved
non-reserved
reserved


NULLS
non-reserved
non-reserved
non-reserved


NUMERIC
non-reserved
non-reserved
non-reserved


OF
non-reserved
non-reserved
reserved


OFFSET
reserved
non-reserved
reserved


ON
reserved
strict-non-reserved
reserved


ONLY
reserved
non-reserved
reserved


OPTION
non-reserved
non-reserved
non-reserved


OPTIONS
non-reserved
non-reserved
non-reserved


OR
reserved
non-reserved
reserved


ORDER
reserved
non-reserved
reserved


OUT
non-reserved
non-reserved
reserved


OUTER
reserved
non-reserved
reserved


OUTPUTFORMAT
non-reserved
non-reserved
non-reserved


OVER
non-reserved
non-reserved
non-reserved


OVERLAPS
reserved
non-reserved
reserved


OVERLAY
non-reserved
non-reserved
non-reserved


OVERWRITE
non-reserved
non-reserved
non-reserved


PARTITION
non-reserved
non-reserved
reserved


PARTITIONED
non-reserved
non-reserved
non-reserved


PARTITIONS
non-reserved
non-reserved
non-reserved


PERCENT
non-reserved
non-reserved
non-reserved


PERCENTILE_CONT
reserved
non-reserved
non-reserved


PERCENTILE_DISC
reserved
non-reserved
non-reserved


PIVOT
non-reserved
non-reserved
non-reserved


PLACING
non-reserved
non-reserved
non-reserved


POSITION
non-reserved
non-reserved
reserved


PRECEDING
non-reserved
non-reserved
non-reserved


PRIMARY
reserved
non-reserved
reserved


PRINCIPALS
non-reserved
non-reserved
non-reserved


PROPERTIES
non-reserved
non-reserved
non-reserved


PURGE
non-reserved
non-reserved
non-reserved


QUARTER
non-reserved
non-reserved
non-reserved


QUERY
non-reserved
non-reserved
non-reserved


RANGE
non-reserved
non-reserved
reserved


REAL
non-reserved
non-reserved
reserved


RECORDREADER
non-reserved
non-reserved
non-reserved


RECORDWRITER
non-reserved
non-reserved
non-reserved


RECOVER
non-reserved
non-reserved
non-reserved


REDUCE
non-reserved
non-reserved
non-reserved


REFERENCES
reserved
non-reserved
reserved


REFRESH
non-reserved
non-reserved
non-reserved


REGEXP
non-reserved
non-reserved
not a keyword


RENAME
non-reserved
non-reserved
non-reserved


REPAIR
non-reserved
non-reserved
non-reserved


REPEATABLE
non-reserved
non-reserved
non-reserved


REPLACE
non-reserved
non-reserved
non-reserved


RESET
non-reserved
non-reserved
non-reserved


RESPECT
non-reserved
non-reserved
non-reserved


RESTRICT
non-reserved
non-reserved
non-reserved


REVOKE
non-reserved
non-reserved
reserved


RIGHT
reserved
strict-non-reserved
reserved


RLIKE
non-reserved
non-reserved
non-reserved


ROLE
non-reserved
non-reserved
non-reserved


ROLES
non-reserved
non-reserved
non-reserved


ROLLBACK
non-reserved
non-reserved
reserved


ROLLUP
non-reserved
non-reserved
reserved


ROW
non-reserved
non-reserved
reserved


ROWS
non-reserved
non-reserved
reserved


SCHEMA
non-reserved
non-reserved
non-reserved


SCHEMAS
non-reserved
non-reserved
non-reserved


SECOND
non-reserved
non-reserved
non-reserved


SECONDS
non-reserved
non-reserved
non-reserved


SELECT
reserved
non-reserved
reserved


SEMI
non-reserved
strict-non-reserved
non-reserved


SEPARATED
non-reserved
non-reserved
non-reserved


SERDE
non-reserved
non-reserved
non-reserved


SERDEPROPERTIES
non-reserved
non-reserved
non-reserved


SESSION_USER
reserved
non-reserved
reserved


SET
non-reserved
non-reserved
reserved


SETS
non-reserved
non-reserved
non-reserved


SHORT
non-reserved
non-reserved
non-reserved


SHOW
non-reserved
non-reserved
non-reserved


SKEWED
non-reserved
non-reserved
non-reserved


SMALLINT
non-reserved
non-reserved
reserved


SOME
reserved
non-reserved
reserved


SORT
non-reserved
non-reserved
non-reserved


SORTED
non-reserved
non-reserved
non-reserved


SOURCE
non-reserved
non-reserved
non-reserved


START
non-reserved
non-reserved
reserved


STATISTICS
non-reserved
non-reserved
non-reserved


STORED
non-reserved
non-reserved
non-reserved


STRATIFY
non-reserved
non-reserved
non-reserved


STRING
non-reserved
non-reserved
non-reserved


STRUCT
non-reserved
non-reserved
non-reserved


SUBSTR
non-reserved
non-reserved
non-reserved


SUBSTRING
non-reserved
non-reserved
non-reserved


SYNC
non-reserved
non-reserved
non-reserved


SYSTEM_TIME
non-reserved
non-reserved
non-reserved


SYSTEM_VERSION
non-reserved
non-reserved
non-reserved


TABLE
reserved
non-reserved
reserved


TABLES
non-reserved
non-reserved
non-reserved


TABLESAMPLE
non-reserved
non-reserved
reserved


TARGET
non-reserved
non-reserved
non-reserved


TBLPROPERTIES
non-reserved
non-reserved
non-reserved


TEMP
non-reserved
non-reserved
not a keyword


TEMPORARY
non-reserved
non-reserved
non-reserved


TERMINATED
non-reserved
non-reserved
non-reserved


THEN
reserved
non-reserved
reserved


TIME
reserved
non-reserved
reserved


TIMESTAMP
non-reserved
non-reserved
non-reserved


TIMESTAMP_LTZ
non-reserved
non-reserved
non-reserved


TIMESTAMP_NTZ
non-reserved
non-reserved
non-reserved


TIMESTAMPADD
non-reserved
non-reserved
non-reserved


TIMESTAMPDIFF
non-reserved
non-reserved
non-reserved


TINYINT
non-reserved
non-reserved
non-reserved


TO
reserved
non-reserved
reserved


TOUCH
non-reserved
non-reserved
non-reserved


TRAILING
reserved
non-reserved
reserved


TRANSACTION
non-reserved
non-reserved
non-reserved


TRANSACTIONS
non-reserved
non-reserved
non-reserved


TRANSFORM
non-reserved
non-reserved
non-reserved


TRIM
non-reserved
non-reserved
non-reserved


TRUE
non-reserved
non-reserved
reserved


TRUNCATE
non-reserved
non-reserved
reserved


TRY_CAST
non-reserved
non-reserved
non-reserved


TYPE
non-reserved
non-reserved
non-reserved


UNARCHIVE
non-reserved
non-reserved
non-reserved


UNBOUNDED
non-reserved
non-reserved
non-reserved


UNCACHE
non-reserved
non-reserved
non-reserved


UNION
reserved
strict-non-reserved
reserved


UNIQUE
reserved
non-reserved
reserved


UNKNOWN
reserved
non-reserved
reserved


UNLOCK
non-reserved
non-reserved
non-reserved


UNPIVOT
non-reserved
non-reserved
non-reserved


UNSET
non-reserved
non-reserved
non-reserved


UPDATE
non-reserved
non-reserved
reserved


USE
non-reserved
non-reserved
non-reserved


USER
reserved
non-reserved
reserved


USING
reserved
strict-non-reserved
reserved


VALUES
non-reserved
non-reserved
reserved


VARCHAR
non-reserved
non-reserved
reserved


VERSION
non-reserved
non-reserved
non-reserved


VIEW
non-reserved
non-reserved
non-reserved


VIEWS
non-reserved
non-reserved
non-reserved


VOID
non-reserved
non-reserved
non-reserved


WEEK
non-reserved
non-reserved
non-reserved


WEEKS
non-reserved
non-reserved
non-reserved


WHEN
reserved
non-reserved
reserved


WHERE
reserved
non-reserved
reserved


WINDOW
non-reserved
non-reserved
reserved


WITH
reserved
non-reserved
reserved


WITHIN
reserved
non-reserved
reserved


X
non-reserved
non-reserved
non-reserved


YEAR
non-reserved
non-reserved
non-reserved


YEARS
non-reserved
non-reserved
non-reserved


ZONE
non-reserved
non-reserved
non-reserved























  




Data Types - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Data Types
Supported Data Types
Spark SQL and DataFrames support the following data types:

Numeric types
    
ByteType: Represents 1-byte signed integer numbers.
The range of numbers is from -128 to 127.
ShortType: Represents 2-byte signed integer numbers.
The range of numbers is from -32768 to 32767.
IntegerType: Represents 4-byte signed integer numbers.
The range of numbers is from -2147483648 to 2147483647.
LongType: Represents 8-byte signed integer numbers.
The range of numbers is from -9223372036854775808 to 9223372036854775807.
FloatType: Represents 4-byte single-precision floating point numbers.
DoubleType: Represents 8-byte double-precision floating point numbers.
DecimalType: Represents arbitrary-precision signed decimal numbers. Backed internally by java.math.BigDecimal. A BigDecimal consists of an arbitrary precision integer unscaled value and a 32-bit integer scale.


String type
    
StringType: Represents character string values.
VarcharType(length): A variant of StringType which has a length limitation. Data writing will fail if the input string exceeds the length limitation. Note: this type can only be used in table schema, not functions/operators.
CharType(length): A variant of VarcharType(length) which is fixed length. Reading column of type CharType(n) always returns string values of length n. Char type column comparison will pad the short one to the longer length.


Binary type
    
BinaryType: Represents byte sequence values.


Boolean type
    
BooleanType: Represents boolean values.


Datetime type
    
DateType: Represents values comprising values of fields year, month and day, without a
time-zone.
TimestampType: Timestamp with local time zone(TIMESTAMP_LTZ). It represents values comprising values of fields year, month, day,
hour, minute, and second, with the session local time-zone. The timestamp value represents an
absolute point in time.
TimestampNTZType: Timestamp without time zone(TIMESTAMP_NTZ). It represents values comprising values of fields year, month, day,
hour, minute, and second. All operations are performed without taking any time zone into account.
        
Note: TIMESTAMP in Spark is a user-specified alias associated with one of the TIMESTAMP_LTZ and TIMESTAMP_NTZ variations.  Users can set the default timestamp type as TIMESTAMP_LTZ(default value) or TIMESTAMP_NTZ via the configuration spark.sql.timestampType.




Interval types
    
YearMonthIntervalType(startField, endField): Represents a year-month interval which is made up of a contiguous subset of the following fields:
        
MONTH, months within years [0..11],
YEAR, years in the range [0..178956970].

Individual interval fields are non-negative, but an interval itself can have a sign, and be negative.
startField is the leftmost field, and endField is the rightmost field of the type. Valid values of startField and endField are 0(MONTH) and 1(YEAR). Supported year-month interval types are:



Year-Month Interval Type
SQL type
An instance of the type




YearMonthIntervalType(YEAR, YEAR) or YearMonthIntervalType(YEAR)
INTERVAL YEAR
INTERVAL '2021' YEAR


YearMonthIntervalType(YEAR, MONTH)
INTERVAL YEAR TO MONTH
INTERVAL '2021-07' YEAR TO MONTH


YearMonthIntervalType(MONTH, MONTH) or YearMonthIntervalType(MONTH)
INTERVAL MONTH
INTERVAL '10' MONTH




DayTimeIntervalType(startField, endField): Represents a day-time interval which is made up of a contiguous subset of the following fields:
        
SECOND, seconds within minutes and possibly fractions of a second [0..59.999999],
MINUTE, minutes within hours [0..59],
HOUR, hours within days [0..23],
DAY, days in the range [0..106751991].

Individual interval fields are non-negative, but an interval itself can have a sign, and be negative.
startField is the leftmost field, and endField is the rightmost field of the type. Valid values of startField and endField are 0 (DAY), 1 (HOUR), 2 (MINUTE), 3 (SECOND). Supported day-time interval types are:



Day-Time Interval Type
SQL type
An instance of the type




DayTimeIntervalType(DAY, DAY) or DayTimeIntervalType(DAY)
INTERVAL DAY
INTERVAL '100' DAY


DayTimeIntervalType(DAY, HOUR)
INTERVAL DAY TO HOUR
INTERVAL '100 10' DAY TO HOUR


DayTimeIntervalType(DAY, MINUTE)
INTERVAL DAY TO MINUTE
INTERVAL '100 10:30' DAY TO MINUTE


DayTimeIntervalType(DAY, SECOND)
INTERVAL DAY TO SECOND
INTERVAL '100 10:30:40.999999' DAY TO SECOND


DayTimeIntervalType(HOUR, HOUR) or DayTimeIntervalType(HOUR)
INTERVAL HOUR
INTERVAL '123' HOUR


DayTimeIntervalType(HOUR, MINUTE)
INTERVAL HOUR TO MINUTE
INTERVAL '123:10' HOUR TO MINUTE


DayTimeIntervalType(HOUR, SECOND)
INTERVAL HOUR TO SECOND
INTERVAL '123:10:59' HOUR TO SECOND


DayTimeIntervalType(MINUTE, MINUTE) or DayTimeIntervalType(MINUTE)
INTERVAL MINUTE
INTERVAL '1000' MINUTE


DayTimeIntervalType(MINUTE, SECOND)
INTERVAL MINUTE TO SECOND
INTERVAL '1000:01.001' MINUTE TO SECOND


DayTimeIntervalType(SECOND, SECOND) or DayTimeIntervalType(SECOND)
INTERVAL SECOND
INTERVAL '1000.000001' SECOND






Complex types
    
ArrayType(elementType, containsNull): Represents values comprising a sequence of
elements with the type of elementType. containsNull is used to indicate if
elements in a ArrayType value can have null values.
MapType(keyType, valueType, valueContainsNull):
Represents values comprising a set of key-value pairs. The data type of keys is
described by keyType and the data type of values is described by valueType.
For a MapType value, keys are not allowed to have null values. valueContainsNull
is used to indicate if values of a MapType value can have null values.
StructType(fields): Represents values with the structure described by
a sequence of StructFields (fields).
        
StructField(name, dataType, nullable): Represents a field in a StructType.
The name of a field is indicated by name. The data type of a field is indicated
by dataType. nullable is used to indicate if values of these fields can have
null values.







All data types of Spark SQL are located in the package of pyspark.sql.types.
You can access them by doing
from pyspark.sql.types import *



Data type
Value type in Python
API to access or create a data type




ByteType
int or longNote: Numbers will be converted to 1-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -128 to 127.
ByteType()


ShortType
int or longNote: Numbers will be converted to 2-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -32768 to 32767.
ShortType()


IntegerType
int or long
IntegerType()


LongType
longNote: Numbers will be converted to 8-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807. Otherwise, please convert data to decimal.Decimal and use DecimalType.
LongType()


FloatType
floatNote: Numbers will be converted to 4-byte single-precision floating point numbers at runtime.
FloatType()


DoubleType
float
DoubleType()


DecimalType
decimal.Decimal
DecimalType()


StringType
string
StringType()


BinaryType
bytearray
BinaryType()


BooleanType
bool
BooleanType()


TimestampType
datetime.datetime
TimestampType()


TimestampNTZType
datetime.datetime
TimestampNTZType()


DateType
datetime.date
DateType()


DayTimeIntervalType
datetime.timedelta
DayTimeIntervalType()


ArrayType
list, tuple, or array
ArrayType(elementType, [containsNull])Note:The default value of containsNull is True.


MapType
dict
MapType(keyType, valueType, [valueContainsNull])Note:The default value of valueContainsNull is True.


StructType
list or tuple
StructType(fields)Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed.


StructField
The value type in Python of the data type of this field(For example, Int for a StructField with the data type IntegerType)
StructField(name, dataType, [nullable])Note: The default value of nullable is True.





All data types of Spark SQL are located in the package org.apache.spark.sql.types.
You can access them by doing
import org.apache.spark.sql.types._
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala" in the Spark repo.



Data type
Value type in Scala
API to access or create a data type




ByteType
Byte
ByteType


ShortType
Short
ShortType


IntegerType
Int
IntegerType


LongType
Long
LongType


FloatType
Float
FloatType


DoubleType
Double
DoubleType


DecimalType
java.math.BigDecimal
DecimalType


StringType
String
StringType


BinaryType
Array[Byte]
BinaryType


BooleanType
Boolean
BooleanType


TimestampType
java.time.Instant or java.sql.Timestamp
TimestampType


TimestampNTZType
java.time.LocalDateTime
TimestampNTZType


DateType
java.time.LocalDate or java.sql.Date
DateType


YearMonthIntervalType
java.time.Period
YearMonthIntervalType


DayTimeIntervalType
java.time.Duration
DayTimeIntervalType


ArrayType
scala.collection.Seq
ArrayType(elementType, [containsNull])Note: The default value of containsNull is true.


MapType
scala.collection.Map
MapType(keyType, valueType, [valueContainsNull])Note: The default value of valueContainsNull is true.


StructType
org.apache.spark.sql.Row
StructType(fields)Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed.


StructField
The value type in Scala of the data type of this field(For example, Int for a StructField with the data type IntegerType)
StructField(name, dataType, [nullable])Note: The default value of nullable is true.





All data types of Spark SQL are located in the package of
org.apache.spark.sql.types. To access or create a data type,
please use factory methods provided in
org.apache.spark.sql.types.DataTypes.



Data type
Value type in Java
API to access or create a data type




ByteType
byte or Byte
DataTypes.ByteType


ShortType
short or Short
DataTypes.ShortType


IntegerType
int or Integer
DataTypes.IntegerType


LongType
long or Long
DataTypes.LongType


FloatType
float or Float
DataTypes.FloatType


DoubleType
double or Double
DataTypes.DoubleType


DecimalType
java.math.BigDecimal
DataTypes.createDecimalType()DataTypes.createDecimalType(precision, scale).


StringType
String
DataTypes.StringType


BinaryType
byte[]
DataTypes.BinaryType


BooleanType
boolean or Boolean
DataTypes.BooleanType


TimestampType
java.time.Instant or java.sql.Timestamp
DataTypes.TimestampType


TimestampNTZType
java.time.LocalDateTime
DataTypes.TimestampNTZType


DateType
java.time.LocalDate or java.sql.Date
DataTypes.DateType


YearMonthIntervalType
java.time.Period
DataTypes.YearMonthIntervalType


DayTimeIntervalType
java.time.Duration
DataTypes.DayTimeIntervalType


ArrayType
java.util.List
DataTypes.createArrayType(elementType)Note: The value of containsNull will be true.DataTypes.createArrayType(elementType, containsNull).


MapType
java.util.Map
DataTypes.createMapType(keyType, valueType)Note: The value of valueContainsNull will be true.DataTypes.createMapType(keyType, valueType, valueContainsNull)


StructType
org.apache.spark.sql.Row
DataTypes.createStructType(fields)Note: fields is a List or an array of StructFields.Also, two fields with the same name are not allowed.


StructField
The value type in Java of the data type of this field (For example, int for a StructField with the data type IntegerType)
DataTypes.createStructField(name, dataType, nullable)








Data type
Value type in R
API to access or create a data type




ByteType
integer Note: Numbers will be converted to 1-byte signed integer numbers at runtime.  Please make sure that numbers are within the range of -128 to 127.
“byte”


ShortType
integer Note: Numbers will be converted to 2-byte signed integer numbers at runtime.  Please make sure that numbers are within the range of -32768 to 32767.
“short”


IntegerType
integer
“integer”


LongType
integer Note: Numbers will be converted to 8-byte signed integer numbers at runtime.  Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807.  Otherwise, please convert data to decimal.Decimal and use DecimalType.
“long”


FloatType
numeric Note: Numbers will be converted to 4-byte single-precision floating point numbers at runtime.
“float”


DoubleType
numeric
“double”


DecimalType
Not supported
Not supported


StringType
character
“string”


BinaryType
raw
“binary”


BooleanType
logical
“bool”


TimestampType
POSIXct
“timestamp”


DateType
Date
“date”


ArrayType
vector or list
list(type=”array”, elementType=elementType, containsNull=[containsNull])Note: The default value of containsNull is TRUE.


MapType
environment
list(type=”map”, keyType=keyType, valueType=valueType, valueContainsNull=[valueContainsNull]) Note: The default value of valueContainsNull is TRUE.


StructType
named list
list(type=”struct”, fields=fields) Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed.


StructField
The value type in R of the data type of this field (For example, integer for a StructField with the data type IntegerType)
list(name=name, type=dataType, nullable=[nullable]) Note: The default value of nullable is TRUE.





The following table shows the type names as well as aliases used in Spark SQL parser for each data type.



Data type
SQL name




BooleanType
BOOLEAN


ByteType
BYTE, TINYINT


ShortType
SHORT, SMALLINT


IntegerType
INT, INTEGER


LongType
LONG, BIGINT


FloatType
FLOAT, REAL


DoubleType
DOUBLE


DateType
DATE


TimestampType
TIMESTAMP, TIMESTAMP_LTZ


TimestampNTZType
TIMESTAMP_NTZ


StringType
STRING


BinaryType
BINARY


DecimalType
DECIMAL, DEC, NUMERIC


YearMonthIntervalType
INTERVAL YEAR, INTERVAL YEAR TO MONTH, INTERVAL MONTH


DayTimeIntervalType
INTERVAL DAY, INTERVAL DAY TO HOUR, INTERVAL DAY TO MINUTE, INTERVAL DAY TO SECOND, INTERVAL HOUR, INTERVAL HOUR TO MINUTE, INTERVAL HOUR TO SECOND, INTERVAL MINUTE, INTERVAL MINUTE TO SECOND, INTERVAL SECOND


ArrayType
ARRAY<element_type>


StructType
STRUCT<field1_name: field1_type, field2_name: field2_type, …> Note: ‘:’ is optional.


MapType
MAP<key_type, value_type>





Floating Point Special Values
Spark SQL supports several special floating point values in a case-insensitive manner:

Inf/+Inf/Infinity/+Infinity: positive infinity
    
FloatType: equivalent to Scala Float.PositiveInfinity.
DoubleType: equivalent to Scala Double.PositiveInfinity.


-Inf/-Infinity: negative infinity
    
FloatType: equivalent to Scala Float.NegativeInfinity.
DoubleType: equivalent to Scala Double.NegativeInfinity.


NaN: not a number
    
FloatType: equivalent to Scala Float.NaN.
DoubleType:  equivalent to Scala Double.NaN.



Positive/Negative Infinity Semantics
There is special handling for positive and negative infinity. They have the following semantics:

Positive infinity multiplied by any positive value returns positive infinity.
Negative infinity multiplied by any positive value returns negative infinity.
Positive infinity multiplied by any negative value returns negative infinity.
Negative infinity multiplied by any negative value returns positive infinity.
Positive/negative infinity multiplied by 0 returns NaN.
Positive/negative infinity is equal to itself.
In aggregations, all positive infinity values are grouped together. Similarly, all negative infinity values are grouped together.
Positive infinity and negative infinity are treated as normal values in join keys.
Positive infinity sorts lower than NaN and higher than any other values.
Negative infinity sorts lower than any other values.

NaN Semantics
There is special handling for not-a-number (NaN) when dealing with float or double types that
do not exactly match standard floating point semantics.
Specifically:

NaN = NaN returns true.
In aggregations, all NaN values are grouped together.
NaN is treated as a normal value in join keys.
NaN values go last when in ascending order, larger than any other numeric value.

Examples
SELECT double('infinity') AS col;
+--------+
|     col|
+--------+
|Infinity|
+--------+

SELECT float('-inf') AS col;
+---------+
|      col|
+---------+
|-Infinity|
+---------+

SELECT float('NaN') AS col;
+---+
|col|
+---+
|NaN|
+---+

SELECT double('infinity') * 0 AS col;
+---+
|col|
+---+
|NaN|
+---+

SELECT double('-infinity') * (-1234567) AS col;
+--------+
|     col|
+--------+
|Infinity|
+--------+

SELECT double('infinity') < double('NaN') AS col;
+----+
| col|
+----+
|true|
+----+

SELECT double('NaN') = double('NaN') AS col;
+----+
| col|
+----+
|true|
+----+

SELECT double('inf') = double('infinity') AS col;
+----+
| col|
+----+
|true|
+----+

CREATE TABLE test (c1 int, c2 double);
INSERT INTO test VALUES (1, double('infinity'));
INSERT INTO test VALUES (2, double('infinity'));
INSERT INTO test VALUES (3, double('inf'));
INSERT INTO test VALUES (4, double('-inf'));
INSERT INTO test VALUES (5, double('NaN'));
INSERT INTO test VALUES (6, double('NaN'));
INSERT INTO test VALUES (7, double('-infinity'));
SELECT COUNT(*), c2 FROM test GROUP BY c2;
+---------+---------+
| count(1)|       c2|
+---------+---------+
|        2|      NaN|
|        2|-Infinity|
|        3| Infinity|
+---------+---------+





















  




Datetime patterns - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Datetime Patterns for Formatting and Parsing
There are several common scenarios for datetime usage in Spark:


CSV/JSON datasources use the pattern string for parsing and formatting datetime content.


Datetime functions related to convert StringType to/from DateType or TimestampType.
For example, unix_timestamp, date_format, to_unix_timestamp, from_unixtime, to_date, to_timestamp, from_utc_timestamp, to_utc_timestamp, etc.


Spark uses pattern letters in the following table for date and timestamp parsing and formatting:



Symbol
Meaning
Presentation
Examples




G
era
text
AD; Anno Domini


y
year
year
2020; 20


D
day-of-year
number(3)
189


M/L
month-of-year
month
7; 07; Jul; July


d
day-of-month
number(2)
28


Q/q
quarter-of-year
number/text
3; 03; Q3; 3rd quarter


E
day-of-week
text
Tue; Tuesday


F
aligned day of week in month
number(1)
3


a
am-pm-of-day
am-pm
PM


h
clock-hour-of-am-pm (1-12)
number(2)
12


K
hour-of-am-pm (0-11)
number(2)
0


k
clock-hour-of-day (1-24)
number(2)
1


H
hour-of-day (0-23)
number(2)
0


m
minute-of-hour
number(2)
30


s
second-of-minute
number(2)
55


S
fraction-of-second
fraction
978


V
time-zone ID
zone-id
America/Los_Angeles; Z; -08:30


z
time-zone name
zone-name
Pacific Standard Time; PST


O
localized zone-offset
offset-O
GMT+8; GMT+08:00; UTC-08:00;


X
zone-offset ‘Z’ for zero
offset-X
Z; -08; -0830; -08:30; -083015; -08:30:15;


x
zone-offset
offset-x
+0000; -08; -0830; -08:30; -083015; -08:30:15;


Z
zone-offset
offset-Z
+0000; -0800; -08:00;


‘
escape for text
delimiter
 


’‘
single quote
literal
’


[
optional section start
 
 


]
optional section end
 
 



The count of pattern letters determines the format.


Text: The text style is determined based on the number of pattern letters used. Less than 4 pattern letters will use the short text form, typically an abbreviation, e.g. day-of-week Monday might output “Mon”. Exactly 4 pattern letters will use the full text form, typically the full description, e.g, day-of-week Monday might output “Monday”. 5 or more letters will fail.

Number(n): The n here represents the maximum count of letters this type of datetime pattern can be used.
    
In formatting, if the count of letters is one, then the value is output using the minimum number of digits and without padding otherwise, the count of digits is used as the width of the output field, with the value zero-padded as necessary.
In parsing, the exact count of digits is expected in the input field.



Number/Text: If the count of pattern letters is 3 or greater, use the Text rules above. Otherwise use the Number rules above.


Fraction: Use one or more (up to 9) contiguous 'S' characters, e,g SSSSSS, to parse and format fraction of second.
For parsing, the acceptable fraction length can be [1, the number of contiguous ‘S’].
For formatting, the fraction length would be padded to the number of contiguous ‘S’ with zeros.
Spark supports datetime of micro-of-second precision, which has up to 6 significant digits, but can parse nano-of-second with exceeded part truncated.


Year: The count of letters determines the minimum field width below which padding is used. If the count of letters is two, then a reduced two digit form is used. For printing, this outputs the rightmost two digits. For parsing, this will parse using the base value of 2000, resulting in a year within the range 2000 to 2099 inclusive. If the count of letters is less than four (but not two), then the sign is only output for negative years. Otherwise, the sign is output if the pad width is exceeded when ‘G’ is not present. 7 or more letters will fail.

Month: It follows the rule of Number/Text. The text form is depend on letters - ‘M’ denotes the ‘standard’ form, and ‘L’ is for ‘stand-alone’ form. These two forms are different only in some certain languages. For example, in Russian, ‘Июль’ is the stand-alone form of July, and ‘Июля’ is the standard form. Here are examples for all supported pattern letters:
    
'M' or 'L': Month number in a year starting from 1. There is no difference between ‘M’ and ‘L’. Month from 1 to 9 are printed without padding.
        spark-sql> select date_format(date '1970-01-01', "M");
1
spark-sql> select date_format(date '1970-12-01', "L");
12
 

'MM' or 'LL': Month number in a year starting from 1. Zero padding is added for month 1-9.
          spark-sql> select date_format(date '1970-1-01', "LL");
  01
  spark-sql> select date_format(date '1970-09-01', "MM");
  09
 

'MMM': Short textual representation in the standard form. The month pattern should be a part of a date pattern not just a stand-alone month except locales where there is no difference between stand and stand-alone forms like in English.
        spark-sql> select date_format(date '1970-01-01', "d MMM");
1 Jan
spark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'dd MMM', 'locale', 'RU'));
01 янв.
 

'LLL': Short textual representation in the stand-alone form. It should be used to format/parse only months without any other date fields.
        spark-sql> select date_format(date '1970-01-01', "LLL");
Jan
spark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'LLL', 'locale', 'RU'));
янв.
 

'MMMM': full textual month representation in the standard form. It is used for parsing/formatting months as a part of dates/timestamps.
        spark-sql> select date_format(date '1970-01-01', "d MMMM");
1 January
spark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'd MMMM', 'locale', 'RU'));
1 января
 

'LLLL': full textual month representation in the stand-alone form. The pattern can be used to format/parse only months.
        spark-sql> select date_format(date '1970-01-01', "LLLL");
January
spark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'LLLL', 'locale', 'RU'));
январь
 




am-pm: This outputs the am-pm-of-day. Pattern letter count must be 1.


Zone ID(V): This outputs the display the time-zone ID. Pattern letter count must be 2.


Zone names(z): This outputs the display textual name of the time-zone ID. If the count of letters is one, two or three, then the short name is output. If the count of letters is four, then the full name is output. Five or more letters will fail.


Offset X and x: This formats the offset based on the number of pattern letters. One letter outputs just the hour, such as ‘+01’, unless the minute is non-zero in which case the minute is also output, such as ‘+0130’. Two letters outputs the hour and minute, without a colon, such as ‘+0130’. Three letters outputs the hour and minute, with a colon, such as ‘+01:30’. Four letters outputs the hour and minute and optional second, without a colon, such as ‘+013015’. Five letters outputs the hour and minute and optional second, with a colon, such as ‘+01:30:15’. Six or more letters will fail. Pattern letter ‘X’ (upper case) will output ‘Z’ when the offset to be output would be zero, whereas pattern letter ‘x’ (lower case) will output ‘+00’, ‘+0000’, or ‘+00:00’.


Offset O: This formats the localized offset based on the number of pattern letters. One letter outputs the short form of the localized offset, which is localized offset text, such as ‘GMT’, with hour without leading zero, optional 2-digit minute and second if non-zero, and colon, for example ‘GMT+8’. Four letters outputs the full form, which is localized offset text, such as ‘GMT, with 2-digit hour and minute field, optional second field if non-zero, and colon, for example ‘GMT+08:00’. Any other count of letters will fail.


Offset Z: This formats the offset based on the number of pattern letters. One, two or three letters outputs the hour and minute, without a colon, such as ‘+0130’. The output will be ‘+0000’ when the offset is zero. Four letters outputs the full form of localized offset, equivalent to four letters of Offset-O. The output will be the corresponding localized offset text if the offset is zero. Five letters outputs the hour, minute, with optional second if non-zero, with colon. It outputs ‘Z’ if the offset is zero. Six or more letters will fail.


Optional section start and end: Use [] to define an optional section and maybe nested.
During formatting, all valid data will be output even it is in the optional section.
During parsing, the whole section may be missing from the parsed string.
An optional section is started by [ and ended using ] (or at the end of the pattern).

Symbols of ‘E’, ‘F’, ‘q’ and ‘Q’ can only be used for datetime formatting, e.g. date_format. They are not allowed used for datetime parsing, e.g. to_timestamp.





















  




Built-in Functions - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Built-in Functions
Aggregate Functions



Function
Description




any(expr)
Returns true if at least one value of `expr` is true.


any_value(expr[, isIgnoreNull])
Returns some value of `expr` for a group of rows.
      If `isIgnoreNull` is true, returns only non-null values.


approx_count_distinct(expr[, relativeSD])
Returns the estimated cardinality by HyperLogLog++.
      `relativeSD` defines the maximum relative standard deviation allowed.


approx_percentile(col, percentage [, accuracy])
Returns the approximate `percentile` of the numeric or
      ansi interval column `col` which is the smallest value in the ordered `col` values (sorted
      from least to greatest) such that no more than `percentage` of `col` values is less than
      the value or equal to that value. The value of percentage must be between 0.0 and 1.0.
      The `accuracy` parameter (default: 10000) is a positive numeric literal which controls
      approximation accuracy at the cost of memory. Higher value of `accuracy` yields better
      accuracy, `1.0/accuracy` is the relative error of the approximation.
      When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0.
      In this case, returns the approximate percentile array of column `col` at the given
      percentage array.


array_agg(expr)
Collects and returns a list of non-unique elements.


avg(expr)
Returns the mean calculated from values of a group.


bit_and(expr)
Returns the bitwise AND of all non-null input values, or null if none.


bit_or(expr)
Returns the bitwise OR of all non-null input values, or null if none.


bit_xor(expr)
Returns the bitwise XOR of all non-null input values, or null if none.


bitmap_construct_agg(child)
Returns a bitmap with the positions of the bits set from all the values from
    the child expression. The child expression will most likely be bitmap_bit_position().


bitmap_or_agg(child)
Returns a bitmap that is the bitwise OR of all of the bitmaps from the child
    expression. The input should be bitmaps created from bitmap_construct_agg().


bool_and(expr)
Returns true if all values of `expr` are true.


bool_or(expr)
Returns true if at least one value of `expr` is true.


collect_list(expr)
Collects and returns a list of non-unique elements.


collect_set(expr)
Collects and returns a set of unique elements.


corr(expr1, expr2)
Returns Pearson coefficient of correlation between a set of number pairs.


count(*)
Returns the total number of retrieved rows, including rows containing null.


    count(expr[, expr...])
Returns the number of rows for which the supplied expression(s) are all non-null.


    count(DISTINCT expr[, expr...])
Returns the number of rows for which the supplied expression(s) are unique and non-null.


count_if(expr)
Returns the number of `TRUE` values for the expression.


count_min_sketch(col, eps, confidence, seed)
Returns a count-min sketch of a column with the given esp,
      confidence and seed. The result is an array of bytes, which can be deserialized to a
      `CountMinSketch` before usage. Count-min sketch is a probabilistic data structure used for
      cardinality estimation using sub-linear space.


covar_pop(expr1, expr2)
Returns the population covariance of a set of number pairs.


covar_samp(expr1, expr2)
Returns the sample covariance of a set of number pairs.


every(expr)
Returns true if all values of `expr` are true.


first(expr[, isIgnoreNull])
Returns the first value of `expr` for a group of rows.
      If `isIgnoreNull` is true, returns only non-null values.


first_value(expr[, isIgnoreNull])
Returns the first value of `expr` for a group of rows.
      If `isIgnoreNull` is true, returns only non-null values.


grouping(col)
indicates whether a specified column in a GROUP BY is aggregated or
      not, returns 1 for aggregated or 0 for not aggregated in the result set.",


grouping_id([col1[, col2 ..]])
returns the level of grouping, equals to
      `(grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)`


histogram_numeric(expr, nb)
Computes a histogram on numeric 'expr' using nb bins.
      The return value is an array of (x,y) pairs representing the centers of the
      histogram's bins. As the value of 'nb' is increased, the histogram approximation
      gets finer-grained, but may yield artifacts around outliers. In practice, 20-40
      histogram bins appear to work well, with more bins being required for skewed or
      smaller datasets. Note that this function creates a histogram with non-uniform
      bin widths. It offers no guarantees in terms of the mean-squared-error of the
      histogram, but in practice is comparable to the histograms produced by the R/S-Plus
      statistical computing packages. Note: the output type of the 'x' field in the return value is
      propagated from the input value consumed in the aggregate function.


hll_sketch_agg(expr, lgConfigK)
Returns the HllSketch's updatable binary representation.
      `lgConfigK` (optional) the log-base-2 of K, with K is the number of buckets or
      slots for the HllSketch.


hll_union_agg(expr, allowDifferentLgConfigK)
Returns the estimated number of unique values.
      `allowDifferentLgConfigK` (optional) Allow sketches with different lgConfigK values
       to be unioned (defaults to false).


kurtosis(expr)
Returns the kurtosis value calculated from values of a group.


last(expr[, isIgnoreNull])
Returns the last value of `expr` for a group of rows.
      If `isIgnoreNull` is true, returns only non-null values


last_value(expr[, isIgnoreNull])
Returns the last value of `expr` for a group of rows.
      If `isIgnoreNull` is true, returns only non-null values


max(expr)
Returns the maximum value of `expr`.


max_by(x, y)
Returns the value of `x` associated with the maximum value of `y`.


mean(expr)
Returns the mean calculated from values of a group.


median(col)
Returns the median of numeric or ANSI interval column `col`.


min(expr)
Returns the minimum value of `expr`.


min_by(x, y)
Returns the value of `x` associated with the minimum value of `y`.


mode(col)
Returns the most frequent value for the values within `col`. NULL values are ignored. If all the values are NULL, or there are 0 rows, returns NULL.


percentile(col, percentage [, frequency])
Returns the exact percentile value of numeric
       or ANSI interval column `col` at the given percentage. The value of percentage must be
       between 0.0 and 1.0. The value of frequency should be positive integral


      percentile(col, array(percentage1 [, percentage2]...) [, frequency])
Returns the exact
      percentile value array of numeric column `col` at the given percentage(s). Each value
      of the percentage array must be between 0.0 and 1.0. The value of frequency should be
      positive integral


percentile_approx(col, percentage [, accuracy])
Returns the approximate `percentile` of the numeric or
      ansi interval column `col` which is the smallest value in the ordered `col` values (sorted
      from least to greatest) such that no more than `percentage` of `col` values is less than
      the value or equal to that value. The value of percentage must be between 0.0 and 1.0.
      The `accuracy` parameter (default: 10000) is a positive numeric literal which controls
      approximation accuracy at the cost of memory. Higher value of `accuracy` yields better
      accuracy, `1.0/accuracy` is the relative error of the approximation.
      When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0.
      In this case, returns the approximate percentile array of column `col` at the given
      percentage array.


regr_avgx(y, x)
Returns the average of the independent variable for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_avgy(y, x)
Returns the average of the dependent variable for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_count(y, x)
Returns the number of non-null number pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_intercept(y, x)
Returns the intercept of the univariate linear regression line for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_r2(y, x)
Returns the coefficient of determination for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_slope(y, x)
Returns the slope of the linear regression line for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_sxx(y, x)
Returns REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_sxy(y, x)
Returns REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


regr_syy(y, x)
Returns REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs in a group, where `y` is the dependent variable and `x` is the independent variable.


skewness(expr)
Returns the skewness value calculated from values of a group.


some(expr)
Returns true if at least one value of `expr` is true.


std(expr)
Returns the sample standard deviation calculated from values of a group.


stddev(expr)
Returns the sample standard deviation calculated from values of a group.


stddev_pop(expr)
Returns the population standard deviation calculated from values of a group.


stddev_samp(expr)
Returns the sample standard deviation calculated from values of a group.


sum(expr)
Returns the sum calculated from values of a group.


try_avg(expr)
Returns the mean calculated from values of a group and the result is null on overflow.


try_sum(expr)
Returns the sum calculated from values of a group and the result is null on overflow.


var_pop(expr)
Returns the population variance calculated from values of a group.


var_samp(expr)
Returns the sample variance calculated from values of a group.


variance(expr)
Returns the sample variance calculated from values of a group.



Examples
-- any
SELECT any(col) FROM VALUES (true), (false), (false) AS tab(col);
+--------+
|any(col)|
+--------+
|    true|
+--------+

SELECT any(col) FROM VALUES (NULL), (true), (false) AS tab(col);
+--------+
|any(col)|
+--------+
|    true|
+--------+

SELECT any(col) FROM VALUES (false), (false), (NULL) AS tab(col);
+--------+
|any(col)|
+--------+
|   false|
+--------+

-- any_value
SELECT any_value(col) FROM VALUES (10), (5), (20) AS tab(col);
+--------------+
|any_value(col)|
+--------------+
|            10|
+--------------+

SELECT any_value(col) FROM VALUES (NULL), (5), (20) AS tab(col);
+--------------+
|any_value(col)|
+--------------+
|          NULL|
+--------------+

SELECT any_value(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
+--------------+
|any_value(col)|
+--------------+
|             5|
+--------------+

-- approx_count_distinct
SELECT approx_count_distinct(col1) FROM VALUES (1), (1), (2), (2), (3) tab(col1);
+---------------------------+
|approx_count_distinct(col1)|
+---------------------------+
|                          3|
+---------------------------+

-- approx_percentile
SELECT approx_percentile(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);
+-------------------------------------------------+
|approx_percentile(col, array(0.5, 0.4, 0.1), 100)|
+-------------------------------------------------+
|                                        [1, 1, 0]|
+-------------------------------------------------+

SELECT approx_percentile(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);
+--------------------------------+
|approx_percentile(col, 0.5, 100)|
+--------------------------------+
|                               7|
+--------------------------------+

SELECT approx_percentile(col, 0.5, 100) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '1' MONTH), (INTERVAL '2' MONTH), (INTERVAL '10' MONTH) AS tab(col);
+--------------------------------+
|approx_percentile(col, 0.5, 100)|
+--------------------------------+
|              INTERVAL '1' MONTH|
+--------------------------------+

SELECT approx_percentile(col, array(0.5, 0.7), 100) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '1' SECOND), (INTERVAL '2' SECOND), (INTERVAL '10' SECOND) AS tab(col);
+--------------------------------------------+
|approx_percentile(col, array(0.5, 0.7), 100)|
+--------------------------------------------+
|                        [INTERVAL '01' SE...|
+--------------------------------------------+

-- array_agg
SELECT array_agg(col) FROM VALUES (1), (2), (1) AS tab(col);
+-----------------+
|collect_list(col)|
+-----------------+
|        [1, 2, 1]|
+-----------------+

-- avg
SELECT avg(col) FROM VALUES (1), (2), (3) AS tab(col);
+--------+
|avg(col)|
+--------+
|     2.0|
+--------+

SELECT avg(col) FROM VALUES (1), (2), (NULL) AS tab(col);
+--------+
|avg(col)|
+--------+
|     1.5|
+--------+

-- bit_and
SELECT bit_and(col) FROM VALUES (3), (5) AS tab(col);
+------------+
|bit_and(col)|
+------------+
|           1|
+------------+

-- bit_or
SELECT bit_or(col) FROM VALUES (3), (5) AS tab(col);
+-----------+
|bit_or(col)|
+-----------+
|          7|
+-----------+

-- bit_xor
SELECT bit_xor(col) FROM VALUES (3), (5) AS tab(col);
+------------+
|bit_xor(col)|
+------------+
|           6|
+------------+

-- bitmap_construct_agg
SELECT substring(hex(bitmap_construct_agg(bitmap_bit_position(col))), 0, 6) FROM VALUES (1), (2), (3) AS tab(col);
+--------------------------------------------------------------------+
|substring(hex(bitmap_construct_agg(bitmap_bit_position(col))), 0, 6)|
+--------------------------------------------------------------------+
|                                                              070000|
+--------------------------------------------------------------------+

SELECT substring(hex(bitmap_construct_agg(bitmap_bit_position(col))), 0, 6) FROM VALUES (1), (1), (1) AS tab(col);
+--------------------------------------------------------------------+
|substring(hex(bitmap_construct_agg(bitmap_bit_position(col))), 0, 6)|
+--------------------------------------------------------------------+
|                                                              010000|
+--------------------------------------------------------------------+

-- bitmap_or_agg
SELECT substring(hex(bitmap_or_agg(col)), 0, 6) FROM VALUES (X '10'), (X '20'), (X '40') AS tab(col);
+----------------------------------------+
|substring(hex(bitmap_or_agg(col)), 0, 6)|
+----------------------------------------+
|                                  700000|
+----------------------------------------+

SELECT substring(hex(bitmap_or_agg(col)), 0, 6) FROM VALUES (X '10'), (X '10'), (X '10') AS tab(col);
+----------------------------------------+
|substring(hex(bitmap_or_agg(col)), 0, 6)|
+----------------------------------------+
|                                  100000|
+----------------------------------------+

-- bool_and
SELECT bool_and(col) FROM VALUES (true), (true), (true) AS tab(col);
+-------------+
|bool_and(col)|
+-------------+
|         true|
+-------------+

SELECT bool_and(col) FROM VALUES (NULL), (true), (true) AS tab(col);
+-------------+
|bool_and(col)|
+-------------+
|         true|
+-------------+

SELECT bool_and(col) FROM VALUES (true), (false), (true) AS tab(col);
+-------------+
|bool_and(col)|
+-------------+
|        false|
+-------------+

-- bool_or
SELECT bool_or(col) FROM VALUES (true), (false), (false) AS tab(col);
+------------+
|bool_or(col)|
+------------+
|        true|
+------------+

SELECT bool_or(col) FROM VALUES (NULL), (true), (false) AS tab(col);
+------------+
|bool_or(col)|
+------------+
|        true|
+------------+

SELECT bool_or(col) FROM VALUES (false), (false), (NULL) AS tab(col);
+------------+
|bool_or(col)|
+------------+
|       false|
+------------+

-- collect_list
SELECT collect_list(col) FROM VALUES (1), (2), (1) AS tab(col);
+-----------------+
|collect_list(col)|
+-----------------+
|        [1, 2, 1]|
+-----------------+

-- collect_set
SELECT collect_set(col) FROM VALUES (1), (2), (1) AS tab(col);
+----------------+
|collect_set(col)|
+----------------+
|          [1, 2]|
+----------------+

-- corr
SELECT corr(c1, c2) FROM VALUES (3, 2), (3, 3), (6, 4) as tab(c1, c2);
+------------------+
|      corr(c1, c2)|
+------------------+
|0.8660254037844387|
+------------------+

-- count
SELECT count(*) FROM VALUES (NULL), (5), (5), (20) AS tab(col);
+--------+
|count(1)|
+--------+
|       4|
+--------+

SELECT count(col) FROM VALUES (NULL), (5), (5), (20) AS tab(col);
+----------+
|count(col)|
+----------+
|         3|
+----------+

SELECT count(DISTINCT col) FROM VALUES (NULL), (5), (5), (10) AS tab(col);
+-------------------+
|count(DISTINCT col)|
+-------------------+
|                  2|
+-------------------+

-- count_if
SELECT count_if(col % 2 = 0) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);
+-------------------------+
|count_if(((col % 2) = 0))|
+-------------------------+
|                        2|
+-------------------------+

SELECT count_if(col IS NULL) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);
+-----------------------+
|count_if((col IS NULL))|
+-----------------------+
|                      1|
+-----------------------+

-- count_min_sketch
SELECT hex(count_min_sketch(col, 0.5d, 0.5d, 1)) FROM VALUES (1), (2), (1) AS tab(col);
+---------------------------------------+
|hex(count_min_sketch(col, 0.5, 0.5, 1))|
+---------------------------------------+
|                   00000001000000000...|
+---------------------------------------+

-- covar_pop
SELECT covar_pop(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);
+------------------+
| covar_pop(c1, c2)|
+------------------+
|0.6666666666666666|
+------------------+

-- covar_samp
SELECT covar_samp(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);
+------------------+
|covar_samp(c1, c2)|
+------------------+
|               1.0|
+------------------+

-- every
SELECT every(col) FROM VALUES (true), (true), (true) AS tab(col);
+----------+
|every(col)|
+----------+
|      true|
+----------+

SELECT every(col) FROM VALUES (NULL), (true), (true) AS tab(col);
+----------+
|every(col)|
+----------+
|      true|
+----------+

SELECT every(col) FROM VALUES (true), (false), (true) AS tab(col);
+----------+
|every(col)|
+----------+
|     false|
+----------+

-- first
SELECT first(col) FROM VALUES (10), (5), (20) AS tab(col);
+----------+
|first(col)|
+----------+
|        10|
+----------+

SELECT first(col) FROM VALUES (NULL), (5), (20) AS tab(col);
+----------+
|first(col)|
+----------+
|      NULL|
+----------+

SELECT first(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
+----------+
|first(col)|
+----------+
|         5|
+----------+

-- first_value
SELECT first_value(col) FROM VALUES (10), (5), (20) AS tab(col);
+----------------+
|first_value(col)|
+----------------+
|              10|
+----------------+

SELECT first_value(col) FROM VALUES (NULL), (5), (20) AS tab(col);
+----------------+
|first_value(col)|
+----------------+
|            NULL|
+----------------+

SELECT first_value(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
+----------------+
|first_value(col)|
+----------------+
|               5|
+----------------+

-- grouping
SELECT name, grouping(name), sum(age) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY cube(name);
+-----+--------------+--------+
| name|grouping(name)|sum(age)|
+-----+--------------+--------+
| NULL|             1|       7|
|Alice|             0|       2|
|  Bob|             0|       5|
+-----+--------------+--------+

-- grouping_id
SELECT name, grouping_id(), sum(age), avg(height) FROM VALUES (2, 'Alice', 165), (5, 'Bob', 180) people(age, name, height) GROUP BY cube(name, height);
+-----+-------------+--------+-----------+
| name|grouping_id()|sum(age)|avg(height)|
+-----+-------------+--------+-----------+
| NULL|            2|       2|      165.0|
|Alice|            0|       2|      165.0|
|Alice|            1|       2|      165.0|
| NULL|            3|       7|      172.5|
|  Bob|            1|       5|      180.0|
|  Bob|            0|       5|      180.0|
| NULL|            2|       5|      180.0|
+-----+-------------+--------+-----------+

-- histogram_numeric
SELECT histogram_numeric(col, 5) FROM VALUES (0), (1), (2), (10) AS tab(col);
+-------------------------+
|histogram_numeric(col, 5)|
+-------------------------+
|     [{0, 1.0}, {1, 1....|
+-------------------------+

-- hll_sketch_agg
SELECT hll_sketch_estimate(hll_sketch_agg(col, 12)) FROM VALUES (1), (1), (2), (2), (3) tab(col);
+--------------------------------------------+
|hll_sketch_estimate(hll_sketch_agg(col, 12))|
+--------------------------------------------+
|                                           3|
+--------------------------------------------+

-- hll_union_agg
SELECT hll_sketch_estimate(hll_union_agg(sketch, true)) FROM (SELECT hll_sketch_agg(col) as sketch FROM VALUES (1) tab(col) UNION ALL SELECT hll_sketch_agg(col, 20) as sketch FROM VALUES (1) tab(col));
+------------------------------------------------+
|hll_sketch_estimate(hll_union_agg(sketch, true))|
+------------------------------------------------+
|                                               1|
+------------------------------------------------+

-- kurtosis
SELECT kurtosis(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);
+-------------------+
|      kurtosis(col)|
+-------------------+
|-0.7014368047529618|
+-------------------+

SELECT kurtosis(col) FROM VALUES (1), (10), (100), (10), (1) as tab(col);
+-------------------+
|      kurtosis(col)|
+-------------------+
|0.19432323191698986|
+-------------------+

-- last
SELECT last(col) FROM VALUES (10), (5), (20) AS tab(col);
+---------+
|last(col)|
+---------+
|       20|
+---------+

SELECT last(col) FROM VALUES (10), (5), (NULL) AS tab(col);
+---------+
|last(col)|
+---------+
|     NULL|
+---------+

SELECT last(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);
+---------+
|last(col)|
+---------+
|        5|
+---------+

-- last_value
SELECT last_value(col) FROM VALUES (10), (5), (20) AS tab(col);
+---------------+
|last_value(col)|
+---------------+
|             20|
+---------------+

SELECT last_value(col) FROM VALUES (10), (5), (NULL) AS tab(col);
+---------------+
|last_value(col)|
+---------------+
|           NULL|
+---------------+

SELECT last_value(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);
+---------------+
|last_value(col)|
+---------------+
|              5|
+---------------+

-- max
SELECT max(col) FROM VALUES (10), (50), (20) AS tab(col);
+--------+
|max(col)|
+--------+
|      50|
+--------+

-- max_by
SELECT max_by(x, y) FROM VALUES ('a', 10), ('b', 50), ('c', 20) AS tab(x, y);
+------------+
|max_by(x, y)|
+------------+
|           b|
+------------+

-- mean
SELECT mean(col) FROM VALUES (1), (2), (3) AS tab(col);
+---------+
|mean(col)|
+---------+
|      2.0|
+---------+

SELECT mean(col) FROM VALUES (1), (2), (NULL) AS tab(col);
+---------+
|mean(col)|
+---------+
|      1.5|
+---------+

-- median
SELECT median(col) FROM VALUES (0), (10) AS tab(col);
+-----------+
|median(col)|
+-----------+
|        5.0|
+-----------+

SELECT median(col) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);
+--------------------+
|         median(col)|
+--------------------+
|INTERVAL '0-5' YE...|
+--------------------+

-- min
SELECT min(col) FROM VALUES (10), (-1), (20) AS tab(col);
+--------+
|min(col)|
+--------+
|      -1|
+--------+

-- min_by
SELECT min_by(x, y) FROM VALUES ('a', 10), ('b', 50), ('c', 20) AS tab(x, y);
+------------+
|min_by(x, y)|
+------------+
|           a|
+------------+

-- mode
SELECT mode(col) FROM VALUES (0), (10), (10) AS tab(col);
+---------+
|mode(col)|
+---------+
|       10|
+---------+

SELECT mode(col) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH), (INTERVAL '10' MONTH) AS tab(col);
+-------------------+
|          mode(col)|
+-------------------+
|INTERVAL '10' MONTH|
+-------------------+

SELECT mode(col) FROM VALUES (0), (10), (10), (null), (null), (null) AS tab(col);
+---------+
|mode(col)|
+---------+
|       10|
+---------+

-- percentile
SELECT percentile(col, 0.3) FROM VALUES (0), (10) AS tab(col);
+-----------------------+
|percentile(col, 0.3, 1)|
+-----------------------+
|                    3.0|
+-----------------------+

SELECT percentile(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col);
+-------------------------------------+
|percentile(col, array(0.25, 0.75), 1)|
+-------------------------------------+
|                           [2.5, 7.5]|
+-------------------------------------+

SELECT percentile(col, 0.5) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);
+-----------------------+
|percentile(col, 0.5, 1)|
+-----------------------+
|   INTERVAL '0-5' YE...|
+-----------------------+

SELECT percentile(col, array(0.2, 0.5)) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '10' SECOND) AS tab(col);
+-----------------------------------+
|percentile(col, array(0.2, 0.5), 1)|
+-----------------------------------+
|               [INTERVAL '0 00:0...|
+-----------------------------------+

-- percentile_approx
SELECT percentile_approx(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);
+-------------------------------------------------+
|percentile_approx(col, array(0.5, 0.4, 0.1), 100)|
+-------------------------------------------------+
|                                        [1, 1, 0]|
+-------------------------------------------------+

SELECT percentile_approx(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);
+--------------------------------+
|percentile_approx(col, 0.5, 100)|
+--------------------------------+
|                               7|
+--------------------------------+

SELECT percentile_approx(col, 0.5, 100) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '1' MONTH), (INTERVAL '2' MONTH), (INTERVAL '10' MONTH) AS tab(col);
+--------------------------------+
|percentile_approx(col, 0.5, 100)|
+--------------------------------+
|              INTERVAL '1' MONTH|
+--------------------------------+

SELECT percentile_approx(col, array(0.5, 0.7), 100) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '1' SECOND), (INTERVAL '2' SECOND), (INTERVAL '10' SECOND) AS tab(col);
+--------------------------------------------+
|percentile_approx(col, array(0.5, 0.7), 100)|
+--------------------------------------------+
|                        [INTERVAL '01' SE...|
+--------------------------------------------+

-- regr_avgx
SELECT regr_avgx(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
+---------------+
|regr_avgx(y, x)|
+---------------+
|           2.75|
+---------------+

SELECT regr_avgx(y, x) FROM VALUES (1, null) AS tab(y, x);
+---------------+
|regr_avgx(y, x)|
+---------------+
|           NULL|
+---------------+

SELECT regr_avgx(y, x) FROM VALUES (null, 1) AS tab(y, x);
+---------------+
|regr_avgx(y, x)|
+---------------+
|           NULL|
+---------------+

SELECT regr_avgx(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
+---------------+
|regr_avgx(y, x)|
+---------------+
|            3.0|
+---------------+

SELECT regr_avgx(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
+---------------+
|regr_avgx(y, x)|
+---------------+
|            3.0|
+---------------+

-- regr_avgy
SELECT regr_avgy(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
+---------------+
|regr_avgy(y, x)|
+---------------+
|           1.75|
+---------------+

SELECT regr_avgy(y, x) FROM VALUES (1, null) AS tab(y, x);
+---------------+
|regr_avgy(y, x)|
+---------------+
|           NULL|
+---------------+

SELECT regr_avgy(y, x) FROM VALUES (null, 1) AS tab(y, x);
+---------------+
|regr_avgy(y, x)|
+---------------+
|           NULL|
+---------------+

SELECT regr_avgy(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
+------------------+
|   regr_avgy(y, x)|
+------------------+
|1.6666666666666667|
+------------------+

SELECT regr_avgy(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
+---------------+
|regr_avgy(y, x)|
+---------------+
|            1.5|
+---------------+

-- regr_count
SELECT regr_count(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
+----------------+
|regr_count(y, x)|
+----------------+
|               4|
+----------------+

SELECT regr_count(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
+----------------+
|regr_count(y, x)|
+----------------+
|               3|
+----------------+

SELECT regr_count(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
+----------------+
|regr_count(y, x)|
+----------------+
|               2|
+----------------+

-- regr_intercept
SELECT regr_intercept(y, x) FROM VALUES (1,1), (2,2), (3,3) AS tab(y, x);
+--------------------+
|regr_intercept(y, x)|
+--------------------+
|                 0.0|
+--------------------+

SELECT regr_intercept(y, x) FROM VALUES (1, null) AS tab(y, x);
+--------------------+
|regr_intercept(y, x)|
+--------------------+
|                NULL|
+--------------------+

SELECT regr_intercept(y, x) FROM VALUES (null, 1) AS tab(y, x);
+--------------------+
|regr_intercept(y, x)|
+--------------------+
|                NULL|
+--------------------+

-- regr_r2
SELECT regr_r2(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
+------------------+
|     regr_r2(y, x)|
+------------------+
|0.2727272727272726|
+------------------+

SELECT regr_r2(y, x) FROM VALUES (1, null) AS tab(y, x);
+-------------+
|regr_r2(y, x)|
+-------------+
|         NULL|
+-------------+

SELECT regr_r2(y, x) FROM VALUES (null, 1) AS tab(y, x);
+-------------+
|regr_r2(y, x)|
+-------------+
|         NULL|
+-------------+

SELECT regr_r2(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
+------------------+
|     regr_r2(y, x)|
+------------------+
|0.7500000000000001|
+------------------+

SELECT regr_r2(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
+-------------+
|regr_r2(y, x)|
+-------------+
|          1.0|
+-------------+

-- regr_slope
SELECT regr_slope(y, x) FROM VALUES (1,1), (2,2), (3,3) AS tab(y, x);
+----------------+
|regr_slope(y, x)|
+----------------+
|             1.0|
+----------------+

SELECT regr_slope(y, x) FROM VALUES (1, null) AS tab(y, x);
+----------------+
|regr_slope(y, x)|
+----------------+
|            NULL|
+----------------+

SELECT regr_slope(y, x) FROM VALUES (null, 1) AS tab(y, x);
+----------------+
|regr_slope(y, x)|
+----------------+
|            NULL|
+----------------+

-- regr_sxx
SELECT regr_sxx(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
+------------------+
|    regr_sxx(y, x)|
+------------------+
|2.7499999999999996|
+------------------+

SELECT regr_sxx(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
+--------------+
|regr_sxx(y, x)|
+--------------+
|           2.0|
+--------------+

SELECT regr_sxx(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
+--------------+
|regr_sxx(y, x)|
+--------------+
|           2.0|
+--------------+

-- regr_sxy
SELECT regr_sxy(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
+------------------+
|    regr_sxy(y, x)|
+------------------+
|0.7499999999999998|
+------------------+

SELECT regr_sxy(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
+--------------+
|regr_sxy(y, x)|
+--------------+
|           1.0|
+--------------+

SELECT regr_sxy(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
+--------------+
|regr_sxy(y, x)|
+--------------+
|           1.0|
+--------------+

-- regr_syy
SELECT regr_syy(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);
+------------------+
|    regr_syy(y, x)|
+------------------+
|0.7499999999999999|
+------------------+

SELECT regr_syy(y, x) FROM VALUES (1, 2), (2, null), (2, 3), (2, 4) AS tab(y, x);
+------------------+
|    regr_syy(y, x)|
+------------------+
|0.6666666666666666|
+------------------+

SELECT regr_syy(y, x) FROM VALUES (1, 2), (2, null), (null, 3), (2, 4) AS tab(y, x);
+--------------+
|regr_syy(y, x)|
+--------------+
|           0.5|
+--------------+

-- skewness
SELECT skewness(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);
+------------------+
|     skewness(col)|
+------------------+
|1.1135657469022013|
+------------------+

SELECT skewness(col) FROM VALUES (-1000), (-100), (10), (20) AS tab(col);
+-------------------+
|      skewness(col)|
+-------------------+
|-1.1135657469022011|
+-------------------+

-- some
SELECT some(col) FROM VALUES (true), (false), (false) AS tab(col);
+---------+
|some(col)|
+---------+
|     true|
+---------+

SELECT some(col) FROM VALUES (NULL), (true), (false) AS tab(col);
+---------+
|some(col)|
+---------+
|     true|
+---------+

SELECT some(col) FROM VALUES (false), (false), (NULL) AS tab(col);
+---------+
|some(col)|
+---------+
|    false|
+---------+

-- std
SELECT std(col) FROM VALUES (1), (2), (3) AS tab(col);
+--------+
|std(col)|
+--------+
|     1.0|
+--------+

-- stddev
SELECT stddev(col) FROM VALUES (1), (2), (3) AS tab(col);
+-----------+
|stddev(col)|
+-----------+
|        1.0|
+-----------+

-- stddev_pop
SELECT stddev_pop(col) FROM VALUES (1), (2), (3) AS tab(col);
+-----------------+
|  stddev_pop(col)|
+-----------------+
|0.816496580927726|
+-----------------+

-- stddev_samp
SELECT stddev_samp(col) FROM VALUES (1), (2), (3) AS tab(col);
+----------------+
|stddev_samp(col)|
+----------------+
|             1.0|
+----------------+

-- sum
SELECT sum(col) FROM VALUES (5), (10), (15) AS tab(col);
+--------+
|sum(col)|
+--------+
|      30|
+--------+

SELECT sum(col) FROM VALUES (NULL), (10), (15) AS tab(col);
+--------+
|sum(col)|
+--------+
|      25|
+--------+

SELECT sum(col) FROM VALUES (NULL), (NULL) AS tab(col);
+--------+
|sum(col)|
+--------+
|    NULL|
+--------+

-- try_avg
SELECT try_avg(col) FROM VALUES (1), (2), (3) AS tab(col);
+------------+
|try_avg(col)|
+------------+
|         2.0|
+------------+

SELECT try_avg(col) FROM VALUES (1), (2), (NULL) AS tab(col);
+------------+
|try_avg(col)|
+------------+
|         1.5|
+------------+

SELECT try_avg(col) FROM VALUES (interval '2147483647 months'), (interval '1 months') AS tab(col);
+------------+
|try_avg(col)|
+------------+
|        NULL|
+------------+

-- try_sum
SELECT try_sum(col) FROM VALUES (5), (10), (15) AS tab(col);
+------------+
|try_sum(col)|
+------------+
|          30|
+------------+

SELECT try_sum(col) FROM VALUES (NULL), (10), (15) AS tab(col);
+------------+
|try_sum(col)|
+------------+
|          25|
+------------+

SELECT try_sum(col) FROM VALUES (NULL), (NULL) AS tab(col);
+------------+
|try_sum(col)|
+------------+
|        NULL|
+------------+

SELECT try_sum(col) FROM VALUES (9223372036854775807L), (1L) AS tab(col);
+------------+
|try_sum(col)|
+------------+
|        NULL|
+------------+

-- var_pop
SELECT var_pop(col) FROM VALUES (1), (2), (3) AS tab(col);
+------------------+
|      var_pop(col)|
+------------------+
|0.6666666666666666|
+------------------+

-- var_samp
SELECT var_samp(col) FROM VALUES (1), (2), (3) AS tab(col);
+-------------+
|var_samp(col)|
+-------------+
|          1.0|
+-------------+

-- variance
SELECT variance(col) FROM VALUES (1), (2), (3) AS tab(col);
+-------------+
|variance(col)|
+-------------+
|          1.0|
+-------------+

Window Functions



Function
Description




cume_dist()
Computes the position of a value relative to all values in the partition.


dense_rank()
Computes the rank of a value in a group of values. The result is one plus the
      previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps
      in the ranking sequence.


lag(input[, offset[, default]])
Returns the value of `input` at the `offset`th row
      before the current row in the window. The default value of `offset` is 1 and the default
      value of `default` is null. If the value of `input` at the `offset`th row is null,
      null is returned. If there is no such offset row (e.g., when the offset is 1, the first
      row of the window does not have any previous row), `default` is returned.


lead(input[, offset[, default]])
Returns the value of `input` at the `offset`th row
      after the current row in the window. The default value of `offset` is 1 and the default
      value of `default` is null. If the value of `input` at the `offset`th row is null,
      null is returned. If there is no such an offset row (e.g., when the offset is 1, the last
      row of the window does not have any subsequent row), `default` is returned.


nth_value(input[, offset])
Returns the value of `input` at the row that is the `offset`th row
      from beginning of the window frame. Offset starts at 1. If ignoreNulls=true, we will skip
      nulls when finding the `offset`th row. Otherwise, every row counts for the `offset`. If
      there is no such an `offset`th row (e.g., when the offset is 10, size of the window frame
      is less than 10), null is returned.


ntile(n)
Divides the rows for each window partition into `n` buckets ranging
      from 1 to at most `n`.


percent_rank()
Computes the percentage ranking of a value in a group of values.


rank()
Computes the rank of a value in a group of values. The result is one plus the number
      of rows preceding or equal to the current row in the ordering of the partition. The values
      will produce gaps in the sequence.


row_number()
Assigns a unique, sequential number to each row, starting with one,
      according to the ordering of rows within the window partition.



Examples
-- cume_dist
SELECT a, b, cume_dist() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+--------------------------------------------------------------------------------------------------------------+
|  a|  b|cume_dist() OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|
+---+---+--------------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                            0.6666666666666666|
| A1|  1|                                                                                            0.6666666666666666|
| A1|  2|                                                                                                           1.0|
| A2|  3|                                                                                                           1.0|
+---+---+--------------------------------------------------------------------------------------------------------------+

-- dense_rank
SELECT a, b, dense_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+--------------------------------------------------------------------------------------------------------------+
|  a|  b|DENSE_RANK() OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|
+---+---+--------------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                             1|
| A1|  1|                                                                                                             1|
| A1|  2|                                                                                                             2|
| A2|  3|                                                                                                             1|
+---+---+--------------------------------------------------------------------------------------------------------------+

-- lag
SELECT a, b, lag(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+-----------------------------------------------------------------------------------------------------------+
|  a|  b|lag(b, 1, NULL) OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST ROWS BETWEEN -1 FOLLOWING AND -1 FOLLOWING)|
+---+---+-----------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                       NULL|
| A1|  1|                                                                                                          1|
| A1|  2|                                                                                                          1|
| A2|  3|                                                                                                       NULL|
+---+---+-----------------------------------------------------------------------------------------------------------+

-- lead
SELECT a, b, lead(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+----------------------------------------------------------------------------------------------------------+
|  a|  b|lead(b, 1, NULL) OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING)|
+---+---+----------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                         1|
| A1|  1|                                                                                                         2|
| A1|  2|                                                                                                      NULL|
| A2|  3|                                                                                                      NULL|
+---+---+----------------------------------------------------------------------------------------------------------+

-- nth_value
SELECT a, b, nth_value(b, 2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+------------------------------------------------------------------------------------------------------------------+
|  a|  b|nth_value(b, 2) OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|
+---+---+------------------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                                 1|
| A1|  1|                                                                                                                 1|
| A1|  2|                                                                                                                 1|
| A2|  3|                                                                                                              NULL|
+---+---+------------------------------------------------------------------------------------------------------------------+

-- ntile
SELECT a, b, ntile(2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+----------------------------------------------------------------------------------------------------------+
|  a|  b|ntile(2) OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|
+---+---+----------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                         1|
| A1|  1|                                                                                                         1|
| A1|  2|                                                                                                         2|
| A2|  3|                                                                                                         1|
+---+---+----------------------------------------------------------------------------------------------------------+

-- percent_rank
SELECT a, b, percent_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+----------------------------------------------------------------------------------------------------------------+
|  a|  b|PERCENT_RANK() OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|
+---+---+----------------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                             0.0|
| A1|  1|                                                                                                             0.0|
| A1|  2|                                                                                                             1.0|
| A2|  3|                                                                                                             0.0|
+---+---+----------------------------------------------------------------------------------------------------------------+

-- rank
SELECT a, b, rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+--------------------------------------------------------------------------------------------------------+
|  a|  b|RANK() OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|
+---+---+--------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                       1|
| A1|  1|                                                                                                       1|
| A1|  2|                                                                                                       3|
| A2|  3|                                                                                                       1|
+---+---+--------------------------------------------------------------------------------------------------------+

-- row_number
SELECT a, b, row_number() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
+---+---+--------------------------------------------------------------------------------------------------------------+
|  a|  b|row_number() OVER (PARTITION BY a ORDER BY b ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|
+---+---+--------------------------------------------------------------------------------------------------------------+
| A1|  1|                                                                                                             1|
| A1|  1|                                                                                                             2|
| A1|  2|                                                                                                             3|
| A2|  3|                                                                                                             1|
+---+---+--------------------------------------------------------------------------------------------------------------+

Array Functions



Function
Description




array(expr, ...)
Returns an array with the given elements.


array_append(array, element)
Add the element at the end of the array passed as first
      argument. Type of element should be similar to type of the elements of the array.
      Null element is also appended into the array. But if the array passed, is NULL
      output is NULL


array_compact(array)
Removes null values from the array.


array_contains(array, value)
Returns true if the array contains the value.


array_distinct(array)
Removes duplicate values from the array.


array_except(array1, array2)
Returns an array of the elements in array1 but not in array2,
    without duplicates.


array_insert(x, pos, val)
Places val into index pos of array x.
      Array indices start at 1. The maximum negative index is -1 for which the function inserts
      new element after the current last element.
      Index above array size appends the array, or prepends the array if index is negative,
      with 'null' elements.


array_intersect(array1, array2)
Returns an array of the elements in the intersection of array1 and
    array2, without duplicates.


array_join(array, delimiter[, nullReplacement])
Concatenates the elements of the given array
      using the delimiter and an optional string to replace nulls. If no value is set for
      nullReplacement, any null value is filtered.


array_max(array)
Returns the maximum value in the array. NaN is greater than
    any non-NaN elements for double/float type. NULL elements are skipped.


array_min(array)
Returns the minimum value in the array. NaN is greater than
    any non-NaN elements for double/float type. NULL elements are skipped.


array_position(array, element)
Returns the (1-based) index of the first matching element of
      the array as long, or 0 if no match is found.


array_prepend(array, element)
Add the element at the beginning of the array passed as first
      argument. Type of element should be the same as the type of the elements of the array.
      Null element is also prepended to the array. But if the array passed is NULL
      output is NULL


array_remove(array, element)
Remove all elements that equal to element from array.


array_repeat(element, count)
Returns the array containing element count times.


array_union(array1, array2)
Returns an array of the elements in the union of array1 and array2,
      without duplicates.


arrays_overlap(a1, a2)
Returns true if a1 contains at least a non-null element present also in a2. If the arrays have no common element and they are both non-empty and either of them contains a null element null is returned, false otherwise.


arrays_zip(a1, a2, ...)
Returns a merged array of structs in which the N-th struct contains all
    N-th values of input arrays.


flatten(arrayOfArrays)
Transforms an array of arrays into a single array.


get(array, index)
Returns element of array at given (0-based) index. If the index points
     outside of the array boundaries, then this function returns NULL.


sequence(start, stop, step)
Generates an array of elements from start to stop (inclusive),
      incrementing by step. The type of the returned elements is the same as the type of argument
      expressions.

      Supported types are: byte, short, integer, long, date, timestamp.

      The start and stop expressions must resolve to the same type.
      If start and stop expressions resolve to the 'date' or 'timestamp' type
      then the step expression must resolve to the 'interval' or 'year-month interval' or
      'day-time interval' type, otherwise to the same type as the start and stop expressions.


shuffle(array)
Returns a random permutation of the given array.


slice(x, start, length)
Subsets array x starting from index start (array indices start at 1, or starting from the end if start is negative) with the specified length.


sort_array(array[, ascendingOrder])
Sorts the input array in ascending or descending order
      according to the natural ordering of the array elements. NaN is greater than any non-NaN
      elements for double/float type. Null elements will be placed at the beginning of the returned
      array in ascending order or at the end of the returned array in descending order.



Examples
-- array
SELECT array(1, 2, 3);
+--------------+
|array(1, 2, 3)|
+--------------+
|     [1, 2, 3]|
+--------------+

-- array_append
SELECT array_append(array('b', 'd', 'c', 'a'), 'd');
+----------------------------------+
|array_append(array(b, d, c, a), d)|
+----------------------------------+
|                   [b, d, c, a, d]|
+----------------------------------+

SELECT array_append(array(1, 2, 3, null), null);
+----------------------------------------+
|array_append(array(1, 2, 3, NULL), NULL)|
+----------------------------------------+
|                    [1, 2, 3, NULL, N...|
+----------------------------------------+

SELECT array_append(CAST(null as Array<Int>), 2);
+---------------------+
|array_append(NULL, 2)|
+---------------------+
|                 NULL|
+---------------------+

-- array_compact
SELECT array_compact(array(1, 2, 3, null));
+-----------------------------------+
|array_compact(array(1, 2, 3, NULL))|
+-----------------------------------+
|                          [1, 2, 3]|
+-----------------------------------+

SELECT array_compact(array("a", "b", "c"));
+-----------------------------+
|array_compact(array(a, b, c))|
+-----------------------------+
|                    [a, b, c]|
+-----------------------------+

-- array_contains
SELECT array_contains(array(1, 2, 3), 2);
+---------------------------------+
|array_contains(array(1, 2, 3), 2)|
+---------------------------------+
|                             true|
+---------------------------------+

-- array_distinct
SELECT array_distinct(array(1, 2, 3, null, 3));
+---------------------------------------+
|array_distinct(array(1, 2, 3, NULL, 3))|
+---------------------------------------+
|                        [1, 2, 3, NULL]|
+---------------------------------------+

-- array_except
SELECT array_except(array(1, 2, 3), array(1, 3, 5));
+--------------------------------------------+
|array_except(array(1, 2, 3), array(1, 3, 5))|
+--------------------------------------------+
|                                         [2]|
+--------------------------------------------+

-- array_insert
SELECT array_insert(array(1, 2, 3, 4), 5, 5);
+-------------------------------------+
|array_insert(array(1, 2, 3, 4), 5, 5)|
+-------------------------------------+
|                      [1, 2, 3, 4, 5]|
+-------------------------------------+

SELECT array_insert(array(5, 4, 3, 2), -1, 1);
+--------------------------------------+
|array_insert(array(5, 4, 3, 2), -1, 1)|
+--------------------------------------+
|                       [5, 4, 3, 2, 1]|
+--------------------------------------+

SELECT array_insert(array(5, 3, 2, 1), -4, 4);
+--------------------------------------+
|array_insert(array(5, 3, 2, 1), -4, 4)|
+--------------------------------------+
|                       [5, 4, 3, 2, 1]|
+--------------------------------------+

-- array_intersect
SELECT array_intersect(array(1, 2, 3), array(1, 3, 5));
+-----------------------------------------------+
|array_intersect(array(1, 2, 3), array(1, 3, 5))|
+-----------------------------------------------+
|                                         [1, 3]|
+-----------------------------------------------+

-- array_join
SELECT array_join(array('hello', 'world'), ' ');
+----------------------------------+
|array_join(array(hello, world),  )|
+----------------------------------+
|                       hello world|
+----------------------------------+

SELECT array_join(array('hello', null ,'world'), ' ');
+----------------------------------------+
|array_join(array(hello, NULL, world),  )|
+----------------------------------------+
|                             hello world|
+----------------------------------------+

SELECT array_join(array('hello', null ,'world'), ' ', ',');
+-------------------------------------------+
|array_join(array(hello, NULL, world),  , ,)|
+-------------------------------------------+
|                              hello , world|
+-------------------------------------------+

-- array_max
SELECT array_max(array(1, 20, null, 3));
+--------------------------------+
|array_max(array(1, 20, NULL, 3))|
+--------------------------------+
|                              20|
+--------------------------------+

-- array_min
SELECT array_min(array(1, 20, null, 3));
+--------------------------------+
|array_min(array(1, 20, NULL, 3))|
+--------------------------------+
|                               1|
+--------------------------------+

-- array_position
SELECT array_position(array(312, 773, 708, 708), 708);
+----------------------------------------------+
|array_position(array(312, 773, 708, 708), 708)|
+----------------------------------------------+
|                                             3|
+----------------------------------------------+

SELECT array_position(array(312, 773, 708, 708), 414);
+----------------------------------------------+
|array_position(array(312, 773, 708, 708), 414)|
+----------------------------------------------+
|                                             0|
+----------------------------------------------+

-- array_prepend
SELECT array_prepend(array('b', 'd', 'c', 'a'), 'd');
+-----------------------------------+
|array_prepend(array(b, d, c, a), d)|
+-----------------------------------+
|                    [d, b, d, c, a]|
+-----------------------------------+

SELECT array_prepend(array(1, 2, 3, null), null);
+-----------------------------------------+
|array_prepend(array(1, 2, 3, NULL), NULL)|
+-----------------------------------------+
|                     [NULL, 1, 2, 3, N...|
+-----------------------------------------+

SELECT array_prepend(CAST(null as Array<Int>), 2);
+----------------------+
|array_prepend(NULL, 2)|
+----------------------+
|                  NULL|
+----------------------+

-- array_remove
SELECT array_remove(array(1, 2, 3, null, 3), 3);
+----------------------------------------+
|array_remove(array(1, 2, 3, NULL, 3), 3)|
+----------------------------------------+
|                            [1, 2, NULL]|
+----------------------------------------+

-- array_repeat
SELECT array_repeat('123', 2);
+--------------------+
|array_repeat(123, 2)|
+--------------------+
|          [123, 123]|
+--------------------+

-- array_union
SELECT array_union(array(1, 2, 3), array(1, 3, 5));
+-------------------------------------------+
|array_union(array(1, 2, 3), array(1, 3, 5))|
+-------------------------------------------+
|                               [1, 2, 3, 5]|
+-------------------------------------------+

-- arrays_overlap
SELECT arrays_overlap(array(1, 2, 3), array(3, 4, 5));
+----------------------------------------------+
|arrays_overlap(array(1, 2, 3), array(3, 4, 5))|
+----------------------------------------------+
|                                          true|
+----------------------------------------------+

-- arrays_zip
SELECT arrays_zip(array(1, 2, 3), array(2, 3, 4));
+------------------------------------------+
|arrays_zip(array(1, 2, 3), array(2, 3, 4))|
+------------------------------------------+
|                      [{1, 2}, {2, 3}, ...|
+------------------------------------------+

SELECT arrays_zip(array(1, 2), array(2, 3), array(3, 4));
+-------------------------------------------------+
|arrays_zip(array(1, 2), array(2, 3), array(3, 4))|
+-------------------------------------------------+
|                             [{1, 2, 3}, {2, 3...|
+-------------------------------------------------+

-- flatten
SELECT flatten(array(array(1, 2), array(3, 4)));
+----------------------------------------+
|flatten(array(array(1, 2), array(3, 4)))|
+----------------------------------------+
|                            [1, 2, 3, 4]|
+----------------------------------------+

-- get
SELECT get(array(1, 2, 3), 0);
+----------------------+
|get(array(1, 2, 3), 0)|
+----------------------+
|                     1|
+----------------------+

SELECT get(array(1, 2, 3), 3);
+----------------------+
|get(array(1, 2, 3), 3)|
+----------------------+
|                  NULL|
+----------------------+

SELECT get(array(1, 2, 3), -1);
+-----------------------+
|get(array(1, 2, 3), -1)|
+-----------------------+
|                   NULL|
+-----------------------+

-- sequence
SELECT sequence(1, 5);
+---------------+
| sequence(1, 5)|
+---------------+
|[1, 2, 3, 4, 5]|
+---------------+

SELECT sequence(5, 1);
+---------------+
| sequence(5, 1)|
+---------------+
|[5, 4, 3, 2, 1]|
+---------------+

SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);
+----------------------------------------------------------------------+
|sequence(to_date(2018-01-01), to_date(2018-03-01), INTERVAL '1' MONTH)|
+----------------------------------------------------------------------+
|                                                  [2018-01-01, 2018...|
+----------------------------------------------------------------------+

SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval '0-1' year to month);
+--------------------------------------------------------------------------------+
|sequence(to_date(2018-01-01), to_date(2018-03-01), INTERVAL '0-1' YEAR TO MONTH)|
+--------------------------------------------------------------------------------+
|                                                            [2018-01-01, 2018...|
+--------------------------------------------------------------------------------+

-- shuffle
SELECT shuffle(array(1, 20, 3, 5));
+---------------------------+
|shuffle(array(1, 20, 3, 5))|
+---------------------------+
|              [20, 3, 1, 5]|
+---------------------------+

SELECT shuffle(array(1, 20, null, 3));
+------------------------------+
|shuffle(array(1, 20, NULL, 3))|
+------------------------------+
|              [1, 3, 20, NULL]|
+------------------------------+

-- slice
SELECT slice(array(1, 2, 3, 4), 2, 2);
+------------------------------+
|slice(array(1, 2, 3, 4), 2, 2)|
+------------------------------+
|                        [2, 3]|
+------------------------------+

SELECT slice(array(1, 2, 3, 4), -2, 2);
+-------------------------------+
|slice(array(1, 2, 3, 4), -2, 2)|
+-------------------------------+
|                         [3, 4]|
+-------------------------------+

-- sort_array
SELECT sort_array(array('b', 'd', null, 'c', 'a'), true);
+-----------------------------------------+
|sort_array(array(b, d, NULL, c, a), true)|
+-----------------------------------------+
|                       [NULL, a, b, c, d]|
+-----------------------------------------+

Map Functions



Function
Description




element_at(array, index)
Returns element of array at given (1-based) index. If Index is 0,
      Spark will throw an error. If index < 0, accesses elements from the last to the first.
      The function returns NULL if the index exceeds the length of the array and
      `spark.sql.ansi.enabled` is set to false.
      If `spark.sql.ansi.enabled` is set to true, it throws ArrayIndexOutOfBoundsException
      for invalid indices.


    element_at(map, key)
Returns value for given key. The function returns NULL if the key is not
       contained in the map.


map(key0, value0, key1, value1, ...)
Creates a map with the given key/value pairs.


map_concat(map, ...)
Returns the union of all the given maps


map_contains_key(map, key)
Returns true if the map contains the key.


map_entries(map)
Returns an unordered array of all entries in the given map.


map_from_arrays(keys, values)
Creates a map with a pair of the given key/value arrays. All elements
      in keys should not be null


map_from_entries(arrayOfEntries)
Returns a map created from the given array of entries.


map_keys(map)
Returns an unordered array containing the keys of the map.


map_values(map)
Returns an unordered array containing the values of the map.


str_to_map(text[, pairDelim[, keyValueDelim]])
Creates a map after splitting the text into key/value pairs using delimiters. Default delimiters are ',' for `pairDelim` and ':' for `keyValueDelim`. Both `pairDelim` and `keyValueDelim` are treated as regular expressions.


try_element_at(array, index)
Returns element of array at given (1-based) index. If Index is 0,
      Spark will throw an error. If index < 0, accesses elements from the last to the first.
      The function always returns NULL if the index exceeds the length of the array.


    try_element_at(map, key)
Returns value for given key. The function always returns NULL
      if the key is not contained in the map.



Examples
-- element_at
SELECT element_at(array(1, 2, 3), 2);
+-----------------------------+
|element_at(array(1, 2, 3), 2)|
+-----------------------------+
|                            2|
+-----------------------------+

SELECT element_at(map(1, 'a', 2, 'b'), 2);
+------------------------------+
|element_at(map(1, a, 2, b), 2)|
+------------------------------+
|                             b|
+------------------------------+

-- map
SELECT map(1.0, '2', 3.0, '4');
+--------------------+
| map(1.0, 2, 3.0, 4)|
+--------------------+
|{1.0 -> 2, 3.0 -> 4}|
+--------------------+

-- map_concat
SELECT map_concat(map(1, 'a', 2, 'b'), map(3, 'c'));
+--------------------------------------+
|map_concat(map(1, a, 2, b), map(3, c))|
+--------------------------------------+
|                  {1 -> a, 2 -> b, ...|
+--------------------------------------+

-- map_contains_key
SELECT map_contains_key(map(1, 'a', 2, 'b'), 1);
+------------------------------------+
|map_contains_key(map(1, a, 2, b), 1)|
+------------------------------------+
|                                true|
+------------------------------------+

SELECT map_contains_key(map(1, 'a', 2, 'b'), 3);
+------------------------------------+
|map_contains_key(map(1, a, 2, b), 3)|
+------------------------------------+
|                               false|
+------------------------------------+

-- map_entries
SELECT map_entries(map(1, 'a', 2, 'b'));
+----------------------------+
|map_entries(map(1, a, 2, b))|
+----------------------------+
|            [{1, a}, {2, b}]|
+----------------------------+

-- map_from_arrays
SELECT map_from_arrays(array(1.0, 3.0), array('2', '4'));
+---------------------------------------------+
|map_from_arrays(array(1.0, 3.0), array(2, 4))|
+---------------------------------------------+
|                         {1.0 -> 2, 3.0 -> 4}|
+---------------------------------------------+

-- map_from_entries
SELECT map_from_entries(array(struct(1, 'a'), struct(2, 'b')));
+---------------------------------------------------+
|map_from_entries(array(struct(1, a), struct(2, b)))|
+---------------------------------------------------+
|                                   {1 -> a, 2 -> b}|
+---------------------------------------------------+

-- map_keys
SELECT map_keys(map(1, 'a', 2, 'b'));
+-------------------------+
|map_keys(map(1, a, 2, b))|
+-------------------------+
|                   [1, 2]|
+-------------------------+

-- map_values
SELECT map_values(map(1, 'a', 2, 'b'));
+---------------------------+
|map_values(map(1, a, 2, b))|
+---------------------------+
|                     [a, b]|
+---------------------------+

-- str_to_map
SELECT str_to_map('a:1,b:2,c:3', ',', ':');
+-----------------------------+
|str_to_map(a:1,b:2,c:3, ,, :)|
+-----------------------------+
|         {a -> 1, b -> 2, ...|
+-----------------------------+

SELECT str_to_map('a');
+-------------------+
|str_to_map(a, ,, :)|
+-------------------+
|        {a -> NULL}|
+-------------------+

-- try_element_at
SELECT try_element_at(array(1, 2, 3), 2);
+---------------------------------+
|try_element_at(array(1, 2, 3), 2)|
+---------------------------------+
|                                2|
+---------------------------------+

SELECT try_element_at(map(1, 'a', 2, 'b'), 2);
+----------------------------------+
|try_element_at(map(1, a, 2, b), 2)|
+----------------------------------+
|                                 b|
+----------------------------------+

Date and Timestamp Functions



Function
Description




add_months(start_date, num_months)
Returns the date that is `num_months` after `start_date`.


convert_timezone([sourceTz, ]targetTz, sourceTs)
Converts the timestamp without time zone `sourceTs` from the `sourceTz` time zone to `targetTz`.


curdate()
Returns the current date at the start of query evaluation. All calls of curdate within the same query return the same value.


current_date()
Returns the current date at the start of query evaluation. All calls of current_date within the same query return the same value.


    current_date
Returns the current date at the start of query evaluation.


current_timestamp()
Returns the current timestamp at the start of query evaluation. All calls of current_timestamp within the same query return the same value.


    current_timestamp
Returns the current timestamp at the start of query evaluation.


current_timezone()
Returns the current session local timezone.


date_add(start_date, num_days)
Returns the date that is `num_days` after `start_date`.


date_diff(endDate, startDate)
Returns the number of days from `startDate` to `endDate`.


date_format(timestamp, fmt)
Converts `timestamp` to a value of string in the format specified by the date format `fmt`.


date_from_unix_date(days)
Create date from the number of days since 1970-01-01.


date_part(field, source)
Extracts a part of the date/timestamp or interval source.


date_sub(start_date, num_days)
Returns the date that is `num_days` before `start_date`.


date_trunc(fmt, ts)
Returns timestamp `ts` truncated to the unit specified by the format model `fmt`.


dateadd(start_date, num_days)
Returns the date that is `num_days` after `start_date`.


datediff(endDate, startDate)
Returns the number of days from `startDate` to `endDate`.


datepart(field, source)
Extracts a part of the date/timestamp or interval source.


day(date)
Returns the day of month of the date/timestamp.


dayofmonth(date)
Returns the day of month of the date/timestamp.


dayofweek(date)
Returns the day of the week for date/timestamp (1 = Sunday, 2 = Monday, ..., 7 = Saturday).


dayofyear(date)
Returns the day of year of the date/timestamp.


extract(field FROM source)
Extracts a part of the date/timestamp or interval source.


from_unixtime(unix_time[, fmt])
Returns `unix_time` in the specified `fmt`.


from_utc_timestamp(timestamp, timezone)
Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders that time as a timestamp in the given time zone. For example, 'GMT+1' would yield '2017-07-14 03:40:00.0'.


hour(timestamp)
Returns the hour component of the string/timestamp.


last_day(date)
Returns the last day of the month which the date belongs to.


localtimestamp()
Returns the current timestamp without time zone at the start of query evaluation. All calls of localtimestamp within the same query return the same value.


    localtimestamp
Returns the current local date-time at the session time zone at the start of query evaluation.


make_date(year, month, day)
Create date from year, month and day fields. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.


make_dt_interval([days[, hours[, mins[, secs]]]])
Make DayTimeIntervalType duration from days, hours, mins and secs.


make_interval([years[, months[, weeks[, days[, hours[, mins[, secs]]]]]]])
Make interval from years, months, weeks, days, hours, mins and secs.


make_timestamp(year, month, day, hour, min, sec[, timezone])
Create timestamp from year, month, day, hour, min, sec and timezone fields. The result data type is consistent with the value of configuration `spark.sql.timestampType`. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.


make_timestamp_ltz(year, month, day, hour, min, sec[, timezone])
Create the current timestamp with local time zone from year, month, day, hour, min, sec and timezone fields. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.


make_timestamp_ntz(year, month, day, hour, min, sec)
Create local date-time from year, month, day, hour, min, sec fields. If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.


make_ym_interval([years[, months]])
Make year-month interval from years, months.


minute(timestamp)
Returns the minute component of the string/timestamp.


month(date)
Returns the month component of the date/timestamp.


months_between(timestamp1, timestamp2[, roundOff])
If `timestamp1` is later than `timestamp2`, then the result
      is positive. If `timestamp1` and `timestamp2` are on the same day of month, or both
      are the last day of month, time of day will be ignored. Otherwise, the difference is
      calculated based on 31 days per month, and rounded to 8 digits unless roundOff=false.


next_day(start_date, day_of_week)
Returns the first date which is later than `start_date` and named as indicated.
      The function returns NULL if at least one of the input parameters is NULL.
      When both of the input parameters are not NULL and day_of_week is an invalid input,
      the function throws IllegalArgumentException if `spark.sql.ansi.enabled` is set to true, otherwise NULL.


now()
Returns the current timestamp at the start of query evaluation.


quarter(date)
Returns the quarter of the year for date, in the range 1 to 4.


second(timestamp)
Returns the second component of the string/timestamp.


session_window(time_column, gap_duration)
Generates session window given a timestamp specifying column and gap duration.
      See 'Types of time windows' in Structured Streaming guide doc for detailed explanation and examples.


timestamp_micros(microseconds)
Creates timestamp from the number of microseconds since UTC epoch.


timestamp_millis(milliseconds)
Creates timestamp from the number of milliseconds since UTC epoch.


timestamp_seconds(seconds)
Creates timestamp from the number of seconds (can be fractional) since UTC epoch.


to_date(date_str[, fmt])
Parses the `date_str` expression with the `fmt` expression to
      a date. Returns null with invalid input. By default, it follows casting rules to a date if
      the `fmt` is omitted.


to_timestamp(timestamp_str[, fmt])
Parses the `timestamp_str` expression with the `fmt` expression
      to a timestamp. Returns null with invalid input. By default, it follows casting rules to
      a timestamp if the `fmt` is omitted. The result data type is consistent with the value of
      configuration `spark.sql.timestampType`.


to_timestamp_ltz(timestamp_str[, fmt])
Parses the `timestamp_str` expression with the `fmt` expression
      to a timestamp with local time zone. Returns null with invalid input. By default, it follows casting rules to
      a timestamp if the `fmt` is omitted.


to_timestamp_ntz(timestamp_str[, fmt])
Parses the `timestamp_str` expression with the `fmt` expression
      to a timestamp without time zone. Returns null with invalid input. By default, it follows casting rules to
      a timestamp if the `fmt` is omitted.


to_unix_timestamp(timeExp[, fmt])
Returns the UNIX timestamp of the given time.


to_utc_timestamp(timestamp, timezone)
Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield '2017-07-14 01:40:00.0'.


trunc(date, fmt)
Returns `date` with the time portion of the day truncated to the unit specified by the format model `fmt`.


try_to_timestamp(timestamp_str[, fmt])
Parses the `timestamp_str` expression with the `fmt` expression
      to a timestamp. The function always returns null on an invalid input with/without ANSI SQL
      mode enabled. By default, it follows casting rules to a timestamp if the `fmt` is omitted.
      The result data type is consistent with the value of configuration `spark.sql.timestampType`.


unix_date(date)
Returns the number of days since 1970-01-01.


unix_micros(timestamp)
Returns the number of microseconds since 1970-01-01 00:00:00 UTC.


unix_millis(timestamp)
Returns the number of milliseconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.


unix_seconds(timestamp)
Returns the number of seconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.


unix_timestamp([timeExp[, fmt]])
Returns the UNIX timestamp of current or specified time.


weekday(date)
Returns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).


weekofyear(date)
Returns the week of the year of the given date. A week is considered to start on a Monday and week 1 is the first week with >3 days.


window(time_column, window_duration[, slide_duration[, start_time]])
Bucketize rows into one or more time windows given a timestamp specifying column.
      Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05).
      Windows can support microsecond precision. Windows in the order of months are not supported.
      See 'Window Operations on Event Time' in Structured Streaming guide doc for detailed explanation and examples.


window_time(window_column)
Extract the time value from time/session window column which can be used for event time value of window.
      The extracted time is (window.end - 1) which reflects the fact that the the aggregating
      windows have exclusive upper bound - [start, end)
      See 'Window Operations on Event Time' in Structured Streaming guide doc for detailed explanation and examples.


year(date)
Returns the year component of the date/timestamp.



Examples
-- add_months
SELECT add_months('2016-08-31', 1);
+-------------------------+
|add_months(2016-08-31, 1)|
+-------------------------+
|               2016-09-30|
+-------------------------+

-- convert_timezone
SELECT convert_timezone('Europe/Brussels', 'America/Los_Angeles', timestamp_ntz'2021-12-06 00:00:00');
+-------------------------------------------------------------------------------------------+
|convert_timezone(Europe/Brussels, America/Los_Angeles, TIMESTAMP_NTZ '2021-12-06 00:00:00')|
+-------------------------------------------------------------------------------------------+
|                                                                        2021-12-05 15:00:00|
+-------------------------------------------------------------------------------------------+

SELECT convert_timezone('Europe/Brussels', timestamp_ntz'2021-12-05 15:00:00');
+------------------------------------------------------------------------------------------+
|convert_timezone(current_timezone(), Europe/Brussels, TIMESTAMP_NTZ '2021-12-05 15:00:00')|
+------------------------------------------------------------------------------------------+
|                                                                       2021-12-05 16:00:00|
+------------------------------------------------------------------------------------------+

-- curdate
SELECT curdate();
+--------------+
|current_date()|
+--------------+
|    2025-02-24|
+--------------+

-- current_date
SELECT current_date();
+--------------+
|current_date()|
+--------------+
|    2025-02-24|
+--------------+

SELECT current_date;
+--------------+
|current_date()|
+--------------+
|    2025-02-24|
+--------------+

-- current_timestamp
SELECT current_timestamp();
+--------------------+
| current_timestamp()|
+--------------------+
|2025-02-24 00:01:...|
+--------------------+

SELECT current_timestamp;
+--------------------+
| current_timestamp()|
+--------------------+
|2025-02-24 00:01:...|
+--------------------+

-- current_timezone
SELECT current_timezone();
+------------------+
|current_timezone()|
+------------------+
|           Etc/UTC|
+------------------+

-- date_add
SELECT date_add('2016-07-30', 1);
+-----------------------+
|date_add(2016-07-30, 1)|
+-----------------------+
|             2016-07-31|
+-----------------------+

-- date_diff
SELECT date_diff('2009-07-31', '2009-07-30');
+---------------------------------+
|date_diff(2009-07-31, 2009-07-30)|
+---------------------------------+
|                                1|
+---------------------------------+

SELECT date_diff('2009-07-30', '2009-07-31');
+---------------------------------+
|date_diff(2009-07-30, 2009-07-31)|
+---------------------------------+
|                               -1|
+---------------------------------+

-- date_format
SELECT date_format('2016-04-08', 'y');
+--------------------------+
|date_format(2016-04-08, y)|
+--------------------------+
|                      2016|
+--------------------------+

-- date_from_unix_date
SELECT date_from_unix_date(1);
+----------------------+
|date_from_unix_date(1)|
+----------------------+
|            1970-01-02|
+----------------------+

-- date_part
SELECT date_part('YEAR', TIMESTAMP '2019-08-12 01:00:00.123456');
+-------------------------------------------------------+
|date_part(YEAR, TIMESTAMP '2019-08-12 01:00:00.123456')|
+-------------------------------------------------------+
|                                                   2019|
+-------------------------------------------------------+

SELECT date_part('week', timestamp'2019-08-12 01:00:00.123456');
+-------------------------------------------------------+
|date_part(week, TIMESTAMP '2019-08-12 01:00:00.123456')|
+-------------------------------------------------------+
|                                                     33|
+-------------------------------------------------------+

SELECT date_part('doy', DATE'2019-08-12');
+---------------------------------+
|date_part(doy, DATE '2019-08-12')|
+---------------------------------+
|                              224|
+---------------------------------+

SELECT date_part('SECONDS', timestamp'2019-10-01 00:00:01.000001');
+----------------------------------------------------------+
|date_part(SECONDS, TIMESTAMP '2019-10-01 00:00:01.000001')|
+----------------------------------------------------------+
|                                                  1.000001|
+----------------------------------------------------------+

SELECT date_part('days', interval 5 days 3 hours 7 minutes);
+-------------------------------------------------+
|date_part(days, INTERVAL '5 03:07' DAY TO MINUTE)|
+-------------------------------------------------+
|                                                5|
+-------------------------------------------------+

SELECT date_part('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
+-------------------------------------------------------------+
|date_part(seconds, INTERVAL '05:00:30.001001' HOUR TO SECOND)|
+-------------------------------------------------------------+
|                                                    30.001001|
+-------------------------------------------------------------+

SELECT date_part('MONTH', INTERVAL '2021-11' YEAR TO MONTH);
+--------------------------------------------------+
|date_part(MONTH, INTERVAL '2021-11' YEAR TO MONTH)|
+--------------------------------------------------+
|                                                11|
+--------------------------------------------------+

SELECT date_part('MINUTE', INTERVAL '123 23:55:59.002001' DAY TO SECOND);
+---------------------------------------------------------------+
|date_part(MINUTE, INTERVAL '123 23:55:59.002001' DAY TO SECOND)|
+---------------------------------------------------------------+
|                                                             55|
+---------------------------------------------------------------+

-- date_sub
SELECT date_sub('2016-07-30', 1);
+-----------------------+
|date_sub(2016-07-30, 1)|
+-----------------------+
|             2016-07-29|
+-----------------------+

-- date_trunc
SELECT date_trunc('YEAR', '2015-03-05T09:32:05.359');
+-----------------------------------------+
|date_trunc(YEAR, 2015-03-05T09:32:05.359)|
+-----------------------------------------+
|                      2015-01-01 00:00:00|
+-----------------------------------------+

SELECT date_trunc('MM', '2015-03-05T09:32:05.359');
+---------------------------------------+
|date_trunc(MM, 2015-03-05T09:32:05.359)|
+---------------------------------------+
|                    2015-03-01 00:00:00|
+---------------------------------------+

SELECT date_trunc('DD', '2015-03-05T09:32:05.359');
+---------------------------------------+
|date_trunc(DD, 2015-03-05T09:32:05.359)|
+---------------------------------------+
|                    2015-03-05 00:00:00|
+---------------------------------------+

SELECT date_trunc('HOUR', '2015-03-05T09:32:05.359');
+-----------------------------------------+
|date_trunc(HOUR, 2015-03-05T09:32:05.359)|
+-----------------------------------------+
|                      2015-03-05 09:00:00|
+-----------------------------------------+

SELECT date_trunc('MILLISECOND', '2015-03-05T09:32:05.123456');
+---------------------------------------------------+
|date_trunc(MILLISECOND, 2015-03-05T09:32:05.123456)|
+---------------------------------------------------+
|                               2015-03-05 09:32:...|
+---------------------------------------------------+

-- dateadd
SELECT dateadd('2016-07-30', 1);
+-----------------------+
|date_add(2016-07-30, 1)|
+-----------------------+
|             2016-07-31|
+-----------------------+

-- datediff
SELECT datediff('2009-07-31', '2009-07-30');
+--------------------------------+
|datediff(2009-07-31, 2009-07-30)|
+--------------------------------+
|                               1|
+--------------------------------+

SELECT datediff('2009-07-30', '2009-07-31');
+--------------------------------+
|datediff(2009-07-30, 2009-07-31)|
+--------------------------------+
|                              -1|
+--------------------------------+

-- datepart
SELECT datepart('YEAR', TIMESTAMP '2019-08-12 01:00:00.123456');
+----------------------------------------------------------+
|datepart(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456')|
+----------------------------------------------------------+
|                                                      2019|
+----------------------------------------------------------+

SELECT datepart('week', timestamp'2019-08-12 01:00:00.123456');
+----------------------------------------------------------+
|datepart(week FROM TIMESTAMP '2019-08-12 01:00:00.123456')|
+----------------------------------------------------------+
|                                                        33|
+----------------------------------------------------------+

SELECT datepart('doy', DATE'2019-08-12');
+------------------------------------+
|datepart(doy FROM DATE '2019-08-12')|
+------------------------------------+
|                                 224|
+------------------------------------+

SELECT datepart('SECONDS', timestamp'2019-10-01 00:00:01.000001');
+-------------------------------------------------------------+
|datepart(SECONDS FROM TIMESTAMP '2019-10-01 00:00:01.000001')|
+-------------------------------------------------------------+
|                                                     1.000001|
+-------------------------------------------------------------+

SELECT datepart('days', interval 5 days 3 hours 7 minutes);
+----------------------------------------------------+
|datepart(days FROM INTERVAL '5 03:07' DAY TO MINUTE)|
+----------------------------------------------------+
|                                                   5|
+----------------------------------------------------+

SELECT datepart('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
+----------------------------------------------------------------+
|datepart(seconds FROM INTERVAL '05:00:30.001001' HOUR TO SECOND)|
+----------------------------------------------------------------+
|                                                       30.001001|
+----------------------------------------------------------------+

SELECT datepart('MONTH', INTERVAL '2021-11' YEAR TO MONTH);
+-----------------------------------------------------+
|datepart(MONTH FROM INTERVAL '2021-11' YEAR TO MONTH)|
+-----------------------------------------------------+
|                                                   11|
+-----------------------------------------------------+

SELECT datepart('MINUTE', INTERVAL '123 23:55:59.002001' DAY TO SECOND);
+------------------------------------------------------------------+
|datepart(MINUTE FROM INTERVAL '123 23:55:59.002001' DAY TO SECOND)|
+------------------------------------------------------------------+
|                                                                55|
+------------------------------------------------------------------+

-- day
SELECT day('2009-07-30');
+---------------+
|day(2009-07-30)|
+---------------+
|             30|
+---------------+

-- dayofmonth
SELECT dayofmonth('2009-07-30');
+----------------------+
|dayofmonth(2009-07-30)|
+----------------------+
|                    30|
+----------------------+

-- dayofweek
SELECT dayofweek('2009-07-30');
+---------------------+
|dayofweek(2009-07-30)|
+---------------------+
|                    5|
+---------------------+

-- dayofyear
SELECT dayofyear('2016-04-09');
+---------------------+
|dayofyear(2016-04-09)|
+---------------------+
|                  100|
+---------------------+

-- extract
SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');
+---------------------------------------------------------+
|extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456')|
+---------------------------------------------------------+
|                                                     2019|
+---------------------------------------------------------+

SELECT extract(week FROM timestamp'2019-08-12 01:00:00.123456');
+---------------------------------------------------------+
|extract(week FROM TIMESTAMP '2019-08-12 01:00:00.123456')|
+---------------------------------------------------------+
|                                                       33|
+---------------------------------------------------------+

SELECT extract(doy FROM DATE'2019-08-12');
+-----------------------------------+
|extract(doy FROM DATE '2019-08-12')|
+-----------------------------------+
|                                224|
+-----------------------------------+

SELECT extract(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');
+------------------------------------------------------------+
|extract(SECONDS FROM TIMESTAMP '2019-10-01 00:00:01.000001')|
+------------------------------------------------------------+
|                                                    1.000001|
+------------------------------------------------------------+

SELECT extract(days FROM interval 5 days 3 hours 7 minutes);
+---------------------------------------------------+
|extract(days FROM INTERVAL '5 03:07' DAY TO MINUTE)|
+---------------------------------------------------+
|                                                  5|
+---------------------------------------------------+

SELECT extract(seconds FROM interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
+---------------------------------------------------------------+
|extract(seconds FROM INTERVAL '05:00:30.001001' HOUR TO SECOND)|
+---------------------------------------------------------------+
|                                                      30.001001|
+---------------------------------------------------------------+

SELECT extract(MONTH FROM INTERVAL '2021-11' YEAR TO MONTH);
+----------------------------------------------------+
|extract(MONTH FROM INTERVAL '2021-11' YEAR TO MONTH)|
+----------------------------------------------------+
|                                                  11|
+----------------------------------------------------+

SELECT extract(MINUTE FROM INTERVAL '123 23:55:59.002001' DAY TO SECOND);
+-----------------------------------------------------------------+
|extract(MINUTE FROM INTERVAL '123 23:55:59.002001' DAY TO SECOND)|
+-----------------------------------------------------------------+
|                                                               55|
+-----------------------------------------------------------------+

-- from_unixtime
SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss');
+-------------------------------------+
|from_unixtime(0, yyyy-MM-dd HH:mm:ss)|
+-------------------------------------+
|                  1970-01-01 00:00:00|
+-------------------------------------+

SELECT from_unixtime(0);
+-------------------------------------+
|from_unixtime(0, yyyy-MM-dd HH:mm:ss)|
+-------------------------------------+
|                  1970-01-01 00:00:00|
+-------------------------------------+

-- from_utc_timestamp
SELECT from_utc_timestamp('2016-08-31', 'Asia/Seoul');
+------------------------------------------+
|from_utc_timestamp(2016-08-31, Asia/Seoul)|
+------------------------------------------+
|                       2016-08-31 09:00:00|
+------------------------------------------+

-- hour
SELECT hour('2009-07-30 12:58:59');
+-------------------------+
|hour(2009-07-30 12:58:59)|
+-------------------------+
|                       12|
+-------------------------+

-- last_day
SELECT last_day('2009-01-12');
+--------------------+
|last_day(2009-01-12)|
+--------------------+
|          2009-01-31|
+--------------------+

-- localtimestamp
SELECT localtimestamp();
+--------------------+
|    localtimestamp()|
+--------------------+
|2025-02-24 00:01:...|
+--------------------+

-- make_date
SELECT make_date(2013, 7, 15);
+----------------------+
|make_date(2013, 7, 15)|
+----------------------+
|            2013-07-15|
+----------------------+

SELECT make_date(2019, 7, NULL);
+------------------------+
|make_date(2019, 7, NULL)|
+------------------------+
|                    NULL|
+------------------------+

-- make_dt_interval
SELECT make_dt_interval(1, 12, 30, 01.001001);
+-------------------------------------+
|make_dt_interval(1, 12, 30, 1.001001)|
+-------------------------------------+
|                 INTERVAL '1 12:30...|
+-------------------------------------+

SELECT make_dt_interval(2);
+-----------------------------------+
|make_dt_interval(2, 0, 0, 0.000000)|
+-----------------------------------+
|               INTERVAL '2 00:00...|
+-----------------------------------+

SELECT make_dt_interval(100, null, 3);
+----------------------------------------+
|make_dt_interval(100, NULL, 3, 0.000000)|
+----------------------------------------+
|                                    NULL|
+----------------------------------------+

-- make_interval
SELECT make_interval(100, 11, 1, 1, 12, 30, 01.001001);
+----------------------------------------------+
|make_interval(100, 11, 1, 1, 12, 30, 1.001001)|
+----------------------------------------------+
|                          100 years 11 mont...|
+----------------------------------------------+

SELECT make_interval(100, null, 3);
+----------------------------------------------+
|make_interval(100, NULL, 3, 0, 0, 0, 0.000000)|
+----------------------------------------------+
|                                          NULL|
+----------------------------------------------+

SELECT make_interval(0, 1, 0, 1, 0, 0, 100.000001);
+-------------------------------------------+
|make_interval(0, 1, 0, 1, 0, 0, 100.000001)|
+-------------------------------------------+
|                       1 months 1 days 1...|
+-------------------------------------------+

-- make_timestamp
SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887);
+-------------------------------------------+
|make_timestamp(2014, 12, 28, 6, 30, 45.887)|
+-------------------------------------------+
|                       2014-12-28 06:30:...|
+-------------------------------------------+

SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887, 'CET');
+------------------------------------------------+
|make_timestamp(2014, 12, 28, 6, 30, 45.887, CET)|
+------------------------------------------------+
|                            2014-12-28 05:30:...|
+------------------------------------------------+

SELECT make_timestamp(2019, 6, 30, 23, 59, 60);
+---------------------------------------+
|make_timestamp(2019, 6, 30, 23, 59, 60)|
+---------------------------------------+
|                    2019-07-01 00:00:00|
+---------------------------------------+

SELECT make_timestamp(2019, 6, 30, 23, 59, 1);
+--------------------------------------+
|make_timestamp(2019, 6, 30, 23, 59, 1)|
+--------------------------------------+
|                   2019-06-30 23:59:01|
+--------------------------------------+

SELECT make_timestamp(null, 7, 22, 15, 30, 0);
+--------------------------------------+
|make_timestamp(NULL, 7, 22, 15, 30, 0)|
+--------------------------------------+
|                                  NULL|
+--------------------------------------+

-- make_timestamp_ltz
SELECT make_timestamp_ltz(2014, 12, 28, 6, 30, 45.887);
+-----------------------------------------------+
|make_timestamp_ltz(2014, 12, 28, 6, 30, 45.887)|
+-----------------------------------------------+
|                           2014-12-28 06:30:...|
+-----------------------------------------------+

SELECT make_timestamp_ltz(2014, 12, 28, 6, 30, 45.887, 'CET');
+----------------------------------------------------+
|make_timestamp_ltz(2014, 12, 28, 6, 30, 45.887, CET)|
+----------------------------------------------------+
|                                2014-12-28 05:30:...|
+----------------------------------------------------+

SELECT make_timestamp_ltz(2019, 6, 30, 23, 59, 60);
+-------------------------------------------+
|make_timestamp_ltz(2019, 6, 30, 23, 59, 60)|
+-------------------------------------------+
|                        2019-07-01 00:00:00|
+-------------------------------------------+

SELECT make_timestamp_ltz(null, 7, 22, 15, 30, 0);
+------------------------------------------+
|make_timestamp_ltz(NULL, 7, 22, 15, 30, 0)|
+------------------------------------------+
|                                      NULL|
+------------------------------------------+

-- make_timestamp_ntz
SELECT make_timestamp_ntz(2014, 12, 28, 6, 30, 45.887);
+-----------------------------------------------+
|make_timestamp_ntz(2014, 12, 28, 6, 30, 45.887)|
+-----------------------------------------------+
|                           2014-12-28 06:30:...|
+-----------------------------------------------+

SELECT make_timestamp_ntz(2019, 6, 30, 23, 59, 60);
+-------------------------------------------+
|make_timestamp_ntz(2019, 6, 30, 23, 59, 60)|
+-------------------------------------------+
|                        2019-07-01 00:00:00|
+-------------------------------------------+

SELECT make_timestamp_ntz(null, 7, 22, 15, 30, 0);
+------------------------------------------+
|make_timestamp_ntz(NULL, 7, 22, 15, 30, 0)|
+------------------------------------------+
|                                      NULL|
+------------------------------------------+

-- make_ym_interval
SELECT make_ym_interval(1, 2);
+----------------------+
|make_ym_interval(1, 2)|
+----------------------+
|  INTERVAL '1-2' YE...|
+----------------------+

SELECT make_ym_interval(1, 0);
+----------------------+
|make_ym_interval(1, 0)|
+----------------------+
|  INTERVAL '1-0' YE...|
+----------------------+

SELECT make_ym_interval(-1, 1);
+-----------------------+
|make_ym_interval(-1, 1)|
+-----------------------+
|   INTERVAL '-0-11' ...|
+-----------------------+

SELECT make_ym_interval(2);
+----------------------+
|make_ym_interval(2, 0)|
+----------------------+
|  INTERVAL '2-0' YE...|
+----------------------+

-- minute
SELECT minute('2009-07-30 12:58:59');
+---------------------------+
|minute(2009-07-30 12:58:59)|
+---------------------------+
|                         58|
+---------------------------+

-- month
SELECT month('2016-07-30');
+-----------------+
|month(2016-07-30)|
+-----------------+
|                7|
+-----------------+

-- months_between
SELECT months_between('1997-02-28 10:30:00', '1996-10-30');
+-----------------------------------------------------+
|months_between(1997-02-28 10:30:00, 1996-10-30, true)|
+-----------------------------------------------------+
|                                           3.94959677|
+-----------------------------------------------------+

SELECT months_between('1997-02-28 10:30:00', '1996-10-30', false);
+------------------------------------------------------+
|months_between(1997-02-28 10:30:00, 1996-10-30, false)|
+------------------------------------------------------+
|                                    3.9495967741935485|
+------------------------------------------------------+

-- next_day
SELECT next_day('2015-01-14', 'TU');
+------------------------+
|next_day(2015-01-14, TU)|
+------------------------+
|              2015-01-20|
+------------------------+

-- now
SELECT now();
+--------------------+
|               now()|
+--------------------+
|2025-02-24 00:01:...|
+--------------------+

-- quarter
SELECT quarter('2016-08-31');
+-------------------+
|quarter(2016-08-31)|
+-------------------+
|                  3|
+-------------------+

-- second
SELECT second('2009-07-30 12:58:59');
+---------------------------+
|second(2009-07-30 12:58:59)|
+---------------------------+
|                         59|
+---------------------------+

-- session_window
SELECT a, session_window.start, session_window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:10:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, session_window(b, '5 minutes') ORDER BY a, start;
+---+-------------------+-------------------+---+
|  a|              start|                end|cnt|
+---+-------------------+-------------------+---+
| A1|2021-01-01 00:00:00|2021-01-01 00:09:30|  2|
| A1|2021-01-01 00:10:00|2021-01-01 00:15:00|  1|
| A2|2021-01-01 00:01:00|2021-01-01 00:06:00|  1|
+---+-------------------+-------------------+---+

SELECT a, session_window.start, session_window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:10:00'), ('A2', '2021-01-01 00:01:00'), ('A2', '2021-01-01 00:04:30') AS tab(a, b) GROUP by a, session_window(b, CASE WHEN a = 'A1' THEN '5 minutes' WHEN a = 'A2' THEN '1 minute' ELSE '10 minutes' END) ORDER BY a, start;
+---+-------------------+-------------------+---+
|  a|              start|                end|cnt|
+---+-------------------+-------------------+---+
| A1|2021-01-01 00:00:00|2021-01-01 00:09:30|  2|
| A1|2021-01-01 00:10:00|2021-01-01 00:15:00|  1|
| A2|2021-01-01 00:01:00|2021-01-01 00:02:00|  1|
| A2|2021-01-01 00:04:30|2021-01-01 00:05:30|  1|
+---+-------------------+-------------------+---+

-- timestamp_micros
SELECT timestamp_micros(1230219000123123);
+----------------------------------+
|timestamp_micros(1230219000123123)|
+----------------------------------+
|              2008-12-25 15:30:...|
+----------------------------------+

-- timestamp_millis
SELECT timestamp_millis(1230219000123);
+-------------------------------+
|timestamp_millis(1230219000123)|
+-------------------------------+
|           2008-12-25 15:30:...|
+-------------------------------+

-- timestamp_seconds
SELECT timestamp_seconds(1230219000);
+-----------------------------+
|timestamp_seconds(1230219000)|
+-----------------------------+
|          2008-12-25 15:30:00|
+-----------------------------+

SELECT timestamp_seconds(1230219000.123);
+---------------------------------+
|timestamp_seconds(1230219000.123)|
+---------------------------------+
|             2008-12-25 15:30:...|
+---------------------------------+

-- to_date
SELECT to_date('2009-07-30 04:17:52');
+----------------------------+
|to_date(2009-07-30 04:17:52)|
+----------------------------+
|                  2009-07-30|
+----------------------------+

SELECT to_date('2016-12-31', 'yyyy-MM-dd');
+-------------------------------+
|to_date(2016-12-31, yyyy-MM-dd)|
+-------------------------------+
|                     2016-12-31|
+-------------------------------+

-- to_timestamp
SELECT to_timestamp('2016-12-31 00:12:00');
+---------------------------------+
|to_timestamp(2016-12-31 00:12:00)|
+---------------------------------+
|              2016-12-31 00:12:00|
+---------------------------------+

SELECT to_timestamp('2016-12-31', 'yyyy-MM-dd');
+------------------------------------+
|to_timestamp(2016-12-31, yyyy-MM-dd)|
+------------------------------------+
|                 2016-12-31 00:00:00|
+------------------------------------+

-- to_timestamp_ltz
SELECT to_timestamp_ltz('2016-12-31 00:12:00');
+-------------------------------------+
|to_timestamp_ltz(2016-12-31 00:12:00)|
+-------------------------------------+
|                  2016-12-31 00:12:00|
+-------------------------------------+

SELECT to_timestamp_ltz('2016-12-31', 'yyyy-MM-dd');
+----------------------------------------+
|to_timestamp_ltz(2016-12-31, yyyy-MM-dd)|
+----------------------------------------+
|                     2016-12-31 00:00:00|
+----------------------------------------+

-- to_timestamp_ntz
SELECT to_timestamp_ntz('2016-12-31 00:12:00');
+-------------------------------------+
|to_timestamp_ntz(2016-12-31 00:12:00)|
+-------------------------------------+
|                  2016-12-31 00:12:00|
+-------------------------------------+

SELECT to_timestamp_ntz('2016-12-31', 'yyyy-MM-dd');
+----------------------------------------+
|to_timestamp_ntz(2016-12-31, yyyy-MM-dd)|
+----------------------------------------+
|                     2016-12-31 00:00:00|
+----------------------------------------+

-- to_unix_timestamp
SELECT to_unix_timestamp('2016-04-08', 'yyyy-MM-dd');
+-----------------------------------------+
|to_unix_timestamp(2016-04-08, yyyy-MM-dd)|
+-----------------------------------------+
|                               1460073600|
+-----------------------------------------+

-- to_utc_timestamp
SELECT to_utc_timestamp('2016-08-31', 'Asia/Seoul');
+----------------------------------------+
|to_utc_timestamp(2016-08-31, Asia/Seoul)|
+----------------------------------------+
|                     2016-08-30 15:00:00|
+----------------------------------------+

-- trunc
SELECT trunc('2019-08-04', 'week');
+-----------------------+
|trunc(2019-08-04, week)|
+-----------------------+
|             2019-07-29|
+-----------------------+

SELECT trunc('2019-08-04', 'quarter');
+--------------------------+
|trunc(2019-08-04, quarter)|
+--------------------------+
|                2019-07-01|
+--------------------------+

SELECT trunc('2009-02-12', 'MM');
+---------------------+
|trunc(2009-02-12, MM)|
+---------------------+
|           2009-02-01|
+---------------------+

SELECT trunc('2015-10-27', 'YEAR');
+-----------------------+
|trunc(2015-10-27, YEAR)|
+-----------------------+
|             2015-01-01|
+-----------------------+

-- try_to_timestamp
SELECT try_to_timestamp('2016-12-31 00:12:00');
+-------------------------------------+
|try_to_timestamp(2016-12-31 00:12:00)|
+-------------------------------------+
|                  2016-12-31 00:12:00|
+-------------------------------------+

SELECT try_to_timestamp('2016-12-31', 'yyyy-MM-dd');
+----------------------------------------+
|try_to_timestamp(2016-12-31, yyyy-MM-dd)|
+----------------------------------------+
|                     2016-12-31 00:00:00|
+----------------------------------------+

SELECT try_to_timestamp('foo', 'yyyy-MM-dd');
+---------------------------------+
|try_to_timestamp(foo, yyyy-MM-dd)|
+---------------------------------+
|                             NULL|
+---------------------------------+

-- unix_date
SELECT unix_date(DATE("1970-01-02"));
+---------------------+
|unix_date(1970-01-02)|
+---------------------+
|                    1|
+---------------------+

-- unix_micros
SELECT unix_micros(TIMESTAMP('1970-01-01 00:00:01Z'));
+---------------------------------+
|unix_micros(1970-01-01 00:00:01Z)|
+---------------------------------+
|                          1000000|
+---------------------------------+

-- unix_millis
SELECT unix_millis(TIMESTAMP('1970-01-01 00:00:01Z'));
+---------------------------------+
|unix_millis(1970-01-01 00:00:01Z)|
+---------------------------------+
|                             1000|
+---------------------------------+

-- unix_seconds
SELECT unix_seconds(TIMESTAMP('1970-01-01 00:00:01Z'));
+----------------------------------+
|unix_seconds(1970-01-01 00:00:01Z)|
+----------------------------------+
|                                 1|
+----------------------------------+

-- unix_timestamp
SELECT unix_timestamp();
+--------------------------------------------------------+
|unix_timestamp(current_timestamp(), yyyy-MM-dd HH:mm:ss)|
+--------------------------------------------------------+
|                                              1740355262|
+--------------------------------------------------------+

SELECT unix_timestamp('2016-04-08', 'yyyy-MM-dd');
+--------------------------------------+
|unix_timestamp(2016-04-08, yyyy-MM-dd)|
+--------------------------------------+
|                            1460073600|
+--------------------------------------+

-- weekday
SELECT weekday('2009-07-30');
+-------------------+
|weekday(2009-07-30)|
+-------------------+
|                  3|
+-------------------+

-- weekofyear
SELECT weekofyear('2008-02-20');
+----------------------+
|weekofyear(2008-02-20)|
+----------------------+
|                     8|
+----------------------+

-- window
SELECT a, window.start, window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:06:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, window(b, '5 minutes') ORDER BY a, start;
+---+-------------------+-------------------+---+
|  a|              start|                end|cnt|
+---+-------------------+-------------------+---+
| A1|2021-01-01 00:00:00|2021-01-01 00:05:00|  2|
| A1|2021-01-01 00:05:00|2021-01-01 00:10:00|  1|
| A2|2021-01-01 00:00:00|2021-01-01 00:05:00|  1|
+---+-------------------+-------------------+---+

SELECT a, window.start, window.end, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:06:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, window(b, '10 minutes', '5 minutes') ORDER BY a, start;
+---+-------------------+-------------------+---+
|  a|              start|                end|cnt|
+---+-------------------+-------------------+---+
| A1|2020-12-31 23:55:00|2021-01-01 00:05:00|  2|
| A1|2021-01-01 00:00:00|2021-01-01 00:10:00|  3|
| A1|2021-01-01 00:05:00|2021-01-01 00:15:00|  1|
| A2|2020-12-31 23:55:00|2021-01-01 00:05:00|  1|
| A2|2021-01-01 00:00:00|2021-01-01 00:10:00|  1|
+---+-------------------+-------------------+---+

-- window_time
SELECT a, window.start as start, window.end as end, window_time(window), cnt FROM (SELECT a, window, count(*) as cnt FROM VALUES ('A1', '2021-01-01 00:00:00'), ('A1', '2021-01-01 00:04:30'), ('A1', '2021-01-01 00:06:00'), ('A2', '2021-01-01 00:01:00') AS tab(a, b) GROUP by a, window(b, '5 minutes') ORDER BY a, window.start);
+---+-------------------+-------------------+--------------------+---+
|  a|              start|                end| window_time(window)|cnt|
+---+-------------------+-------------------+--------------------+---+
| A1|2021-01-01 00:00:00|2021-01-01 00:05:00|2021-01-01 00:04:...|  2|
| A1|2021-01-01 00:05:00|2021-01-01 00:10:00|2021-01-01 00:09:...|  1|
| A2|2021-01-01 00:00:00|2021-01-01 00:05:00|2021-01-01 00:04:...|  1|
+---+-------------------+-------------------+--------------------+---+

-- year
SELECT year('2016-07-30');
+----------------+
|year(2016-07-30)|
+----------------+
|            2016|
+----------------+

JSON Functions



Function
Description




from_json(jsonStr, schema[, options])
Returns a struct value with the given `jsonStr` and `schema`.


get_json_object(json_txt, path)
Extracts a json object from `path`.


json_array_length(jsonArray)
Returns the number of elements in the outermost JSON array.


json_object_keys(json_object)
Returns all the keys of the outermost JSON object as an array.


json_tuple(jsonStr, p1, p2, ..., pn)
Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.


schema_of_json(json[, options])
Returns schema in the DDL format of JSON string.


to_json(expr[, options])
Returns a JSON string with a given struct value



Examples
-- from_json
SELECT from_json('{"a":1, "b":0.8}', 'a INT, b DOUBLE');
+---------------------------+
|from_json({"a":1, "b":0.8})|
+---------------------------+
|                   {1, 0.8}|
+---------------------------+

SELECT from_json('{"time":"26/08/2015"}', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));
+--------------------------------+
|from_json({"time":"26/08/2015"})|
+--------------------------------+
|            {2015-08-26 00:00...|
+--------------------------------+

SELECT from_json('{"teacher": "Alice", "student": [{"name": "Bob", "rank": 1}, {"name": "Charlie", "rank": 2}]}', 'STRUCT<teacher: STRING, student: ARRAY<STRUCT<name: STRING, rank: INT>>>');
+--------------------------------------------------------------------------------------------------------+
|from_json({"teacher": "Alice", "student": [{"name": "Bob", "rank": 1}, {"name": "Charlie", "rank": 2}]})|
+--------------------------------------------------------------------------------------------------------+
|                                                                                    {Alice, [{Bob, 1}...|
+--------------------------------------------------------------------------------------------------------+

-- get_json_object
SELECT get_json_object('{"a":"b"}', '$.a');
+-------------------------------+
|get_json_object({"a":"b"}, $.a)|
+-------------------------------+
|                              b|
+-------------------------------+

-- json_array_length
SELECT json_array_length('[1,2,3,4]');
+----------------------------+
|json_array_length([1,2,3,4])|
+----------------------------+
|                           4|
+----------------------------+

SELECT json_array_length('[1,2,3,{"f1":1,"f2":[5,6]},4]');
+------------------------------------------------+
|json_array_length([1,2,3,{"f1":1,"f2":[5,6]},4])|
+------------------------------------------------+
|                                               5|
+------------------------------------------------+

SELECT json_array_length('[1,2');
+-----------------------+
|json_array_length([1,2)|
+-----------------------+
|                   NULL|
+-----------------------+

-- json_object_keys
SELECT json_object_keys('{}');
+--------------------+
|json_object_keys({})|
+--------------------+
|                  []|
+--------------------+

SELECT json_object_keys('{"key": "value"}');
+----------------------------------+
|json_object_keys({"key": "value"})|
+----------------------------------+
|                             [key]|
+----------------------------------+

SELECT json_object_keys('{"f1":"abc","f2":{"f3":"a", "f4":"b"}}');
+--------------------------------------------------------+
|json_object_keys({"f1":"abc","f2":{"f3":"a", "f4":"b"}})|
+--------------------------------------------------------+
|                                                [f1, f2]|
+--------------------------------------------------------+

-- json_tuple
SELECT json_tuple('{"a":1, "b":2}', 'a', 'b');
+---+---+
| c0| c1|
+---+---+
|  1|  2|
+---+---+

-- schema_of_json
SELECT schema_of_json('[{"col":0}]');
+---------------------------+
|schema_of_json([{"col":0}])|
+---------------------------+
|       ARRAY<STRUCT<col:...|
+---------------------------+

SELECT schema_of_json('[{"col":01}]', map('allowNumericLeadingZeros', 'true'));
+----------------------------+
|schema_of_json([{"col":01}])|
+----------------------------+
|        ARRAY<STRUCT<col:...|
+----------------------------+

-- to_json
SELECT to_json(named_struct('a', 1, 'b', 2));
+---------------------------------+
|to_json(named_struct(a, 1, b, 2))|
+---------------------------------+
|                    {"a":1,"b":2}|
+---------------------------------+

SELECT to_json(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));
+-----------------------------------------------------------------+
|to_json(named_struct(time, to_timestamp(2015-08-26, yyyy-MM-dd)))|
+-----------------------------------------------------------------+
|                                             {"time":"26/08/20...|
+-----------------------------------------------------------------+

SELECT to_json(array(named_struct('a', 1, 'b', 2)));
+----------------------------------------+
|to_json(array(named_struct(a, 1, b, 2)))|
+----------------------------------------+
|                         [{"a":1,"b":2}]|
+----------------------------------------+

SELECT to_json(map('a', named_struct('b', 1)));
+-----------------------------------+
|to_json(map(a, named_struct(b, 1)))|
+-----------------------------------+
|                      {"a":{"b":1}}|
+-----------------------------------+

SELECT to_json(map(named_struct('a', 1),named_struct('b', 2)));
+----------------------------------------------------+
|to_json(map(named_struct(a, 1), named_struct(b, 2)))|
+----------------------------------------------------+
|                                     {"[1]":{"b":2}}|
+----------------------------------------------------+

SELECT to_json(map('a', 1));
+------------------+
|to_json(map(a, 1))|
+------------------+
|           {"a":1}|
+------------------+

SELECT to_json(array(map('a', 1)));
+-------------------------+
|to_json(array(map(a, 1)))|
+-------------------------+
|                [{"a":1}]|
+-------------------------+

Mathematical Functions



Function
Description




expr1 % expr2
Returns the remainder after `expr1`/`expr2`.


expr1 * expr2
Returns `expr1`*`expr2`.


expr1 + expr2
Returns `expr1`+`expr2`.


expr1 - expr2
Returns `expr1`-`expr2`.


expr1 / expr2
Returns `expr1`/`expr2`. It always performs floating point division.


abs(expr)
Returns the absolute value of the numeric or interval value.


acos(expr)
Returns the inverse cosine (a.k.a. arc cosine) of `expr`, as if computed by
      `java.lang.Math.acos`.


acosh(expr)
Returns inverse hyperbolic cosine of `expr`.


asin(expr)
Returns the inverse sine (a.k.a. arc sine) the arc sin of `expr`,
      as if computed by `java.lang.Math.asin`.


asinh(expr)
Returns inverse hyperbolic sine of `expr`.


atan(expr)
Returns the inverse tangent (a.k.a. arc tangent) of `expr`, as if computed by
      `java.lang.Math.atan`


atan2(exprY, exprX)
Returns the angle in radians between the positive x-axis of a plane
      and the point given by the coordinates (`exprX`, `exprY`), as if computed by
      `java.lang.Math.atan2`.


atanh(expr)
Returns inverse hyperbolic tangent of `expr`.


bin(expr)
Returns the string representation of the long value `expr` represented in binary.


bround(expr, d)
Returns `expr` rounded to `d` decimal places using HALF_EVEN rounding mode.


cbrt(expr)
Returns the cube root of `expr`.


ceil(expr[, scale])
Returns the smallest number after rounding up that is not smaller than `expr`. An optional `scale` parameter can be specified to control the rounding behavior.


ceiling(expr[, scale])
Returns the smallest number after rounding up that is not smaller than `expr`. An optional `scale` parameter can be specified to control the rounding behavior.


conv(num, from_base, to_base)
Convert `num` from `from_base` to `to_base`.


cos(expr)
Returns the cosine of `expr`, as if computed by
      `java.lang.Math.cos`.


cosh(expr)
Returns the hyperbolic cosine of `expr`, as if computed by
        `java.lang.Math.cosh`.


cot(expr)
Returns the cotangent of `expr`, as if computed by `1/java.lang.Math.tan`.


csc(expr)
Returns the cosecant of `expr`, as if computed by `1/java.lang.Math.sin`.


degrees(expr)
Converts radians to degrees.


expr1 div expr2
Divide `expr1` by `expr2`. It returns NULL if an operand is NULL or `expr2` is 0. The result is casted to long.


e()
Returns Euler's number, e.


exp(expr)
Returns e to the power of `expr`.


expm1(expr) - Returns exp(`expr`)
1.


factorial(expr)
Returns the factorial of `expr`. `expr` is [0..20]. Otherwise, null.


floor(expr[, scale])
Returns the largest number after rounding down that is not greater than `expr`. An optional `scale` parameter can be specified to control the rounding behavior.


greatest(expr, ...)
Returns the greatest value of all parameters, skipping null values.


hex(expr)
Converts `expr` to hexadecimal.


hypot(expr1, expr2)
Returns sqrt(`expr1`**2 + `expr2`**2).


least(expr, ...)
Returns the least value of all parameters, skipping null values.


ln(expr)
Returns the natural logarithm (base e) of `expr`.


log(base, expr)
Returns the logarithm of `expr` with `base`.


log10(expr)
Returns the logarithm of `expr` with base 10.


log1p(expr)
Returns log(1 + `expr`).


log2(expr)
Returns the logarithm of `expr` with base 2.


expr1 mod expr2
Returns the remainder after `expr1`/`expr2`.


negative(expr)
Returns the negated value of `expr`.


pi()
Returns pi.


pmod(expr1, expr2)
Returns the positive value of `expr1` mod `expr2`.


positive(expr)
Returns the value of `expr`.


pow(expr1, expr2)
Raises `expr1` to the power of `expr2`.


power(expr1, expr2)
Raises `expr1` to the power of `expr2`.


radians(expr)
Converts degrees to radians.


rand([seed])
Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).


randn([seed])
Returns a random value with independent and identically distributed (i.i.d.) values drawn from the standard normal distribution.


random([seed])
Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).


rint(expr)
Returns the double value that is closest in value to the argument and is equal to a mathematical integer.


round(expr, d)
Returns `expr` rounded to `d` decimal places using HALF_UP rounding mode.


sec(expr)
Returns the secant of `expr`, as if computed by `1/java.lang.Math.cos`.


shiftleft(base, expr)
Bitwise left shift.


sign(expr)
Returns -1.0, 0.0 or 1.0 as `expr` is negative, 0 or positive.


signum(expr)
Returns -1.0, 0.0 or 1.0 as `expr` is negative, 0 or positive.


sin(expr)
Returns the sine of `expr`, as if computed by `java.lang.Math.sin`.


sinh(expr)
Returns hyperbolic sine of `expr`, as if computed by `java.lang.Math.sinh`.


sqrt(expr)
Returns the square root of `expr`.


tan(expr)
Returns the tangent of `expr`, as if computed by `java.lang.Math.tan`.


tanh(expr)
Returns the hyperbolic tangent of `expr`, as if computed by
      `java.lang.Math.tanh`.


try_add(expr1, expr2)
Returns the sum of `expr1`and `expr2` and the result is null on overflow. The acceptable input types are the same with the `+` operator.


try_divide(dividend, divisor)
Returns `dividend`/`divisor`. It always performs floating point division. Its result is always null if `expr2` is 0. `dividend` must be a numeric or an interval. `divisor` must be a numeric.


try_multiply(expr1, expr2)
Returns `expr1`*`expr2` and the result is null on overflow. The acceptable input types are the same with the `*` operator.


try_subtract(expr1, expr2)
Returns `expr1`-`expr2` and the result is null on overflow. The acceptable input types are the same with the `-` operator.


unhex(expr)
Converts hexadecimal `expr` to binary.


width_bucket(value, min_value, max_value, num_bucket)
Returns the bucket number to which
      `value` would be assigned in an equiwidth histogram with `num_bucket` buckets,
      in the range `min_value` to `max_value`."



Examples
-- %
SELECT 2 % 1.8;
+---------+
|(2 % 1.8)|
+---------+
|      0.2|
+---------+

SELECT MOD(2, 1.8);
+-----------+
|mod(2, 1.8)|
+-----------+
|        0.2|
+-----------+

-- *
SELECT 2 * 3;
+-------+
|(2 * 3)|
+-------+
|      6|
+-------+

-- +
SELECT 1 + 2;
+-------+
|(1 + 2)|
+-------+
|      3|
+-------+

-- -
SELECT 2 - 1;
+-------+
|(2 - 1)|
+-------+
|      1|
+-------+

-- /
SELECT 3 / 2;
+-------+
|(3 / 2)|
+-------+
|    1.5|
+-------+

SELECT 2L / 2L;
+-------+
|(2 / 2)|
+-------+
|    1.0|
+-------+

-- abs
SELECT abs(-1);
+-------+
|abs(-1)|
+-------+
|      1|
+-------+

SELECT abs(INTERVAL -'1-1' YEAR TO MONTH);
+----------------------------------+
|abs(INTERVAL '-1-1' YEAR TO MONTH)|
+----------------------------------+
|              INTERVAL '1-1' YE...|
+----------------------------------+

-- acos
SELECT acos(1);
+-------+
|ACOS(1)|
+-------+
|    0.0|
+-------+

SELECT acos(2);
+-------+
|ACOS(2)|
+-------+
|    NaN|
+-------+

-- acosh
SELECT acosh(1);
+--------+
|ACOSH(1)|
+--------+
|     0.0|
+--------+

SELECT acosh(0);
+--------+
|ACOSH(0)|
+--------+
|     NaN|
+--------+

-- asin
SELECT asin(0);
+-------+
|ASIN(0)|
+-------+
|    0.0|
+-------+

SELECT asin(2);
+-------+
|ASIN(2)|
+-------+
|    NaN|
+-------+

-- asinh
SELECT asinh(0);
+--------+
|ASINH(0)|
+--------+
|     0.0|
+--------+

-- atan
SELECT atan(0);
+-------+
|ATAN(0)|
+-------+
|    0.0|
+-------+

-- atan2
SELECT atan2(0, 0);
+-----------+
|ATAN2(0, 0)|
+-----------+
|        0.0|
+-----------+

-- atanh
SELECT atanh(0);
+--------+
|ATANH(0)|
+--------+
|     0.0|
+--------+

SELECT atanh(2);
+--------+
|ATANH(2)|
+--------+
|     NaN|
+--------+

-- bin
SELECT bin(13);
+-------+
|bin(13)|
+-------+
|   1101|
+-------+

SELECT bin(-13);
+--------------------+
|            bin(-13)|
+--------------------+
|11111111111111111...|
+--------------------+

SELECT bin(13.3);
+---------+
|bin(13.3)|
+---------+
|     1101|
+---------+

-- bround
SELECT bround(2.5, 0);
+--------------+
|bround(2.5, 0)|
+--------------+
|             2|
+--------------+

SELECT bround(25, -1);
+--------------+
|bround(25, -1)|
+--------------+
|            20|
+--------------+

-- cbrt
SELECT cbrt(27.0);
+----------+
|CBRT(27.0)|
+----------+
|       3.0|
+----------+

-- ceil
SELECT ceil(-0.1);
+----------+
|CEIL(-0.1)|
+----------+
|         0|
+----------+

SELECT ceil(5);
+-------+
|CEIL(5)|
+-------+
|      5|
+-------+

SELECT ceil(3.1411, 3);
+---------------+
|ceil(3.1411, 3)|
+---------------+
|          3.142|
+---------------+

SELECT ceil(3.1411, -3);
+----------------+
|ceil(3.1411, -3)|
+----------------+
|            1000|
+----------------+

-- ceiling
SELECT ceiling(-0.1);
+-------------+
|ceiling(-0.1)|
+-------------+
|            0|
+-------------+

SELECT ceiling(5);
+----------+
|ceiling(5)|
+----------+
|         5|
+----------+

SELECT ceiling(3.1411, 3);
+------------------+
|ceiling(3.1411, 3)|
+------------------+
|             3.142|
+------------------+

SELECT ceiling(3.1411, -3);
+-------------------+
|ceiling(3.1411, -3)|
+-------------------+
|               1000|
+-------------------+

-- conv
SELECT conv('100', 2, 10);
+----------------+
|conv(100, 2, 10)|
+----------------+
|               4|
+----------------+

SELECT conv(-10, 16, -10);
+------------------+
|conv(-10, 16, -10)|
+------------------+
|               -16|
+------------------+

-- cos
SELECT cos(0);
+------+
|COS(0)|
+------+
|   1.0|
+------+

-- cosh
SELECT cosh(0);
+-------+
|COSH(0)|
+-------+
|    1.0|
+-------+

-- cot
SELECT cot(1);
+------------------+
|            COT(1)|
+------------------+
|0.6420926159343306|
+------------------+

-- csc
SELECT csc(1);
+------------------+
|            CSC(1)|
+------------------+
|1.1883951057781212|
+------------------+

-- degrees
SELECT degrees(3.141592653589793);
+--------------------------+
|DEGREES(3.141592653589793)|
+--------------------------+
|                     180.0|
+--------------------------+

-- div
SELECT 3 div 2;
+---------+
|(3 div 2)|
+---------+
|        1|
+---------+

SELECT INTERVAL '1-1' YEAR TO MONTH div INTERVAL '-1' MONTH;
+------------------------------------------------------+
|(INTERVAL '1-1' YEAR TO MONTH div INTERVAL '-1' MONTH)|
+------------------------------------------------------+
|                                                   -13|
+------------------------------------------------------+

-- e
SELECT e();
+-----------------+
|              E()|
+-----------------+
|2.718281828459045|
+-----------------+

-- exp
SELECT exp(0);
+------+
|EXP(0)|
+------+
|   1.0|
+------+

-- expm1
SELECT expm1(0);
+--------+
|EXPM1(0)|
+--------+
|     0.0|
+--------+

-- factorial
SELECT factorial(5);
+------------+
|factorial(5)|
+------------+
|         120|
+------------+

-- floor
SELECT floor(-0.1);
+-----------+
|FLOOR(-0.1)|
+-----------+
|         -1|
+-----------+

SELECT floor(5);
+--------+
|FLOOR(5)|
+--------+
|       5|
+--------+

SELECT floor(3.1411, 3);
+----------------+
|floor(3.1411, 3)|
+----------------+
|           3.141|
+----------------+

SELECT floor(3.1411, -3);
+-----------------+
|floor(3.1411, -3)|
+-----------------+
|                0|
+-----------------+

-- greatest
SELECT greatest(10, 9, 2, 4, 3);
+------------------------+
|greatest(10, 9, 2, 4, 3)|
+------------------------+
|                      10|
+------------------------+

-- hex
SELECT hex(17);
+-------+
|hex(17)|
+-------+
|     11|
+-------+

SELECT hex('Spark SQL');
+------------------+
|    hex(Spark SQL)|
+------------------+
|537061726B2053514C|
+------------------+

-- hypot
SELECT hypot(3, 4);
+-----------+
|HYPOT(3, 4)|
+-----------+
|        5.0|
+-----------+

-- least
SELECT least(10, 9, 2, 4, 3);
+---------------------+
|least(10, 9, 2, 4, 3)|
+---------------------+
|                    2|
+---------------------+

-- ln
SELECT ln(1);
+-----+
|ln(1)|
+-----+
|  0.0|
+-----+

-- log
SELECT log(10, 100);
+------------+
|LOG(10, 100)|
+------------+
|         2.0|
+------------+

-- log10
SELECT log10(10);
+---------+
|LOG10(10)|
+---------+
|      1.0|
+---------+

-- log1p
SELECT log1p(0);
+--------+
|LOG1P(0)|
+--------+
|     0.0|
+--------+

-- log2
SELECT log2(2);
+-------+
|LOG2(2)|
+-------+
|    1.0|
+-------+

-- mod
SELECT 2 % 1.8;
+---------+
|(2 % 1.8)|
+---------+
|      0.2|
+---------+

SELECT MOD(2, 1.8);
+-----------+
|mod(2, 1.8)|
+-----------+
|        0.2|
+-----------+

-- negative
SELECT negative(1);
+-----------+
|negative(1)|
+-----------+
|         -1|
+-----------+

-- pi
SELECT pi();
+-----------------+
|             PI()|
+-----------------+
|3.141592653589793|
+-----------------+

-- pmod
SELECT pmod(10, 3);
+-----------+
|pmod(10, 3)|
+-----------+
|          1|
+-----------+

SELECT pmod(-10, 3);
+------------+
|pmod(-10, 3)|
+------------+
|           2|
+------------+

-- positive
SELECT positive(1);
+-----+
|(+ 1)|
+-----+
|    1|
+-----+

-- pow
SELECT pow(2, 3);
+---------+
|pow(2, 3)|
+---------+
|      8.0|
+---------+

-- power
SELECT power(2, 3);
+-----------+
|POWER(2, 3)|
+-----------+
|        8.0|
+-----------+

-- radians
SELECT radians(180);
+-----------------+
|     RADIANS(180)|
+-----------------+
|3.141592653589793|
+-----------------+

-- rand
SELECT rand();
+------------------+
|            rand()|
+------------------+
|0.9087689485284804|
+------------------+

SELECT rand(0);
+------------------+
|           rand(0)|
+------------------+
|0.7604953758285915|
+------------------+

SELECT rand(null);
+------------------+
|        rand(NULL)|
+------------------+
|0.7604953758285915|
+------------------+

-- randn
SELECT randn();
+-------------------+
|            randn()|
+-------------------+
|0.26951669182254523|
+-------------------+

SELECT randn(0);
+------------------+
|          randn(0)|
+------------------+
|1.6034991609278433|
+------------------+

SELECT randn(null);
+------------------+
|       randn(NULL)|
+------------------+
|1.6034991609278433|
+------------------+

-- random
SELECT random();
+------------------+
|            rand()|
+------------------+
|0.6225967149264997|
+------------------+

SELECT random(0);
+------------------+
|           rand(0)|
+------------------+
|0.7604953758285915|
+------------------+

SELECT random(null);
+------------------+
|        rand(NULL)|
+------------------+
|0.7604953758285915|
+------------------+

-- rint
SELECT rint(12.3456);
+-------------+
|rint(12.3456)|
+-------------+
|         12.0|
+-------------+

-- round
SELECT round(2.5, 0);
+-------------+
|round(2.5, 0)|
+-------------+
|            3|
+-------------+

-- sec
SELECT sec(0);
+------+
|SEC(0)|
+------+
|   1.0|
+------+

-- shiftleft
SELECT shiftleft(2, 1);
+---------------+
|shiftleft(2, 1)|
+---------------+
|              4|
+---------------+

-- sign
SELECT sign(40);
+--------+
|sign(40)|
+--------+
|     1.0|
+--------+

SELECT sign(INTERVAL -'100' YEAR);
+--------------------------+
|sign(INTERVAL '-100' YEAR)|
+--------------------------+
|                      -1.0|
+--------------------------+

-- signum
SELECT signum(40);
+----------+
|SIGNUM(40)|
+----------+
|       1.0|
+----------+

SELECT signum(INTERVAL -'100' YEAR);
+----------------------------+
|SIGNUM(INTERVAL '-100' YEAR)|
+----------------------------+
|                        -1.0|
+----------------------------+

-- sin
SELECT sin(0);
+------+
|SIN(0)|
+------+
|   0.0|
+------+

-- sinh
SELECT sinh(0);
+-------+
|SINH(0)|
+-------+
|    0.0|
+-------+

-- sqrt
SELECT sqrt(4);
+-------+
|SQRT(4)|
+-------+
|    2.0|
+-------+

-- tan
SELECT tan(0);
+------+
|TAN(0)|
+------+
|   0.0|
+------+

-- tanh
SELECT tanh(0);
+-------+
|TANH(0)|
+-------+
|    0.0|
+-------+

-- try_add
SELECT try_add(1, 2);
+-------------+
|try_add(1, 2)|
+-------------+
|            3|
+-------------+

SELECT try_add(2147483647, 1);
+----------------------+
|try_add(2147483647, 1)|
+----------------------+
|                  NULL|
+----------------------+

SELECT try_add(date'2021-01-01', 1);
+-----------------------------+
|try_add(DATE '2021-01-01', 1)|
+-----------------------------+
|                   2021-01-02|
+-----------------------------+

SELECT try_add(date'2021-01-01', interval 1 year);
+---------------------------------------------+
|try_add(DATE '2021-01-01', INTERVAL '1' YEAR)|
+---------------------------------------------+
|                                   2022-01-01|
+---------------------------------------------+

SELECT try_add(timestamp'2021-01-01 00:00:00', interval 1 day);
+----------------------------------------------------------+
|try_add(TIMESTAMP '2021-01-01 00:00:00', INTERVAL '1' DAY)|
+----------------------------------------------------------+
|                                       2021-01-02 00:00:00|
+----------------------------------------------------------+

SELECT try_add(interval 1 year, interval 2 year);
+---------------------------------------------+
|try_add(INTERVAL '1' YEAR, INTERVAL '2' YEAR)|
+---------------------------------------------+
|                            INTERVAL '3' YEAR|
+---------------------------------------------+

-- try_divide
SELECT try_divide(3, 2);
+----------------+
|try_divide(3, 2)|
+----------------+
|             1.5|
+----------------+

SELECT try_divide(2L, 2L);
+----------------+
|try_divide(2, 2)|
+----------------+
|             1.0|
+----------------+

SELECT try_divide(1, 0);
+----------------+
|try_divide(1, 0)|
+----------------+
|            NULL|
+----------------+

SELECT try_divide(interval 2 month, 2);
+---------------------------------+
|try_divide(INTERVAL '2' MONTH, 2)|
+---------------------------------+
|             INTERVAL '0-1' YE...|
+---------------------------------+

SELECT try_divide(interval 2 month, 0);
+---------------------------------+
|try_divide(INTERVAL '2' MONTH, 0)|
+---------------------------------+
|                             NULL|
+---------------------------------+

-- try_multiply
SELECT try_multiply(2, 3);
+------------------+
|try_multiply(2, 3)|
+------------------+
|                 6|
+------------------+

SELECT try_multiply(-2147483648, 10);
+-----------------------------+
|try_multiply(-2147483648, 10)|
+-----------------------------+
|                         NULL|
+-----------------------------+

SELECT try_multiply(interval 2 year, 3);
+----------------------------------+
|try_multiply(INTERVAL '2' YEAR, 3)|
+----------------------------------+
|              INTERVAL '6-0' YE...|
+----------------------------------+

-- try_subtract
SELECT try_subtract(2, 1);
+------------------+
|try_subtract(2, 1)|
+------------------+
|                 1|
+------------------+

SELECT try_subtract(-2147483648, 1);
+----------------------------+
|try_subtract(-2147483648, 1)|
+----------------------------+
|                        NULL|
+----------------------------+

SELECT try_subtract(date'2021-01-02', 1);
+----------------------------------+
|try_subtract(DATE '2021-01-02', 1)|
+----------------------------------+
|                        2021-01-01|
+----------------------------------+

SELECT try_subtract(date'2021-01-01', interval 1 year);
+--------------------------------------------------+
|try_subtract(DATE '2021-01-01', INTERVAL '1' YEAR)|
+--------------------------------------------------+
|                                        2020-01-01|
+--------------------------------------------------+

SELECT try_subtract(timestamp'2021-01-02 00:00:00', interval 1 day);
+---------------------------------------------------------------+
|try_subtract(TIMESTAMP '2021-01-02 00:00:00', INTERVAL '1' DAY)|
+---------------------------------------------------------------+
|                                            2021-01-01 00:00:00|
+---------------------------------------------------------------+

SELECT try_subtract(interval 2 year, interval 1 year);
+--------------------------------------------------+
|try_subtract(INTERVAL '2' YEAR, INTERVAL '1' YEAR)|
+--------------------------------------------------+
|                                 INTERVAL '1' YEAR|
+--------------------------------------------------+

-- unhex
SELECT decode(unhex('537061726B2053514C'), 'UTF-8');
+----------------------------------------+
|decode(unhex(537061726B2053514C), UTF-8)|
+----------------------------------------+
|                               Spark SQL|
+----------------------------------------+

-- width_bucket
SELECT width_bucket(5.3, 0.2, 10.6, 5);
+-------------------------------+
|width_bucket(5.3, 0.2, 10.6, 5)|
+-------------------------------+
|                              3|
+-------------------------------+

SELECT width_bucket(-2.1, 1.3, 3.4, 3);
+-------------------------------+
|width_bucket(-2.1, 1.3, 3.4, 3)|
+-------------------------------+
|                              0|
+-------------------------------+

SELECT width_bucket(8.1, 0.0, 5.7, 4);
+------------------------------+
|width_bucket(8.1, 0.0, 5.7, 4)|
+------------------------------+
|                             5|
+------------------------------+

SELECT width_bucket(-0.9, 5.2, 0.5, 2);
+-------------------------------+
|width_bucket(-0.9, 5.2, 0.5, 2)|
+-------------------------------+
|                              3|
+-------------------------------+

SELECT width_bucket(INTERVAL '0' YEAR, INTERVAL '0' YEAR, INTERVAL '10' YEAR, 10);
+--------------------------------------------------------------------------+
|width_bucket(INTERVAL '0' YEAR, INTERVAL '0' YEAR, INTERVAL '10' YEAR, 10)|
+--------------------------------------------------------------------------+
|                                                                         1|
+--------------------------------------------------------------------------+

SELECT width_bucket(INTERVAL '1' YEAR, INTERVAL '0' YEAR, INTERVAL '10' YEAR, 10);
+--------------------------------------------------------------------------+
|width_bucket(INTERVAL '1' YEAR, INTERVAL '0' YEAR, INTERVAL '10' YEAR, 10)|
+--------------------------------------------------------------------------+
|                                                                         2|
+--------------------------------------------------------------------------+

SELECT width_bucket(INTERVAL '0' DAY, INTERVAL '0' DAY, INTERVAL '10' DAY, 10);
+-----------------------------------------------------------------------+
|width_bucket(INTERVAL '0' DAY, INTERVAL '0' DAY, INTERVAL '10' DAY, 10)|
+-----------------------------------------------------------------------+
|                                                                      1|
+-----------------------------------------------------------------------+

SELECT width_bucket(INTERVAL '1' DAY, INTERVAL '0' DAY, INTERVAL '10' DAY, 10);
+-----------------------------------------------------------------------+
|width_bucket(INTERVAL '1' DAY, INTERVAL '0' DAY, INTERVAL '10' DAY, 10)|
+-----------------------------------------------------------------------+
|                                                                      2|
+-----------------------------------------------------------------------+

String Functions



Function
Description




ascii(str)
Returns the numeric value of the first character of `str`.


base64(bin)
Converts the argument from a binary `bin` to a base 64 string.


bit_length(expr)
Returns the bit length of string data or number of bits of binary data.


btrim(str)
Removes the leading and trailing space characters from `str`.


    btrim(str, trimStr)
Remove the leading and trailing `trimStr` characters from `str`.


char(expr)
Returns the ASCII character having the binary equivalent to `expr`. If n is larger than 256 the result is equivalent to chr(n % 256)


char_length(expr)
Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.


character_length(expr)
Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.


chr(expr)
Returns the ASCII character having the binary equivalent to `expr`. If n is larger than 256 the result is equivalent to chr(n % 256)


concat_ws(sep[, str | array(str)]+)
Returns the concatenation of the strings separated by `sep`, skipping null values.


contains(left, right)
Returns a boolean. The value is True if right is found inside left.
    Returns NULL if either input expression is NULL. Otherwise, returns False.
    Both left or right must be of STRING or BINARY type.


decode(bin, charset)
Decodes the first argument using the second argument character set.


    decode(expr, search, result [, search, result ] ... [, default])
Compares expr
      to each search value in order. If expr is equal to a search value, decode returns
      the corresponding result. If no match is found, then it returns default. If default
      is omitted, it returns null.


elt(n, input1, input2, ...)
Returns the `n`-th input, e.g., returns `input2` when `n` is 2.
    The function returns NULL if the index exceeds the length of the array
    and `spark.sql.ansi.enabled` is set to false. If `spark.sql.ansi.enabled` is set to true,
    it throws ArrayIndexOutOfBoundsException for invalid indices.


encode(str, charset)
Encodes the first argument using the second argument character set.


endswith(left, right)
Returns a boolean. The value is True if left ends with right.
    Returns NULL if either input expression is NULL. Otherwise, returns False.
    Both left or right must be of STRING or BINARY type.


find_in_set(str, str_array)
Returns the index (1-based) of the given string (`str`) in the comma-delimited list (`str_array`).
      Returns 0, if the string was not found or if the given string (`str`) contains a comma.


format_number(expr1, expr2)
Formats the number `expr1` like '#,###,###.##', rounded to `expr2`
      decimal places. If `expr2` is 0, the result has no decimal point or fractional part.
      `expr2` also accept a user specified format.
      This is supposed to function like MySQL's FORMAT.


format_string(strfmt, obj, ...)
Returns a formatted string from printf-style format strings.


initcap(str)
Returns `str` with the first letter of each word in uppercase.
      All other letters are in lowercase. Words are delimited by white space.


instr(str, substr)
Returns the (1-based) index of the first occurrence of `substr` in `str`.


lcase(str)
Returns `str` with all characters changed to lowercase.


left(str, len)
Returns the leftmost `len`(`len` can be string type) characters from the string `str`,if `len` is less or equal than 0 the result is an empty string.


len(expr)
Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.


length(expr)
Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.


levenshtein(str1, str2[, threshold])
Returns the Levenshtein distance between the two given strings. If threshold is set and distance more than it, return -1.


locate(substr, str[, pos])
Returns the position of the first occurrence of `substr` in `str` after position `pos`.
      The given `pos` and return value are 1-based.


lower(str)
Returns `str` with all characters changed to lowercase.


lpad(str, len[, pad])
Returns `str`, left-padded with `pad` to a length of `len`.
      If `str` is longer than `len`, the return value is shortened to `len` characters or bytes.
      If `pad` is not specified, `str` will be padded to the left with space characters if it is
      a character string, and with zeros if it is a byte sequence.


ltrim(str)
Removes the leading space characters from `str`.


luhn_check(str )
Checks that a string of digits is valid according to the Luhn algorithm.
    This checksum function is widely applied on credit card numbers and government identification
    numbers to distinguish valid numbers from mistyped, incorrect numbers.


mask(input[, upperChar, lowerChar, digitChar, otherChar])
masks the given string value.
       The function replaces characters with 'X' or 'x', and numbers with 'n'.
       This can be useful for creating copies of tables with sensitive information removed.


octet_length(expr)
Returns the byte length of string data or number of bytes of binary data.


overlay(input, replace, pos[, len])
Replace `input` with `replace` that starts at `pos` and is of length `len`.


position(substr, str[, pos])
Returns the position of the first occurrence of `substr` in `str` after position `pos`.
      The given `pos` and return value are 1-based.


printf(strfmt, obj, ...)
Returns a formatted string from printf-style format strings.


regexp_count(str, regexp)
Returns a count of the number of times that the regular expression pattern `regexp` is matched in the string `str`.


regexp_extract(str, regexp[, idx])
Extract the first string in the `str` that match the `regexp`
    expression and corresponding to the regex group index.


regexp_extract_all(str, regexp[, idx])
Extract all strings in the `str` that match the `regexp`
    expression and corresponding to the regex group index.


regexp_instr(str, regexp)
Searches a string for a regular expression and returns an integer that indicates the beginning position of the matched substring. Positions are 1-based, not 0-based. If no match is found, returns 0.


regexp_replace(str, regexp, rep[, position])
Replaces all substrings of `str` that match `regexp` with `rep`.


regexp_substr(str, regexp)
Returns the substring that matches the regular expression `regexp` within the string `str`. If the regular expression is not found, the result is null.


repeat(str, n)
Returns the string which repeats the given string value n times.


replace(str, search[, replace])
Replaces all occurrences of `search` with `replace`.


right(str, len)
Returns the rightmost `len`(`len` can be string type) characters from the string `str`,if `len` is less or equal than 0 the result is an empty string.


rpad(str, len[, pad])
Returns `str`, right-padded with `pad` to a length of `len`.
      If `str` is longer than `len`, the return value is shortened to `len` characters.
      If `pad` is not specified, `str` will be padded to the right with space characters if it is
      a character string, and with zeros if it is a binary string.


rtrim(str)
Removes the trailing space characters from `str`.


sentences(str[, lang, country])
Splits `str` into an array of array of words.


soundex(str)
Returns Soundex code of the string.


space(n)
Returns a string consisting of `n` spaces.


split(str, regex, limit)
Splits `str` around occurrences that match `regex` and returns an array with a length of at most `limit`


split_part(str, delimiter, partNum)
Splits `str` by delimiter and return
      requested part of the split (1-based). If any input is null, returns null.
      if `partNum` is out of range of split parts, returns empty string. If `partNum` is 0,
      throws an error. If `partNum` is negative, the parts are counted backward from the
      end of the string. If the `delimiter` is an empty string, the `str` is not split.


startswith(left, right)
Returns a boolean. The value is True if left starts with right.
    Returns NULL if either input expression is NULL. Otherwise, returns False.
    Both left or right must be of STRING or BINARY type.


substr(str, pos[, len])
Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.


    substr(str FROM pos[ FOR len]])
Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.


substring(str, pos[, len])
Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.


    substring(str FROM pos[ FOR len]])
Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.


substring_index(str, delim, count)
Returns the substring from `str` before `count` occurrences of the delimiter `delim`.
      If `count` is positive, everything to the left of the final delimiter (counting from the
      left) is returned. If `count` is negative, everything to the right of the final delimiter
      (counting from the right) is returned. The function substring_index performs a case-sensitive match
      when searching for `delim`.


to_binary(str[, fmt])
Converts the input `str` to a binary value based on the supplied `fmt`.
      `fmt` can be a case-insensitive string literal of "hex", "utf-8", "utf8", or "base64".
      By default, the binary format for conversion is "hex" if `fmt` is omitted.
      The function returns NULL if at least one of the input parameters is NULL.


to_char(numberExpr, formatExpr)
Convert `numberExpr` to a string based on the `formatExpr`.
      Throws an exception if the conversion fails. The format can consist of the following
      characters, case insensitive:
        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format
          string matches a sequence of digits in the input value, generating a result string of the
          same length as the corresponding sequence in the format string. The result string is
          left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of
          the decimal value, starts with 0, and is before the decimal point. Otherwise, it is
          padded with spaces.
        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).
        ',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be
          a 0 or 9 to the left and right of each grouping separator.
        '$': Specifies the location of the $ currency sign. This character may only be specified
          once.
        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at
          the beginning or end of the format string). Note that 'S' prints '+' for positive values
          but 'MI' prints a space.
        'PR': Only allowed at the end of the format string; specifies that the result string will be
          wrapped by angle brackets if the input value is negative.
          ('<1>').


to_number(expr, fmt)
Convert string 'expr' to a number based on the string format 'fmt'.
       Throws an exception if the conversion fails. The format can consist of the following
       characters, case insensitive:
         '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format
           string matches a sequence of digits in the input string. If the 0/9 sequence starts with
           0 and is before the decimal point, it can only match a digit sequence of the same size.
           Otherwise, if the sequence starts with 9 or is after the decimal point, it can match a
           digit sequence that has the same or smaller size.
         '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).
         ',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be
           a 0 or 9 to the left and right of each grouping separator. 'expr' must match the
           grouping separator relevant for the size of the number.
         '$': Specifies the location of the $ currency sign. This character may only be specified
           once.
         'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at
           the beginning or end of the format string). Note that 'S' allows '-' but 'MI' does not.
         'PR': Only allowed at the end of the format string; specifies that 'expr' indicates a
           negative number with wrapping angled brackets.
           ('<1>').


to_varchar(numberExpr, formatExpr)
Convert `numberExpr` to a string based on the `formatExpr`.
      Throws an exception if the conversion fails. The format can consist of the following
      characters, case insensitive:
        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format
          string matches a sequence of digits in the input value, generating a result string of the
          same length as the corresponding sequence in the format string. The result string is
          left-padded with zeros if the 0/9 sequence comprises more digits than the matching part of
          the decimal value, starts with 0, and is before the decimal point. Otherwise, it is
          padded with spaces.
        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).
        ',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be
          a 0 or 9 to the left and right of each grouping separator.
        '$': Specifies the location of the $ currency sign. This character may only be specified
          once.
        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at
          the beginning or end of the format string). Note that 'S' prints '+' for positive values
          but 'MI' prints a space.
        'PR': Only allowed at the end of the format string; specifies that the result string will be
          wrapped by angle brackets if the input value is negative.
          ('<1>').


translate(input, from, to)
Translates the `input` string by replacing the characters present in the `from` string with the corresponding characters in the `to` string.


trim(str)
Removes the leading and trailing space characters from `str`.


    trim(BOTH FROM str)
Removes the leading and trailing space characters from `str`.


    trim(LEADING FROM str)
Removes the leading space characters from `str`.


    trim(TRAILING FROM str)
Removes the trailing space characters from `str`.


    trim(trimStr FROM str)
Remove the leading and trailing `trimStr` characters from `str`.


    trim(BOTH trimStr FROM str)
Remove the leading and trailing `trimStr` characters from `str`.


    trim(LEADING trimStr FROM str)
Remove the leading `trimStr` characters from `str`.


    trim(TRAILING trimStr FROM str)
Remove the trailing `trimStr` characters from `str`.


try_to_binary(str[, fmt])
This is a special version of `to_binary` that performs the same operation, but returns a NULL value instead of raising an error if the conversion cannot be performed.


try_to_number(expr, fmt)
Convert string 'expr' to a number based on the string format `fmt`.
       Returns NULL if the string 'expr' does not match the expected format. The format follows the
       same semantics as the to_number function.


ucase(str)
Returns `str` with all characters changed to uppercase.


unbase64(str)
Converts the argument from a base 64 string `str` to a binary.


upper(str)
Returns `str` with all characters changed to uppercase.



Examples
-- ascii
SELECT ascii('222');
+----------+
|ascii(222)|
+----------+
|        50|
+----------+

SELECT ascii(2);
+--------+
|ascii(2)|
+--------+
|      50|
+--------+

-- base64
SELECT base64('Spark SQL');
+-----------------+
|base64(Spark SQL)|
+-----------------+
|     U3BhcmsgU1FM|
+-----------------+

SELECT base64(x'537061726b2053514c');
+-----------------------------+
|base64(X'537061726B2053514C')|
+-----------------------------+
|                 U3BhcmsgU1FM|
+-----------------------------+

-- bit_length
SELECT bit_length('Spark SQL');
+---------------------+
|bit_length(Spark SQL)|
+---------------------+
|                   72|
+---------------------+

SELECT bit_length(x'537061726b2053514c');
+---------------------------------+
|bit_length(X'537061726B2053514C')|
+---------------------------------+
|                               72|
+---------------------------------+

-- btrim
SELECT btrim('    SparkSQL   ');
+----------------------+
|btrim(    SparkSQL   )|
+----------------------+
|              SparkSQL|
+----------------------+

SELECT btrim(encode('    SparkSQL   ', 'utf-8'));
+-------------------------------------+
|btrim(encode(    SparkSQL   , utf-8))|
+-------------------------------------+
|                             SparkSQL|
+-------------------------------------+

SELECT btrim('SSparkSQLS', 'SL');
+---------------------+
|btrim(SSparkSQLS, SL)|
+---------------------+
|               parkSQ|
+---------------------+

SELECT btrim(encode('SSparkSQLS', 'utf-8'), encode('SL', 'utf-8'));
+---------------------------------------------------+
|btrim(encode(SSparkSQLS, utf-8), encode(SL, utf-8))|
+---------------------------------------------------+
|                                             parkSQ|
+---------------------------------------------------+

-- char
SELECT char(65);
+--------+
|char(65)|
+--------+
|       A|
+--------+

-- char_length
SELECT char_length('Spark SQL ');
+-----------------------+
|char_length(Spark SQL )|
+-----------------------+
|                     10|
+-----------------------+

SELECT char_length(x'537061726b2053514c');
+----------------------------------+
|char_length(X'537061726B2053514C')|
+----------------------------------+
|                                 9|
+----------------------------------+

SELECT CHAR_LENGTH('Spark SQL ');
+-----------------------+
|char_length(Spark SQL )|
+-----------------------+
|                     10|
+-----------------------+

SELECT CHARACTER_LENGTH('Spark SQL ');
+----------------------------+
|character_length(Spark SQL )|
+----------------------------+
|                          10|
+----------------------------+

-- character_length
SELECT character_length('Spark SQL ');
+----------------------------+
|character_length(Spark SQL )|
+----------------------------+
|                          10|
+----------------------------+

SELECT character_length(x'537061726b2053514c');
+---------------------------------------+
|character_length(X'537061726B2053514C')|
+---------------------------------------+
|                                      9|
+---------------------------------------+

SELECT CHAR_LENGTH('Spark SQL ');
+-----------------------+
|char_length(Spark SQL )|
+-----------------------+
|                     10|
+-----------------------+

SELECT CHARACTER_LENGTH('Spark SQL ');
+----------------------------+
|character_length(Spark SQL )|
+----------------------------+
|                          10|
+----------------------------+

-- chr
SELECT chr(65);
+-------+
|chr(65)|
+-------+
|      A|
+-------+

-- concat_ws
SELECT concat_ws(' ', 'Spark', 'SQL');
+------------------------+
|concat_ws( , Spark, SQL)|
+------------------------+
|               Spark SQL|
+------------------------+

SELECT concat_ws('s');
+------------+
|concat_ws(s)|
+------------+
|            |
+------------+

SELECT concat_ws('/', 'foo', null, 'bar');
+----------------------------+
|concat_ws(/, foo, NULL, bar)|
+----------------------------+
|                     foo/bar|
+----------------------------+

SELECT concat_ws(null, 'Spark', 'SQL');
+---------------------------+
|concat_ws(NULL, Spark, SQL)|
+---------------------------+
|                       NULL|
+---------------------------+

-- contains
SELECT contains('Spark SQL', 'Spark');
+--------------------------+
|contains(Spark SQL, Spark)|
+--------------------------+
|                      true|
+--------------------------+

SELECT contains('Spark SQL', 'SPARK');
+--------------------------+
|contains(Spark SQL, SPARK)|
+--------------------------+
|                     false|
+--------------------------+

SELECT contains('Spark SQL', null);
+-------------------------+
|contains(Spark SQL, NULL)|
+-------------------------+
|                     NULL|
+-------------------------+

SELECT contains(x'537061726b2053514c', x'537061726b');
+----------------------------------------------+
|contains(X'537061726B2053514C', X'537061726B')|
+----------------------------------------------+
|                                          true|
+----------------------------------------------+

-- decode
SELECT decode(encode('abc', 'utf-8'), 'utf-8');
+---------------------------------+
|decode(encode(abc, utf-8), utf-8)|
+---------------------------------+
|                              abc|
+---------------------------------+

SELECT decode(2, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');
+----------------------------------------------------------------------------------+
|decode(2, 1, Southlake, 2, San Francisco, 3, New Jersey, 4, Seattle, Non domestic)|
+----------------------------------------------------------------------------------+
|                                                                     San Francisco|
+----------------------------------------------------------------------------------+

SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle', 'Non domestic');
+----------------------------------------------------------------------------------+
|decode(6, 1, Southlake, 2, San Francisco, 3, New Jersey, 4, Seattle, Non domestic)|
+----------------------------------------------------------------------------------+
|                                                                      Non domestic|
+----------------------------------------------------------------------------------+

SELECT decode(6, 1, 'Southlake', 2, 'San Francisco', 3, 'New Jersey', 4, 'Seattle');
+--------------------------------------------------------------------+
|decode(6, 1, Southlake, 2, San Francisco, 3, New Jersey, 4, Seattle)|
+--------------------------------------------------------------------+
|                                                                NULL|
+--------------------------------------------------------------------+

SELECT decode(null, 6, 'Spark', NULL, 'SQL', 4, 'rocks');
+-------------------------------------------+
|decode(NULL, 6, Spark, NULL, SQL, 4, rocks)|
+-------------------------------------------+
|                                        SQL|
+-------------------------------------------+

-- elt
SELECT elt(1, 'scala', 'java');
+-------------------+
|elt(1, scala, java)|
+-------------------+
|              scala|
+-------------------+

SELECT elt(2, 'a', 1);
+------------+
|elt(2, a, 1)|
+------------+
|           1|
+------------+

-- encode
SELECT encode('abc', 'utf-8');
+------------------+
|encode(abc, utf-8)|
+------------------+
|        [61 62 63]|
+------------------+

-- endswith
SELECT endswith('Spark SQL', 'SQL');
+------------------------+
|endswith(Spark SQL, SQL)|
+------------------------+
|                    true|
+------------------------+

SELECT endswith('Spark SQL', 'Spark');
+--------------------------+
|endswith(Spark SQL, Spark)|
+--------------------------+
|                     false|
+--------------------------+

SELECT endswith('Spark SQL', null);
+-------------------------+
|endswith(Spark SQL, NULL)|
+-------------------------+
|                     NULL|
+-------------------------+

SELECT endswith(x'537061726b2053514c', x'537061726b');
+----------------------------------------------+
|endswith(X'537061726B2053514C', X'537061726B')|
+----------------------------------------------+
|                                         false|
+----------------------------------------------+

SELECT endswith(x'537061726b2053514c', x'53514c');
+------------------------------------------+
|endswith(X'537061726B2053514C', X'53514C')|
+------------------------------------------+
|                                      true|
+------------------------------------------+

-- find_in_set
SELECT find_in_set('ab','abc,b,ab,c,def');
+-------------------------------+
|find_in_set(ab, abc,b,ab,c,def)|
+-------------------------------+
|                              3|
+-------------------------------+

-- format_number
SELECT format_number(12332.123456, 4);
+------------------------------+
|format_number(12332.123456, 4)|
+------------------------------+
|                   12,332.1235|
+------------------------------+

SELECT format_number(12332.123456, '##################.###');
+---------------------------------------------------+
|format_number(12332.123456, ##################.###)|
+---------------------------------------------------+
|                                          12332.123|
+---------------------------------------------------+

-- format_string
SELECT format_string("Hello World %d %s", 100, "days");
+-------------------------------------------+
|format_string(Hello World %d %s, 100, days)|
+-------------------------------------------+
|                       Hello World 100 days|
+-------------------------------------------+

-- initcap
SELECT initcap('sPark sql');
+------------------+
|initcap(sPark sql)|
+------------------+
|         Spark Sql|
+------------------+

-- instr
SELECT instr('SparkSQL', 'SQL');
+--------------------+
|instr(SparkSQL, SQL)|
+--------------------+
|                   6|
+--------------------+

-- lcase
SELECT lcase('SparkSql');
+---------------+
|lcase(SparkSql)|
+---------------+
|       sparksql|
+---------------+

-- left
SELECT left('Spark SQL', 3);
+------------------+
|left(Spark SQL, 3)|
+------------------+
|               Spa|
+------------------+

SELECT left(encode('Spark SQL', 'utf-8'), 3);
+---------------------------------+
|left(encode(Spark SQL, utf-8), 3)|
+---------------------------------+
|                       [53 70 61]|
+---------------------------------+

-- len
SELECT len('Spark SQL ');
+---------------+
|len(Spark SQL )|
+---------------+
|             10|
+---------------+

SELECT len(x'537061726b2053514c');
+--------------------------+
|len(X'537061726B2053514C')|
+--------------------------+
|                         9|
+--------------------------+

SELECT CHAR_LENGTH('Spark SQL ');
+-----------------------+
|char_length(Spark SQL )|
+-----------------------+
|                     10|
+-----------------------+

SELECT CHARACTER_LENGTH('Spark SQL ');
+----------------------------+
|character_length(Spark SQL )|
+----------------------------+
|                          10|
+----------------------------+

-- length
SELECT length('Spark SQL ');
+------------------+
|length(Spark SQL )|
+------------------+
|                10|
+------------------+

SELECT length(x'537061726b2053514c');
+-----------------------------+
|length(X'537061726B2053514C')|
+-----------------------------+
|                            9|
+-----------------------------+

SELECT CHAR_LENGTH('Spark SQL ');
+-----------------------+
|char_length(Spark SQL )|
+-----------------------+
|                     10|
+-----------------------+

SELECT CHARACTER_LENGTH('Spark SQL ');
+----------------------------+
|character_length(Spark SQL )|
+----------------------------+
|                          10|
+----------------------------+

-- levenshtein
SELECT levenshtein('kitten', 'sitting');
+----------------------------+
|levenshtein(kitten, sitting)|
+----------------------------+
|                           3|
+----------------------------+

SELECT levenshtein('kitten', 'sitting', 2);
+-------------------------------+
|levenshtein(kitten, sitting, 2)|
+-------------------------------+
|                             -1|
+-------------------------------+

-- locate
SELECT locate('bar', 'foobarbar');
+-------------------------+
|locate(bar, foobarbar, 1)|
+-------------------------+
|                        4|
+-------------------------+

SELECT locate('bar', 'foobarbar', 5);
+-------------------------+
|locate(bar, foobarbar, 5)|
+-------------------------+
|                        7|
+-------------------------+

SELECT POSITION('bar' IN 'foobarbar');
+-------------------------+
|locate(bar, foobarbar, 1)|
+-------------------------+
|                        4|
+-------------------------+

-- lower
SELECT lower('SparkSql');
+---------------+
|lower(SparkSql)|
+---------------+
|       sparksql|
+---------------+

-- lpad
SELECT lpad('hi', 5, '??');
+---------------+
|lpad(hi, 5, ??)|
+---------------+
|          ???hi|
+---------------+

SELECT lpad('hi', 1, '??');
+---------------+
|lpad(hi, 1, ??)|
+---------------+
|              h|
+---------------+

SELECT lpad('hi', 5);
+--------------+
|lpad(hi, 5,  )|
+--------------+
|            hi|
+--------------+

SELECT hex(lpad(unhex('aabb'), 5));
+--------------------------------+
|hex(lpad(unhex(aabb), 5, X'00'))|
+--------------------------------+
|                      000000AABB|
+--------------------------------+

SELECT hex(lpad(unhex('aabb'), 5, unhex('1122')));
+--------------------------------------+
|hex(lpad(unhex(aabb), 5, unhex(1122)))|
+--------------------------------------+
|                            112211AABB|
+--------------------------------------+

-- ltrim
SELECT ltrim('    SparkSQL   ');
+----------------------+
|ltrim(    SparkSQL   )|
+----------------------+
|           SparkSQL   |
+----------------------+

-- luhn_check
SELECT luhn_check('8112189876');
+----------------------+
|luhn_check(8112189876)|
+----------------------+
|                  true|
+----------------------+

SELECT luhn_check('79927398713');
+-----------------------+
|luhn_check(79927398713)|
+-----------------------+
|                   true|
+-----------------------+

SELECT luhn_check('79927398714');
+-----------------------+
|luhn_check(79927398714)|
+-----------------------+
|                  false|
+-----------------------+

-- mask
SELECT mask('abcd-EFGH-8765-4321');
+----------------------------------------+
|mask(abcd-EFGH-8765-4321, X, x, n, NULL)|
+----------------------------------------+
|                     xxxx-XXXX-nnnn-nnnn|
+----------------------------------------+

SELECT mask('abcd-EFGH-8765-4321', 'Q');
+----------------------------------------+
|mask(abcd-EFGH-8765-4321, Q, x, n, NULL)|
+----------------------------------------+
|                     xxxx-QQQQ-nnnn-nnnn|
+----------------------------------------+

SELECT mask('AbCD123-@$#', 'Q', 'q');
+--------------------------------+
|mask(AbCD123-@$#, Q, q, n, NULL)|
+--------------------------------+
|                     QqQQnnn-@$#|
+--------------------------------+

SELECT mask('AbCD123-@$#');
+--------------------------------+
|mask(AbCD123-@$#, X, x, n, NULL)|
+--------------------------------+
|                     XxXXnnn-@$#|
+--------------------------------+

SELECT mask('AbCD123-@$#', 'Q');
+--------------------------------+
|mask(AbCD123-@$#, Q, x, n, NULL)|
+--------------------------------+
|                     QxQQnnn-@$#|
+--------------------------------+

SELECT mask('AbCD123-@$#', 'Q', 'q');
+--------------------------------+
|mask(AbCD123-@$#, Q, q, n, NULL)|
+--------------------------------+
|                     QqQQnnn-@$#|
+--------------------------------+

SELECT mask('AbCD123-@$#', 'Q', 'q', 'd');
+--------------------------------+
|mask(AbCD123-@$#, Q, q, d, NULL)|
+--------------------------------+
|                     QqQQddd-@$#|
+--------------------------------+

SELECT mask('AbCD123-@$#', 'Q', 'q', 'd', 'o');
+-----------------------------+
|mask(AbCD123-@$#, Q, q, d, o)|
+-----------------------------+
|                  QqQQdddoooo|
+-----------------------------+

SELECT mask('AbCD123-@$#', NULL, 'q', 'd', 'o');
+--------------------------------+
|mask(AbCD123-@$#, NULL, q, d, o)|
+--------------------------------+
|                     AqCDdddoooo|
+--------------------------------+

SELECT mask('AbCD123-@$#', NULL, NULL, 'd', 'o');
+-----------------------------------+
|mask(AbCD123-@$#, NULL, NULL, d, o)|
+-----------------------------------+
|                        AbCDdddoooo|
+-----------------------------------+

SELECT mask('AbCD123-@$#', NULL, NULL, NULL, 'o');
+--------------------------------------+
|mask(AbCD123-@$#, NULL, NULL, NULL, o)|
+--------------------------------------+
|                           AbCD123oooo|
+--------------------------------------+

SELECT mask(NULL, NULL, NULL, NULL, 'o');
+-------------------------------+
|mask(NULL, NULL, NULL, NULL, o)|
+-------------------------------+
|                           NULL|
+-------------------------------+

SELECT mask(NULL);
+-------------------------+
|mask(NULL, X, x, n, NULL)|
+-------------------------+
|                     NULL|
+-------------------------+

SELECT mask('AbCD123-@$#', NULL, NULL, NULL, NULL);
+-----------------------------------------+
|mask(AbCD123-@$#, NULL, NULL, NULL, NULL)|
+-----------------------------------------+
|                              AbCD123-@$#|
+-----------------------------------------+

-- octet_length
SELECT octet_length('Spark SQL');
+-----------------------+
|octet_length(Spark SQL)|
+-----------------------+
|                      9|
+-----------------------+

SELECT octet_length(x'537061726b2053514c');
+-----------------------------------+
|octet_length(X'537061726B2053514C')|
+-----------------------------------+
|                                  9|
+-----------------------------------+

-- overlay
SELECT overlay('Spark SQL' PLACING '_' FROM 6);
+----------------------------+
|overlay(Spark SQL, _, 6, -1)|
+----------------------------+
|                   Spark_SQL|
+----------------------------+

SELECT overlay('Spark SQL' PLACING 'CORE' FROM 7);
+-------------------------------+
|overlay(Spark SQL, CORE, 7, -1)|
+-------------------------------+
|                     Spark CORE|
+-------------------------------+

SELECT overlay('Spark SQL' PLACING 'ANSI ' FROM 7 FOR 0);
+-------------------------------+
|overlay(Spark SQL, ANSI , 7, 0)|
+-------------------------------+
|                 Spark ANSI SQL|
+-------------------------------+

SELECT overlay('Spark SQL' PLACING 'tructured' FROM 2 FOR 4);
+-----------------------------------+
|overlay(Spark SQL, tructured, 2, 4)|
+-----------------------------------+
|                     Structured SQL|
+-----------------------------------+

SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('_', 'utf-8') FROM 6);
+----------------------------------------------------------+
|overlay(encode(Spark SQL, utf-8), encode(_, utf-8), 6, -1)|
+----------------------------------------------------------+
|                                      [53 70 61 72 6B 5...|
+----------------------------------------------------------+

SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('CORE', 'utf-8') FROM 7);
+-------------------------------------------------------------+
|overlay(encode(Spark SQL, utf-8), encode(CORE, utf-8), 7, -1)|
+-------------------------------------------------------------+
|                                         [53 70 61 72 6B 2...|
+-------------------------------------------------------------+

SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('ANSI ', 'utf-8') FROM 7 FOR 0);
+-------------------------------------------------------------+
|overlay(encode(Spark SQL, utf-8), encode(ANSI , utf-8), 7, 0)|
+-------------------------------------------------------------+
|                                         [53 70 61 72 6B 2...|
+-------------------------------------------------------------+

SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('tructured', 'utf-8') FROM 2 FOR 4);
+-----------------------------------------------------------------+
|overlay(encode(Spark SQL, utf-8), encode(tructured, utf-8), 2, 4)|
+-----------------------------------------------------------------+
|                                             [53 74 72 75 63 7...|
+-----------------------------------------------------------------+

-- position
SELECT position('bar', 'foobarbar');
+---------------------------+
|position(bar, foobarbar, 1)|
+---------------------------+
|                          4|
+---------------------------+

SELECT position('bar', 'foobarbar', 5);
+---------------------------+
|position(bar, foobarbar, 5)|
+---------------------------+
|                          7|
+---------------------------+

SELECT POSITION('bar' IN 'foobarbar');
+-------------------------+
|locate(bar, foobarbar, 1)|
+-------------------------+
|                        4|
+-------------------------+

-- printf
SELECT printf("Hello World %d %s", 100, "days");
+------------------------------------+
|printf(Hello World %d %s, 100, days)|
+------------------------------------+
|                Hello World 100 days|
+------------------------------------+

-- regexp_count
SELECT regexp_count('Steven Jones and Stephen Smith are the best players', 'Ste(v|ph)en');
+------------------------------------------------------------------------------+
|regexp_count(Steven Jones and Stephen Smith are the best players, Ste(v|ph)en)|
+------------------------------------------------------------------------------+
|                                                                             2|
+------------------------------------------------------------------------------+

SELECT regexp_count('abcdefghijklmnopqrstuvwxyz', '[a-z]{3}');
+--------------------------------------------------+
|regexp_count(abcdefghijklmnopqrstuvwxyz, [a-z]{3})|
+--------------------------------------------------+
|                                                 8|
+--------------------------------------------------+

-- regexp_extract
SELECT regexp_extract('100-200', '(\\d+)-(\\d+)', 1);
+---------------------------------------+
|regexp_extract(100-200, (\d+)-(\d+), 1)|
+---------------------------------------+
|                                    100|
+---------------------------------------+

-- regexp_extract_all
SELECT regexp_extract_all('100-200, 300-400', '(\\d+)-(\\d+)', 1);
+----------------------------------------------------+
|regexp_extract_all(100-200, 300-400, (\d+)-(\d+), 1)|
+----------------------------------------------------+
|                                          [100, 300]|
+----------------------------------------------------+

-- regexp_instr
SELECT regexp_instr('user@spark.apache.org', '@[^.]*');
+----------------------------------------------+
|regexp_instr(user@spark.apache.org, @[^.]*, 0)|
+----------------------------------------------+
|                                             5|
+----------------------------------------------+

-- regexp_replace
SELECT regexp_replace('100-200', '(\\d+)', 'num');
+--------------------------------------+
|regexp_replace(100-200, (\d+), num, 1)|
+--------------------------------------+
|                               num-num|
+--------------------------------------+

-- regexp_substr
SELECT regexp_substr('Steven Jones and Stephen Smith are the best players', 'Ste(v|ph)en');
+-------------------------------------------------------------------------------+
|regexp_substr(Steven Jones and Stephen Smith are the best players, Ste(v|ph)en)|
+-------------------------------------------------------------------------------+
|                                                                         Steven|
+-------------------------------------------------------------------------------+

SELECT regexp_substr('Steven Jones and Stephen Smith are the best players', 'Jeck');
+------------------------------------------------------------------------+
|regexp_substr(Steven Jones and Stephen Smith are the best players, Jeck)|
+------------------------------------------------------------------------+
|                                                                    NULL|
+------------------------------------------------------------------------+

-- repeat
SELECT repeat('123', 2);
+--------------+
|repeat(123, 2)|
+--------------+
|        123123|
+--------------+

-- replace
SELECT replace('ABCabc', 'abc', 'DEF');
+-------------------------+
|replace(ABCabc, abc, DEF)|
+-------------------------+
|                   ABCDEF|
+-------------------------+

-- right
SELECT right('Spark SQL', 3);
+-------------------+
|right(Spark SQL, 3)|
+-------------------+
|                SQL|
+-------------------+

-- rpad
SELECT rpad('hi', 5, '??');
+---------------+
|rpad(hi, 5, ??)|
+---------------+
|          hi???|
+---------------+

SELECT rpad('hi', 1, '??');
+---------------+
|rpad(hi, 1, ??)|
+---------------+
|              h|
+---------------+

SELECT rpad('hi', 5);
+--------------+
|rpad(hi, 5,  )|
+--------------+
|         hi   |
+--------------+

SELECT hex(rpad(unhex('aabb'), 5));
+--------------------------------+
|hex(rpad(unhex(aabb), 5, X'00'))|
+--------------------------------+
|                      AABB000000|
+--------------------------------+

SELECT hex(rpad(unhex('aabb'), 5, unhex('1122')));
+--------------------------------------+
|hex(rpad(unhex(aabb), 5, unhex(1122)))|
+--------------------------------------+
|                            AABB112211|
+--------------------------------------+

-- rtrim
SELECT rtrim('    SparkSQL   ');
+----------------------+
|rtrim(    SparkSQL   )|
+----------------------+
|              SparkSQL|
+----------------------+

-- sentences
SELECT sentences('Hi there! Good morning.');
+--------------------------------------+
|sentences(Hi there! Good morning., , )|
+--------------------------------------+
|                  [[Hi, there], [Go...|
+--------------------------------------+

-- soundex
SELECT soundex('Miller');
+---------------+
|soundex(Miller)|
+---------------+
|           M460|
+---------------+

-- space
SELECT concat(space(2), '1');
+-------------------+
|concat(space(2), 1)|
+-------------------+
|                  1|
+-------------------+

-- split
SELECT split('oneAtwoBthreeC', '[ABC]');
+--------------------------------+
|split(oneAtwoBthreeC, [ABC], -1)|
+--------------------------------+
|             [one, two, three, ]|
+--------------------------------+

SELECT split('oneAtwoBthreeC', '[ABC]', -1);
+--------------------------------+
|split(oneAtwoBthreeC, [ABC], -1)|
+--------------------------------+
|             [one, two, three, ]|
+--------------------------------+

SELECT split('oneAtwoBthreeC', '[ABC]', 2);
+-------------------------------+
|split(oneAtwoBthreeC, [ABC], 2)|
+-------------------------------+
|              [one, twoBthreeC]|
+-------------------------------+

-- split_part
SELECT split_part('11.12.13', '.', 3);
+--------------------------+
|split_part(11.12.13, ., 3)|
+--------------------------+
|                        13|
+--------------------------+

-- startswith
SELECT startswith('Spark SQL', 'Spark');
+----------------------------+
|startswith(Spark SQL, Spark)|
+----------------------------+
|                        true|
+----------------------------+

SELECT startswith('Spark SQL', 'SQL');
+--------------------------+
|startswith(Spark SQL, SQL)|
+--------------------------+
|                     false|
+--------------------------+

SELECT startswith('Spark SQL', null);
+---------------------------+
|startswith(Spark SQL, NULL)|
+---------------------------+
|                       NULL|
+---------------------------+

SELECT startswith(x'537061726b2053514c', x'537061726b');
+------------------------------------------------+
|startswith(X'537061726B2053514C', X'537061726B')|
+------------------------------------------------+
|                                            true|
+------------------------------------------------+

SELECT startswith(x'537061726b2053514c', x'53514c');
+--------------------------------------------+
|startswith(X'537061726B2053514C', X'53514C')|
+--------------------------------------------+
|                                       false|
+--------------------------------------------+

-- substr
SELECT substr('Spark SQL', 5);
+--------------------------------+
|substr(Spark SQL, 5, 2147483647)|
+--------------------------------+
|                           k SQL|
+--------------------------------+

SELECT substr('Spark SQL', -3);
+---------------------------------+
|substr(Spark SQL, -3, 2147483647)|
+---------------------------------+
|                              SQL|
+---------------------------------+

SELECT substr('Spark SQL', 5, 1);
+-----------------------+
|substr(Spark SQL, 5, 1)|
+-----------------------+
|                      k|
+-----------------------+

SELECT substr('Spark SQL' FROM 5);
+-----------------------------------+
|substring(Spark SQL, 5, 2147483647)|
+-----------------------------------+
|                              k SQL|
+-----------------------------------+

SELECT substr('Spark SQL' FROM -3);
+------------------------------------+
|substring(Spark SQL, -3, 2147483647)|
+------------------------------------+
|                                 SQL|
+------------------------------------+

SELECT substr('Spark SQL' FROM 5 FOR 1);
+--------------------------+
|substring(Spark SQL, 5, 1)|
+--------------------------+
|                         k|
+--------------------------+

SELECT substr(encode('Spark SQL', 'utf-8'), 5);
+-----------------------------------------------+
|substr(encode(Spark SQL, utf-8), 5, 2147483647)|
+-----------------------------------------------+
|                               [6B 20 53 51 4C]|
+-----------------------------------------------+

-- substring
SELECT substring('Spark SQL', 5);
+-----------------------------------+
|substring(Spark SQL, 5, 2147483647)|
+-----------------------------------+
|                              k SQL|
+-----------------------------------+

SELECT substring('Spark SQL', -3);
+------------------------------------+
|substring(Spark SQL, -3, 2147483647)|
+------------------------------------+
|                                 SQL|
+------------------------------------+

SELECT substring('Spark SQL', 5, 1);
+--------------------------+
|substring(Spark SQL, 5, 1)|
+--------------------------+
|                         k|
+--------------------------+

SELECT substring('Spark SQL' FROM 5);
+-----------------------------------+
|substring(Spark SQL, 5, 2147483647)|
+-----------------------------------+
|                              k SQL|
+-----------------------------------+

SELECT substring('Spark SQL' FROM -3);
+------------------------------------+
|substring(Spark SQL, -3, 2147483647)|
+------------------------------------+
|                                 SQL|
+------------------------------------+

SELECT substring('Spark SQL' FROM 5 FOR 1);
+--------------------------+
|substring(Spark SQL, 5, 1)|
+--------------------------+
|                         k|
+--------------------------+

SELECT substring(encode('Spark SQL', 'utf-8'), 5);
+--------------------------------------------------+
|substring(encode(Spark SQL, utf-8), 5, 2147483647)|
+--------------------------------------------------+
|                                  [6B 20 53 51 4C]|
+--------------------------------------------------+

-- substring_index
SELECT substring_index('www.apache.org', '.', 2);
+-------------------------------------+
|substring_index(www.apache.org, ., 2)|
+-------------------------------------+
|                           www.apache|
+-------------------------------------+

-- to_binary
SELECT to_binary('abc', 'utf-8');
+---------------------+
|to_binary(abc, utf-8)|
+---------------------+
|           [61 62 63]|
+---------------------+

-- to_char
SELECT to_char(454, '999');
+-----------------+
|to_char(454, 999)|
+-----------------+
|              454|
+-----------------+

SELECT to_char(454.00, '000D00');
+-----------------------+
|to_char(454.00, 000D00)|
+-----------------------+
|                 454.00|
+-----------------------+

SELECT to_char(12454, '99G999');
+----------------------+
|to_char(12454, 99G999)|
+----------------------+
|                12,454|
+----------------------+

SELECT to_char(78.12, '$99.99');
+----------------------+
|to_char(78.12, $99.99)|
+----------------------+
|                $78.12|
+----------------------+

SELECT to_char(-12454.8, '99G999D9S');
+----------------------------+
|to_char(-12454.8, 99G999D9S)|
+----------------------------+
|                   12,454.8-|
+----------------------------+

-- to_number
SELECT to_number('454', '999');
+-------------------+
|to_number(454, 999)|
+-------------------+
|                454|
+-------------------+

SELECT to_number('454.00', '000.00');
+-------------------------+
|to_number(454.00, 000.00)|
+-------------------------+
|                   454.00|
+-------------------------+

SELECT to_number('12,454', '99,999');
+-------------------------+
|to_number(12,454, 99,999)|
+-------------------------+
|                    12454|
+-------------------------+

SELECT to_number('$78.12', '$99.99');
+-------------------------+
|to_number($78.12, $99.99)|
+-------------------------+
|                    78.12|
+-------------------------+

SELECT to_number('12,454.8-', '99,999.9S');
+-------------------------------+
|to_number(12,454.8-, 99,999.9S)|
+-------------------------------+
|                       -12454.8|
+-------------------------------+

-- to_varchar
SELECT to_varchar(454, '999');
+-----------------+
|to_char(454, 999)|
+-----------------+
|              454|
+-----------------+

SELECT to_varchar(454.00, '000D00');
+-----------------------+
|to_char(454.00, 000D00)|
+-----------------------+
|                 454.00|
+-----------------------+

SELECT to_varchar(12454, '99G999');
+----------------------+
|to_char(12454, 99G999)|
+----------------------+
|                12,454|
+----------------------+

SELECT to_varchar(78.12, '$99.99');
+----------------------+
|to_char(78.12, $99.99)|
+----------------------+
|                $78.12|
+----------------------+

SELECT to_varchar(-12454.8, '99G999D9S');
+----------------------------+
|to_char(-12454.8, 99G999D9S)|
+----------------------------+
|                   12,454.8-|
+----------------------------+

-- translate
SELECT translate('AaBbCc', 'abc', '123');
+---------------------------+
|translate(AaBbCc, abc, 123)|
+---------------------------+
|                     A1B2C3|
+---------------------------+

-- trim
SELECT trim('    SparkSQL   ');
+---------------------+
|trim(    SparkSQL   )|
+---------------------+
|             SparkSQL|
+---------------------+

SELECT trim(BOTH FROM '    SparkSQL   ');
+---------------------+
|trim(    SparkSQL   )|
+---------------------+
|             SparkSQL|
+---------------------+

SELECT trim(LEADING FROM '    SparkSQL   ');
+----------------------+
|ltrim(    SparkSQL   )|
+----------------------+
|           SparkSQL   |
+----------------------+

SELECT trim(TRAILING FROM '    SparkSQL   ');
+----------------------+
|rtrim(    SparkSQL   )|
+----------------------+
|              SparkSQL|
+----------------------+

SELECT trim('SL' FROM 'SSparkSQLS');
+-----------------------------+
|TRIM(BOTH SL FROM SSparkSQLS)|
+-----------------------------+
|                       parkSQ|
+-----------------------------+

SELECT trim(BOTH 'SL' FROM 'SSparkSQLS');
+-----------------------------+
|TRIM(BOTH SL FROM SSparkSQLS)|
+-----------------------------+
|                       parkSQ|
+-----------------------------+

SELECT trim(LEADING 'SL' FROM 'SSparkSQLS');
+--------------------------------+
|TRIM(LEADING SL FROM SSparkSQLS)|
+--------------------------------+
|                        parkSQLS|
+--------------------------------+

SELECT trim(TRAILING 'SL' FROM 'SSparkSQLS');
+---------------------------------+
|TRIM(TRAILING SL FROM SSparkSQLS)|
+---------------------------------+
|                         SSparkSQ|
+---------------------------------+

-- try_to_binary
SELECT try_to_binary('abc', 'utf-8');
+-------------------------+
|try_to_binary(abc, utf-8)|
+-------------------------+
|               [61 62 63]|
+-------------------------+

select try_to_binary('a!', 'base64');
+-------------------------+
|try_to_binary(a!, base64)|
+-------------------------+
|                     NULL|
+-------------------------+

select try_to_binary('abc', 'invalidFormat');
+---------------------------------+
|try_to_binary(abc, invalidFormat)|
+---------------------------------+
|                             NULL|
+---------------------------------+

-- try_to_number
SELECT try_to_number('454', '999');
+-----------------------+
|try_to_number(454, 999)|
+-----------------------+
|                    454|
+-----------------------+

SELECT try_to_number('454.00', '000.00');
+-----------------------------+
|try_to_number(454.00, 000.00)|
+-----------------------------+
|                       454.00|
+-----------------------------+

SELECT try_to_number('12,454', '99,999');
+-----------------------------+
|try_to_number(12,454, 99,999)|
+-----------------------------+
|                        12454|
+-----------------------------+

SELECT try_to_number('$78.12', '$99.99');
+-----------------------------+
|try_to_number($78.12, $99.99)|
+-----------------------------+
|                        78.12|
+-----------------------------+

SELECT try_to_number('12,454.8-', '99,999.9S');
+-----------------------------------+
|try_to_number(12,454.8-, 99,999.9S)|
+-----------------------------------+
|                           -12454.8|
+-----------------------------------+

-- ucase
SELECT ucase('SparkSql');
+---------------+
|ucase(SparkSql)|
+---------------+
|       SPARKSQL|
+---------------+

-- unbase64
SELECT unbase64('U3BhcmsgU1FM');
+----------------------+
|unbase64(U3BhcmsgU1FM)|
+----------------------+
|  [53 70 61 72 6B 2...|
+----------------------+

-- upper
SELECT upper('SparkSql');
+---------------+
|upper(SparkSql)|
+---------------+
|       SPARKSQL|
+---------------+

Conditional Functions



Function
Description




coalesce(expr1, expr2, ...)
Returns the first non-null argument if exists. Otherwise, null.


if(expr1, expr2, expr3)
If `expr1` evaluates to true, then returns `expr2`; otherwise returns `expr3`.


ifnull(expr1, expr2)
Returns `expr2` if `expr1` is null, or `expr1` otherwise.


nanvl(expr1, expr2)
Returns `expr1` if it's not NaN, or `expr2` otherwise.


nullif(expr1, expr2)
Returns null if `expr1` equals to `expr2`, or `expr1` otherwise.


nvl(expr1, expr2)
Returns `expr2` if `expr1` is null, or `expr1` otherwise.


nvl2(expr1, expr2, expr3)
Returns `expr2` if `expr1` is not null, or `expr3` otherwise.


CASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END
When `expr1` = true, returns `expr2`; else when `expr3` = true, returns `expr4`; else returns `expr5`.



Examples
-- coalesce
SELECT coalesce(NULL, 1, NULL);
+-----------------------+
|coalesce(NULL, 1, NULL)|
+-----------------------+
|                      1|
+-----------------------+

-- if
SELECT if(1 < 2, 'a', 'b');
+-------------------+
|(IF((1 < 2), a, b))|
+-------------------+
|                  a|
+-------------------+

-- ifnull
SELECT ifnull(NULL, array('2'));
+----------------------+
|ifnull(NULL, array(2))|
+----------------------+
|                   [2]|
+----------------------+

-- nanvl
SELECT nanvl(cast('NaN' as double), 123);
+-------------------------------+
|nanvl(CAST(NaN AS DOUBLE), 123)|
+-------------------------------+
|                          123.0|
+-------------------------------+

-- nullif
SELECT nullif(2, 2);
+------------+
|nullif(2, 2)|
+------------+
|        NULL|
+------------+

-- nvl
SELECT nvl(NULL, array('2'));
+-------------------+
|nvl(NULL, array(2))|
+-------------------+
|                [2]|
+-------------------+

-- nvl2
SELECT nvl2(NULL, 2, 1);
+----------------+
|nvl2(NULL, 2, 1)|
+----------------+
|               1|
+----------------+

-- when
SELECT CASE WHEN 1 > 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;
+-----------------------------------------------------------+
|CASE WHEN (1 > 0) THEN 1 WHEN (2 > 0) THEN 2.0 ELSE 1.2 END|
+-----------------------------------------------------------+
|                                                        1.0|
+-----------------------------------------------------------+

SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;
+-----------------------------------------------------------+
|CASE WHEN (1 < 0) THEN 1 WHEN (2 > 0) THEN 2.0 ELSE 1.2 END|
+-----------------------------------------------------------+
|                                                        2.0|
+-----------------------------------------------------------+

SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 < 0 THEN 2.0 END;
+--------------------------------------------------+
|CASE WHEN (1 < 0) THEN 1 WHEN (2 < 0) THEN 2.0 END|
+--------------------------------------------------+
|                                              NULL|
+--------------------------------------------------+

Bitwise Functions



Function
Description




expr1 & expr2
Returns the result of bitwise AND of `expr1` and `expr2`.


expr1 ^ expr2
Returns the result of bitwise exclusive OR of `expr1` and `expr2`.


bit_count(expr)
Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer, or NULL if the argument is NULL.


bit_get(expr, pos)
Returns the value of the bit (0 or 1) at the specified position.
      The positions are numbered from right to left, starting at zero.
      The position argument cannot be negative.


getbit(expr, pos)
Returns the value of the bit (0 or 1) at the specified position.
      The positions are numbered from right to left, starting at zero.
      The position argument cannot be negative.


shiftright(base, expr)
Bitwise (signed) right shift.


shiftrightunsigned(base, expr)
Bitwise unsigned right shift.


expr1 | expr2
Returns the result of bitwise OR of `expr1` and `expr2`.


~ expr
Returns the result of bitwise NOT of `expr`.



Examples
-- &
SELECT 3 & 5;
+-------+
|(3 & 5)|
+-------+
|      1|
+-------+

-- ^
SELECT 3 ^ 5;
+-------+
|(3 ^ 5)|
+-------+
|      6|
+-------+

-- bit_count
SELECT bit_count(0);
+------------+
|bit_count(0)|
+------------+
|           0|
+------------+

-- bit_get
SELECT bit_get(11, 0);
+--------------+
|bit_get(11, 0)|
+--------------+
|             1|
+--------------+

SELECT bit_get(11, 2);
+--------------+
|bit_get(11, 2)|
+--------------+
|             0|
+--------------+

-- getbit
SELECT getbit(11, 0);
+-------------+
|getbit(11, 0)|
+-------------+
|            1|
+-------------+

SELECT getbit(11, 2);
+-------------+
|getbit(11, 2)|
+-------------+
|            0|
+-------------+

-- shiftright
SELECT shiftright(4, 1);
+----------------+
|shiftright(4, 1)|
+----------------+
|               2|
+----------------+

-- shiftrightunsigned
SELECT shiftrightunsigned(4, 1);
+------------------------+
|shiftrightunsigned(4, 1)|
+------------------------+
|                       2|
+------------------------+

-- |
SELECT 3 | 5;
+-------+
|(3 | 5)|
+-------+
|      7|
+-------+

-- ~
SELECT ~ 0;
+---+
| ~0|
+---+
| -1|
+---+

Conversion Functions



Function
Description




bigint(expr)
Casts the value `expr` to the target data type `bigint`.


binary(expr)
Casts the value `expr` to the target data type `binary`.


boolean(expr)
Casts the value `expr` to the target data type `boolean`.


cast(expr AS type)
Casts the value `expr` to the target data type `type`.


date(expr)
Casts the value `expr` to the target data type `date`.


decimal(expr)
Casts the value `expr` to the target data type `decimal`.


double(expr)
Casts the value `expr` to the target data type `double`.


float(expr)
Casts the value `expr` to the target data type `float`.


int(expr)
Casts the value `expr` to the target data type `int`.


smallint(expr)
Casts the value `expr` to the target data type `smallint`.


string(expr)
Casts the value `expr` to the target data type `string`.


timestamp(expr)
Casts the value `expr` to the target data type `timestamp`.


tinyint(expr)
Casts the value `expr` to the target data type `tinyint`.



Examples
-- cast
SELECT cast('10' as int);
+---------------+
|CAST(10 AS INT)|
+---------------+
|             10|
+---------------+

Predicate Functions



Function
Description




! expr
Logical not.


expr1 < expr2
Returns true if `expr1` is less than `expr2`.


expr1 <= expr2
Returns true if `expr1` is less than or equal to `expr2`.


expr1 <=> expr2
Returns same result as the EQUAL(=) operator for non-null operands,
      but returns true if both are null, false if one of the them is null.


expr1 = expr2
Returns true if `expr1` equals `expr2`, or false otherwise.


expr1 == expr2
Returns true if `expr1` equals `expr2`, or false otherwise.


expr1 > expr2
Returns true if `expr1` is greater than `expr2`.


expr1 >= expr2
Returns true if `expr1` is greater than or equal to `expr2`.


expr1 and expr2
Logical AND.


str ilike pattern[ ESCAPE escape]
Returns true if str matches `pattern` with `escape` case-insensitively, null if any arguments are null, false otherwise.


expr1 in(expr2, expr3, ...)
Returns true if `expr` equals to any valN.


isnan(expr)
Returns true if `expr` is NaN, or false otherwise.


isnotnull(expr)
Returns true if `expr` is not null, or false otherwise.


isnull(expr)
Returns true if `expr` is null, or false otherwise.


str like pattern[ ESCAPE escape]
Returns true if str matches `pattern` with `escape`, null if any arguments are null, false otherwise.


not expr
Logical not.


expr1 or expr2
Logical OR.


regexp(str, regexp)
Returns true if `str` matches `regexp`, or false otherwise.


regexp_like(str, regexp)
Returns true if `str` matches `regexp`, or false otherwise.


rlike(str, regexp)
Returns true if `str` matches `regexp`, or false otherwise.



Examples
-- !
SELECT ! true;
+----------+
|(NOT true)|
+----------+
|     false|
+----------+

SELECT ! false;
+-----------+
|(NOT false)|
+-----------+
|       true|
+-----------+

SELECT ! NULL;
+----------+
|(NOT NULL)|
+----------+
|      NULL|
+----------+

-- <
SELECT 1 < 2;
+-------+
|(1 < 2)|
+-------+
|   true|
+-------+

SELECT 1.1 < '1';
+---------+
|(1.1 < 1)|
+---------+
|    false|
+---------+

SELECT to_date('2009-07-30 04:17:52') < to_date('2009-07-30 04:17:52');
+-------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) < to_date(2009-07-30 04:17:52))|
+-------------------------------------------------------------+
|                                                        false|
+-------------------------------------------------------------+

SELECT to_date('2009-07-30 04:17:52') < to_date('2009-08-01 04:17:52');
+-------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) < to_date(2009-08-01 04:17:52))|
+-------------------------------------------------------------+
|                                                         true|
+-------------------------------------------------------------+

SELECT 1 < NULL;
+----------+
|(1 < NULL)|
+----------+
|      NULL|
+----------+

-- <=
SELECT 2 <= 2;
+--------+
|(2 <= 2)|
+--------+
|    true|
+--------+

SELECT 1.0 <= '1';
+----------+
|(1.0 <= 1)|
+----------+
|      true|
+----------+

SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-07-30 04:17:52');
+--------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) <= to_date(2009-07-30 04:17:52))|
+--------------------------------------------------------------+
|                                                          true|
+--------------------------------------------------------------+

SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-08-01 04:17:52');
+--------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) <= to_date(2009-08-01 04:17:52))|
+--------------------------------------------------------------+
|                                                          true|
+--------------------------------------------------------------+

SELECT 1 <= NULL;
+-----------+
|(1 <= NULL)|
+-----------+
|       NULL|
+-----------+

-- <=>
SELECT 2 <=> 2;
+---------+
|(2 <=> 2)|
+---------+
|     true|
+---------+

SELECT 1 <=> '1';
+---------+
|(1 <=> 1)|
+---------+
|     true|
+---------+

SELECT true <=> NULL;
+---------------+
|(true <=> NULL)|
+---------------+
|          false|
+---------------+

SELECT NULL <=> NULL;
+---------------+
|(NULL <=> NULL)|
+---------------+
|           true|
+---------------+

-- =
SELECT 2 = 2;
+-------+
|(2 = 2)|
+-------+
|   true|
+-------+

SELECT 1 = '1';
+-------+
|(1 = 1)|
+-------+
|   true|
+-------+

SELECT true = NULL;
+-------------+
|(true = NULL)|
+-------------+
|         NULL|
+-------------+

SELECT NULL = NULL;
+-------------+
|(NULL = NULL)|
+-------------+
|         NULL|
+-------------+

-- ==
SELECT 2 == 2;
+-------+
|(2 = 2)|
+-------+
|   true|
+-------+

SELECT 1 == '1';
+-------+
|(1 = 1)|
+-------+
|   true|
+-------+

SELECT true == NULL;
+-------------+
|(true = NULL)|
+-------------+
|         NULL|
+-------------+

SELECT NULL == NULL;
+-------------+
|(NULL = NULL)|
+-------------+
|         NULL|
+-------------+

-- >
SELECT 2 > 1;
+-------+
|(2 > 1)|
+-------+
|   true|
+-------+

SELECT 2 > 1.1;
+-------+
|(2 > 1)|
+-------+
|   true|
+-------+

SELECT to_date('2009-07-30 04:17:52') > to_date('2009-07-30 04:17:52');
+-------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) > to_date(2009-07-30 04:17:52))|
+-------------------------------------------------------------+
|                                                        false|
+-------------------------------------------------------------+

SELECT to_date('2009-07-30 04:17:52') > to_date('2009-08-01 04:17:52');
+-------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) > to_date(2009-08-01 04:17:52))|
+-------------------------------------------------------------+
|                                                        false|
+-------------------------------------------------------------+

SELECT 1 > NULL;
+----------+
|(1 > NULL)|
+----------+
|      NULL|
+----------+

-- >=
SELECT 2 >= 1;
+--------+
|(2 >= 1)|
+--------+
|    true|
+--------+

SELECT 2.0 >= '2.1';
+------------+
|(2.0 >= 2.1)|
+------------+
|       false|
+------------+

SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-07-30 04:17:52');
+--------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) >= to_date(2009-07-30 04:17:52))|
+--------------------------------------------------------------+
|                                                          true|
+--------------------------------------------------------------+

SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-08-01 04:17:52');
+--------------------------------------------------------------+
|(to_date(2009-07-30 04:17:52) >= to_date(2009-08-01 04:17:52))|
+--------------------------------------------------------------+
|                                                         false|
+--------------------------------------------------------------+

SELECT 1 >= NULL;
+-----------+
|(1 >= NULL)|
+-----------+
|       NULL|
+-----------+

-- and
SELECT true and true;
+---------------+
|(true AND true)|
+---------------+
|           true|
+---------------+

SELECT true and false;
+----------------+
|(true AND false)|
+----------------+
|           false|
+----------------+

SELECT true and NULL;
+---------------+
|(true AND NULL)|
+---------------+
|           NULL|
+---------------+

SELECT false and NULL;
+----------------+
|(false AND NULL)|
+----------------+
|           false|
+----------------+

-- ilike
SELECT ilike('Spark', '_Park');
+-------------------+
|ilike(Spark, _Park)|
+-------------------+
|               true|
+-------------------+

SET spark.sql.parser.escapedStringLiterals=true;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....| true|
+--------------------+-----+

SELECT '%SystemDrive%\Users\John' ilike '\%SystemDrive\%\\users%';
+--------------------------------------------------------+
|ilike(%SystemDrive%\Users\John, \%SystemDrive\%\\users%)|
+--------------------------------------------------------+
|                                                    true|
+--------------------------------------------------------+

SET spark.sql.parser.escapedStringLiterals=false;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....|false|
+--------------------+-----+

SELECT '%SystemDrive%\\USERS\\John' ilike '\%SystemDrive\%\\\\Users%';
+--------------------------------------------------------+
|ilike(%SystemDrive%\USERS\John, \%SystemDrive\%\\Users%)|
+--------------------------------------------------------+
|                                                    true|
+--------------------------------------------------------+

SELECT '%SystemDrive%/Users/John' ilike '/%SYSTEMDrive/%//Users%' ESCAPE '/';
+--------------------------------------------------------+
|ilike(%SystemDrive%/Users/John, /%SYSTEMDrive/%//Users%)|
+--------------------------------------------------------+
|                                                    true|
+--------------------------------------------------------+

-- in
SELECT 1 in(1, 2, 3);
+----------------+
|(1 IN (1, 2, 3))|
+----------------+
|            true|
+----------------+

SELECT 1 in(2, 3, 4);
+----------------+
|(1 IN (2, 3, 4))|
+----------------+
|           false|
+----------------+

SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 1), named_struct('a', 1, 'b', 3));
+----------------------------------------------------------------------------------+
|(named_struct(a, 1, b, 2) IN (named_struct(a, 1, b, 1), named_struct(a, 1, b, 3)))|
+----------------------------------------------------------------------------------+
|                                                                             false|
+----------------------------------------------------------------------------------+

SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 2), named_struct('a', 1, 'b', 3));
+----------------------------------------------------------------------------------+
|(named_struct(a, 1, b, 2) IN (named_struct(a, 1, b, 2), named_struct(a, 1, b, 3)))|
+----------------------------------------------------------------------------------+
|                                                                              true|
+----------------------------------------------------------------------------------+

-- isnan
SELECT isnan(cast('NaN' as double));
+--------------------------+
|isnan(CAST(NaN AS DOUBLE))|
+--------------------------+
|                      true|
+--------------------------+

-- isnotnull
SELECT isnotnull(1);
+---------------+
|(1 IS NOT NULL)|
+---------------+
|           true|
+---------------+

-- isnull
SELECT isnull(1);
+-----------+
|(1 IS NULL)|
+-----------+
|      false|
+-----------+

-- like
SELECT like('Spark', '_park');
+----------------+
|Spark LIKE _park|
+----------------+
|            true|
+----------------+

SET spark.sql.parser.escapedStringLiterals=true;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....| true|
+--------------------+-----+

SELECT '%SystemDrive%\Users\John' like '\%SystemDrive\%\\Users%';
+-----------------------------------------------------+
|%SystemDrive%\Users\John LIKE \%SystemDrive\%\\Users%|
+-----------------------------------------------------+
|                                                 true|
+-----------------------------------------------------+

SET spark.sql.parser.escapedStringLiterals=false;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....|false|
+--------------------+-----+

SELECT '%SystemDrive%\\Users\\John' like '\%SystemDrive\%\\\\Users%';
+-----------------------------------------------------+
|%SystemDrive%\Users\John LIKE \%SystemDrive\%\\Users%|
+-----------------------------------------------------+
|                                                 true|
+-----------------------------------------------------+

SELECT '%SystemDrive%/Users/John' like '/%SystemDrive/%//Users%' ESCAPE '/';
+-----------------------------------------------------+
|%SystemDrive%/Users/John LIKE /%SystemDrive/%//Users%|
+-----------------------------------------------------+
|                                                 true|
+-----------------------------------------------------+

-- not
SELECT not true;
+----------+
|(NOT true)|
+----------+
|     false|
+----------+

SELECT not false;
+-----------+
|(NOT false)|
+-----------+
|       true|
+-----------+

SELECT not NULL;
+----------+
|(NOT NULL)|
+----------+
|      NULL|
+----------+

-- or
SELECT true or false;
+---------------+
|(true OR false)|
+---------------+
|           true|
+---------------+

SELECT false or false;
+----------------+
|(false OR false)|
+----------------+
|           false|
+----------------+

SELECT true or NULL;
+--------------+
|(true OR NULL)|
+--------------+
|          true|
+--------------+

SELECT false or NULL;
+---------------+
|(false OR NULL)|
+---------------+
|           NULL|
+---------------+

-- regexp
SET spark.sql.parser.escapedStringLiterals=true;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....| true|
+--------------------+-----+

SELECT regexp('%SystemDrive%\Users\John', '%SystemDrive%\\Users.*');
+--------------------------------------------------------+
|REGEXP(%SystemDrive%\Users\John, %SystemDrive%\\Users.*)|
+--------------------------------------------------------+
|                                                    true|
+--------------------------------------------------------+

SET spark.sql.parser.escapedStringLiterals=false;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....|false|
+--------------------+-----+

SELECT regexp('%SystemDrive%\\Users\\John', '%SystemDrive%\\\\Users.*');
+--------------------------------------------------------+
|REGEXP(%SystemDrive%\Users\John, %SystemDrive%\\Users.*)|
+--------------------------------------------------------+
|                                                    true|
+--------------------------------------------------------+

-- regexp_like
SET spark.sql.parser.escapedStringLiterals=true;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....| true|
+--------------------+-----+

SELECT regexp_like('%SystemDrive%\Users\John', '%SystemDrive%\\Users.*');
+-------------------------------------------------------------+
|REGEXP_LIKE(%SystemDrive%\Users\John, %SystemDrive%\\Users.*)|
+-------------------------------------------------------------+
|                                                         true|
+-------------------------------------------------------------+

SET spark.sql.parser.escapedStringLiterals=false;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....|false|
+--------------------+-----+

SELECT regexp_like('%SystemDrive%\\Users\\John', '%SystemDrive%\\\\Users.*');
+-------------------------------------------------------------+
|REGEXP_LIKE(%SystemDrive%\Users\John, %SystemDrive%\\Users.*)|
+-------------------------------------------------------------+
|                                                         true|
+-------------------------------------------------------------+

-- rlike
SET spark.sql.parser.escapedStringLiterals=true;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....| true|
+--------------------+-----+

SELECT rlike('%SystemDrive%\Users\John', '%SystemDrive%\\Users.*');
+-------------------------------------------------------+
|RLIKE(%SystemDrive%\Users\John, %SystemDrive%\\Users.*)|
+-------------------------------------------------------+
|                                                   true|
+-------------------------------------------------------+

SET spark.sql.parser.escapedStringLiterals=false;
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.parser....|false|
+--------------------+-----+

SELECT rlike('%SystemDrive%\\Users\\John', '%SystemDrive%\\\\Users.*');
+-------------------------------------------------------+
|RLIKE(%SystemDrive%\Users\John, %SystemDrive%\\Users.*)|
+-------------------------------------------------------+
|                                                   true|
+-------------------------------------------------------+

Csv Functions



Function
Description




from_csv(csvStr, schema[, options])
Returns a struct value with the given `csvStr` and `schema`.


schema_of_csv(csv[, options])
Returns schema in the DDL format of CSV string.


to_csv(expr[, options])
Returns a CSV string with a given struct value



Examples
-- from_csv
SELECT from_csv('1, 0.8', 'a INT, b DOUBLE');
+----------------+
|from_csv(1, 0.8)|
+----------------+
|        {1, 0.8}|
+----------------+

SELECT from_csv('26/08/2015', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));
+--------------------+
|from_csv(26/08/2015)|
+--------------------+
|{2015-08-26 00:00...|
+--------------------+

-- schema_of_csv
SELECT schema_of_csv('1,abc');
+--------------------+
|schema_of_csv(1,abc)|
+--------------------+
|STRUCT<_c0: INT, ...|
+--------------------+

-- to_csv
SELECT to_csv(named_struct('a', 1, 'b', 2));
+--------------------------------+
|to_csv(named_struct(a, 1, b, 2))|
+--------------------------------+
|                             1,2|
+--------------------------------+

SELECT to_csv(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));
+----------------------------------------------------------------+
|to_csv(named_struct(time, to_timestamp(2015-08-26, yyyy-MM-dd)))|
+----------------------------------------------------------------+
|                                                      26/08/2015|
+----------------------------------------------------------------+

Misc Functions



Function
Description




aes_decrypt(expr, key[, mode[, padding[, aad]]])
Returns a decrypted value of `expr` using AES in `mode` with `padding`.
      Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS').
      Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption.
      The default mode is GCM.


aes_encrypt(expr, key[, mode[, padding[, iv[, aad]]]])
Returns an encrypted value of `expr` using AES in given `mode` with the specified `padding`.
      Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS').
      Optional initialization vectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12 bytes for GCM. If not provided, a random vector will be generated and prepended to the output.
      Optional additional authenticated data (AAD) is only supported for GCM. If provided for encryption, the identical AAD value must be provided for decryption.
      The default mode is GCM.


assert_true(expr)
Throws an exception if `expr` is not true.


bitmap_bit_position(child)
Returns the bit position for the given input child expression.


bitmap_bucket_number(child)
Returns the bucket number for the given input child expression.


bitmap_count(child)
Returns the number of set bits in the child bitmap.


current_catalog()
Returns the current catalog.


current_database()
Returns the current database.


current_schema()
Returns the current database.


current_user()
user name of current execution context.


equal_null(expr1, expr2)
Returns same result as the EQUAL(=) operator for non-null operands,
      but returns true if both are null, false if one of the them is null.


hll_sketch_estimate(expr)
Returns the estimated number of unique values given the binary representation
    of a Datasketches HllSketch.


hll_union(first, second, allowDifferentLgConfigK)
Merges two binary representations of
    Datasketches HllSketch objects, using a Datasketches Union object. Set
    allowDifferentLgConfigK to true to allow unions of sketches with different
    lgConfigK values (defaults to false).


input_file_block_length()
Returns the length of the block being read, or -1 if not available.


input_file_block_start()
Returns the start offset of the block being read, or -1 if not available.


input_file_name()
Returns the name of the file being read, or empty string if not available.


java_method(class, method[, arg1[, arg2 ..]])
Calls a method with reflection.


monotonically_increasing_id()
Returns monotonically increasing 64-bit integers. The generated ID is guaranteed
      to be monotonically increasing and unique, but not consecutive. The current implementation
      puts the partition ID in the upper 31 bits, and the lower 33 bits represent the record number
      within each partition. The assumption is that the data frame has less than 1 billion
      partitions, and each partition has less than 8 billion records.
      The function is non-deterministic because its result depends on partition IDs.


reflect(class, method[, arg1[, arg2 ..]])
Calls a method with reflection.


spark_partition_id()
Returns the current partition id.


try_aes_decrypt(expr, key[, mode[, padding[, aad]]])
This is a special version of `aes_decrypt` that performs the same operation, but returns a NULL value instead of raising an error if the decryption cannot be performed.


typeof(expr)
Return DDL-formatted type string for the data type of the input.


user()
user name of current execution context.


uuid()
Returns an universally unique identifier (UUID) string. The value is returned as a canonical UUID 36-character string.


version()
Returns the Spark version. The string contains 2 fields, the first being a release version and the second being a git revision.



Examples
-- aes_decrypt
SELECT aes_decrypt(unhex('83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94'), '0000111122223333');
+------------------------------------------------------------------------------------------------------------------------+
|aes_decrypt(unhex(83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94), 0000111122223333, GCM, DEFAULT, )|
+------------------------------------------------------------------------------------------------------------------------+
|                                                                                                        [53 70 61 72 6B]|
+------------------------------------------------------------------------------------------------------------------------+

SELECT aes_decrypt(unhex('6E7CA17BBB468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210'), '0000111122223333', 'GCM');
+--------------------------------------------------------------------------------------------------------------------------------+
|aes_decrypt(unhex(6E7CA17BBB468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210), 0000111122223333, GCM, DEFAULT, )|
+--------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                            [53 70 61 72 6B 2...|
+--------------------------------------------------------------------------------------------------------------------------------+

SELECT aes_decrypt(unbase64('3lmwu+Mw0H3fi5NDvcu9lg=='), '1234567890abcdef', 'ECB', 'PKCS');
+------------------------------------------------------------------------------+
|aes_decrypt(unbase64(3lmwu+Mw0H3fi5NDvcu9lg==), 1234567890abcdef, ECB, PKCS, )|
+------------------------------------------------------------------------------+
|                                                          [53 70 61 72 6B 2...|
+------------------------------------------------------------------------------+

SELECT aes_decrypt(unbase64('2NYmDCjgXTbbxGA3/SnJEfFC/JQ7olk2VQWReIAAFKo='), '1234567890abcdef', 'CBC');
+-----------------------------------------------------------------------------------------------------+
|aes_decrypt(unbase64(2NYmDCjgXTbbxGA3/SnJEfFC/JQ7olk2VQWReIAAFKo=), 1234567890abcdef, CBC, DEFAULT, )|
+-----------------------------------------------------------------------------------------------------+
|                                                                                 [41 70 61 63 68 6...|
+-----------------------------------------------------------------------------------------------------+

SELECT aes_decrypt(unbase64('AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg='), 'abcdefghijklmnop12345678ABCDEFGH', 'CBC', 'DEFAULT');
+---------------------------------------------------------------------------------------------------------------------+
|aes_decrypt(unbase64(AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=), abcdefghijklmnop12345678ABCDEFGH, CBC, DEFAULT, )|
+---------------------------------------------------------------------------------------------------------------------+
|                                                                                                     [53 70 61 72 6B]|
+---------------------------------------------------------------------------------------------------------------------+

SELECT aes_decrypt(unbase64('AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4'), 'abcdefghijklmnop12345678ABCDEFGH', 'GCM', 'DEFAULT', 'This is an AAD mixed into the input');
+--------------------------------------------------------------------------------------------------------------------------------------------------------+
|aes_decrypt(unbase64(AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4), abcdefghijklmnop12345678ABCDEFGH, GCM, DEFAULT, This is an AAD mixed into the input)|
+--------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                        [53 70 61 72 6B]|
+--------------------------------------------------------------------------------------------------------------------------------------------------------+

-- aes_encrypt
SELECT hex(aes_encrypt('Spark', '0000111122223333'));
+-----------------------------------------------------------+
|hex(aes_encrypt(Spark, 0000111122223333, GCM, DEFAULT, , ))|
+-----------------------------------------------------------+
|                                       B10980E61DCCA0A8F...|
+-----------------------------------------------------------+

SELECT hex(aes_encrypt('Spark SQL', '0000111122223333', 'GCM'));
+---------------------------------------------------------------+
|hex(aes_encrypt(Spark SQL, 0000111122223333, GCM, DEFAULT, , ))|
+---------------------------------------------------------------+
|                                           FA491A4505D296754...|
+---------------------------------------------------------------+

SELECT base64(aes_encrypt('Spark SQL', '1234567890abcdef', 'ECB', 'PKCS'));
+---------------------------------------------------------------+
|base64(aes_encrypt(Spark SQL, 1234567890abcdef, ECB, PKCS, , ))|
+---------------------------------------------------------------+
|                                           3lmwu+Mw0H3fi5NDv...|
+---------------------------------------------------------------+

SELECT base64(aes_encrypt('Apache Spark', '1234567890abcdef', 'CBC', 'DEFAULT'));
+---------------------------------------------------------------------+
|base64(aes_encrypt(Apache Spark, 1234567890abcdef, CBC, DEFAULT, , ))|
+---------------------------------------------------------------------+
|                                                 8pDrRMN5AmrPC52Zp...|
+---------------------------------------------------------------------+

SELECT base64(aes_encrypt('Spark', 'abcdefghijklmnop12345678ABCDEFGH', 'CBC', 'DEFAULT', unhex('00000000000000000000000000000000')));
+---------------------------------------------------------------------------------------------------------------------+
|base64(aes_encrypt(Spark, abcdefghijklmnop12345678ABCDEFGH, CBC, DEFAULT, unhex(00000000000000000000000000000000), ))|
+---------------------------------------------------------------------------------------------------------------------+
|                                                                                                 AAAAAAAAAAAAAAAAA...|
+---------------------------------------------------------------------------------------------------------------------+

SELECT base64(aes_encrypt('Spark', 'abcdefghijklmnop12345678ABCDEFGH', 'GCM', 'DEFAULT', unhex('000000000000000000000000'), 'This is an AAD mixed into the input'));
+------------------------------------------------------------------------------------------------------------------------------------------------+
|base64(aes_encrypt(Spark, abcdefghijklmnop12345678ABCDEFGH, GCM, DEFAULT, unhex(000000000000000000000000), This is an AAD mixed into the input))|
+------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                            AAAAAAAAAAAAAAAAQ...|
+------------------------------------------------------------------------------------------------------------------------------------------------+

-- assert_true
SELECT assert_true(0 < 1);
+--------------------------------------------+
|assert_true((0 < 1), '(0 < 1)' is not true!)|
+--------------------------------------------+
|                                        NULL|
+--------------------------------------------+

-- bitmap_bit_position
SELECT bitmap_bit_position(1);
+----------------------+
|bitmap_bit_position(1)|
+----------------------+
|                     0|
+----------------------+

SELECT bitmap_bit_position(123);
+------------------------+
|bitmap_bit_position(123)|
+------------------------+
|                     122|
+------------------------+

-- bitmap_bucket_number
SELECT bitmap_bucket_number(123);
+-------------------------+
|bitmap_bucket_number(123)|
+-------------------------+
|                        1|
+-------------------------+

SELECT bitmap_bucket_number(0);
+-----------------------+
|bitmap_bucket_number(0)|
+-----------------------+
|                      0|
+-----------------------+

-- bitmap_count
SELECT bitmap_count(X '1010');
+---------------------+
|bitmap_count(X'1010')|
+---------------------+
|                    2|
+---------------------+

SELECT bitmap_count(X 'FFFF');
+---------------------+
|bitmap_count(X'FFFF')|
+---------------------+
|                   16|
+---------------------+

SELECT bitmap_count(X '0');
+-------------------+
|bitmap_count(X'00')|
+-------------------+
|                  0|
+-------------------+

-- current_catalog
SELECT current_catalog();
+-----------------+
|current_catalog()|
+-----------------+
|    spark_catalog|
+-----------------+

-- current_database
SELECT current_database();
+------------------+
|current_database()|
+------------------+
|           default|
+------------------+

-- current_schema
SELECT current_schema();
+------------------+
|current_database()|
+------------------+
|           default|
+------------------+

-- current_user
SELECT current_user();
+--------------+
|current_user()|
+--------------+
|      spark-rm|
+--------------+

-- equal_null
SELECT equal_null(3, 3);
+----------------+
|equal_null(3, 3)|
+----------------+
|            true|
+----------------+

SELECT equal_null(1, '11');
+-----------------+
|equal_null(1, 11)|
+-----------------+
|            false|
+-----------------+

SELECT equal_null(true, NULL);
+----------------------+
|equal_null(true, NULL)|
+----------------------+
|                 false|
+----------------------+

SELECT equal_null(NULL, 'abc');
+---------------------+
|equal_null(NULL, abc)|
+---------------------+
|                false|
+---------------------+

SELECT equal_null(NULL, NULL);
+----------------------+
|equal_null(NULL, NULL)|
+----------------------+
|                  true|
+----------------------+

-- hll_sketch_estimate
SELECT hll_sketch_estimate(hll_sketch_agg(col)) FROM VALUES (1), (1), (2), (2), (3) tab(col);
+--------------------------------------------+
|hll_sketch_estimate(hll_sketch_agg(col, 12))|
+--------------------------------------------+
|                                           3|
+--------------------------------------------+

-- hll_union
SELECT hll_sketch_estimate(hll_union(hll_sketch_agg(col1), hll_sketch_agg(col2))) FROM VALUES (1, 4), (1, 4), (2, 5), (2, 5), (3, 6) tab(col1, col2);
+-----------------------------------------------------------------------------------------+
|hll_sketch_estimate(hll_union(hll_sketch_agg(col1, 12), hll_sketch_agg(col2, 12), false))|
+-----------------------------------------------------------------------------------------+
|                                                                                        6|
+-----------------------------------------------------------------------------------------+

-- input_file_block_length
SELECT input_file_block_length();
+-------------------------+
|input_file_block_length()|
+-------------------------+
|                       -1|
+-------------------------+

-- input_file_block_start
SELECT input_file_block_start();
+------------------------+
|input_file_block_start()|
+------------------------+
|                      -1|
+------------------------+

-- input_file_name
SELECT input_file_name();
+-----------------+
|input_file_name()|
+-----------------+
|                 |
+-----------------+

-- java_method
SELECT java_method('java.util.UUID', 'randomUUID');
+---------------------------------------+
|java_method(java.util.UUID, randomUUID)|
+---------------------------------------+
|                   a86e23f5-636c-44e...|
+---------------------------------------+

SELECT java_method('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');
+-----------------------------------------------------------------------------+
|java_method(java.util.UUID, fromString, a5cf6c42-0c85-418f-af6c-3e4e5b1328f2)|
+-----------------------------------------------------------------------------+
|                                                         a5cf6c42-0c85-418...|
+-----------------------------------------------------------------------------+

-- monotonically_increasing_id
SELECT monotonically_increasing_id();
+-----------------------------+
|monotonically_increasing_id()|
+-----------------------------+
|                            0|
+-----------------------------+

-- reflect
SELECT reflect('java.util.UUID', 'randomUUID');
+-----------------------------------+
|reflect(java.util.UUID, randomUUID)|
+-----------------------------------+
|               287d8696-4979-4b1...|
+-----------------------------------+

SELECT reflect('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');
+-------------------------------------------------------------------------+
|reflect(java.util.UUID, fromString, a5cf6c42-0c85-418f-af6c-3e4e5b1328f2)|
+-------------------------------------------------------------------------+
|                                                     a5cf6c42-0c85-418...|
+-------------------------------------------------------------------------+

-- spark_partition_id
SELECT spark_partition_id();
+--------------------+
|SPARK_PARTITION_ID()|
+--------------------+
|                   0|
+--------------------+

-- try_aes_decrypt
SELECT try_aes_decrypt(unhex('6E7CA17BBB468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210'), '0000111122223333', 'GCM');
+------------------------------------------------------------------------------------------------------------------------------------+
|try_aes_decrypt(unhex(6E7CA17BBB468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210), 0000111122223333, GCM, DEFAULT, )|
+------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                [53 70 61 72 6B 2...|
+------------------------------------------------------------------------------------------------------------------------------------+

SELECT try_aes_decrypt(unhex('----------468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210'), '0000111122223333', 'GCM');
+------------------------------------------------------------------------------------------------------------------------------------+
|try_aes_decrypt(unhex(----------468D3084B5744BCA729FB7B2B7BCB8E4472847D02670489D95FA97DBBA7D3210), 0000111122223333, GCM, DEFAULT, )|
+------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                NULL|
+------------------------------------------------------------------------------------------------------------------------------------+

-- typeof
SELECT typeof(1);
+---------+
|typeof(1)|
+---------+
|      int|
+---------+

SELECT typeof(array(1));
+----------------+
|typeof(array(1))|
+----------------+
|      array<int>|
+----------------+

-- user
SELECT user();
+--------------+
|current_user()|
+--------------+
|      spark-rm|
+--------------+

-- uuid
SELECT uuid();
+--------------------+
|              uuid()|
+--------------------+
|6a017d79-b5bc-489...|
+--------------------+

-- version
SELECT version();
+--------------------+
|           version()|
+--------------------+
|3.5.5 7c29c664cdc...|
+--------------------+

Generator Functions



Function
Description




explode(expr)
Separates the elements of array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns. Unless specified otherwise, uses the default column name `col` for elements of the array or `key` and `value` for the elements of the map.


explode_outer(expr)
Separates the elements of array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns. Unless specified otherwise, uses the default column name `col` for elements of the array or `key` and `value` for the elements of the map.


inline(expr)
Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.


inline_outer(expr)
Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.


posexplode(expr)
Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Unless specified otherwise, uses the column name `pos` for position, `col` for elements of the array or `key` and `value` for elements of the map.


posexplode_outer(expr)
Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Unless specified otherwise, uses the column name `pos` for position, `col` for elements of the array or `key` and `value` for elements of the map.


stack(n, expr1, ..., exprk)
Separates `expr1`, ..., `exprk` into `n` rows. Uses column names col0, col1, etc. by default unless specified otherwise.



Examples
-- explode
SELECT explode(array(10, 20));
+---+
|col|
+---+
| 10|
| 20|
+---+

SELECT explode(collection => array(10, 20));
+---+
|col|
+---+
| 10|
| 20|
+---+

SELECT * FROM explode(collection => array(10, 20));
+---+
|col|
+---+
| 10|
| 20|
+---+

-- explode_outer
SELECT explode_outer(array(10, 20));
+---+
|col|
+---+
| 10|
| 20|
+---+

SELECT explode_outer(collection => array(10, 20));
+---+
|col|
+---+
| 10|
| 20|
+---+

SELECT * FROM explode_outer(collection => array(10, 20));
+---+
|col|
+---+
| 10|
| 20|
+---+

-- inline
SELECT inline(array(struct(1, 'a'), struct(2, 'b')));
+----+----+
|col1|col2|
+----+----+
|   1|   a|
|   2|   b|
+----+----+

-- inline_outer
SELECT inline_outer(array(struct(1, 'a'), struct(2, 'b')));
+----+----+
|col1|col2|
+----+----+
|   1|   a|
|   2|   b|
+----+----+

-- posexplode
SELECT posexplode(array(10,20));
+---+---+
|pos|col|
+---+---+
|  0| 10|
|  1| 20|
+---+---+

SELECT * FROM posexplode(array(10,20));
+---+---+
|pos|col|
+---+---+
|  0| 10|
|  1| 20|
+---+---+

-- posexplode_outer
SELECT posexplode_outer(array(10,20));
+---+---+
|pos|col|
+---+---+
|  0| 10|
|  1| 20|
+---+---+

SELECT * FROM posexplode_outer(array(10,20));
+---+---+
|pos|col|
+---+---+
|  0| 10|
|  1| 20|
+---+---+

-- stack
SELECT stack(2, 1, 2, 3);
+----+----+
|col0|col1|
+----+----+
|   1|   2|
|   3|NULL|
+----+----+





















  




User Defined Aggregate Functions (UDAFs) - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







User Defined Aggregate Functions (UDAFs)
Description
User-Defined Aggregate Functions (UDAFs) are user-programmable routines that act on multiple rows at once and return a single aggregated value as a result. This documentation lists the classes that are required for creating and registering UDAFs. It also contains examples that demonstrate how to define and register UDAFs in Scala and invoke them in Spark SQL.
Aggregator[-IN, BUF, OUT]
A base class for user-defined aggregations, which can be used in Dataset operations to take all of the elements of a group and reduce them to a single value.
IN - The input type for the aggregation.
BUF - The type of the intermediate value of the reduction.
OUT - The type of the final output result.


bufferEncoder: Encoder[BUF]
Specifies the Encoder for the intermediate value type.


finish(reduction: BUF): OUT
Transform the output of the reduction.


merge(b1: BUF, b2: BUF): BUF
Merge two intermediate values.


outputEncoder: Encoder[OUT]
Specifies the Encoder for the final output value type.


reduce(b: BUF, a: IN): BUF
Aggregate input value a into current intermediate value. For performance, the function may modify b and return it instead of constructing new object for b.


zero: BUF
The initial value of the intermediate result for this aggregation.


Examples
Type-Safe User-Defined Aggregate Functions
User-defined aggregations for strongly typed Datasets revolve around the Aggregator abstract class.
For example, a type-safe user-defined average can look like:


import org.apache.spark.sql.{Encoder, Encoders, SparkSession}
import org.apache.spark.sql.expressions.Aggregator

case class Employee(name: String, salary: Long)
case class Average(var sum: Long, var count: Long)

object MyAverage extends Aggregator[Employee, Average, Double] {
  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
  def zero: Average = Average(0L, 0L)
  // Combine two values to produce a new value. For performance, the function may modify `buffer`
  // and return it instead of constructing a new object
  def reduce(buffer: Average, employee: Employee): Average = {
    buffer.sum += employee.salary
    buffer.count += 1
    buffer
  }
  // Merge two intermediate values
  def merge(b1: Average, b2: Average): Average = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }
  // Transform the output of the reduction
  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
  // Specifies the Encoder for the intermediate value type
  def bufferEncoder: Encoder[Average] = Encoders.product
  // Specifies the Encoder for the final output value type
  def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}

val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]
ds.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

// Convert the function to a `TypedColumn` and give it a name
val averageSalary = MyAverage.toColumn.name("average_salary")
val result = ds.select(averageSalary)
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala" in the Spark repo.


import java.io.Serializable;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoder;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.TypedColumn;
import org.apache.spark.sql.expressions.Aggregator;

public static class Employee implements Serializable {
  private String name;
  private long salary;

  // Constructors, getters, setters...

}

public static class Average implements Serializable  {
  private long sum;
  private long count;

  // Constructors, getters, setters...

}

public static class MyAverage extends Aggregator<Employee, Average, Double> {
  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
  @Override
  public Average zero() {
    return new Average(0L, 0L);
  }
  // Combine two values to produce a new value. For performance, the function may modify `buffer`
  // and return it instead of constructing a new object
  @Override
  public Average reduce(Average buffer, Employee employee) {
    long newSum = buffer.getSum() + employee.getSalary();
    long newCount = buffer.getCount() + 1;
    buffer.setSum(newSum);
    buffer.setCount(newCount);
    return buffer;
  }
  // Merge two intermediate values
  @Override
  public Average merge(Average b1, Average b2) {
    long mergedSum = b1.getSum() + b2.getSum();
    long mergedCount = b1.getCount() + b2.getCount();
    b1.setSum(mergedSum);
    b1.setCount(mergedCount);
    return b1;
  }
  // Transform the output of the reduction
  @Override
  public Double finish(Average reduction) {
    return ((double) reduction.getSum()) / reduction.getCount();
  }
  // Specifies the Encoder for the intermediate value type
  @Override
  public Encoder<Average> bufferEncoder() {
    return Encoders.bean(Average.class);
  }
  // Specifies the Encoder for the final output value type
  @Override
  public Encoder<Double> outputEncoder() {
    return Encoders.DOUBLE();
  }
}

Encoder<Employee> employeeEncoder = Encoders.bean(Employee.class);
String path = "examples/src/main/resources/employees.json";
Dataset<Employee> ds = spark.read().json(path).as(employeeEncoder);
ds.show();
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

MyAverage myAverage = new MyAverage();
// Convert the function to a `TypedColumn` and give it a name
TypedColumn<Employee, Double> averageSalary = myAverage.toColumn().name("average_salary");
Dataset<Double> result = ds.select(averageSalary);
result.show();
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java" in the Spark repo.


Untyped User-Defined Aggregate Functions
Typed aggregations, as described above, may also be registered as untyped aggregating UDFs for use with DataFrames.
For example, a user-defined average for untyped DataFrames can look like:


import org.apache.spark.sql.{Encoder, Encoders, SparkSession}
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.functions

case class Average(var sum: Long, var count: Long)

object MyAverage extends Aggregator[Long, Average, Double] {
  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
  def zero: Average = Average(0L, 0L)
  // Combine two values to produce a new value. For performance, the function may modify `buffer`
  // and return it instead of constructing a new object
  def reduce(buffer: Average, data: Long): Average = {
    buffer.sum += data
    buffer.count += 1
    buffer
  }
  // Merge two intermediate values
  def merge(b1: Average, b2: Average): Average = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }
  // Transform the output of the reduction
  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
  // Specifies the Encoder for the intermediate value type
  def bufferEncoder: Encoder[Average] = Encoders.product
  // Specifies the Encoder for the final output value type
  def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}

// Register the function to access it
spark.udf.register("myAverage", functions.udaf(MyAverage))

val df = spark.read.json("examples/src/main/resources/employees.json")
df.createOrReplaceTempView("employees")
df.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala" in the Spark repo.


import java.io.Serializable;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoder;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.expressions.Aggregator;
import org.apache.spark.sql.functions;

public static class Average implements Serializable  {
  private long sum;
  private long count;

  // Constructors, getters, setters...
  public Average() {
  }

  public Average(long sum, long count) {
    this.sum = sum;
    this.count = count;
  }

  public long getSum() {
    return sum;
  }

  public void setSum(long sum) {
    this.sum = sum;
  }

  public long getCount() {
    return count;
  }

  public void setCount(long count) {
    this.count = count;
  }
}

public static class MyAverage extends Aggregator<Long, Average, Double> {
  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
  @Override
  public Average zero() {
    return new Average(0L, 0L);
  }
  // Combine two values to produce a new value. For performance, the function may modify `buffer`
  // and return it instead of constructing a new object
  @Override
  public Average reduce(Average buffer, Long data) {
    long newSum = buffer.getSum() + data;
    long newCount = buffer.getCount() + 1;
    buffer.setSum(newSum);
    buffer.setCount(newCount);
    return buffer;
  }
  // Merge two intermediate values
  @Override
  public Average merge(Average b1, Average b2) {
    long mergedSum = b1.getSum() + b2.getSum();
    long mergedCount = b1.getCount() + b2.getCount();
    b1.setSum(mergedSum);
    b1.setCount(mergedCount);
    return b1;
  }
  // Transform the output of the reduction
  @Override
  public Double finish(Average reduction) {
    return ((double) reduction.getSum()) / reduction.getCount();
  }
  // Specifies the Encoder for the intermediate value type
  @Override
  public Encoder<Average> bufferEncoder() {
    return Encoders.bean(Average.class);
  }
  // Specifies the Encoder for the final output value type
  @Override
  public Encoder<Double> outputEncoder() {
    return Encoders.DOUBLE();
  }
}

// Register the function to access it
spark.udf().register("myAverage", functions.udaf(new MyAverage(), Encoders.LONG()));

Dataset<Row> df = spark.read().json("examples/src/main/resources/employees.json");
df.createOrReplaceTempView("employees");
df.show();
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

Dataset<Row> result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees");
result.show();
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java" in the Spark repo.


-- Compile and place UDAF MyAverage in a JAR file called `MyAverage.jar` in /tmp.
CREATE FUNCTION myAverage AS 'MyAverage' USING JAR '/tmp/MyAverage.jar';

SHOW USER FUNCTIONS;
+------------------+
|          function|
+------------------+
| default.myAverage|
+------------------+

CREATE TEMPORARY VIEW employees
USING org.apache.spark.sql.json
OPTIONS (
    path "examples/src/main/resources/employees.json"
);

SELECT * FROM employees;
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+

SELECT myAverage(salary) as average_salary FROM employees;
+--------------+
|average_salary|
+--------------+
|        3750.0|
+--------------+
 


Related Statements

Scalar User Defined Functions (UDFs)
Integration with Hive UDFs/UDAFs/UDTFs





















  




Integration with Hive UDFs/UDAFs/UDTFs - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Integration with Hive UDFs/UDAFs/UDTFs
Description
Spark SQL supports integration of Hive UDFs, UDAFs and UDTFs. Similar to Spark UDFs and UDAFs, Hive UDFs work on a single row as input and generate a single row as output, while Hive UDAFs operate on multiple rows and return a single aggregated row as a result. In addition, Hive also supports UDTFs (User Defined Tabular Functions) that act on one row as input and return multiple rows as output. To use Hive UDFs/UDAFs/UTFs, the user should register them in Spark, and then use them in Spark SQL queries.
Examples
Hive has two UDF interfaces: UDF and GenericUDF.
An example below uses GenericUDFAbs derived from GenericUDF.
-- Register `GenericUDFAbs` and use it in Spark SQL.
-- Note that, if you use your own programmed one, you need to add a JAR containing it
-- into a classpath,
-- e.g., ADD JAR yourHiveUDF.jar;
CREATE TEMPORARY FUNCTION testUDF AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs';

SELECT * FROM t;
+-----+
|value|
+-----+
| -1.0|
|  2.0|
| -3.0|
+-----+

SELECT testUDF(value) FROM t;
+--------------+
|testUDF(value)|
+--------------+
|           1.0|
|           2.0|
|           3.0|
+--------------+

-- Register `UDFSubstr` and use it in Spark SQL.
-- Note that, it can achieve better performance if the return types and method parameters use Java primitives.
-- e.g., UDFSubstr. The data processing method is UTF8String <-> Text <-> String. we can avoid UTF8String <-> Text. 
CREATE TEMPORARY FUNCTION hive_substr AS 'org.apache.hadoop.hive.ql.udf.UDFSubstr';

select hive_substr('Spark SQL', 1, 5) as value;
+-----+
|value|
+-----+
|Spark|
+-----+

An example below uses GenericUDTFExplode derived from GenericUDTF.
-- Register `GenericUDTFExplode` and use it in Spark SQL
CREATE TEMPORARY FUNCTION hiveUDTF
    AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode';

SELECT * FROM t;
+------+
| value|
+------+
|[1, 2]|
|[3, 4]|
+------+

SELECT hiveUDTF(value) FROM t;
+---+
|col|
+---+
|  1|
|  2|
|  3|
|  4|
+---+

Hive has two UDAF interfaces: UDAF and GenericUDAFResolver.
An example below uses GenericUDAFSum derived from GenericUDAFResolver.
-- Register `GenericUDAFSum` and use it in Spark SQL
CREATE TEMPORARY FUNCTION hiveUDAF
    AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum';

SELECT * FROM t;
+---+-----+
|key|value|
+---+-----+
|  a|    1|
|  a|    2|
|  b|    3|
+---+-----+

SELECT key, hiveUDAF(value) FROM t GROUP BY key;
+---+---------------+
|key|hiveUDAF(value)|
+---+---------------+
|  b|              3|
|  a|              3|
+---+---------------+





















  




Scalar User Defined Functions (UDFs) - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Scalar User Defined Functions (UDFs)
Description
User-Defined Functions (UDFs) are user-programmable routines that act on one row. This documentation lists the classes that are required for creating and registering UDFs. It also contains examples that demonstrate how to define and register UDFs and invoke them in Spark SQL.
UserDefinedFunction
To define the properties of a user-defined function, the user can use some of the methods defined in this class.


asNonNullable(): UserDefinedFunction
Updates UserDefinedFunction to non-nullable.


asNondeterministic(): UserDefinedFunction
Updates UserDefinedFunction to nondeterministic.


withName(name: String): UserDefinedFunction
Updates UserDefinedFunction with a given name.


Examples


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.udf

val spark = SparkSession
  .builder()
  .appName("Spark SQL UDF scalar example")
  .getOrCreate()

// Define and register a zero-argument non-deterministic UDF
// UDF is deterministic by default, i.e. produces the same result for the same input.
val random = udf(() => Math.random())
spark.udf.register("random", random.asNondeterministic())
spark.sql("SELECT random()").show()
// +-------+
// |UDF()  |
// +-------+
// |xxxxxxx|
// +-------+

// Define and register a one-argument UDF
val plusOne = udf((x: Int) => x + 1)
spark.udf.register("plusOne", plusOne)
spark.sql("SELECT plusOne(5)").show()
// +------+
// |UDF(5)|
// +------+
// |     6|
// +------+

// Define a two-argument UDF and register it with Spark in one step
spark.udf.register("strLenScala", (_: String).length + (_: Int))
spark.sql("SELECT strLenScala('test', 1)").show()
// +--------------------+
// |strLenScala(test, 1)|
// +--------------------+
// |                   5|
// +--------------------+

// UDF in a WHERE clause
spark.udf.register("oneArgFilter", (n: Int) => { n > 5 })
spark.range(1, 10).createOrReplaceTempView("test")
spark.sql("SELECT * FROM test WHERE oneArgFilter(id)").show()
// +---+
// | id|
// +---+
// |  6|
// |  7|
// |  8|
// |  9|
// +---+
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala" in the Spark repo.


import org.apache.spark.sql.*;
import org.apache.spark.sql.api.java.UDF1;
import org.apache.spark.sql.expressions.UserDefinedFunction;
import static org.apache.spark.sql.functions.udf;
import org.apache.spark.sql.types.DataTypes;

SparkSession spark = SparkSession
  .builder()
  .appName("Java Spark SQL UDF scalar example")
  .getOrCreate();

// Define and register a zero-argument non-deterministic UDF
// UDF is deterministic by default, i.e. produces the same result for the same input.
UserDefinedFunction random = udf(
  () -> Math.random(), DataTypes.DoubleType
);
random.asNondeterministic();
spark.udf().register("random", random);
spark.sql("SELECT random()").show();
// +-------+
// |UDF()  |
// +-------+
// |xxxxxxx|
// +-------+

// Define and register a one-argument UDF
spark.udf().register("plusOne",
  (UDF1<Integer, Integer>) x -> x + 1, DataTypes.IntegerType);
spark.sql("SELECT plusOne(5)").show();
// +----------+
// |plusOne(5)|
// +----------+
// |         6|
// +----------+

// Define and register a two-argument UDF
UserDefinedFunction strLen = udf(
  (String s, Integer x) -> s.length() + x, DataTypes.IntegerType
);
spark.udf().register("strLen", strLen);
spark.sql("SELECT strLen('test', 1)").show();
// +------------+
// |UDF(test, 1)|
// +------------+
// |           5|
// +------------+

// UDF in a WHERE clause
spark.udf().register("oneArgFilter",
  (UDF1<Long, Boolean>) x -> x > 5, DataTypes.BooleanType);
spark.range(1, 10).createOrReplaceTempView("test");
spark.sql("SELECT * FROM test WHERE oneArgFilter(id)").show();
// +---+
// | id|
// +---+
// |  6|
// |  7|
// |  8|
// |  9|
// +---+
Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java" in the Spark repo.


Related Statements

User Defined Aggregate Functions (UDAFs)
Integration with Hive UDFs/UDAFs/UDTFs





















  




Functions - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Functions
Spark SQL provides two function features to meet a wide range of user needs: built-in functions and user-defined functions (UDFs).
Built-in functions are commonly used routines that Spark SQL predefines and a complete list of the functions can be found in the Built-in Functions API document. UDFs allow users to define their own functions when the system’s built-in functions are not enough to perform the desired task.
Built-in Functions
Spark SQL has some categories of frequently-used built-in functions for aggregation, arrays/maps, date/timestamp, and JSON data.
This subsection presents the usages and descriptions of these functions.
Scalar Functions

Array Functions
Map Functions
Date and Timestamp Functions
JSON Functions
Mathematical Functions
String Functions
Bitwise Functions
Conversion Functions
Conditional Functions
Predicate Functions
Csv Functions
Misc Functions

Aggregate-like Functions

Aggregate Functions
Window Functions

Generator Functions

Generator Functions

UDFs (User-Defined Functions)
User-Defined Functions (UDFs) are a feature of Spark SQL that allows users to define their own functions when the system’s built-in functions are not enough to perform the desired task. To use UDFs in Spark SQL, users must first define the function, then register the function with Spark, and finally call the registered function. The User-Defined Functions can act on a single row or act on multiple rows at once. Spark SQL also supports integration of existing Hive implementations of UDFs, UDAFs and UDTFs.

Scalar User-Defined Functions (UDFs)
User-Defined Aggregate Functions (UDAFs)
Integration with Hive UDFs/UDAFs/UDTFs





















  




Identifier clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







IDENTIFIER clause
Description
Converts a constant STRING expression into a SQL object name.
The purpose of this clause is to allow for templating of identifiers in SQL statements without opening up the risk of SQL injection attacks.
Typically, this clause is used with a parameter marker as argument.
Syntax
IDENTIFIER ( strExpr )

Parameters

strExpr: A constant STRING expression. Typically, the expression includes a parameter marker.

Returns
A (qualified) identifier which can be used as a:

qualified table name
namespace name
function name
qualified column or attribute reference

Examples
These examples use named parameter markers to templatize queries.
// Creation of a table using parameter marker.
spark.sql("CREATE TABLE IDENTIFIER(:mytab)(c1 INT)", args = Map("mytab" -> "tab1")).show()

spark.sql("DESCRIBE IDENTIFIER(:mytab)", args = Map("mytab" -> "tab1")).show()
+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|      c1|      int|   NULL|
+--------+---------+-------+

// Altering a table with a fixed schema and a parameterized table name. 
spark.sql("ALTER TABLE IDENTIFIER('default.' || :mytab) ADD COLUMN c2 INT", args = Map("mytab" -> "tab1")).show()

spark.sql("DESCRIBE IDENTIFIER(:mytab)", args = Map("mytab" -> "default.tab1")).show()
+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|      c1|      int|   NULL|
|      c2|      int|   NULL|
+--------+---------+-------+

// A parameterized reference to a table in a query. This table name is qualified and uses back-ticks.
spark.sql("SELECT * FROM IDENTIFIER(:mytab)", args = Map("mytab" -> "`default`.`tab1`")).show()
+---+---+
| c1| c2|
+---+---+
+---+---+


// You cannot qualify the IDENTIFIER clause or use it as a qualifier itself.
spark.sql("SELECT * FROM myschema.IDENTIFIER(:mytab)", args = Map("mytab" -> "`tab1`")).show()
[INVALID_SQL_SYNTAX.INVALID_TABLE_VALUED_FUNC_NAME] `myschema`.`IDENTIFIER`.

spark.sql("SELECT * FROM IDENTIFIER(:myschema).mytab", args = Map("mychema" -> "`default`")).show()
[PARSE_SYNTAX_ERROR]

// Dropping a table with separate schema and table parameters.
spark.sql("DROP TABLE IDENTIFIER(:myschema || '.' || :mytab)", args = Map("myschema" -> "default", "mytab" -> "tab1")).show()

// A parameterized column reference
spark.sql("SELECT IDENTIFIER(:col) FROM VALUES(1) AS T(c1)", args = Map("col" -> "t.c1")).show()
+---+
| c1|
+---+
|  1|
+---+

// Passing in a function name as a parameter
spark.sql("SELECT IDENTIFIER(:func)(-1)", args = Map("func" -> "abs")).show();
+-------+
|abs(-1)|
+-------+
|      1|
+-------+





















  




Identifiers - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Identifiers
Description
An identifier is a string used to identify a database object such as a table, view, schema, column, etc. Spark SQL has regular identifiers and delimited identifiers, which are enclosed within backticks. Both regular identifiers and delimited identifiers are case-insensitive.
Syntax
Regular Identifier
{ letter | digit | '_' } [ , ... ]

Note: If spark.sql.ansi.enabled is set to true, ANSI SQL reserved keywords cannot be used as identifiers. For more details, please refer to ANSI Compliance.
Delimited Identifier
`c [ ... ]`

Parameters


letter
Any letter from A-Z or a-z.


digit
Any numeral from 0 to 9.


c
Any character from the character set. Use ` to escape special characters (e.g., `).


Examples
-- This CREATE TABLE fails with ParseException because of the illegal identifier name a.b
CREATE TABLE test (a.b int);
Error in query:
[PARSE_SYNTAX_ERROR] Syntax error at or near '.': extra input '.'(line 1, pos 20)

== SQL ==
CREATE TABLE test (a.b int)
--------------------^^^

-- This CREATE TABLE works
CREATE TABLE test (`a.b` int);

-- This CREATE TABLE fails with ParseException because special character ` is not escaped
CREATE TABLE test1 (`a`b` int);
Error in query:
[PARSE_SYNTAX_ERROR] Syntax error at or near '`'(line 1, pos 24)

== SQL ==
CREATE TABLE test1 (`a`b` int)
------------------------^^^

-- This CREATE TABLE works
CREATE TABLE test (`a``b` int);





















  




Literals - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Literals
A literal (also known as a constant) represents a fixed data value. Spark SQL supports the following literals:

String Literal
Binary Literal
Null Literal
Boolean Literal
Numeric Literal
Datetime Literal
Interval Literal

String Literal
A string literal is used to specify a character string value.
Syntax
[ r ] { 'char [ ... ]' | "char [ ... ]" }

Parameters


char
One character from the character set. Use \ to escape special characters (e.g., ' or \).
  To represent unicode characters, use 16-bit or 32-bit unicode escape of the form \uxxxx or \Uxxxxxxxx,
  where xxxx and xxxxxxxx are 16-bit and 32-bit code points in hexadecimal respectively (e.g., \u3042 for あ and \U0001F44D for 👍).


r
Case insensitive, indicates RAW. If a string literal starts with r prefix, neither special characters nor unicode characters are escaped by \.


Examples
SELECT 'Hello, World!' AS col;
+-------------+
|          col|
+-------------+
|Hello, World!|
+-------------+

SELECT "SPARK SQL" AS col;
+---------+
|      col|
+---------+
|Spark SQL|
+---------+

SELECT 'it\'s $10.' AS col;
+---------+
|      col|
+---------+
|It's $10.|
+---------+

SELECT r"'\n' represents newline character." AS col;
+----------------------------------+
|                               col|
+----------------------------------+
|'\n' represents newline character.|
+----------------------------------+

Binary Literal
A binary literal is used to specify a byte sequence value.
Syntax
X { 'num [ ... ]' | "num [ ... ]" }

Parameters


num
Any hexadecimal number from 0 to F.


Examples
SELECT X'123456' AS col;
+----------+
|       col|
+----------+
|[12 34 56]|
+----------+

Null Literal
A null literal is used to specify a null value.
Syntax
NULL

Examples
SELECT NULL AS col;
+----+
| col|
+----+
|NULL|
+----+

Boolean Literal
A boolean literal is used to specify a boolean value.
Syntax
TRUE | FALSE

Examples
SELECT TRUE AS col;
+----+
| col|
+----+
|true|
+----+

Numeric Literal
A numeric literal is used to specify a fixed or floating-point number.
There are two kinds of numeric literals: integral literal and fractional literal.
Integral Literal Syntax
[ + | - ] digit [ ... ] [ L | S | Y ]

Integral Literal Parameters


digit
Any numeral from 0 to 9.


L
Case insensitive, indicates BIGINT, which is an 8-byte signed integer number.


S
Case insensitive, indicates SMALLINT, which is a 2-byte signed integer number.


Y
Case insensitive, indicates TINYINT, which is a 1-byte signed integer number.


default (no postfix)
Indicates a 4-byte signed integer number.


Integral Literal Examples
SELECT -2147483648 AS col;
+-----------+
|        col|
+-----------+
|-2147483648|
+-----------+

SELECT 9223372036854775807l AS col;
+-------------------+
|                col|
+-------------------+
|9223372036854775807|
+-------------------+

SELECT -32Y AS col;
+---+
|col|
+---+
|-32|
+---+

SELECT 482S AS col;
+---+
|col|
+---+
|482|
+---+

Fractional Literals Syntax
decimal literals:
decimal_digits { [ BD ] | [ exponent BD ] } | digit [ ... ] [ exponent ] BD

double literals:
decimal_digits  { D | exponent [ D ] }  | digit [ ... ] { exponent [ D ] | [ exponent ] D }

float literals:
decimal_digits  { F | exponent [ F ] }  | digit [ ... ] { exponent [ F ] | [ exponent ] F }

While decimal_digits is defined as
[ + | - ] { digit [ ... ] . [ digit [ ... ] ] | . digit [ ... ] }

and exponent is defined as
E [ + | - ] digit [ ... ]

Fractional Literals Parameters


digit
Any numeral from 0 to 9.


D
Case insensitive, indicates DOUBLE, which is an 8-byte double-precision floating point number.


F
Case insensitive, indicates FLOAT, which is a 4-byte single-precision floating point number.


BD
Case insensitive, indicates DECIMAL, with the total number of digits as precision and the number of digits to right of decimal point as scale.


Fractional Literals Examples
SELECT 12.578 AS col;
+------+
|   col|
+------+
|12.578|
+------+

SELECT -0.1234567 AS col;
+----------+
|       col|
+----------+
|-0.1234567|
+----------+

SELECT -.1234567 AS col;
+----------+
|       col|
+----------+
|-0.1234567|
+----------+

SELECT 123. AS col;
+---+
|col|
+---+
|123|
+---+

SELECT 123.BD AS col;
+---+
|col|
+---+
|123|
+---+

SELECT 5E2 AS col;
+-----+
|  col|
+-----+
|500.0|
+-----+

SELECT 5D AS col;
+---+
|col|
+---+
|5.0|
+---+

SELECT -5BD AS col;
+---+
|col|
+---+
| -5|
+---+

SELECT 12.578e-2d AS col;
+-------+
|    col|
+-------+
|0.12578|
+-------+

SELECT -.1234567E+2BD AS col;
+---------+
|      col|
+---------+
|-12.34567|
+---------+

SELECT +3.e+3 AS col;
+------+
|   col|
+------+
|3000.0|
+------+

SELECT -3.E-3D AS col;
+------+
|   col|
+------+
|-0.003|
+------+

Datetime Literal
A datetime literal is used to specify a date or timestamp value.
Date Syntax
DATE { 'yyyy' |
       'yyyy-[m]m' |
       'yyyy-[m]m-[d]d' |
       'yyyy-[m]m-[d]d[T]' }

Note: defaults to 01 if month or day is not specified.
Date Examples
SELECT DATE '1997' AS col;
+----------+
|       col|
+----------+
|1997-01-01|
+----------+

SELECT DATE '1997-01' AS col;
+----------+
|       col|
+----------+
|1997-01-01|
+----------+

SELECT DATE '2011-11-11' AS col;
+----------+
|       col|
+----------+
|2011-11-11|
+----------+

Timestamp Syntax
TIMESTAMP { 'yyyy' |
            'yyyy-[m]m' |
            'yyyy-[m]m-[d]d' |
            'yyyy-[m]m-[d]d ' |
            'yyyy-[m]m-[d]d[T][h]h[:]' |
            'yyyy-[m]m-[d]d[T][h]h:[m]m[:]' |
            'yyyy-[m]m-[d]d[T][h]h:[m]m:[s]s[.]' |
            'yyyy-[m]m-[d]d[T][h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]'}

Note: defaults to 00 if hour, minute or second is not specified.
zone_id should have one of the forms:

Z - Zulu time zone UTC+0
+|-[h]h:[m]m
An id with one of the prefixes UTC+, UTC-, GMT+, GMT-, UT+ or UT-, and a suffix in the formats:
    
+|-h[h]
+|-hh[:]mm
+|-hh:mm:ss
+|-hhmmss


Region-based zone IDs in the form area/city, such as Europe/Paris

Note: defaults to the session local timezone (set via spark.sql.session.timeZone) if zone_id is not specified.
Timestamp Examples
SELECT TIMESTAMP '1997-01-31 09:26:56.123' AS col;
+-----------------------+
|                    col|
+-----------------------+
|1997-01-31 09:26:56.123|
+-----------------------+

SELECT TIMESTAMP '1997-01-31 09:26:56.66666666UTC+08:00' AS col;
+--------------------------+
|                      col |
+--------------------------+
|1997-01-30 17:26:56.666666|
+--------------------------+

SELECT TIMESTAMP '1997-01' AS col;
+-------------------+
|                col|
+-------------------+
|1997-01-01 00:00:00|
+-------------------+

Interval Literal
An interval literal is used to specify a fixed period of time.
The interval literal supports two syntaxes: ANSI syntax and multi-units syntax.
ANSI Syntax
The ANSI SQL standard defines interval literals in the form:
INTERVAL [ <sign> ] <interval string> <interval qualifier>

where <interval qualifier> can be a single field or in the field-to-field form:
<interval qualifier> ::= <start field> TO <end field> | <single field>

The field name is case-insensitive, and can be one of YEAR, MONTH, DAY, HOUR, MINUTE and SECOND.
An interval literal can have either year-month or day-time interval type. The interval sub-type defines format of <interval string>:
<interval string> ::= <quote> [ <sign> ] { <year-month literal> | <day-time literal> } <quote>
<year-month literal> ::= <years value> [ <minus sign> <months value> ] | <months value>
<day-time literal> ::= <day-time interval> | <time interval>
<day-time interval> ::= <days value> [ <space> <hours value> [ <colon> <minutes value> [ <colon> <seconds value> ] ] ]
<time interval> ::= <hours value> [ <colon> <minutes value> [ <colon> <seconds value> ] ]
  | <minutes value> [ <colon> <seconds value> ]
  | <seconds value>

Supported year-month interval literals and theirs formats:



<interval qualifier>
Interval string pattern
An instance of the literal




YEAR
[+|-]'[+|-]y'
INTERVAL -'2021' YEAR


YEAR TO MONTH
[+|-]'[+|-]y-m'
INTERVAL '-2021-07' YEAR TO MONTH


MONTH
[+|-]'[+|-]m'
interval '10' month



Formats of supported day-time interval literals:



<interval qualifier>
Interval string pattern
An instance of the literal




DAY
[+|-]'[+|-]d'
INTERVAL -'100' DAY


DAY TO HOUR
[+|-]'[+|-]d h'
INTERVAL '-100 10' DAY TO HOUR


DAY TO MINUTE
[+|-]'[+|-]d h:m'
INTERVAL '100 10:30' DAY TO MINUTE


DAY TO SECOND
[+|-]'[+|-]d h:m:s.n'
INTERVAL '100 10:30:40.999999' DAY TO SECOND


HOUR
[+|-]'[+|-]h'
INTERVAL '123' HOUR


HOUR TO MINUTE
[+|-]'[+|-]h:m'
INTERVAL -'-123:10' HOUR TO MINUTE


HOUR TO SECOND
[+|-]'[+|-]h:m:s.n'
INTERVAL '123:10:59' HOUR TO SECOND


MINUTE
[+|-]'[+|-]m'
interval '1000' minute


MINUTE TO SECOND
[+|-]'[+|-]m:s.n'
INTERVAL '1000:01.001' MINUTE TO SECOND


SECOND
[+|-]'[+|-]s.n'
INTERVAL '1000.000001' SECOND



ANSI Examples
SELECT INTERVAL '2-3' YEAR TO MONTH AS col;
+----------------------------+
|col                         |
+----------------------------+
|INTERVAL '2-3' YEAR TO MONTH|
+----------------------------+

SELECT INTERVAL -'20 15:40:32.99899999' DAY TO SECOND AS col;
+--------------------------------------------+
|col                                         |
+--------------------------------------------+
|INTERVAL '-20 15:40:32.998999' DAY TO SECOND|
+--------------------------------------------+

Multi-units Syntax
INTERVAL interval_value interval_unit [ interval_value interval_unit ... ] |
INTERVAL 'interval_value interval_unit [ interval_value interval_unit ... ]' |

Multi-units Parameters


interval_value
Syntax:
[ + | - ] number_value | '[ + | - ] number_value'
 


interval_unit
Syntax:
YEAR[S] | MONTH[S] | WEEK[S] | DAY[S] | HOUR[S] | MINUTE[S] | SECOND[S] |
MILLISECOND[S] | MICROSECOND[S]

Mix of the YEAR[S] or MONTH[S] interval units with other units is not allowed.
 


Multi-units Examples
SELECT INTERVAL 3 YEAR AS col;
+-------+
|    col|
+-------+
|3 years|
+-------+

SELECT INTERVAL -2 HOUR '3' MINUTE AS col;
+--------------------+
|                 col|
+--------------------+
|-1 hours -57 minutes|
+--------------------+

SELECT INTERVAL '1 YEAR 2 DAYS 3 HOURS';
+----------------------+
|                   col|
+----------------------+
|1 years 2 days 3 hours|
+----------------------+

SELECT INTERVAL 1 YEARS 2 MONTH 3 WEEK 4 DAYS 5 HOUR 6 MINUTES 7 SECOND 8
    MILLISECOND 9 MICROSECONDS AS col;
+-----------------------------------------------------------+
|                                                        col|
+-----------------------------------------------------------+
|1 years 2 months 25 days 5 hours 6 minutes 7.008009 seconds|
+-----------------------------------------------------------+





















  




NULL Semantics - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







NULL Semantics
Description
A table consists of a set of rows and each row contains a set of columns.
A column is associated with a data type and represents
a specific attribute of an entity (for example, age is a column of an
entity called person). Sometimes, the value of a column
specific to a row is not known at the time the row comes into existence.
In SQL, such values are represented as NULL. This section details the
semantics of NULL values handling in various operators, expressions and
other SQL constructs.

Null handling in comparison operators
Null handling in Logical operators
Null handling in Expressions

Null handling in null-intolerant expressions
Null handling Expressions that can process null value operands
Null handling in built-in aggregate expressions


Null handling in WHERE, HAVING and JOIN conditions
Null handling in GROUP BY and DISTINCT
Null handling in ORDER BY
Null handling in UNION, INTERSECT, EXCEPT
Null handling in EXISTS and NOT EXISTS subquery
Null handling in IN and NOT IN subquery


The following illustrates the schema layout and data of a table named person. The data contains NULL values in
the age column and this table will be used in various examples in the sections below.
TABLE: person



Id
Name
Age




100
Joe
30


200
Marry
NULL


300
Mike
18


400
Fred
50


500
Albert
NULL


600
Michelle
30


700
Dan
50



Comparison Operators 
Apache spark supports the standard comparison operators such as ‘>’, ‘>=’, ‘=’, ‘<’ and ‘<=’.
The result of these operators is unknown or NULL when one of the operands or both the operands are
unknown or NULL. In order to compare the NULL values for equality, Spark provides a null-safe
equal operator (‘<=>’), which returns False when one of the operand is NULL and returns ‘True when
both the operands are NULL. The following table illustrates the behaviour of comparison operators when
one or both operands are NULL`:



Left Operand
Right  Operand
>
>=
=
<
<=
<=>




NULL
Any value
NULL
NULL
NULL
NULL
NULL
False


Any value
NULL
NULL
NULL
NULL
NULL
NULL
False


NULL
NULL
NULL
NULL
NULL
NULL
NULL
True



Examples
-- Normal comparison operators return `NULL` when one of the operand is `NULL`.
SELECT 5 > null AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

-- Normal comparison operators return `NULL` when both the operands are `NULL`.
SELECT null = null AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

-- Null-safe equal operator return `False` when one of the operand is `NULL`
SELECT 5 <=> null AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|            false|
+-----------------+

-- Null-safe equal operator return `True` when one of the operand is `NULL`
SELECT NULL <=> NULL;
+-----------------+
|expression_output|
+-----------------+
|             true|
+-----------------+

Logical Operators 
Spark supports standard logical operators such as AND, OR and NOT. These operators take Boolean expressions
as the arguments and return a Boolean value.
The following tables illustrate the behavior of logical operators when one or both operands are NULL.



Left Operand
Right Operand
OR
AND




True
NULL
True
NULL


False
NULL
NULL
False


NULL
True
True
NULL


NULL
False
NULL
False


NULL
NULL
NULL
NULL






operand
NOT




NULL
NULL



Examples
-- Normal comparison operators return `NULL` when one of the operands is `NULL`.
SELECT (true OR null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             true|
+-----------------+

-- Normal comparison operators return `NULL` when both the operands are `NULL`.
SELECT (null OR false) AS expression_output
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

-- Null-safe equal operator returns `False` when one of the operands is `NULL`
SELECT NOT(null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

Expressions 
The comparison operators and logical operators are treated as expressions in
Spark. Other than these two kinds of expressions, Spark supports other form of
expressions such as function expressions, cast expressions, etc. The expressions
in Spark can be broadly classified as :

Null intolerant expressions
Expressions that can process NULL value operands
    
The result of these expressions depends on the expression itself.



Null Intolerant Expressions 
Null intolerant expressions return NULL when one or more arguments of 
expression are NULL and most of the expressions fall in this category.
Examples
SELECT concat('John', null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

SELECT positive(null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

SELECT to_date(null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

Expressions That Can Process Null Value Operands 
This class of expressions are designed to handle NULL values. The result of the 
expressions depends on the expression itself. As an example, function expression isnull
returns a true on null input and false on non null input where as function coalesce
returns the first non NULL value in its list of operands. However, coalesce returns
NULL when all its operands are NULL. Below is an incomplete list of expressions of this category.

COALESCE
NULLIF
IFNULL
NVL
NVL2
ISNAN
NANVL
ISNULL
ISNOTNULL
ATLEASTNNONNULLS
IN

Examples
SELECT isnull(null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             true|
+-----------------+

-- Returns the first occurrence of non `NULL` value.
SELECT coalesce(null, null, 3, null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|                3|
+-----------------+

-- Returns `NULL` as all its operands are `NULL`. 
SELECT coalesce(null, null, null, null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|             null|
+-----------------+

SELECT isnan(null) AS expression_output;
+-----------------+
|expression_output|
+-----------------+
|            false|
+-----------------+

Builtin Aggregate Expressions 
Aggregate functions compute a single result by processing a set of input rows. Below are
the rules of how NULL values are handled by aggregate functions.

NULL values are ignored from processing by all the aggregate functions.
    
Only exception to this rule is COUNT(*) function.


Some aggregate functions return NULL when all input values are NULL or the input data set
is empty. The list of these functions is:
    
MAX
MIN
SUM
AVG
EVERY
ANY
SOME



Examples
-- `count(*)` does not skip `NULL` values.
SELECT count(*) FROM person;
+--------+
|count(1)|
+--------+
|       7|
+--------+

-- `NULL` values in column `age` are skipped from processing.
SELECT count(age) FROM person;
+----------+
|count(age)|
+----------+
|         5|
+----------+

-- `count(*)` on an empty input set returns 0. This is unlike the other
-- aggregate functions, such as `max`, which return `NULL`.
SELECT count(*) FROM person where 1 = 0;
+--------+
|count(1)|
+--------+
|       0|
+--------+

-- `NULL` values are excluded from computation of maximum value.
SELECT max(age) FROM person;
+--------+
|max(age)|
+--------+
|      50|
+--------+

-- `max` returns `NULL` on an empty input set.
SELECT max(age) FROM person where 1 = 0;
+--------+
|max(age)|
+--------+
|    null|
+--------+

Condition Expressions in WHERE, HAVING and JOIN Clauses 
WHERE, HAVING operators filter rows based on the user specified condition.
A JOIN operator is used to combine rows from two tables based on a join condition.
For all the three operators, a condition expression is a boolean expression and can return
 True, False or Unknown (NULL). They are “satisfied” if the result of the condition is True.
Examples
-- Persons whose age is unknown (`NULL`) are filtered out from the result set.
SELECT * FROM person WHERE age > 0;
+--------+---+
|    name|age|
+--------+---+
|Michelle| 30|
|    Fred| 50|
|    Mike| 18|
|     Dan| 50|
|     Joe| 30|
+--------+---+

-- `IS NULL` expression is used in disjunction to select the persons
-- with unknown (`NULL`) records.
SELECT * FROM person WHERE age > 0 OR age IS NULL;
+--------+----+
|    name| age|
+--------+----+
|  Albert|null|
|Michelle|  30|
|    Fred|  50|
|    Mike|  18|
|     Dan|  50|
|   Marry|null|
|     Joe|  30|
+--------+----+

-- Person with unknown(`NULL`) ages are skipped from processing.
SELECT age, count(*) FROM person GROUP BY age HAVING max(age) > 18;
+---+--------+
|age|count(1)|
+---+--------+
| 50|       2|
| 30|       2|
+---+--------+

-- A self join case with a join condition `p1.age = p2.age AND p1.name = p2.name`.
-- The persons with unknown age (`NULL`) are filtered out by the join operator.
SELECT * FROM person p1, person p2
    WHERE p1.age = p2.age
    AND p1.name = p2.name;
+--------+---+--------+---+
|    name|age|    name|age|
+--------+---+--------+---+
|Michelle| 30|Michelle| 30|
|    Fred| 50|    Fred| 50|
|    Mike| 18|    Mike| 18|
|     Dan| 50|     Dan| 50|
|     Joe| 30|     Joe| 30|
+--------+---+--------+---+

-- The age column from both legs of join are compared using null-safe equal which
-- is why the persons with unknown age (`NULL`) are qualified by the join.
SELECT * FROM person p1, person p2
    WHERE p1.age <=> p2.age
    AND p1.name = p2.name;
+--------+----+--------+----+
|    name| age|    name| age|
+--------+----+--------+----+
|  Albert|null|  Albert|null|
|Michelle|  30|Michelle|  30|
|    Fred|  50|    Fred|  50|
|    Mike|  18|    Mike|  18|
|     Dan|  50|     Dan|  50|
|   Marry|null|   Marry|null|
|     Joe|  30|     Joe|  30|
+--------+----+--------+----+

Aggregate Operator (GROUP BY, DISTINCT) 
As discussed in the previous section comparison operator,
two NULL values are not equal. However, for the purpose of grouping and distinct processing, the two or more
values with NULL dataare grouped together into the same bucket. This behaviour is conformant with SQL
standard and with other enterprise database management systems.
Examples
-- `NULL` values are put in one bucket in `GROUP BY` processing.
SELECT age, count(*) FROM person GROUP BY age;
+----+--------+
| age|count(1)|
+----+--------+
|null|       2|
|  50|       2|
|  30|       2|
|  18|       1|
+----+--------+

-- All `NULL` ages are considered one distinct value in `DISTINCT` processing.
SELECT DISTINCT age FROM person;
+----+
| age|
+----+
|null|
|  50|
|  30|
|  18|
+----+

Sort Operator (ORDER BY Clause) 
Spark SQL supports null ordering specification in ORDER BY clause. Spark processes the ORDER BY clause by
placing all the NULL values at first or at last depending on the null ordering specification. By default, all
the NULL values are placed at first.
Examples
-- `NULL` values are shown at first and other values
-- are sorted in ascending way.
SELECT age, name FROM person ORDER BY age;
+----+--------+
| age|    name|
+----+--------+
|null|   Marry|
|null|  Albert|
|  18|    Mike|
|  30|Michelle|
|  30|     Joe|
|  50|    Fred|
|  50|     Dan|
+----+--------+

-- Column values other than `NULL` are sorted in ascending
-- way and `NULL` values are shown at the last.
SELECT age, name FROM person ORDER BY age NULLS LAST;
+----+--------+
| age|    name|
+----+--------+
|  18|    Mike|
|  30|Michelle|
|  30|     Joe|
|  50|     Dan|
|  50|    Fred|
|null|   Marry|
|null|  Albert|
+----+--------+

-- Columns other than `NULL` values are sorted in descending
-- and `NULL` values are shown at the last.
SELECT age, name FROM person ORDER BY age DESC NULLS LAST;
+----+--------+
| age|    name|
+----+--------+
|  50|    Fred|
|  50|     Dan|
|  30|Michelle|
|  30|     Joe|
|  18|    Mike|
|null|   Marry|
|null|  Albert|
+----+--------+

Set Operators (UNION, INTERSECT, EXCEPT) 
NULL values are compared in a null-safe manner for equality in the context of
set operations. That means when comparing rows, two NULL values are considered 
equal unlike the regular EqualTo(=) operator.
Examples
CREATE VIEW unknown_age SELECT * FROM person WHERE age IS NULL;

-- Only common rows between two legs of `INTERSECT` are in the 
-- result set. The comparison between columns of the row are done
-- in a null-safe manner.
SELECT name, age FROM person
    INTERSECT
    SELECT name, age from unknown_age;
+------+----+
|  name| age|
+------+----+
|Albert|null|
| Marry|null|
+------+----+

-- `NULL` values from two legs of the `EXCEPT` are not in output. 
-- This basically shows that the comparison happens in a null-safe manner.
SELECT age, name FROM person
    EXCEPT
    SELECT age FROM unknown_age;
+---+--------+
|age|    name|
+---+--------+
| 30|     Joe|
| 50|    Fred|
| 30|Michelle|
| 18|    Mike|
| 50|     Dan|
+---+--------+

-- Performs `UNION` operation between two sets of data. 
-- The comparison between columns of the row ae done in
-- null-safe manner.
SELECT name, age FROM person
    UNION 
    SELECT name, age FROM unknown_age;
+--------+----+
|    name| age|
+--------+----+
|  Albert|null|
|     Joe|  30|
|Michelle|  30|
|   Marry|null|
|    Fred|  50|
|    Mike|  18|
|     Dan|  50|
+--------+----+

EXISTS/NOT EXISTS Subquery 
In Spark, EXISTS and NOT EXISTS expressions are allowed inside a WHERE clause. 
These are boolean expressions which return either TRUE or
FALSE. In other words, EXISTS is a membership condition and returns TRUE
when the subquery it refers to returns one or more rows. Similarly, NOT EXISTS
is a non-membership condition and returns TRUE when no rows or zero rows are
returned from the subquery.
These two expressions are not affected by presence of NULL in the result of
the subquery. They are normally faster because they can be converted to
semijoins / anti-semijoins without special provisions for null awareness.
Examples
-- Even if subquery produces rows with `NULL` values, the `EXISTS` expression
-- evaluates to `TRUE` as the subquery produces 1 row.
SELECT * FROM person WHERE EXISTS (SELECT null);
+--------+----+
|    name| age|
+--------+----+
|  Albert|null|
|Michelle|  30|
|    Fred|  50|
|    Mike|  18|
|     Dan|  50|
|   Marry|null|
|     Joe|  30|
+--------+----+

-- `NOT EXISTS` expression returns `FALSE`. It returns `TRUE` only when
-- subquery produces no rows. In this case, it returns 1 row.
SELECT * FROM person WHERE NOT EXISTS (SELECT null);
+----+---+
|name|age|
+----+---+
+----+---+

-- `NOT EXISTS` expression returns `TRUE`.
SELECT * FROM person WHERE NOT EXISTS (SELECT 1 WHERE 1 = 0);
+--------+----+
|    name| age|
+--------+----+
|  Albert|null|
|Michelle|  30|
|    Fred|  50|
|    Mike|  18|
|     Dan|  50|
|   Marry|null|
|     Joe|  30|
+--------+----+

IN/NOT IN Subquery 
In Spark, IN and NOT IN expressions are allowed inside a WHERE clause of
a query. Unlike the EXISTS expression, IN expression can return a TRUE,
FALSE or UNKNOWN (NULL) value. Conceptually a IN expression is semantically
equivalent to a set of equality condition separated by a disjunctive operator (OR).
For example, c1 IN (1, 2, 3) is semantically equivalent to (C1 = 1 OR c1 = 2 OR c1 = 3).
As far as handling NULL values are concerned, the semantics can be deduced from
the NULL value handling in comparison operators(=) and logical operators(OR).
To summarize, below are the rules for computing the result of an IN expression.

TRUE is returned when the non-NULL value in question is found in the list
FALSE is returned when the non-NULL value is not found in the list and the
list does not contain NULL values
UNKNOWN is returned when the value is NULL, or the non-NULL value is not found in the list
and the list contains at least one NULL value

NOT IN always returns UNKNOWN when the list contains NULL, regardless of the input value.
This is because IN returns UNKNOWN if the value is not in the list containing NULL,
and because NOT UNKNOWN is again UNKNOWN.
Examples
-- The subquery has only `NULL` value in its result set. Therefore,
-- the result of `IN` predicate is UNKNOWN.
SELECT * FROM person WHERE age IN (SELECT null);
+----+---+
|name|age|
+----+---+
+----+---+

-- The subquery has `NULL` value in the result set as well as a valid 
-- value `50`. Rows with age = 50 are returned. 
SELECT * FROM person
    WHERE age IN (SELECT age FROM VALUES (50), (null) sub(age));
+----+---+
|name|age|
+----+---+
|Fred| 50|
| Dan| 50|
+----+---+

-- Since subquery has `NULL` value in the result set, the `NOT IN`
-- predicate would return UNKNOWN. Hence, no rows are
-- qualified for this query.
SELECT * FROM person
    WHERE age NOT IN (SELECT age FROM VALUES (50), (null) sub(age));
+----+---+
|name|age|
+----+---+
+----+---+





















  




Number patterns - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







Number Patterns for Formatting and Parsing
Description
Functions such as to_number and to_char support converting between values of string and
Decimal type. Such functions accept format strings indicating how to map between these types.
Syntax
Number format strings support the following syntax:
  { ' [ MI | S ] [ $ ] 
      [ 0 | 9 | G | , ] [...] 
      [ . | D ] 
      [ 0 | 9 ] [...] 
      [ $ ] [ PR | MI | S ] ' }

Elements
Each number format string can contain the following elements (case insensitive):


0 or 9
Specifies an expected digit between 0 and 9.
A sequence of 0 or 9 in the format string matches a sequence of digits with the same or smaller
size. If the 0/9 sequence starts with 0 and is before the decimal point, it requires matching the
number of digits exactly: when parsing, it matches only a digit sequence of the same size; when
formatting, the result string adds left-padding with zeros to the digit sequence to reach the
same size. Otherwise, the 0/9 sequence matches any digit sequence with the same or smaller size
when parsing, and pads the digit sequence with spaces (if before the decimal point) or zeros (if
after the decimal point) in the result string when formatting. Note that the digit sequence will
become a ‘#’ sequence when formatting if the size is larger than the 0/9 sequence.


. or D
Specifies the position of the decimal point. This character may only be specified once.
When parsing, the input string does not need to include a decimal point.


, or G
Specifies the position of the , grouping (thousands) separator.
There must be a 0 or 9 to the left and right of each grouping separator. When parsing,
the input string must match the grouping separator relevant for the size of the number.


$
Specifies the location of the $ currency sign. This character may only be specified once.


S
Specifies the position of an optional ‘+’ or ‘-‘ sign. This character may only be specified once.


MI
Specifies the position of an optional ‘-‘ sign (no ‘+’). This character may only be specified once.
When formatting, it prints a space for positive values.


PR
Maps negative input values to wrapping angle brackets (<1>) in the corresponding string.
Positive input values do not receive wrapping angle brackets.


Function types and error handling

The to_number function accepts an input string and a format string argument. It requires that
the input string matches the provided format and raises an error otherwise. The function then
returns the corresponding Decimal value.
The try_to_number function accepts an input string and a format string argument. It works the
same as the to_number function except that it returns NULL instead of raising an error if the
input string does not match the given number format.
The to_char function accepts an input decimal and a format string argument. The function then
returns the corresponding string value.
All functions will fail if the given format string is invalid.

Examples
The following examples use the to_number, try_to_number, and to_char SQL
functions.
Note that the format string used in most of these examples expects:

an optional sign at the beginning,
followed by a dollar sign,
followed by a number between 3 and 6 digits long,
thousands separators,
up to two digits beyond the decimal point.

The to_number function
-- The negative number with currency symbol maps to characters in the format string.
> SELECT to_number('-$12,345.67', 'S$999,099.99');
  -12345.67
 
-- The '$' sign is not optional.
> SELECT to_number('5', '$9');
  Error: the input string does not match the given number format
 
-- The plus sign is optional, and so are fractional digits.
> SELECT to_number('$345', 'S$999,099.99');
  345.00
 
-- The format requires at least three digits.
> SELECT to_number('$45', 'S$999,099.99');
  Error: the input string does not match the given number format
 
-- The format requires at least three digits.
> SELECT to_number('$045', 'S$999,099.99');
  45.00
 
-- MI indicates an optional minus sign at the beginning or end of the input string.
> SELECT to_number('1234-', '999999MI');
  -1234
 
-- PR indicates optional wrapping angel brakets.
> SELECT to_number('9', '999PR')
  9

The try_to_number function:
-- The '$' sign is not optional.
> SELECT try_to_number('5', '$9');
  NULL
 
-- The format requires at least three digits.
> SELECT try_to_number('$45', 'S$999,099.99');
  NULL

The to_char function:
> SELECT to_char(decimal(454), '999');
  "454"

-- '99' can format digit sequence with a smaller size.
> SELECT to_char(decimal(1), '99.9');
  " 1.0"

-- '000' left-pads 0 for digit sequence with a smaller size.
> SELECT to_char(decimal(45.1), '000.00');
  "045.10"

> SELECT to_char(decimal(12454), '99,999');
  "12,454"

-- digit sequence with a larger size leads to '#' sequence.
> SELECT to_char(decimal(78.12), '$9.99');
  "$#.##"

-- 'S' can be at the end.
> SELECT to_char(decimal(-12454.8), '99,999.9S');
  "12,454.8-"

> SELECT to_char(decimal(12454.8), 'L99,999.9');
  Error: cannot resolve 'to_char(Decimal(12454.8), 'L99,999.9')' due to data type mismatch:
  Unexpected character 'L' found in the format string 'L99,999.9'; the structure of the format
  string must match: [MI|S] [$] [0|9|G|,]* [.|D] [0|9]* [$] [PR|MI|S]; line 1 pos 25





















  




ANALYZE TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







ANALYZE TABLE
Description
The ANALYZE TABLE statement collects statistics about one specific table or all the tables in one specified database,
that are to be used by the query optimizer to find a better query execution plan.
Syntax
ANALYZE TABLE table_identifier [ partition_spec ]
    COMPUTE STATISTICS [ NOSCAN | FOR COLUMNS col [ , ... ] | FOR ALL COLUMNS ]

ANALYZE TABLES [ { FROM | IN } database_name ] COMPUTE STATISTICS [ NOSCAN ]

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
An optional parameter that specifies a comma separated list of key and value pairs
  for partitions. When specified, partition statistics is returned.
Syntax: PARTITION ( partition_col_name [ = partition_col_val ] [ , ... ] )


{ FROM | IN } database_name
Specifies the name of the database to be analyzed. Without a database name, ANALYZE collects all tables in the current database that the current user has permission to analyze.


NOSCAN
Collects only the table’s size in bytes (which does not require scanning the entire table).


FOR COLUMNS col [ , … ] | FOR ALL COLUMNS
Collects column statistics for each column specified, or alternatively for every column, as well as table statistics.


If no analyze option is specified, both number of rows and size in bytes are collected.
Examples
CREATE DATABASE school_db;
USE school_db;

CREATE TABLE teachers (name STRING, teacher_id INT);
INSERT INTO teachers VALUES ('Tom', 1), ('Jerry', 2);

CREATE TABLE students (name STRING, student_id INT) PARTITIONED BY (student_id);
INSERT INTO students VALUES ('Mark', 111111), ('John', 222222);

ANALYZE TABLE students COMPUTE STATISTICS NOSCAN;

DESC EXTENDED students;
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                name|              string|   null|
|          student_id|                 int|   null|
|                 ...|                 ...|    ...|
|          Statistics|           864 bytes|       |
|                 ...|                 ...|    ...|
+--------------------+--------------------+-------+

ANALYZE TABLE students COMPUTE STATISTICS;

DESC EXTENDED students;
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                name|              string|   null|
|          student_id|                 int|   null|
|                 ...|                 ...|    ...|
|          Statistics|   864 bytes, 2 rows|       |
|                 ...|                 ...|    ...|
+--------------------+--------------------+-------+

ANALYZE TABLE students PARTITION (student_id = 111111) COMPUTE STATISTICS;

DESC EXTENDED students PARTITION (student_id = 111111);
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                name|              string|   null|
|          student_id|                 int|   null|
|                 ...|                 ...|    ...|
|Partition Statistics|   432 bytes, 1 rows|       |
|                 ...|                 ...|    ...|
+--------------------+--------------------+-------+

ANALYZE TABLE students COMPUTE STATISTICS FOR COLUMNS name;

DESC EXTENDED students name;
+--------------+----------+
|     info_name|info_value|
+--------------+----------+
|      col_name|      name|
|     data_type|    string|
|       comment|      NULL|
|           min|      NULL|
|           max|      NULL|
|     num_nulls|         0|
|distinct_count|         2|
|   avg_col_len|         4|
|   max_col_len|         4|
|     histogram|      NULL|
+--------------+----------+

ANALYZE TABLES IN school_db COMPUTE STATISTICS NOSCAN;

DESC EXTENDED teachers;
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                name|              string|   null|
|          teacher_id|                 int|   null|
|                 ...|                 ...|    ...|
|          Statistics|          1382 bytes|       |
|                 ...|                 ...|    ...|
+--------------------+--------------------+-------+

DESC EXTENDED students;
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                name|              string|   null|
|          student_id|                 int|   null|
|                 ...|                 ...|    ...|
|          Statistics|           864 bytes|       |
|                 ...|                 ...|    ...|
+--------------------+--------------------+-------+

ANALYZE TABLES COMPUTE STATISTICS;

DESC EXTENDED teachers;
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                name|              string|   null|
|          teacher_id|                 int|   null|
|                 ...|                 ...|    ...|
|          Statistics|  1382 bytes, 2 rows|       |
|                 ...|                 ...|    ...|
+--------------------+--------------------+-------+

DESC EXTENDED students;
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                name|              string|   null|
|          student_id|                 int|   null|
|                 ...|                 ...|    ...|
|          Statistics|   864 bytes, 2 rows|       |
|                 ...|                 ...|    ...|
+--------------------+--------------------+-------+





















  




CACHE TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CACHE TABLE
Description
CACHE TABLE statement caches contents of a table or output of a query with the given storage level. If a query is cached, then a temp view will be created for this query.
This reduces scanning of the original files in future queries.
Syntax
CACHE [ LAZY ] TABLE table_identifier
    [ OPTIONS ( 'storageLevel' [ = ] value ) ] [ [ AS ] query ]

Parameters


LAZY
Only cache the table when it is first used, instead of immediately.


table_identifier
Specifies the table or view name to be cached. The table or view name may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


OPTIONS ( ‘storageLevel’ [ = ] value )
OPTIONS clause with storageLevel key and value pair. A Warning is issued when a key other than storageLevel is used. The valid options for storageLevel are:

NONE
DISK_ONLY
DISK_ONLY_2
DISK_ONLY_3
MEMORY_ONLY
MEMORY_ONLY_2
MEMORY_ONLY_SER
MEMORY_ONLY_SER_2
MEMORY_AND_DISK
MEMORY_AND_DISK_2
MEMORY_AND_DISK_SER
MEMORY_AND_DISK_SER_2
OFF_HEAP

An Exception is thrown when an invalid value is set for storageLevel. If storageLevel is not explicitly set using OPTIONS clause, the default storageLevel is set to MEMORY_AND_DISK.


query
A query that produces the rows to be cached. It can be in one of following formats:

a SELECT statement
a TABLE statement
a FROM statement



Examples
CACHE TABLE testCache OPTIONS ('storageLevel' 'DISK_ONLY') SELECT * FROM testData;

Related Statements

CLEAR CACHE
UNCACHE TABLE
REFRESH TABLE
REFRESH
REFRESH FUNCTION





















  




CLEAR CACHE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CLEAR CACHE
Description
CLEAR CACHE removes the entries and associated data from the in-memory and/or on-disk cache for all cached tables and views.
Syntax
CLEAR CACHE

Examples
CLEAR CACHE;

Related Statements

CACHE TABLE
UNCACHE TABLE
REFRESH TABLE
REFRESH
REFRESH FUNCTION





















  




REFRESH FUNCTION - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







REFRESH FUNCTION
Description
REFRESH FUNCTION statement invalidates the cached function entry, which includes a class name
and resource location of the given function. The invalidated cache is populated right away.
Note that REFRESH FUNCTION only works for permanent functions. Refreshing native functions or temporary functions will cause an exception.
Syntax
REFRESH FUNCTION function_identifier

Parameters


function_identifier
Specifies a function name, which is either a qualified or unqualified name. If no database identifier is provided, uses the current database.
Syntax: [ database_name. ] function_name


Examples
-- The cached entry of the function will be refreshed
-- The function is resolved from the current database as the function name is unqualified.
REFRESH FUNCTION func1;

-- The cached entry of the function will be refreshed
-- The function is resolved from tempDB database as the function name is qualified.
REFRESH FUNCTION db1.func1;   

Related Statements

CACHE TABLE
CLEAR CACHE
UNCACHE TABLE
REFRESH TABLE
REFRESH





















  




REFRESH TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







REFRESH TABLE
Description
REFRESH TABLE statement invalidates the cached entries, which include data
and metadata of the given table or view. The invalidated cache is populated in
lazy manner when the cached table or the query associated with it is executed again.
Syntax
REFRESH [TABLE] table_identifier

Parameters


table_identifier
Specifies a table name, which is either a qualified or unqualified name that designates a table/view. If no database identifier is provided, it refers to a temporary view or a table/view in the current database.
Syntax: [ database_name. ] table_name


Examples
-- The cached entries of the table will be refreshed  
-- The table is resolved from the current database as the table name is unqualified.
REFRESH TABLE tbl1;

-- The cached entries of the view will be refreshed or invalidated
-- The view is resolved from tempDB database, as the view name is qualified.
REFRESH TABLE tempDB.view1;   

Related Statements

CACHE TABLE
CLEAR CACHE
UNCACHE TABLE
REFRESH
REFRESH FUNCTION





















  




REFRESH - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







REFRESH
Description
REFRESH is used to invalidate and refresh all the cached data (and the associated metadata) for
all Datasets that contains the given data source path. Path matching is by prefix, i.e. “/” would
invalidate everything that is cached.
Syntax
REFRESH resource_path

Parameters


resource_path
The path of the resource that is to be refreshed.


Examples
-- The Path is resolved using the datasource's File Index.
CREATE TABLE test(ID INT) using parquet;
INSERT INTO test SELECT 1000;
CACHE TABLE test;
INSERT INTO test SELECT 100;
REFRESH "hdfs://path/to/table";

Related Statements

CACHE TABLE
CLEAR CACHE
UNCACHE TABLE
REFRESH TABLE
REFRESH FUNCTION





















  




UNCACHE TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







UNCACHE TABLE
Description
UNCACHE TABLE removes the entries and associated data from the in-memory and/or on-disk cache for a given table or view. The
underlying entries should already have been brought to cache by previous CACHE TABLE operation. UNCACHE TABLE on a non-existent table throws an exception if IF EXISTS is not specified.
Syntax
UNCACHE TABLE [ IF EXISTS ] table_identifier

Parameters


table_identifier
Specifies the table or view name to be uncached. The table or view name may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


Examples
UNCACHE TABLE t1;

Related Statements

CACHE TABLE
CLEAR CACHE
REFRESH TABLE
REFRESH
REFRESH FUNCTION





















  




RESET - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







RESET
Description
The RESET command resets runtime configurations specific to the current session which were set via the SET command to their default values.
Syntax
RESET;

RESET configuration_key;

Parameters


(none)
Reset any runtime configurations specific to the current session which were set via the SET command to their default values.


configuration_key
Restore the value of the configuration_key to the default value. If the default value is undefined, the configuration_key will be removed.


Examples
-- Reset any runtime configurations specific to the current session which were set via the SET command to their default values.
RESET;

-- If you start your application with --conf spark.foo=bar and set spark.foo=foobar in runtime, the example below will restore it to 'bar'. If spark.foo is not specified during starting, the example below will remove this config from the SQLConf. It will ignore nonexistent keys.
RESET spark.abc;

Related Statements

SET





















  




SET - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SET
Description
The SET command sets a property, returns the value of an existing property or returns all SQLConf properties with value and meaning.
Syntax
SET
SET [ -v ]
SET property_key[ = property_value ]

Parameters


-v
Outputs the key, value and meaning of existing SQLConf properties.


property_key
Returns the value of specified property key.


property_key=property_value
Sets the value for a given property key. If an old value exists for a given property key, then it gets overridden by the new value.


Examples
-- Set a property.
SET spark.sql.variable.substitute=false;

-- List all SQLConf properties with value and meaning.
SET -v;

-- List all SQLConf properties with value for current session.
SET;

-- List the value of specified property key.
SET spark.sql.variable.substitute;
+-----------------------------+-----+
|                          key|value|
+-----------------------------+-----+
|spark.sql.variable.substitute|false|
+-----------------------------+-----+

Related Statements

RESET





















  




DESCRIBE DATABASE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DESCRIBE DATABASE
Description
​
DESCRIBE DATABASE statement returns the metadata of an existing database. The metadata information includes database
name, database comment, and database location on the filesystem. If the optional EXTENDED option is specified, it
returns the basic metadata information along with the database properties. The DATABASE and SCHEMA are
interchangeable.
Syntax
{ DESC | DESCRIBE } DATABASE [ EXTENDED ] db_name

Parameters


db_name
Specifies a name of an existing database or an existing schema in the system. If the name does not exist, an
  exception is thrown.


Examples
-- Create employees DATABASE
CREATE DATABASE employees COMMENT 'For software companies';

-- Describe employees DATABASE.
-- Returns Database Name, Description and Root location of the filesystem
-- for the employees DATABASE.
DESCRIBE DATABASE employees;
+-------------------------+-----------------------------+
|database_description_item|   database_description_value|
+-------------------------+-----------------------------+
|            Database Name|                    employees|
|              Description|       For software companies|
|                 Location|file:/Users/Temp/employees.db|
+-------------------------+-----------------------------+

-- Create employees DATABASE
CREATE DATABASE employees COMMENT 'For software companies';

-- Alter employees database to set DBPROPERTIES
ALTER DATABASE employees SET DBPROPERTIES ('Create-by' = 'Kevin', 'Create-date' = '09/01/2019');

-- Describe employees DATABASE with EXTENDED option to return additional database properties
DESCRIBE DATABASE EXTENDED employees;
+-------------------------+---------------------------------------------+
|database_description_item|                   database_description_value|
+-------------------------+---------------------------------------------+
|            Database Name|                                    employees|
|              Description|                       For software companies|
|                 Location|                file:/Users/Temp/employees.db|
|               Properties|((Create-by,kevin), (Create-date,09/01/2019))|
+-------------------------+---------------------------------------------+

-- Create deployment SCHEMA
CREATE SCHEMA deployment COMMENT 'Deployment environment';

-- Describe deployment, the DATABASE and SCHEMA are interchangeable, their meaning are the same.
DESC DATABASE deployment;
+-------------------------+------------------------------+
|database_description_item|database_description_value    |
+-------------------------+------------------------------+
|            Database Name|                    deployment|
|              Description|        Deployment environment|
|                 Location|file:/Users/Temp/deployment.db|
+-------------------------+------------------------------+

Related Statements

DESCRIBE FUNCTION
DESCRIBE TABLE
DESCRIBE QUERY





















  




DESCRIBE FUNCTION - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DESCRIBE FUNCTION
Description
DESCRIBE FUNCTION statement returns the basic metadata information of an
existing function. The metadata information includes the function name, implementing
class and the usage details.  If the optional EXTENDED option is specified, the basic
metadata information is returned along with the extended usage information.
Syntax
{ DESC | DESCRIBE } FUNCTION [ EXTENDED ] function_name

Parameters


function_name
Specifies a name of an existing function in the system. The function name may be
  optionally qualified with a database name. If function_name is qualified with
  a database then the function is resolved from the user specified database, otherwise
  it is resolved from the current database.
Syntax: [ database_name. ] function_name


Examples
-- Describe a builtin scalar function.
-- Returns function name, implementing class and usage
DESC FUNCTION abs;
+-------------------------------------------------------------------+
|function_desc                                                      |
+-------------------------------------------------------------------+
|Function: abs                                                      |
|Class: org.apache.spark.sql.catalyst.expressions.Abs               |
|Usage: abs(expr) - Returns the absolute value of the numeric value.|
+-------------------------------------------------------------------+

-- Describe a builtin scalar function.
-- Returns function name, implementing class and usage and examples.
DESC FUNCTION EXTENDED abs;
+-------------------------------------------------------------------+
|function_desc                                                      |
+-------------------------------------------------------------------+
|Function: abs                                                      |
|Class: org.apache.spark.sql.catalyst.expressions.Abs               |
|Usage: abs(expr) - Returns the absolute value of the numeric value.|
|Extended Usage:                                                    |
|    Examples:                                                      |
|      > SELECT abs(-1);                                            |
|       1                                                           |
|                                                                   |
+-------------------------------------------------------------------+

-- Describe a builtin aggregate function
DESC FUNCTION max;
+--------------------------------------------------------------+
|function_desc                                                 |
+--------------------------------------------------------------+
|Function: max                                                 |
|Class: org.apache.spark.sql.catalyst.expressions.aggregate.Max|
|Usage: max(expr) - Returns the maximum value of `expr`.       |
+--------------------------------------------------------------+

-- Describe a builtin user defined aggregate function
-- Returns function name, implementing class and usage and examples.
DESC FUNCTION EXTENDED explode
+---------------------------------------------------------------+
|function_desc                                                  |
+---------------------------------------------------------------+
|Function: explode                                              |
|Class: org.apache.spark.sql.catalyst.expressions.Explode       |
|Usage: explode(expr) - Separates the elements of array `expr`  |
| into multiple rows, or the elements of map `expr` into        |
| multiple rows and columns. Unless specified otherwise, uses   |
| the default column name `col` for elements of the array or    |
| `key` and `value` for the elements of the map.                |
|Extended Usage:                                                |
|    Examples:                                                  |
|      > SELECT explode(array(10, 20));                         |
|       10                                                      |
|       20                                                      |
+---------------------------------------------------------------+

Related Statements

DESCRIBE DATABASE
DESCRIBE TABLE
DESCRIBE QUERY





















  




DESCRIBE QUERY - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DESCRIBE QUERY
Description
The DESCRIBE QUERY statement is used to return the metadata of output
of a query. A shorthand DESC may be used instead of DESCRIBE to
describe the query output.
Syntax
{ DESC | DESCRIBE } [ QUERY ] input_statement

Parameters


QUERY
  This clause is optional and may be omitted.


input_statement
Specifies a result set producing statement and may be one of the following:

a SELECT statement
a CTE(Common table expression) statement
an INLINE TABLE statement
a TABLE statement
a FROM statement`

Please refer to select-statement
  for a detailed syntax of the query parameter.


Examples
-- Create table `person`
CREATE TABLE person (name STRING , age INT COMMENT 'Age column', address STRING);

-- Returns column metadata information for a simple select query
DESCRIBE QUERY SELECT age, sum(age) FROM person GROUP BY age;
+--------+---------+----------+
|col_name|data_type|   comment|
+--------+---------+----------+
|     age|      int|Age column|
|sum(age)|   bigint|      null|
+--------+---------+----------+

-- Returns column metadata information for common table expression (`CTE`).
DESCRIBE QUERY WITH all_names_cte
    AS (SELECT name from person) SELECT * FROM all_names_cte;
+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|    name|   string|   null|
+--------+---------+-------+

-- Returns column metadata information for an inline table.
DESC QUERY VALUES(100, 'John', 10000.20D) AS employee(id, name, salary);
+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|      id|      int|   null|
|    name|   string|   null|
|  salary|   double|   null|
+--------+---------+-------+

-- Returns column metadata information for `TABLE` statement.
DESC QUERY TABLE person;
+--------+---------+----------+
|col_name|data_type|   comment|
+--------+---------+----------+
|    name|   string|      null|
|     age|      int| Agecolumn|
| address|   string|      null|
+--------+---------+----------+

-- Returns column metadata information for a `FROM` statement.
-- `QUERY` clause is optional and can be omitted.
DESCRIBE FROM person SELECT age;
+--------+---------+----------+
|col_name|data_type|   comment|
+--------+---------+----------+
|     age|      int| Agecolumn|
+--------+---------+----------+

Related Statements

DESCRIBE DATABASE
DESCRIBE TABLE
DESCRIBE FUNCTION





















  




DESCRIBE TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DESCRIBE TABLE
Description
DESCRIBE TABLE statement returns the basic metadata information of a
table. The metadata information includes column name, column type
and column comment. Optionally a partition spec or column name may be specified
to return the metadata pertaining to a partition or column respectively.
Syntax
{ DESC | DESCRIBE } [ TABLE ] [ format ] table_identifier [ partition_spec ] [ col_name ]

Parameters


format
Specifies the optional format of describe output. If EXTENDED is specified
  then additional metadata information (such as parent database, owner, and access time)
  is returned.


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
An optional parameter that specifies a comma separated list of key and value pairs
  for partitions. When specified, additional partition metadata is returned.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


col_name
An optional parameter that specifies the column name that needs to be described.
  The supplied column name may be optionally qualified. Parameters partition_spec
  and col_name are  mutually exclusive and can not be specified together. Currently
  nested columns are not allowed to be specified.
Syntax: [ database_name. ] [ table_name. ] column_name


Examples
-- Creates a table `customer`. Assumes current database is `salesdb`.
CREATE TABLE customer(
        cust_id INT,
        state VARCHAR(20),
        name STRING COMMENT 'Short name'
    )
    USING parquet
    PARTITIONED BY (state);
    
INSERT INTO customer PARTITION (state = 'AR') VALUES (100, 'Mike');
    
-- Returns basic metadata information for unqualified table `customer`
DESCRIBE TABLE customer;
+-----------------------+---------+----------+
|               col_name|data_type|   comment|
+-----------------------+---------+----------+
|                cust_id|      int|      null|
|                   name|   string|Short name|
|                  state|   string|      null|
|# Partition Information|         |          |
|             # col_name|data_type|   comment|
|                  state|   string|      null|
+-----------------------+---------+----------+

-- Returns basic metadata information for qualified table `customer`
DESCRIBE TABLE salesdb.customer;
+-----------------------+---------+----------+
|               col_name|data_type|   comment|
+-----------------------+---------+----------+
|                cust_id|      int|      null|
|                   name|   string|Short name|
|                  state|   string|      null|
|# Partition Information|         |          |
|             # col_name|data_type|   comment|
|                  state|   string|      null|
+-----------------------+---------+----------+

-- Returns additional metadata such as parent database, owner, access time etc.
DESCRIBE TABLE EXTENDED customer;
+----------------------------+------------------------------+----------+
|                    col_name|                     data_type|   comment|
+----------------------------+------------------------------+----------+
|                     cust_id|                           int|      null|
|                        name|                        string|Short name|
|                       state|                        string|      null|
|     # Partition Information|                              |          |
|                  # col_name|                     data_type|   comment|
|                       state|                        string|      null|
|                            |                              |          |
|# Detailed Table Information|                              |          |
|                    Database|                       default|          |
|                       Table|                      customer|          |
|                       Owner|                 <TABLE OWNER>|          |
|                Created Time|  Tue Apr 07 22:56:34 JST 2020|          |
|                 Last Access|                       UNKNOWN|          |
|                  Created By|               <SPARK VERSION>|          |
|                        Type|                       MANAGED|          |
|                    Provider|                       parquet|          |
|                    Location|file:/tmp/salesdb.db/custom...|          |
|               Serde Library|org.apache.hadoop.hive.ql.i...|          |
|                 InputFormat|org.apache.hadoop.hive.ql.i...|          |
|                OutputFormat|org.apache.hadoop.hive.ql.i...|          |
|          Partition Provider|                       Catalog|          |
+----------------------------+------------------------------+----------+

-- Returns partition metadata such as partitioning column name, column type and comment.
DESCRIBE TABLE EXTENDED customer PARTITION (state = 'AR');
+------------------------------+------------------------------+----------+
|                      col_name|                     data_type|   comment|
+------------------------------+------------------------------+----------+
|                       cust_id|                           int|      null|
|                          name|                        string|Short name|
|                         state|                        string|      null|
|       # Partition Information|                              |          |
|                    # col_name|                     data_type|   comment|
|                         state|                        string|      null|
|                              |                              |          |
|# Detailed Partition Inform...|                              |          |
|                      Database|                       default|          |
|                         Table|                      customer|          |
|              Partition Values|                    [state=AR]|          |
|                      Location|file:/tmp/salesdb.db/custom...|          |
|                 Serde Library|org.apache.hadoop.hive.ql.i...|          |
|                   InputFormat|org.apache.hadoop.hive.ql.i...|          |
|                  OutputFormat|org.apache.hadoop.hive.ql.i...|          |
|            Storage Properties|[serialization.format=1, pa...|          |
|          Partition Parameters|{transient_lastDdlTime=1586...|          |
|                  Created Time|  Tue Apr 07 23:05:43 JST 2020|          |
|                   Last Access|                       UNKNOWN|          |
|          Partition Statistics|                     659 bytes|          |
|                              |                              |          |
|         # Storage Information|                              |          |
|                      Location|file:/tmp/salesdb.db/custom...|          |
|                 Serde Library|org.apache.hadoop.hive.ql.i...|          |
|                   InputFormat|org.apache.hadoop.hive.ql.i...|          |
|                  OutputFormat|org.apache.hadoop.hive.ql.i...|          |
+------------------------------+------------------------------+----------+

-- Returns the metadata for `name` column.
-- Optional `TABLE` clause is omitted and column is fully qualified.
DESCRIBE customer salesdb.customer.name;
+---------+----------+
|info_name|info_value|
+---------+----------+
| col_name|      name|
|data_type|    string|
|  comment|Short name|
+---------+----------+

Related Statements

DESCRIBE DATABASE
DESCRIBE QUERY
DESCRIBE FUNCTION





















  




ADD FILE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







ADD FILE
Description
ADD FILE can be used to add a single file as well as a directory to the list of resources. The added resource can be listed using LIST FILE.
Syntax
ADD { FILE | FILES } resource_name [ ... ]

Parameters


resource_name
The name of the file or directory to be added.


Examples
ADD FILE /tmp/test;
ADD FILE "/path/to/file/abc.txt";
ADD FILE '/another/test.txt';
ADD FILE "/path with space/abc.txt";
ADD FILE "/path/to/some/directory";
ADD FILES "/path with space/cde.txt" '/path with space/fgh.txt';

Related Statements

LIST FILE
LIST JAR
LIST ARCHIVE
ADD JAR
ADD ARCHIVE





















  




ADD JAR - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







ADD JAR
Description
ADD JAR adds a JAR file to the list of resources. The added JAR file can be listed using LIST JAR.
Syntax
ADD { JAR | JARS } file_name [ ... ]

Parameters


file_name
The name of the JAR file to be added. It could be either on a local file system or a distributed file system or an Ivy URI.
  Apache Ivy is a popular dependency manager focusing on flexibility and simplicity. Now we support two parameter in URI query string:

transitive: whether to download dependent jars related to your ivy URL. The parameter name is case-sensitive, and the parameter value is case-insensitive. If multiple transitive parameters are specified, the last one wins.
exclude: exclusion list during downloading Ivy URI jar and dependent jars.

User can write Ivy URI such as:
ivy://group:module:version
ivy://group:module:version?transitive=[true|false]
ivy://group:module:version?transitive=[true|false]&exclude=group:module,group:module
 


Examples
ADD JAR /tmp/test.jar;
ADD JAR "/path/to/some.jar";
ADD JAR '/some/other.jar';
ADD JAR "/path with space/abc.jar";
ADD JARS "/path with space/def.jar" '/path with space/ghi.jar';
ADD JAR "ivy://group:module:version";
ADD JAR "ivy://group:module:version?transitive=false"
ADD JAR "ivy://group:module:version?transitive=true"
ADD JAR "ivy://group:module:version?exclude=group:module&transitive=true"

Related Statements

LIST JAR
ADD FILE
LIST FILE
ADD ARCHIVE
LIST ARCHIVE





















  




LIST FILE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







LIST FILE
Description
LIST FILE lists the resources added by ADD FILE.
Syntax
LIST { FILE | FILES } file_name [ ... ]

Examples
ADD FILE /tmp/test;
ADD FILE /tmp/test_2;
LIST FILE;
-- output for LIST FILE
file:/private/tmp/test
file:/private/tmp/test_2

LIST FILE /tmp/test /some/random/file /another/random/file
--output
file:/private/tmp/test

Related Statements

ADD FILE
ADD JAR
ADD ARCHIVE
LIST JAR
LIST ARCHIVE





















  




LIST JAR - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







LIST JAR
Description
LIST JAR lists the JARs added by ADD JAR.
Syntax
LIST { JAR | JARS } file_name [ ... ]


Examples
ADD JAR /tmp/test.jar;
ADD JAR /tmp/test_2.jar;
LIST JAR;
-- output for LIST JAR
spark://192.168.1.112:62859/jars/test.jar
spark://192.168.1.112:62859/jars/test_2.jar

LIST JAR /tmp/test.jar /some/random.jar /another/random.jar;
-- output
spark://192.168.1.112:62859/jars/test.jar

Related Statements

ADD JAR
ADD FILE
ADD ARCHIVE
LIST FILE
LIST ARCHIVE





















  




SHOW COLUMNS - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW COLUMNS
Description
Returns the list of columns in a table. If the table does not exist, an exception is thrown.
Syntax
SHOW COLUMNS table_identifier [ database ]

Parameters


table_identifier
Specifies the table name of an existing table. The table may be optionally qualified
  with a database name.
Syntax: { IN | FROM } [ database_name . ] table_name
Note: Keywords IN and FROM are interchangeable.


database
Specifies an optional database name. The table is resolved from this database when it
  is specified. When this parameter is specified then table
  name should not be qualified with a different database name.
Syntax: { IN | FROM } database_name
Note: Keywords IN and FROM are interchangeable.


Examples
-- Create `customer` table in `salesdb` database;
USE salesdb;
CREATE TABLE customer(
    cust_cd INT,
    name VARCHAR(100),
    cust_addr STRING);

-- List the columns of `customer` table in current database.
SHOW COLUMNS IN customer;
+---------+
| col_name|
+---------+
|  cust_cd|
|     name|
|cust_addr|
+---------+

-- List the columns of `customer` table in `salesdb` database.
SHOW COLUMNS IN salesdb.customer;
+---------+
| col_name|
+---------+
|  cust_cd|
|     name|
|cust_addr|
+---------+

-- List the columns of `customer` table in `salesdb` database
SHOW COLUMNS IN customer IN salesdb;
+---------+
| col_name|
+---------+
|  cust_cd|
|     name|
|cust_addr|
+---------+

Related Statements

DESCRIBE TABLE
SHOW TABLE





















  




SHOW CREATE TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW CREATE TABLE
Description
SHOW CREATE TABLE returns the CREATE TABLE statement or CREATE VIEW statement that was used to create a given table or view. SHOW CREATE TABLE on a non-existent table or a temporary view throws an exception.
Syntax
SHOW CREATE TABLE table_identifier [ AS SERDE ]

Parameters


table_identifier
Specifies a table or view name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


AS SERDE
Generates Hive DDL for a Hive SerDe table.


Examples
CREATE TABLE test (c INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
    STORED AS TEXTFILE
    TBLPROPERTIES ('prop1' = 'value1', 'prop2' = 'value2');

SHOW CREATE TABLE test;
+----------------------------------------------------+
|                                      createtab_stmt|
+----------------------------------------------------+
|CREATE TABLE `default`.`test` (`c` INT)
 USING text
 TBLPROPERTIES (
   'transient_lastDdlTime' = '1586269021',
   'prop1' = 'value1',
   'prop2' = 'value2')
+----------------------------------------------------+

SHOW CREATE TABLE test AS SERDE;
+------------------------------------------------------------------------------+
|                                                                createtab_stmt|
+------------------------------------------------------------------------------+
|CREATE TABLE `default`.`test`(
  `c` INT)
 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
 WITH SERDEPROPERTIES (
   'serialization.format' = ',',
   'field.delim' = ',')
 STORED AS
   INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
   OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
 TBLPROPERTIES (
   'prop1' = 'value1',
   'prop2' = 'value2',
   'transient_lastDdlTime' = '1641800515')
+------------------------------------------------------------------------------+

Related Statements

CREATE TABLE
CREATE VIEW





















  




SHOW DATABASES - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW DATABASES
Description
Lists the databases that match an optionally supplied regular expression pattern. If no
pattern is supplied then the command lists all the databases in the system.
Please note that the usage of SCHEMAS and DATABASES are interchangeable
and mean the same thing.
Syntax
SHOW { DATABASES | SCHEMAS } [ LIKE regex_pattern ]

Parameters


regex_pattern
Specifies a regular expression pattern that is used to filter the results of the
  statement.

Except for * and | character, the pattern works like a regular expression.
* alone matches 0 or more characters and | is used to separate multiple different regular expressions,
 any of which can match.
The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive.



Examples
-- Create database. Assumes a database named `default` already exists in
-- the system. 
CREATE DATABASE payroll_db;
CREATE DATABASE payments_db;

-- Lists all the databases. 
SHOW DATABASES;
+------------+
|databaseName|
+------------+
|     default|
| payments_db|
|  payroll_db|
+------------+
  
-- Lists databases with name starting with string pattern `pay`
SHOW DATABASES LIKE 'pay*';
+------------+
|databaseName|
+------------+
| payments_db|
|  payroll_db|
+------------+
  
-- Lists all databases. Keywords SCHEMAS and DATABASES are interchangeable. 
SHOW SCHEMAS;
+------------+
|databaseName|
+------------+
|     default|
| payments_db|
|  payroll_db|
+------------+

Related Statements

DESCRIBE DATABASE
CREATE DATABASE
ALTER DATABASE





















  




SHOW FUNCTIONS - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW FUNCTIONS
Description
Returns the list of functions after applying an optional regex pattern.
Given number of functions supported by Spark is quite large, this statement
in conjunction with describe function
may be used to quickly find the function and understand its usage. The LIKE 
clause is optional and supported only for compatibility with other systems.
Syntax
SHOW [ function_kind ] FUNCTIONS [ { FROM | IN } database_name ] [ LIKE regex_pattern ]

Parameters


function_kind
Specifies the name space of the function to be searched upon. The valid name spaces are :

USER - Looks up the function(s) among the user defined functions.
SYSTEM - Looks up the function(s) among the system defined functions.
ALL -  Looks up the function(s) among both user and system defined functions.



{ FROM | IN } database_name
Specifies the database name from which functions are listed.


regex_pattern
Specifies a regular expression pattern that is used to filter the results of the
  statement.

Except for * and | character, the pattern works like a regular expression.
* alone matches 0 or more characters and | is used to separate multiple different regular expressions,
 any of which can match.
The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive.



Examples
-- List a system function `trim` by searching both user defined and system
-- defined functions.
SHOW FUNCTIONS trim;
+--------+
|function|
+--------+
|    trim|
+--------+

-- List a system function `concat` by searching system defined functions.
SHOW SYSTEM FUNCTIONS concat;
+--------+
|function|
+--------+
|  concat|
+--------+

-- List a qualified function `max` from database `salesdb`. 
SHOW SYSTEM FUNCTIONS FROM salesdb LIKE 'max';
+--------+
|function|
+--------+
|     max|
+--------+

-- List all functions starting with `t`
SHOW FUNCTIONS LIKE 't*';
+-----------------+
|         function|
+-----------------+
|              tan|
|             tanh|
|        timestamp|
|          tinyint|
|           to_csv|
|          to_date|
|          to_json|
|     to_timestamp|
|to_unix_timestamp|
| to_utc_timestamp|
|        transform|
|   transform_keys|
| transform_values|
|        translate|
|             trim|
|            trunc|
|           typeof|
+-----------------+

-- List all functions starting with `yea` or `windo`
SHOW FUNCTIONS LIKE 'yea*|windo*';
+--------+
|function|
+--------+
|  window|
|    year|
+--------+

-- Use normal regex pattern to list function names that has 4 characters
-- with `t` as the starting character.
SHOW FUNCTIONS LIKE 't[a-z][a-z][a-z]';
+--------+
|function|
+--------+
|    tanh|
|    trim|
+--------+

Related Statements

DESCRIBE FUNCTION





















  




SHOW PARTITIONS - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW PARTITIONS
Description
The SHOW PARTITIONS statement is used to list partitions of a table. An optional
partition spec may be specified to return the partitions matching the supplied
partition spec.
Syntax
SHOW PARTITIONS table_identifier [ partition_spec ]

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
An optional parameter that specifies a comma separated list of key and value pairs
  for partitions. When specified, the partitions that match the partition specification are returned.
Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] )


Examples
-- create a partitioned table and insert a few rows.
USE salesdb;
CREATE TABLE customer(id INT, name STRING) PARTITIONED BY (state STRING, city STRING);
INSERT INTO customer PARTITION (state = 'CA', city = 'Fremont') VALUES (100, 'John');
INSERT INTO customer PARTITION (state = 'CA', city = 'San Jose') VALUES (200, 'Marry');
INSERT INTO customer PARTITION (state = 'AZ', city = 'Peoria') VALUES (300, 'Daniel');

-- Lists all partitions for table `customer`
SHOW PARTITIONS customer;
+----------------------+
|             partition|
+----------------------+
|  state=AZ/city=Peoria|
| state=CA/city=Fremont|
|state=CA/city=San Jose|
+----------------------+

-- Lists all partitions for the qualified table `customer`
SHOW PARTITIONS salesdb.customer;
+----------------------+
|             partition|
+----------------------+
|  state=AZ/city=Peoria|
| state=CA/city=Fremont|
|state=CA/city=San Jose|
+----------------------+

-- Specify a full partition spec to list specific partition
SHOW PARTITIONS customer PARTITION (state = 'CA', city = 'Fremont');
+---------------------+
|            partition|
+---------------------+
|state=CA/city=Fremont|
+---------------------+

-- Specify a partial partition spec to list the specific partitions
SHOW PARTITIONS customer PARTITION (state = 'CA');
+----------------------+
|             partition|
+----------------------+
| state=CA/city=Fremont|
|state=CA/city=San Jose|
+----------------------+

-- Specify a partial spec to list specific partition
SHOW PARTITIONS customer PARTITION (city =  'San Jose');
+----------------------+
|             partition|
+----------------------+
|state=CA/city=San Jose|
+----------------------+

Related Statements

CREATE TABLE
INSERT STATEMENT
DESCRIBE TABLE
SHOW TABLE





















  




SHOW TABLE EXTENDED - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW TABLE EXTENDED
Description
SHOW TABLE EXTENDED will show information for all tables matching the given regular expression.
Output includes basic table information and file system information like Last Access, 
Created By, Type, Provider, Table Properties, Location, Serde Library, InputFormat, 
OutputFormat, Storage Properties, Partition Provider, Partition Columns and Schema.
If a partition specification is present, it outputs the given partition’s file-system-specific 
information such as Partition Parameters and Partition Statistics. Note that a table regex 
cannot be used with a partition specification.
Syntax
SHOW TABLE EXTENDED [ { IN | FROM } database_name ] LIKE regex_pattern
    [ partition_spec ]

Parameters


{ IN|FROM } database_name
Specifies database name. If not provided, will use the current database.


regex_pattern
Specifies the regular expression pattern that is used to filter out unwanted tables.

Except for * and | character, the pattern works like a regular expression.
* alone matches 0 or more characters and | is used to separate multiple different regular expressions,
any of which can match.
The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive.



partition_spec
An optional parameter that specifies a comma separated list of key and value pairs
  for partitions. Note that a table regex cannot be used with a partition specification.
Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] )


Examples
-- Assumes `employee` table created with partitioned by column `grade`
CREATE TABLE employee(name STRING, grade INT) PARTITIONED BY (grade);
INSERT INTO employee PARTITION (grade = 1) VALUES ('sam');
INSERT INTO employee PARTITION (grade = 2) VALUES ('suj');

 -- Show the details of the table
SHOW TABLE EXTENDED LIKE 'employee';
+--------+---------+-----------+--------------------------------------------------------------+
|database|tableName|isTemporary|                         information                          |
+--------+---------+-----------+--------------------------------------------------------------+
|default |employee |false      |Database: default
                                Table: employee
                                Owner: root
                                Created Time: Fri Aug 30 15:10:21 IST 2019
                                Last Access: Thu Jan 01 05:30:00 IST 1970
                                Created By: Spark 3.0.0-SNAPSHOT
                                Type: MANAGED
                                Provider: hive
                                Table Properties: [transient_lastDdlTime=1567158021]
                                Location: file:/opt/spark1/spark/spark-warehouse/employee
                                Serde Library: org.apache.hadoop.hive.serde2.lazy
                                .LazySimpleSerDe
                                InputFormat: org.apache.hadoop.mapred.TextInputFormat
                                OutputFormat: org.apache.hadoop.hive.ql.io
                                .HiveIgnoreKeyTextOutputFormat
                                Storage Properties: [serialization.format=1]
                                Partition Provider: Catalog
                                Partition Columns: [`grade`]
                                Schema: root
                                 |-- name: string (nullable = true)
                                 |-- grade: integer (nullable = true)
                                                                                                            
+--------+---------+-----------+--------------------------------------------------------------+

-- showing the multiple table details with pattern matching
SHOW TABLE EXTENDED  LIKE 'employe*';
+--------+---------+-----------+--------------------------------------------------------------+
|database|tableName|isTemporary|                         information                          |
+--------+---------+-----------+--------------------------------------------------------------+
|default |employee |false      |Database: default
                                Table: employee
                                Owner: root
                                Created Time: Fri Aug 30 15:10:21 IST 2019
                                Last Access: Thu Jan 01 05:30:00 IST 1970
                                Created By: Spark 3.0.0-SNAPSHOT
                                Type: MANAGED
                                Provider: hive
                                Table Properties: [transient_lastDdlTime=1567158021]
                                Location: file:/opt/spark1/spark/spark-warehouse/employee
                                Serde Library: org.apache.hadoop.hive.serde2.lazy
                                .LazySimpleSerDe
                                InputFormat: org.apache.hadoop.mapred.TextInputFormat
                                OutputFormat: org.apache.hadoop.hive.ql.io
                                .HiveIgnoreKeyTextOutputFormat
                                Storage Properties: [serialization.format=1]
                                Partition Provider: Catalog
                                Partition Columns: [`grade`]
                                Schema: root
                                 |-- name: string (nullable = true)
                                 |-- grade: integer (nullable = true)
  
|default |employee1|false      |Database: default
                                Table: employee1
                                Owner: root
                                Created Time: Fri Aug 30 15:22:33 IST 2019
                                Last Access: Thu Jan 01 05:30:00 IST 1970
                                Created By: Spark 3.0.0-SNAPSHOT
                                Type: MANAGED
                                Provider: hive
                                Table Properties: [transient_lastDdlTime=1567158753]
                                Location: file:/opt/spark1/spark/spark-warehouse/employee1
                                Serde Library: org.apache.hadoop.hive.serde2.lazy
                                .LazySimpleSerDe
                                InputFormat: org.apache.hadoop.mapred.TextInputFormat
                                OutputFormat: org.apache.hadoop.hive.ql.io
                                .HiveIgnoreKeyTextOutputFormat
                                Storage Properties: [serialization.format=1]
                                Partition Provider: Catalog
                                Schema: root
                                 |-- name: string (nullable = true)
                                                                                                               
+--------+---------+----------+---------------------------------------------------------------+
  
-- show partition file system details
SHOW TABLE EXTENDED  IN default LIKE 'employee' PARTITION (grade=1);
+--------+---------+-----------+--------------------------------------------------------------+
|database|tableName|isTemporary|                         information                          |
+--------+---------+-----------+--------------------------------------------------------------+
|default |employee |false      |Partition Values: [grade=1]
                                Location: file:/opt/spark1/spark/spark-warehouse/employee
                                /grade=1
                                Serde Library: org.apache.hadoop.hive.serde2.lazy
                                .LazySimpleSerDe
                                InputFormat: org.apache.hadoop.mapred.TextInputFormat
                                OutputFormat: org.apache.hadoop.hive.ql.io
                                .HiveIgnoreKeyTextOutputFormat
                                Storage Properties: [serialization.format=1]
                                Partition Parameters: {rawDataSize=-1, numFiles=1,
                                transient_lastDdlTime=1567158221, totalSize=4,
                                COLUMN_STATS_ACCURATE=false, numRows=-1}
                                Created Time: Fri Aug 30 15:13:41 IST 2019
                                Last Access: Thu Jan 01 05:30:00 IST 1970
                                Partition Statistics: 4 bytes
                                                                                                                                                                          |
+--------+---------+-----------+--------------------------------------------------------------+

-- show partition file system details with regex fails as shown below
SHOW TABLE EXTENDED  IN default LIKE 'empl*' PARTITION (grade=1);
Error: Error running query: org.apache.spark.sql.catalyst.analysis.NoSuchTableException:
 Table or view 'emplo*' not found in database 'default'; (state=,code=0)

Related Statements

CREATE TABLE
DESCRIBE TABLE





















  




SHOW TABLES - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW TABLES
Description
The SHOW TABLES statement returns all the tables for an optionally specified database.
Additionally, the output of this statement may be filtered by an optional matching
pattern. If no database is specified then the tables are returned from the 
current database.
Syntax
SHOW TABLES [ { FROM | IN } database_name ] [ LIKE regex_pattern ]

Parameters


{ FROM | IN } database_name
Specifies the database name from which tables are listed.


regex_pattern
Specifies the regular expression pattern that is used to filter out unwanted tables.

Except for * and | character, the pattern works like a regular expression.
* alone matches 0 or more characters and | is used to separate multiple different regular expressions,
any of which can match.
The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive.



Examples
-- List all tables in default database
SHOW TABLES;
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|      sam|      false|
| default|     sam1|      false|
| default|      suj|      false|
+--------+---------+-----------+

-- List all tables from userdb database 
SHOW TABLES FROM userdb;
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
|  userdb|    user1|      false|
|  userdb|    user2|      false|
+--------+---------+-----------+

-- List all tables in userdb database
SHOW TABLES IN userdb;
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
|  userdb|    user1|      false|
|  userdb|    user2|      false|
+--------+---------+-----------+

-- List all tables from default database matching the pattern `sam*`
SHOW TABLES FROM default LIKE 'sam*';
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|      sam|      false|
| default|     sam1|      false|
+--------+---------+-----------+
  
-- List all tables matching the pattern `sam*|suj`
SHOW TABLES LIKE 'sam*|suj';
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|      sam|      false|
| default|     sam1|      false|
| default|      suj|      false|
+--------+---------+-----------+

Related Statements

CREATE TABLE
DROP TABLE
CREATE DATABASE
DROP DATABASE





















  




SHOW TBLPROPERTIES - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW TBLPROPERTIES
Description
This statement returns the value of a table property given an optional value for
a property key. If no key is specified then all the properties are returned.
Syntax
SHOW TBLPROPERTIES table_identifier 
   [ ( unquoted_property_key | property_key_as_string_literal ) ]

Parameters


table_identifier
Specifies the table name of an existing table. The table may be optionally qualified
  with a database name.
Syntax: [ database_name. ] table_name


unquoted_property_key
Specifies the property key in unquoted form. The key may consists of multiple
  parts separated by dot.
Syntax: [ key_part1 ] [ .key_part2 ] [ ... ]


property_key_as_string_literal
Specifies a property key value as a string literal.


Note

Property value returned by this statement excludes some properties 
that are internal to spark and hive. The excluded properties are :
    
All the properties that start with prefix spark.sql
Property keys such as:  EXTERNAL, comment
All the properties generated internally by hive to store statistics. Some of these
properties are: numFiles, numPartitions, numRows.



Examples
-- create a table `customer` in database `salesdb`
USE salesdb;
CREATE TABLE customer(cust_code INT, name VARCHAR(100), cust_addr STRING)
    TBLPROPERTIES ('created.by.user' = 'John', 'created.date' = '01-01-2001');

-- show all the user specified properties for table `customer`
SHOW TBLPROPERTIES customer;
+---------------------+----------+
|                  key|     value|
+---------------------+----------+
|      created.by.user|      John|
|         created.date|01-01-2001|
|transient_lastDdlTime|1567554931|
+---------------------+----------+

-- show all the user specified properties for a qualified table `customer`
-- in database `salesdb`
SHOW TBLPROPERTIES salesdb.customer;
+---------------------+----------+
|                  key|     value|
+---------------------+----------+
|      created.by.user|      John|
|         created.date|01-01-2001|
|transient_lastDdlTime|1567554931|
+---------------------+----------+

-- show value for unquoted property key `created.by.user`
SHOW TBLPROPERTIES customer (created.by.user);
+-----+
|value|
+-----+
| John|
+-----+

-- show value for property `created.date`` specified as string literal
SHOW TBLPROPERTIES customer ('created.date');
+----------+
|     value|
+----------+
|01-01-2001|
+----------+

Related Statements

CREATE TABLE
ALTER TABLE SET TBLPROPERTIES
SHOW TABLES
SHOW TABLE EXTENDED





















  




SHOW VIEWS - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SHOW VIEWS
Description
The SHOW VIEWS statement returns all the views for an optionally specified database.
Additionally, the output of this statement may be filtered by an optional matching
pattern. If no database is specified then the views are returned from the 
current database. If the specified database is global temporary view database, we will
list global temporary views. Note that the command also lists local temporary views 
regardless of a given database.
Syntax
SHOW VIEWS [ { FROM | IN } database_name ] [ LIKE regex_pattern ]

Parameters


{ FROM | IN } database_name
Specifies the database name from which views are listed.


regex_pattern
Specifies the regular expression pattern that is used to filter out unwanted views.

Except for * and | character, the pattern works like a regular expression.
* alone matches 0 or more characters and | is used to separate multiple different regular expressions,
any of which can match.
The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive.



Examples
-- Create views in different databases, also create global/local temp views.
CREATE VIEW sam AS SELECT id, salary FROM employee WHERE name = 'sam';
CREATE VIEW sam1 AS SELECT id, salary FROM employee WHERE name = 'sam1';
CREATE VIEW suj AS SELECT id, salary FROM employee WHERE name = 'suj';
USE userdb;
CREATE VIEW user1 AS SELECT id, salary FROM default.employee WHERE name = 'user1';
CREATE VIEW user2 AS SELECT id, salary FROM default.employee WHERE name = 'user2';
USE default;
CREATE GLOBAL TEMP VIEW temp1 AS SELECT 1 AS col1;
CREATE TEMP VIEW temp2 AS SELECT 1 AS col1;

-- List all views in default database
SHOW VIEWS;
+-------------+------------+--------------+
| namespace   | viewName   | isTemporary  |
+-------------+------------+--------------+
| default     | sam        | false        |
| default     | sam1       | false        |
| default     | suj        | false        |
|             | temp2      | true         |
+-------------+------------+--------------+

-- List all views from userdb database 
SHOW VIEWS FROM userdb;
+-------------+------------+--------------+
| namespace   | viewName   | isTemporary  |
+-------------+------------+--------------+
| userdb      | user1      | false        |
| userdb      | user2      | false        |
|             | temp2      | true         |
+-------------+------------+--------------+
  
-- List all views in global temp view database 
SHOW VIEWS IN global_temp;
+-------------+------------+--------------+
| namespace   | viewName   | isTemporary  |
+-------------+------------+--------------+
| global_temp | temp1      | true         |
|             | temp2      | true         |
+-------------+------------+--------------+

-- List all views from default database matching the pattern `sam*`
SHOW VIEWS FROM default LIKE 'sam*';
+-----------+------------+--------------+
| namespace | viewName   | isTemporary  |
+-----------+------------+--------------+
| default   | sam        | false        |
| default   | sam1       | false        |
+-----------+------------+--------------+

-- List all views from the current database matching the pattern `sam|suj｜temp*`
SHOW VIEWS LIKE 'sam|suj|temp*';
+-------------+------------+--------------+
| namespace   | viewName   | isTemporary  |
+-------------+------------+--------------+
| default     | sam        | false        |
| default     | suj        | false        |
|             | temp2      | true         |
+-------------+------------+--------------+

Related statements

CREATE VIEW
DROP VIEW
CREATE DATABASE
DROP DATABASE





















  




ALTER DATABASE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







ALTER DATABASE
Description
ALTER DATABASE statement changes the properties or location of a database. Please note that the usage of
DATABASE, SCHEMA and NAMESPACE are interchangeable and one can be used in place of the others. An error message
is issued if the database is not found in the system.
ALTER PROPERTIES
ALTER DATABASE SET DBPROPERTIES statement changes the properties associated with a database.
The specified property values override any existing value with the same property name. 
This command is mostly used to record the metadata for a database and may be used for auditing purposes.
Syntax
ALTER { DATABASE | SCHEMA | NAMESPACE } database_name
    SET { DBPROPERTIES | PROPERTIES } ( property_name = property_value [ , ... ] )

Parameters


database_name
Specifies the name of the database to be altered.


ALTER LOCATION
ALTER DATABASE SET LOCATION statement changes the default parent-directory where new tables will be added 
for a database. Please note that it does not move the contents of the database’s current directory to the newly 
specified location or change the locations associated with any tables/partitions under the specified database 
(available since Spark 3.0.0 with the Hive metastore version 3.0.0 and later).
Syntax
ALTER { DATABASE | SCHEMA | NAMESPACE } database_name
    SET LOCATION 'new_location'

Parameters


database_name
Specifies the name of the database to be altered.


Examples
-- Creates a database named `inventory`.
CREATE DATABASE inventory;

-- Alters the database to set properties `Edited-by` and `Edit-date`.
ALTER DATABASE inventory SET DBPROPERTIES ('Edited-by' = 'John', 'Edit-date' = '01/01/2001');

-- Verify that properties are set.
DESCRIBE DATABASE EXTENDED inventory;
+-------------------------+------------------------------------------+
|database_description_item|                database_description_value|
+-------------------------+------------------------------------------+
|            Database Name|                                 inventory|
|              Description|                                          |
|                 Location|   file:/temp/spark-warehouse/inventory.db|
|               Properties|((Edit-date,01/01/2001), (Edited-by,John))|
+-------------------------+------------------------------------------+

-- Alters the database to set a new location.
ALTER DATABASE inventory SET LOCATION 'file:/temp/spark-warehouse/new_inventory.db';

-- Verify that a new location is set.
DESCRIBE DATABASE EXTENDED inventory;
+-------------------------+-------------------------------------------+
|database_description_item|                 database_description_value|
+-------------------------+-------------------------------------------+
|            Database Name|                                  inventory|
|              Description|                                           |
|                 Location|file:/temp/spark-warehouse/new_inventory.db|
|               Properties| ((Edit-date,01/01/2001), (Edited-by,John))|
+-------------------------+-------------------------------------------+

Related Statements

DESCRIBE DATABASE





















  




ALTER TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







ALTER TABLE
Description
ALTER TABLE statement changes the schema or properties of a table.
RENAME
ALTER TABLE RENAME TO statement changes the table name of an existing table in the database. The table rename command cannot be used to move a table between databases, only to rename a table within the same database.
If the table is cached, the commands clear cached data of the table. The cache will be lazily filled when the next time the table is accessed. Additionally:

the table rename command uncaches all table’s dependents such as views that refer to the table. The dependents should be cached again explicitly.
the partition rename command clears caches of all table dependents while keeping them as cached. So, their caches will be lazily filled when the next time they are accessed.

Syntax
ALTER TABLE table_identifier RENAME TO table_identifier

ALTER TABLE table_identifier partition_spec RENAME TO partition_spec

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
Partition to be renamed. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


ADD COLUMNS
ALTER TABLE ADD COLUMNS statement adds mentioned columns to an existing table.
Syntax
ALTER TABLE table_identifier ADD COLUMNS ( col_spec [ , ... ] )

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


COLUMNS ( col_spec )
Specifies the columns to be added.


DROP COLUMNS
ALTER TABLE DROP COLUMNS statement drops mentioned columns from an existing table.
Note that this statement is only supported with v2 tables.
Syntax
ALTER TABLE table_identifier DROP { COLUMN | COLUMNS } [ ( ] col_name [ , ... ] [ ) ]

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


col_name
Specifies the name of the column.


RENAME COLUMN
ALTER TABLE RENAME COLUMN statement changes the column name of an existing table.
Note that this statement is only supported with v2 tables.
Syntax
ALTER TABLE table_identifier RENAME COLUMN col_name TO col_name

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


col_name
Specifies the name of the column.


ALTER OR CHANGE COLUMN
ALTER TABLE ALTER COLUMN or ALTER TABLE CHANGE COLUMN statement changes column’s definition.
Syntax
ALTER TABLE table_identifier { ALTER | CHANGE } [ COLUMN ] col_name alterColumnAction

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


col_name
Specifies the name of the column.


alterColumnAction
Change column’s definition.


REPLACE COLUMNS
ALTER TABLE REPLACE COLUMNS statement removes all existing columns and adds the new set of columns.
Note that this statement is only supported with v2 tables.
Syntax
ALTER TABLE table_identifier [ partition_spec ] REPLACE COLUMNS  
  [ ( ] qualified_col_type_with_position_list [ ) ]

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
Partition to be replaced. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


qualified_col_type_with_position_list
The list of the column(s) to be added
Syntax: col_name col_type [ col_comment ] [ col_position ] [ , ... ]


ADD AND DROP PARTITION
ADD PARTITION
ALTER TABLE ADD statement adds partition to the partitioned table.
If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed.
Syntax
ALTER TABLE table_identifier ADD [IF NOT EXISTS] 
    ( partition_spec [ partition_spec ... ] )

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
Partition to be added. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


DROP PARTITION
ALTER TABLE DROP statement drops the partition of the table.
If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed.
Syntax
ALTER TABLE table_identifier DROP [ IF EXISTS ] partition_spec [PURGE]

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
Partition to be dropped. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


SET AND UNSET
SET TABLE PROPERTIES
ALTER TABLE SET command is used for setting the table properties. If a particular property was already set, 
this overrides the old value with the new one.
ALTER TABLE UNSET is used to drop the table property.
Syntax
-- Set Table Properties 
ALTER TABLE table_identifier SET TBLPROPERTIES ( key1 = val1, key2 = val2, ... )

-- Unset Table Properties
ALTER TABLE table_identifier UNSET TBLPROPERTIES [ IF EXISTS ] ( key1, key2, ... )

SET SERDE
ALTER TABLE SET command is used for setting the SERDE or SERDE properties in Hive tables. If a particular property was already set, this overrides the old value with the new one.
Syntax
-- Set SERDE Properties
ALTER TABLE table_identifier [ partition_spec ]
    SET SERDEPROPERTIES ( key1 = val1, key2 = val2, ... )

ALTER TABLE table_identifier [ partition_spec ] SET SERDE serde_class_name
    [ WITH SERDEPROPERTIES ( key1 = val1, key2 = val2, ... ) ]

SET LOCATION And SET FILE FORMAT
ALTER TABLE SET command can also be used for changing the file location and file format for 
existing tables.
If the table is cached, the ALTER TABLE .. SET LOCATION command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed.
Syntax
-- Changing File Format
ALTER TABLE table_identifier [ partition_spec ] SET FILEFORMAT file_format

-- Changing File Location
ALTER TABLE table_identifier [ partition_spec ] SET LOCATION 'new_location'

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
Specifies the partition on which the property has to be set. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


SERDEPROPERTIES ( key1 = val1, key2 = val2, … )
Specifies the SERDE properties to be set.


RECOVER PARTITIONS
ALTER TABLE RECOVER PARTITIONS statement recovers all the partitions in the directory of a table and updates the Hive metastore.
Another way to recover partitions is to use MSCK REPAIR TABLE.
Syntax
ALTER TABLE table_identifier RECOVER PARTITIONS

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


Examples
-- RENAME table 
DESC student;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|                   name|   string|   NULL|
|                 rollno|      int|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

ALTER TABLE Student RENAME TO StudentInfo;

-- After Renaming the table
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|                   name|   string|   NULL|
|                 rollno|      int|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

-- RENAME partition

SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=10|
|   age=11|
|   age=12|
+---------+

ALTER TABLE default.StudentInfo PARTITION (age='10') RENAME TO PARTITION (age='15');

-- After renaming Partition
SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=11|
|   age=12|
|   age=15|
+---------+

-- Add new columns to a table
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|                   name|   string|   NULL|
|                 rollno|      int|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

ALTER TABLE StudentInfo ADD columns (LastName string, DOB timestamp);

-- After Adding New columns to the table
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|                   name|   string|   NULL|
|                 rollno|      int|   NULL|
|               LastName|   string|   NULL|
|                    DOB|timestamp|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

-- Drop columns of a table
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|                   name|   string|   NULL|
|                 rollno|      int|   NULL|
|               LastName|   string|   NULL|
|                    DOB|timestamp|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

ALTER TABLE StudentInfo DROP columns (LastName, DOB);

-- After dropping columns of the table
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|                   name|   string|   NULL|
|                 rollno|      int|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

-- Rename a column of a table
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|                   name|   string|   NULL|
|                 rollno|      int|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

ALTER TABLE StudentInfo RENAME COLUMN name TO FirstName;

-- After renaming a column of the table
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|              FirstName|   string|   NULL|
|                 rollno|      int|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

-- ALTER OR CHANGE COLUMNS
DESC StudentInfo;
+-----------------------+---------+-------+
|               col_name|data_type|comment|
+-----------------------+---------+-------+
|              FirstName|   string|   NULL|
|                 rollno|      int|   NULL|
|                    age|      int|   NULL|
|# Partition Information|         |       |
|             # col_name|data_type|comment|
|                    age|      int|   NULL|
+-----------------------+---------+-------+

ALTER TABLE StudentInfo ALTER COLUMN FirstName COMMENT "new comment";

-- After ALTER or CHANGE COLUMNS
DESC StudentInfo;
+-----------------------+---------+-----------+
|               col_name|data_type|    comment|
+-----------------------+---------+-----------+
|              FirstName|   string|new comment|
|                 rollno|      int|       NULL|
|                    age|      int|       NULL|
|# Partition Information|         |           |
|             # col_name|data_type|    comment|
|                    age|      int|       NULL|
+-----------------------+---------+-----------+

-- REPLACE COLUMNS
DESC StudentInfo;
+-----------------------+---------+-----------+
|               col_name|data_type|    comment|
+-----------------------+---------+-----------+
|              FirstName|   string|new comment|
|                 rollno|      int|       NULL|
|                    age|      int|       NULL|
|# Partition Information|         |           |
|             # col_name|data_type|    comment|
|                    age|      int|       NULL|
+-----------------------+---------+-----------+

ALTER TABLE StudentInfo REPLACE COLUMNS (name string, ID int COMMENT 'new comment');

-- After replacing COLUMNS
DESC StudentInfo;
+-----=---------+---------+-----------+
|       col_name|data_type|    comment|
+---------------+---------+-----------+
|           name|   string|       NULL|
|             ID|      int|new comment|
| # Partitioning|         |           |
|Not partitioned|         |           |
+---------------+---------+-----------+

-- Add a new partition to a table 
SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=11|
|   age=12|
|   age=15|
+---------+

ALTER TABLE StudentInfo ADD IF NOT EXISTS PARTITION (age=18);

-- After adding a new partition to the table
SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=11|
|   age=12|
|   age=15|
|   age=18|
+---------+

-- Drop a partition from the table 
SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=11|
|   age=12|
|   age=15|
|   age=18|
+---------+

ALTER TABLE StudentInfo DROP IF EXISTS PARTITION (age=18);

-- After dropping the partition of the table
SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=11|
|   age=12|
|   age=15|
+---------+

-- Adding multiple partitions to the table
SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=11|
|   age=12|
|   age=15|
+---------+

ALTER TABLE StudentInfo ADD IF NOT EXISTS PARTITION (age=18) PARTITION (age=20);

-- After adding multiple partitions to the table
SHOW PARTITIONS StudentInfo;
+---------+
|partition|
+---------+
|   age=11|
|   age=12|
|   age=15|
|   age=18|
|   age=20|
+---------+

-- Change the fileformat
ALTER TABLE loc_orc SET fileformat orc;

ALTER TABLE p1 partition (month=2, day=2) SET fileformat parquet;

-- Change the file Location
ALTER TABLE dbx.tab1 PARTITION (a='1', b='2') SET LOCATION '/path/to/part/ways'

-- SET SERDE/ SERDE Properties
ALTER TABLE test_tab SET SERDE 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

ALTER TABLE dbx.tab1 SET SERDE 'org.apache.hadoop' WITH SERDEPROPERTIES ('k' = 'v', 'kay' = 'vee')

-- SET TABLE PROPERTIES
ALTER TABLE dbx.tab1 SET TBLPROPERTIES ('winner' = 'loser');

-- SET TABLE COMMENT Using SET PROPERTIES
ALTER TABLE dbx.tab1 SET TBLPROPERTIES ('comment' = 'A table comment.');

-- Alter TABLE COMMENT Using SET PROPERTIES
ALTER TABLE dbx.tab1 SET TBLPROPERTIES ('comment' = 'This is a new comment.');

-- DROP TABLE PROPERTIES
ALTER TABLE dbx.tab1 UNSET TBLPROPERTIES ('winner');

-- RECOVER PARTITIONS
ALTER TABLE dbx.tab1 RECOVER PARTITIONS;

Related Statements

CREATE TABLE
DROP TABLE





















  




ALTER VIEW - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







ALTER VIEW
Description
The ALTER VIEW statement can alter metadata associated with the view. It can change the definition of the view, change
the name of a view to a different name, set and unset the metadata of the view by setting TBLPROPERTIES.
RENAME View
Renames the existing view. If the new view name already exists in the source database, a TableAlreadyExistsException is thrown. This operation
does not support moving the views across databases.
If the view is cached, the command clears cached data of the view and all its dependents that refer to it. View’s cache will be lazily filled when the next time the view is accessed. The command leaves view’s dependents as uncached.
Syntax
ALTER VIEW view_identifier RENAME TO view_identifier

Parameters


view_identifier
Specifies a view name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] view_name


SET View Properties
Set one or more properties of an existing view. The properties are the key value pairs. If the properties’ keys exist, 
the values are replaced with the new values. If the properties’ keys do not exist, the key value pairs are added into
the properties.
Syntax
ALTER VIEW view_identifier SET TBLPROPERTIES ( property_key = property_val [ , ... ] )

Parameters


view_identifier
Specifies a view name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] view_name


property_key
Specifies the property key. The key may consists of multiple parts separated by dot.
Syntax: [ key_part1 ] [ .key_part2 ] [ ... ]


UNSET View Properties
Drop one or more properties of an existing view. If the specified keys do not exist, an exception is thrown. Use 
IF EXISTS to avoid the exception.
Syntax
ALTER VIEW view_identifier UNSET TBLPROPERTIES [ IF EXISTS ]  ( property_key [ , ... ] )

Parameters


view_identifier
Specifies a view name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] view_name


property_key
Specifies the property key. The key may consists of multiple parts separated by dot.
Syntax: [ key_part1 ] [ .key_part2 ] [ ... ]


ALTER View AS SELECT
ALTER VIEW view_identifier AS SELECT statement changes the definition of a view. The SELECT statement must be valid,
and the view_identifier must exist.
Syntax
ALTER VIEW view_identifier AS select_statement

Note that ALTER VIEW statement does not support SET SERDE or SET SERDEPROPERTIES properties.
Parameters


view_identifier
Specifies a view name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] view_name


select_statement
Specifies the definition of the view. Check select_statement for details.


Examples
-- Rename only changes the view name.
-- The source and target databases of the view have to be the same.
-- Use qualified or unqualified name for the source and target view.
ALTER VIEW tempdb1.v1 RENAME TO tempdb1.v2;

-- Verify that the new view is created.
DESCRIBE TABLE EXTENDED tempdb1.v2;
+----------------------------+----------+-------+
|                    col_name|data_type |comment|
+----------------------------+----------+-------+
|                          c1|       int|   null|
|                          c2|    string|   null|
|                            |          |       |
|# Detailed Table Information|          |       |
|                    Database|   tempdb1|       |
|                       Table|        v2|       |
+----------------------------+----------+-------+

-- Before ALTER VIEW SET TBLPROPERTIES
DESC TABLE EXTENDED tempdb1.v2;
+----------------------------+----------+-------+
|                    col_name| data_type|comment|
+----------------------------+----------+-------+
|                          c1|       int|   null|
|                          c2|    string|   null|
|                            |          |       |
|# Detailed Table Information|          |       |
|                    Database|   tempdb1|       |
|                       Table|        v2|       |
|            Table Properties|    [....]|       |
+----------------------------+----------+-------+

-- Set properties in TBLPROPERTIES
ALTER VIEW tempdb1.v2 SET TBLPROPERTIES ('created.by.user' = "John", 'created.date' = '01-01-2001' );

-- Use `DESCRIBE TABLE EXTENDED tempdb1.v2` to verify
DESC TABLE EXTENDED tempdb1.v2;
+----------------------------+-----------------------------------------------------+-------+
|                    col_name|                                            data_type|comment|
+----------------------------+-----------------------------------------------------+-------+
|                          c1|                                                  int|   null|
|                          c2|                                               string|   null|
|                            |                                                     |       |
|# Detailed Table Information|                                                     |       |
|                    Database|                                              tempdb1|       |
|                       Table|                                                   v2|       |
|            Table Properties|[created.by.user=John, created.date=01-01-2001, ....]|       |
+----------------------------+-----------------------------------------------------+-------+

-- Remove the key `created.by.user` and `created.date` from `TBLPROPERTIES`
ALTER VIEW tempdb1.v2 UNSET TBLPROPERTIES ('created.by.user', 'created.date');

--Use `DESC TABLE EXTENDED tempdb1.v2` to verify the changes
DESC TABLE EXTENDED tempdb1.v2;
+----------------------------+----------+-------+
|                    col_name| data_type|comment|
+----------------------------+----------+-------+
|                          c1|       int|   null|
|                          c2|    string|   null|
|                            |          |       |
|# Detailed Table Information|          |       |
|                    Database|   tempdb1|       |
|                       Table|        v2|       |
|            Table Properties|    [....]|       |
+----------------------------+----------+-------+

-- Change the view definition
ALTER VIEW tempdb1.v2 AS SELECT * FROM tempdb1.v1;

-- Use `DESC TABLE EXTENDED` to verify
DESC TABLE EXTENDED tempdb1.v2;
+----------------------------+---------------------------+-------+
|                    col_name|                  data_type|comment|
+----------------------------+---------------------------+-------+
|                          c1|                        int|   null|
|                          c2|                     string|   null|
|                            |                           |       |
|# Detailed Table Information|                           |       |
|                    Database|                    tempdb1|       |
|                       Table|                         v2|       |
|                        Type|                       VIEW|       |
|                   View Text|   select * from tempdb1.v1|       |
|          View Original Text|   select * from tempdb1.v1|       |
+----------------------------+---------------------------+-------+

Related Statements

describe-table
create-view
drop-view
show-views





















  




CREATE DATABASE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CREATE DATABASE
Description
Creates a database with the specified name. If database with the same name already exists, an exception will be thrown.
Syntax
CREATE { DATABASE | SCHEMA } [ IF NOT EXISTS ] database_name
    [ COMMENT database_comment ]
    [ LOCATION database_directory ]
    [ WITH DBPROPERTIES ( property_name = property_value [ , ... ] ) ]

Parameters


database_name
Specifies the name of the database to be created.


IF NOT EXISTS
Creates a database with the given name if it does not exist. If a database with the same name already exists, nothing will happen.


database_directory
Path of the file system in which the specified database is to be created. If the specified path does not exist in the underlying file system, this command creates a directory with the path. If the location is not specified, the database will be created in the default warehouse directory, whose path is configured by the static configuration spark.sql.warehouse.dir.


database_comment
Specifies the description for the database.


WITH DBPROPERTIES ( property_name=property_value [ , … ] )
Specifies the properties for the database in key-value pairs.


Examples
-- Create database `customer_db`. This throws exception if database with name customer_db
-- already exists.
CREATE DATABASE customer_db;

-- Create database `customer_db` only if database with same name doesn't exist.
CREATE DATABASE IF NOT EXISTS customer_db;

-- Create database `customer_db` only if database with same name doesn't exist with 
-- `Comments`,`Specific Location` and `Database properties`.
CREATE DATABASE IF NOT EXISTS customer_db COMMENT 'This is customer database' LOCATION '/user'
    WITH DBPROPERTIES (ID=001, Name='John');

-- Verify that properties are set.
DESCRIBE DATABASE EXTENDED customer_db;
+-------------------------+--------------------------+
|database_description_item|database_description_value|
+-------------------------+--------------------------+
|            Database Name|               customer_db|
|              Description| This is customer database|
|                 Location|     hdfs://hacluster/user|
|               Properties|   ((ID,001), (Name,John))|
+-------------------------+--------------------------+

Related Statements

DESCRIBE DATABASE
DROP DATABASE





















  




CREATE FUNCTION - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CREATE FUNCTION
Description
The CREATE FUNCTION statement is used to create a temporary or permanent function
in Spark. Temporary functions are scoped at a session level where as permanent
functions are created in the persistent catalog and are made available to
all sessions. The resources specified in the USING clause are made available
to all executors when they are executed for the first time. In addition to the
SQL interface, spark allows users to create custom user defined scalar and
aggregate functions using Scala, Python and Java APIs. Please refer to 
Scalar UDFs and
UDAFs for more information.
Syntax
CREATE [ OR REPLACE ] [ TEMPORARY ] FUNCTION [ IF NOT EXISTS ]
    function_name AS class_name [ resource_locations ]

Parameters


OR REPLACE
If specified, the resources for the function are reloaded. This is mainly useful
  to pick up any changes made to the implementation of the function. This
  parameter is mutually exclusive to IF NOT EXISTS and can not
  be specified together.


TEMPORARY
Indicates the scope of function being created. When TEMPORARY is specified, the
  created function is valid and visible in the current session. No persistent
  entry is made in the catalog for these kind of functions.


IF NOT EXISTS
If specified, creates the function only when it does not exist. The creation
  of function succeeds (no error is thrown) if the specified function already
  exists in the system. This parameter is mutually exclusive to OR REPLACE
  and can not be specified together.


function_name
Specifies a name of function to be created. The function name may be optionally qualified with a database name.
Syntax: [ database_name. ] function_name


class_name
Specifies the name of the class that provides the implementation for function to be created.
  The implementing class should extend one of the base classes as follows:

Should extend UDF or UDAF in org.apache.hadoop.hive.ql.exec package.
Should extend AbstractGenericUDAFResolver, GenericUDF, or
GenericUDTF in org.apache.hadoop.hive.ql.udf.generic package.
Should extend UserDefinedAggregateFunction in org.apache.spark.sql.expressions package.



resource_locations
Specifies the list of resources that contain the implementation of the function
  along with its dependencies.
Syntax: USING { { (JAR | FILE | ARCHIVE) resource_uri } , ... }


Examples
-- 1. Create a simple UDF `SimpleUdf` that increments the supplied integral value by 10.
--    import org.apache.hadoop.hive.ql.exec.UDF;
--    public class SimpleUdf extends UDF {
--      public int evaluate(int value) {
--        return value + 10;
--      }
--    }
-- 2. Compile and place it in a JAR file called `SimpleUdf.jar` in /tmp.

-- Create a table called `test` and insert two rows.
CREATE TABLE test(c1 INT);
INSERT INTO test VALUES (1), (2);

-- Create a permanent function called `simple_udf`. 
CREATE FUNCTION simple_udf AS 'SimpleUdf'
    USING JAR '/tmp/SimpleUdf.jar';

-- Verify that the function is in the registry.
SHOW USER FUNCTIONS;
+------------------+
|          function|
+------------------+
|default.simple_udf|
+------------------+

-- Invoke the function. Every selected value should be incremented by 10.
SELECT simple_udf(c1) AS function_return_value FROM test;
+---------------------+
|function_return_value|
+---------------------+
|                   11|
|                   12|
+---------------------+

-- Created a temporary function.
CREATE TEMPORARY FUNCTION simple_temp_udf AS 'SimpleUdf' 
    USING JAR '/tmp/SimpleUdf.jar';

-- Verify that the newly created temporary function is in the registry.
-- Please note that the temporary function does not have a qualified
-- database associated with it.
SHOW USER FUNCTIONS;
+------------------+
|          function|
+------------------+
|default.simple_udf|
|   simple_temp_udf|
+------------------+

-- 1. Modify `SimpleUdf`'s implementation to add supplied integral value by 20.
--    import org.apache.hadoop.hive.ql.exec.UDF;
  
--    public class SimpleUdfR extends UDF {
--      public int evaluate(int value) {
--        return value + 20;
--      }
--    }
-- 2. Compile and place it in a jar file called `SimpleUdfR.jar` in /tmp.

-- Replace the implementation of `simple_udf`
CREATE OR REPLACE FUNCTION simple_udf AS 'SimpleUdfR'
    USING JAR '/tmp/SimpleUdfR.jar';

-- Invoke the function. Every selected value should be incremented by 20.
SELECT simple_udf(c1) AS function_return_value FROM test;
+---------------------+
|function_return_value|
+---------------------+
|                   21|
|                   22|
+---------------------+

Related Statements

SHOW FUNCTIONS
DESCRIBE FUNCTION
DROP FUNCTION





















  




CREATE TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CREATE TABLE
Description
CREATE TABLE statement is used to define a table in an existing database.
The CREATE statements:

CREATE TABLE USING DATA_SOURCE
CREATE TABLE USING HIVE FORMAT
CREATE TABLE LIKE

Related Statements

ALTER TABLE
DROP TABLE





















  




CREATE VIEW - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CREATE VIEW
Description
Views are based on the result-set of an SQL query. CREATE VIEW constructs
a virtual table that has no physical data therefore other operations like
ALTER VIEW and DROP VIEW only change metadata.
Syntax
CREATE [ OR REPLACE ] [ [ GLOBAL ] TEMPORARY ] VIEW [ IF NOT EXISTS ] view_identifier
    create_view_clauses AS query

Parameters


OR REPLACE
If a view of same name already exists, it will be replaced.


[ GLOBAL ] TEMPORARY
TEMPORARY views are session-scoped and will be dropped when session ends
  because it skips persisting the definition in the underlying metastore, if any.
  GLOBAL TEMPORARY views are tied to a system preserved temporary database global_temp.


IF NOT EXISTS
Creates a view if it does not exist.


view_identifier
Specifies a view name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] view_name


create_view_clauses
These clauses are optional and order insensitive. It can be of following formats.

[ ( column_name [ COMMENT column_comment ], ... ) ] to specify column-level comments.
[ COMMENT view_comment ] to specify view-level comments.
[ TBLPROPERTIES ( property_name = property_value [ , ... ] ) ] to add metadata key-value pairs.



query
A SELECT statement that constructs the view from base tables or other views.


Examples
-- Create or replace view for `experienced_employee` with comments.
CREATE OR REPLACE VIEW experienced_employee
    (ID COMMENT 'Unique identification number', Name) 
    COMMENT 'View for experienced employees'
    AS SELECT id, name FROM all_employee
        WHERE working_years > 5;

-- Create a global temporary view `subscribed_movies` if it does not exist.
CREATE GLOBAL TEMPORARY VIEW IF NOT EXISTS subscribed_movies 
    AS SELECT mo.member_id, mb.full_name, mo.movie_title
        FROM movies AS mo INNER JOIN members AS mb 
        ON mo.member_id = mb.id;

Related Statements

ALTER VIEW
DROP VIEW
SHOW VIEWS





















  




DROP DATABASE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DROP DATABASE
Description
Drop a database and delete the directory associated with the database from the file system. An 
exception will be thrown if the database does not exist in the system.
Syntax
DROP { DATABASE | SCHEMA } [ IF EXISTS ] dbname [ RESTRICT | CASCADE ]

Parameters


DATABASE | SCHEMA
DATABASE and SCHEMA mean the same thing, either of them can be used.


IF EXISTS
If specified, no exception is thrown when the database does not exist.


RESTRICT
If specified, will restrict dropping a non-empty database and is enabled by default.


CASCADE
If specified, will drop all the associated tables and functions.


Examples
-- Create `inventory_db` Database
CREATE DATABASE inventory_db COMMENT 'This database is used to maintain Inventory';

-- Drop the database and it's tables
DROP DATABASE inventory_db CASCADE;

-- Drop the database using IF EXISTS
DROP DATABASE IF EXISTS inventory_db CASCADE;

Related Statements

CREATE DATABASE
DESCRIBE DATABASE
SHOW DATABASES





















  




DROP FUNCTION - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DROP FUNCTION
Description
The DROP FUNCTION statement drops a temporary or user defined function (UDF). An exception will
be thrown if the function does not exist.
Syntax
DROP [ TEMPORARY ] FUNCTION [ IF EXISTS ] function_name

Parameters


function_name
Specifies the name of an existing function. The function name may be
  optionally qualified with a database name.
Syntax: [ database_name. ] function_name


TEMPORARY
Should be used to delete the TEMPORARY function.


IF EXISTS
If specified, no exception is thrown when the function does not exist.


Examples
-- Create a permanent function `test_avg`
CREATE FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';

-- List user functions
SHOW USER FUNCTIONS;
+----------------+
|        function|
+----------------+
|default.test_avg|
+----------------+

-- Create Temporary function `test_avg`
CREATE TEMPORARY FUNCTION test_avg AS
    'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';

-- List user functions
SHOW USER FUNCTIONS;
+----------------+
|        function|
+----------------+
|default.test_avg|
|        test_avg|
+----------------+

-- Drop Permanent function
DROP FUNCTION test_avg;

-- Try to drop Permanent function which is not present
DROP FUNCTION test_avg;
Error: Error running query:
org.apache.spark.sql.catalyst.analysis.NoSuchPermanentFunctionException:
Function 'default.test_avg' not found in database 'default'; (state=,code=0)

-- List the functions after dropping, it should list only temporary function
SHOW USER FUNCTIONS;
+--------+
|function|
+--------+
|test_avg|
+--------+
  
-- Drop Temporary function
DROP TEMPORARY FUNCTION IF EXISTS test_avg;

Related Statements

CREATE FUNCTION
DESCRIBE FUNCTION
SHOW FUNCTION





















  




DROP TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DROP TABLE
Description
DROP TABLE deletes the table and removes the directory associated with the table from the file system
if the table is not EXTERNAL table. If the table is not present it throws an exception.
In case of an external table, only the associated metadata information is removed from the metastore database.
If the table is cached, the command uncaches the table and all its dependents.
Syntax
DROP TABLE [ IF EXISTS ] table_identifier [ PURGE ]

Parameter


IF EXISTS
If specified, no exception is thrown when the table does not exist.


table_identifier
Specifies the table name to be dropped. The table name may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


PURGE
If specified, completely purge the table skipping trash while dropping table(Note: PURGE available in Hive Metastore 0.14.0 and later).


Examples
-- Assumes a table named `employeetable` exists.
DROP TABLE employeetable;

-- Assumes a table named `employeetable` exists in the `userdb` database
DROP TABLE userdb.employeetable;

-- Assumes a table named `employeetable` does not exist.
-- Throws exception
DROP TABLE employeetable;
Error: org.apache.spark.sql.AnalysisException: Table or view not found: employeetable;
(state=,code=0)

-- Assumes a table named `employeetable` does not exist,Try with IF EXISTS
-- this time it will not throw exception
DROP TABLE IF EXISTS employeetable;

-- Completely purge the table skipping trash.
DROP TABLE employeetable PURGE;

Related Statements

CREATE TABLE
CREATE DATABASE
DROP DATABASE





















  




DROP VIEW - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DROP VIEW
Description
DROP VIEW removes the metadata associated with a specified view from the catalog.
Syntax
DROP VIEW [ IF EXISTS ] view_identifier

Parameter


IF EXISTS
If specified, no exception is thrown when the view does not exist.


view_identifier
Specifies the view name to be dropped. The view name may be optionally qualified with a database name.
Syntax: [ database_name. ] view_name


Examples
-- Assumes a view named `employeeView` exists.
DROP VIEW employeeView;

-- Assumes a view named `employeeView` exists in the `userdb` database
DROP VIEW userdb.employeeView;

-- Assumes a view named `employeeView` does not exist.
-- Throws exception
DROP VIEW employeeView;
Error: org.apache.spark.sql.AnalysisException: Table or view not found: employeeView;
(state=,code=0)

-- Assumes a view named `employeeView` does not exist,Try with IF EXISTS
-- this time it will not throw exception
DROP VIEW IF EXISTS employeeView;

Related Statements

CREATE VIEW
ALTER VIEW
SHOW VIEWS
CREATE DATABASE
DROP DATABASE





















  




REPAIR TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







REPAIR TABLE
Description
REPAIR TABLE recovers all the partitions in the directory of a table and updates the Hive metastore. When creating a table using PARTITIONED BY clause, partitions are generated and registered in the Hive metastore. However, if the partitioned table is created from existing data, partitions are not registered automatically in the Hive metastore. User needs to run REPAIR TABLE to register the partitions. REPAIR TABLE on a non-existent table or a table without partitions throws an exception. Another way to recover partitions is to use ALTER TABLE RECOVER PARTITIONS. This command can also be invoked using MSCK REPAIR TABLE, for Hive compatibility.
If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed.
Syntax
[MSCK] REPAIR TABLE table_identifier [{ADD|DROP|SYNC} PARTITIONS]

Parameters


table_identifier
Specifies the name of the table to be repaired. The table name may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


{ADD|DROP|SYNC} PARTITIONS
Specifies how to recover partitions. If not specified, ADD is the default.

ADD, the command adds new partitions to the session catalog for all sub-folder in the base table folder that don’t belong to any table partitions.
DROP, the command drops all partitions from the session catalog that have non-existing locations in the file system.
SYNC is the combination of DROP and ADD.



Examples
-- create a partitioned table from existing data /tmp/namesAndAges.parquet
CREATE TABLE t1 (name STRING, age INT) USING parquet PARTITIONED BY (age)
    LOCATION "/tmp/namesAndAges.parquet";

-- SELECT * FROM t1 does not return results
SELECT * FROM t1;

-- run REPAIR TABLE to recovers all the partitions
REPAIR TABLE t1;

-- SELECT * FROM t1 returns results
SELECT * FROM t1;
+-------+---+
|   name|age|
+-------+---+
|Michael| 20|
+-------+---+
| Justin| 19|
+-------+---+
|   Andy| 30|
+-------+---+

Related Statements

ALTER TABLE





















  




TRUNCATE TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







TRUNCATE TABLE
Description
The TRUNCATE TABLE statement removes all the rows from a table or partition(s). The table must not be a view 
or an external/temporary table. In order to truncate multiple partitions at once, the user can specify the partitions 
in partition_spec. If no partition_spec is specified it will remove all partitions in the table.
If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed.
Syntax
TRUNCATE TABLE table_identifier [ partition_spec ]

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
An optional parameter that specifies a comma separated list of key and value pairs
  for partitions.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


Examples
-- Create table Student with partition
CREATE TABLE Student (name STRING, rollno INT) PARTITIONED BY (age INT);

SELECT * FROM Student;
+----+------+---+
|name|rollno|age|
+----+------+---+
| ABC|     1| 10|
| DEF|     2| 10|
| XYZ|     3| 12|
+----+------+---+

-- Removes all rows from the table in the partition specified
TRUNCATE TABLE Student partition(age=10);

-- After truncate execution, records belonging to partition age=10 are removed
SELECT * FROM Student;
+----+------+---+
|name|rollno|age|
+----+------+---+
| XYZ|     3| 12|
+----+------+---+

-- Removes all rows from the table from all partitions
TRUNCATE TABLE Student;

SELECT * FROM Student;
+----+------+---+
|name|rollno|age|
+----+------+---+
+----+------+---+

Related Statements

DROP TABLE
ALTER TABLE





















  




USE Database - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







USE Database
Description
USE statement is used to set the current database. After the current database is set,
the unqualified database artifacts such as tables, functions and views that are 
referenced by SQLs are resolved from the current database. 
The default database name is ‘default’.
Syntax
USE database_name

Parameter


database_name
Name of the database will be used. If the database does not exist, an exception will be thrown.


Examples
-- Use the 'userdb' which exists.
USE userdb;

-- Use the 'userdb1' which doesn't exist
USE userdb1;
Error: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'userdb1' not found;
(state=,code=0)

Related Statements

CREATE DATABASE
DROP DATABASE
CREATE TABLE 





















  




INSERT OVERWRITE DIRECTORY - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







INSERT OVERWRITE DIRECTORY
Description
The INSERT OVERWRITE DIRECTORY statement overwrites the existing data in the directory with the new values using either spark file format or Hive Serde. 
Hive support must be enabled to use Hive Serde. The inserted rows can be specified by value expressions or result from a query.
Syntax
INSERT OVERWRITE [ LOCAL ] DIRECTORY [ directory_path ]
    { spark_format | hive_format }
    { VALUES ( { value | NULL } [ , ... ] ) [ , ( ... ) ] | query }

While spark_format is defined as
USING file_format [ OPTIONS ( key = val [ , ... ] ) ]

hive_format is defined as
[ ROW FORMAT row_format ] [ STORED AS hive_serde ]

Parameters


directory_path
Specifies the destination directory. The LOCAL keyword is used to specify that the directory is on the local file system.
  In spark file format, it can also be specified in OPTIONS using path, but directory_path and path option can not be both specified.


file_format
Specifies the file format to use for the insert. Valid options are TEXT, CSV, JSON, JDBC, PARQUET, ORC, HIVE, LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.execution.datasources.FileFormat.


OPTIONS ( key = val [ , … ] )
Specifies one or more options for the writing of the file format.


hive_format
Specifies the file format to use for the insert. Both row_format and hive_serde are optional. ROW FORMAT SERDE can only be used with TEXTFILE, SEQUENCEFILE, or RCFILE, while ROW FORMAT DELIMITED can only be used with TEXTFILE. If both are not defined, spark uses TEXTFILE.


row_format
Specifies the row format for this insert. Valid options are SERDE clause and DELIMITED clause. SERDE clause can be used to specify a custom SerDe for this insert. Alternatively, DELIMITED clause can be used to specify the native SerDe and state the delimiter, escape character, null character, and so on.


hive_serde
Specifies the file format for this insert. Valid options are TEXTFILE, SEQUENCEFILE, RCFILE, ORC, PARQUET, and AVRO. You can also specify your own input and output format using INPUTFORMAT and OUTPUTFORMAT.


VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ]
Specifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted.
  A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.


query
A query that produces the rows to be inserted. It can be in one of following formats:

a SELECT statement
a Inline Table statement
a FROM statement



Examples
Spark format
INSERT OVERWRITE DIRECTORY '/tmp/destination'
    USING parquet
    OPTIONS (col1 1, col2 2, col3 'test')
    SELECT * FROM test_table;

INSERT OVERWRITE DIRECTORY
    USING parquet
    OPTIONS ('path' '/tmp/destination', col1 1, col2 2, col3 'test')
    SELECT * FROM test_table;

Hive format
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/destination'
    STORED AS orc
    SELECT * FROM test_table;

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/destination'
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
    SELECT * FROM test_table;

Related Statements

INSERT TABLE statement





















  




INSERT TABLE - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







INSERT TABLE
Description
The INSERT statement inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query.
Syntax
INSERT [ INTO | OVERWRITE ] [ TABLE ] table_identifier [ partition_spec ] [ ( column_list ) ]
    { VALUES ( { value | NULL } [ , ... ] ) [ , ( ... ) ] | query }

INSERT INTO [ TABLE ] table_identifier REPLACE WHERE boolean_expression query

Parameters


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
An optional parameter that specifies a comma-separated list of key and value pairs
  for partitions. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec.
Syntax: PARTITION ( partition_col_name  = partition_col_val [ , ... ] )


column_list
An optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table. Spark will reorder the columns of the input query to match the table schema according to the specified column list.
Note:The current behaviour has some limitations:

All specified columns should exist in the table and not be duplicated from each other. It includes all columns except the static partition columns.
The size of the column list should be exactly the size of the data from VALUES clause or query.



VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ]
Specifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted.
  A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows.


boolean_expression
Specifies any expression that evaluates to a result type boolean. Two or
more expressions may be combined together using the logical
operators ( AND, OR ).


query
A query that produces the rows to be inserted. It can be in one of following formats:

a SELECT statement
a Inline Table statement
a FROM statement



Examples
Insert Into
Single Row Insert Using a VALUES Clause
CREATE TABLE students (name VARCHAR(64), address VARCHAR(64))
    USING PARQUET PARTITIONED BY (student_id INT);

INSERT INTO students VALUES
    ('Amy Smith', '123 Park Ave, San Jose', 111111);

SELECT * FROM students;
+---------+----------------------+----------+
|     name|               address|student_id|
+---------+----------------------+----------+
|Amy Smith|123 Park Ave, San Jose|    111111|
+---------+----------------------+----------+

Multi-Row Insert Using a VALUES Clause
INSERT INTO students VALUES
    ('Bob Brown', '456 Taylor St, Cupertino', 222222),
    ('Cathy Johnson', '789 Race Ave, Palo Alto', 333333);

SELECT * FROM students;
+-------------+------------------------+----------+
|         name|                 address|student_id|
+-------------+------------------------+----------+
|    Amy Smith|  123 Park Ave, San Jose|    111111|
+-------------+------------------------+----------+
|    Bob Brown|456 Taylor St, Cupertino|    222222|
+-------------+------------------------+----------+
|Cathy Johnson| 789 Race Ave, Palo Alto|    333333|
+--------------+-----------------------+----------+

Insert Using a SELECT Statement
-- Assuming the persons table has already been created and populated.
SELECT * FROM persons;
+-------------+--------------------------+---------+
|         name|                   address|      ssn|
+-------------+--------------------------+---------+
|Dora Williams|134 Forest Ave, Menlo Park|123456789|
+-------------+--------------------------+---------+
|  Eddie Davis|   245 Market St, Milpitas|345678901|
+-------------+--------------------------+---------+

INSERT INTO students PARTITION (student_id = 444444)
    SELECT name, address FROM persons WHERE name = "Dora Williams";

SELECT * FROM students;
+-------------+--------------------------+----------+
|         name|                   address|student_id|
+-------------+--------------------------+----------+
|    Amy Smith|    123 Park Ave, San Jose|    111111|
+-------------+--------------------------+----------+
|    Bob Brown|  456 Taylor St, Cupertino|    222222|
+-------------+--------------------------+----------+
|Cathy Johnson|   789 Race Ave, Palo Alto|    333333|
+-------------+--------------------------+----------+
|Dora Williams|134 Forest Ave, Menlo Park|    444444|
+-------------+--------------------------+----------+

Insert Using a TABLE Statement
-- Assuming the visiting_students table has already been created and populated.
SELECT * FROM visiting_students;
+-------------+---------------------+----------+
|         name|              address|student_id|
+-------------+---------------------+----------+
|Fleur Laurent|345 Copper St, London|    777777|
+-------------+---------------------+----------+
|Gordon Martin| 779 Lake Ave, Oxford|    888888|
+-------------+---------------------+----------+

INSERT INTO students TABLE visiting_students;

SELECT * FROM students;
+-------------+--------------------------+----------+
|         name|                   address|student_id|
+-------------+--------------------------+----------+
|    Amy Smith|    123 Park Ave, San Jose|    111111|
+-------------+--------------------------+----------+
|    Bob Brown|  456 Taylor St, Cupertino|    222222|
+-------------+--------------------------+----------+
|Cathy Johnson|   789 Race Ave, Palo Alto|    333333|
+-------------+--------------------------+----------+
|Dora Williams|134 Forest Ave, Menlo Park|    444444|
+-------------+--------------------------+----------+
|Fleur Laurent|     345 Copper St, London|    777777|
+-------------+--------------------------+----------+
|Gordon Martin|      779 Lake Ave, Oxford|    888888|
+-------------+--------------------------+----------+

Insert Using a FROM Statement
-- Assuming the applicants table has already been created and populated.
SELECT * FROM applicants;
+-----------+--------------------------+----------+---------+
|       name|                   address|student_id|qualified|
+-----------+--------------------------+----------+---------+
|Helen Davis| 469 Mission St, San Diego|    999999|     true|
+-----------+--------------------------+----------+---------+
|   Ivy King|367 Leigh Ave, Santa Clara|    101010|    false|
+-----------+--------------------------+----------+---------+
| Jason Wang|     908 Bird St, Saratoga|    121212|     true|
+-----------+--------------------------+----------+---------+

INSERT INTO students
     FROM applicants SELECT name, address, student_id WHERE qualified = true;

SELECT * FROM students;
+-------------+--------------------------+----------+
|         name|                   address|student_id|
+-------------+--------------------------+----------+
|    Amy Smith|    123 Park Ave, San Jose|    111111|
+-------------+--------------------------+----------+
|    Bob Brown|  456 Taylor St, Cupertino|    222222|
+-------------+--------------------------+----------+
|Cathy Johnson|   789 Race Ave, Palo Alto|    333333|
+-------------+--------------------------+----------+
|Dora Williams|134 Forest Ave, Menlo Park|    444444|
+-------------+--------------------------+----------+
|Fleur Laurent|     345 Copper St, London|    777777|
+-------------+--------------------------+----------+
|Gordon Martin|      779 Lake Ave, Oxford|    888888|
+-------------+--------------------------+----------+
|  Helen Davis| 469 Mission St, San Diego|    999999|
+-------------+--------------------------+----------+
|   Jason Wang|     908 Bird St, Saratoga|    121212|
+-------------+--------------------------+----------+

Insert Using a Typed Date Literal for a Partition Column Value
CREATE TABLE students (name STRING, address  STRING) PARTITIONED BY (birthday DATE);

INSERT INTO students PARTITION (birthday = date'2019-01-02')
    VALUES ('Amy Smith', '123 Park Ave, San Jose');

SELECT * FROM students;
+-------------+-------------------------+-----------+
|         name|                  address|   birthday|
+-------------+-------------------------+-----------+
|    Amy Smith|   123 Park Ave, San Jose| 2019-01-02|
+-------------+-------------------------+-----------+

Insert with a column list
INSERT INTO students (address, name, student_id) VALUES
    ('Hangzhou, China', 'Kent Yao', 11215016);

SELECT * FROM students WHERE name = 'Kent Yao';
+---------+----------------------+----------+
|     name|               address|student_id|
+---------+----------------------+----------+
|Kent Yao |       Hangzhou, China|  11215016|
+---------+----------------------+----------+

Insert with both a partition spec and a column list
INSERT INTO students PARTITION (student_id = 11215017) (address, name) VALUES
    ('Hangzhou, China', 'Kent Yao Jr.');

SELECT * FROM students WHERE student_id = 11215017;
+------------+----------------------+----------+
|        name|               address|student_id|
+------------+----------------------+----------+
|Kent Yao Jr.|       Hangzhou, China|  11215017|
+------------+----------------------+----------+

Insert Overwrite
Insert Using a VALUES Clause
-- Assuming the students table has already been created and populated.
SELECT * FROM students;
+-------------+--------------------------+----------+
|         name|                   address|student_id|
+-------------+--------------------------+----------+
|    Amy Smith|    123 Park Ave, San Jose|    111111|
|    Bob Brown|  456 Taylor St, Cupertino|    222222|
|Cathy Johnson|   789 Race Ave, Palo Alto|    333333|
|Dora Williams|134 Forest Ave, Menlo Park|    444444|
|Fleur Laurent|     345 Copper St, London|    777777|
|Gordon Martin|      779 Lake Ave, Oxford|    888888|
|  Helen Davis| 469 Mission St, San Diego|    999999|
|   Jason Wang|     908 Bird St, Saratoga|    121212|
+-------------+--------------------------+----------+

INSERT OVERWRITE students VALUES
    ('Ashua Hill', '456 Erica Ct, Cupertino', 111111),
    ('Brian Reed', '723 Kern Ave, Palo Alto', 222222);

SELECT * FROM students;
+----------+-----------------------+----------+
|      name|                address|student_id|
+----------+-----------------------+----------+
|Ashua Hill|456 Erica Ct, Cupertino|    111111|
|Brian Reed|723 Kern Ave, Palo Alto|    222222|
+----------+-----------------------+----------+

Insert Using a SELECT Statement
-- Assuming the persons table has already been created and populated.
SELECT * FROM persons;
+-------------+--------------------------+---------+
|         name|                   address|      ssn|
+-------------+--------------------------+---------+
|Dora Williams|134 Forest Ave, Menlo Park|123456789|
+-------------+--------------------------+---------+
|  Eddie Davis|   245 Market St, Milpitas|345678901|
+-------------+--------------------------+---------+

INSERT OVERWRITE students PARTITION (student_id = 222222)
    SELECT name, address FROM persons WHERE name = "Dora Williams";

SELECT * FROM students;
+-------------+--------------------------+----------+
|         name|                   address|student_id|
+-------------+--------------------------+----------+
|   Ashua Hill|   456 Erica Ct, Cupertino|    111111|
+-------------+--------------------------+----------+
|Dora Williams|134 Forest Ave, Menlo Park|    222222|
+-------------+--------------------------+----------+

Insert Using a REPLACE WHERE Statement
-- Assuming the persons and persons2 table has already been created and populated.
SELECT * FROM persons;
+-------------+--------------------------+---------+
|         name|                   address|      ssn|
+-------------+--------------------------+---------+
|Dora Williams|134 Forest Ave, Menlo Park|123456789|
+-------------+--------------------------+---------+
|  Eddie Davis|   245 Market St, Milpitas|345678901|
+-------------+--------------------------+---------+
    
SELECT * FROM persons2;
+-------------+--------------------------+---------+
|         name|                   address|      ssn|
+-------------+--------------------------+---------+
|   Ashua Hill|   456 Erica Ct, Cupertino|432795921|
+-------------+--------------------------+---------+

-- in an atomic operation, 1) delete rows with ssn = 123456789 and 2) insert rows from persons2 
INSERT INTO persons REPLACE WHERE ssn = 123456789 SELECT * FROM persons2

SELECT * FROM persons;
+-------------+--------------------------+---------+
|         name|                   address|      ssn|
+-------------+--------------------------+---------+
|  Eddie Davis|   245 Market St, Milpitas|345678901|
+-------------+--------------------------+---------+
|   Ashua Hill|   456 Erica Ct, Cupertino|432795921|
+-------------+--------------------------+---------+

Insert Using a TABLE Statement
-- Assuming the visiting_students table has already been created and populated.
SELECT * FROM visiting_students;
+-------------+---------------------+----------+
|         name|              address|student_id|
+-------------+---------------------+----------+
|Fleur Laurent|345 Copper St, London|    777777|
+-------------+---------------------+----------+
|Gordon Martin| 779 Lake Ave, Oxford|    888888|
+-------------+---------------------+----------+

INSERT OVERWRITE students TABLE visiting_students;

SELECT * FROM students;
+-------------+---------------------+----------+
|         name|              address|student_id|
+-------------+---------------------+----------+
|Fleur Laurent|345 Copper St, London|    777777|
+-------------+---------------------+----------+
|Gordon Martin| 779 Lake Ave, Oxford|    888888|
+-------------+---------------------+----------+

Insert Using a FROM Statement
-- Assuming the applicants table has already been created and populated.
SELECT * FROM applicants;
+-----------+--------------------------+----------+---------+
|       name|                   address|student_id|qualified|
+-----------+--------------------------+----------+---------+
|Helen Davis| 469 Mission St, San Diego|    999999|     true|
+-----------+--------------------------+----------+---------+
|   Ivy King|367 Leigh Ave, Santa Clara|    101010|    false|
+-----------+--------------------------+----------+---------+
| Jason Wang|     908 Bird St, Saratoga|    121212|     true|
+-----------+--------------------------+----------+---------+

INSERT OVERWRITE students
    FROM applicants SELECT name, address, student_id WHERE qualified = true;

SELECT * FROM students;
+-----------+-------------------------+----------+
|       name|                  address|student_id|
+-----------+-------------------------+----------+
|Helen Davis|469 Mission St, San Diego|    999999|
+-----------+-------------------------+----------+
| Jason Wang|    908 Bird St, Saratoga|    121212|
+-----------+-------------------------+----------+

Insert Using a Typed Date Literal for a Partition Column Value
CREATE TABLE students (name STRING, address  STRING) PARTITIONED BY (birthday DATE);

INSERT INTO students PARTITION (birthday = date'2019-01-02')
    VALUES ('Amy Smith', '123 Park Ave, San Jose');

SELECT * FROM students;
+-------------+-------------------------+-----------+
|         name|                  address|   birthday|
+-------------+-------------------------+-----------+
|    Amy Smith|   123 Park Ave, San Jose| 2019-01-02|
+-------------+-------------------------+-----------+

INSERT OVERWRITE students PARTITION (birthday = date'2019-01-02')
    VALUES('Jason Wang', '908 Bird St, Saratoga');

SELECT * FROM students;
+-----------+-------------------------+-----------+
|       name|                  address|   birthday|
+-----------+-------------------------+-----------+
| Jason Wang|    908 Bird St, Saratoga| 2019-01-02|
+-----------+-------------------------+-----------+

Insert with a column list
INSERT OVERWRITE students (address, name, student_id) VALUES
    ('Hangzhou, China', 'Kent Yao', 11215016);

SELECT * FROM students WHERE name = 'Kent Yao';
+---------+----------------------+----------+
|     name|               address|student_id|
+---------+----------------------+----------+
|Kent Yao |       Hangzhou, China|  11215016|
+---------+----------------------+----------+

Insert with both a partition spec and a column list
INSERT OVERWRITE students PARTITION (student_id = 11215016) (address, name) VALUES
    ('Hangzhou, China', 'Kent Yao Jr.');

SELECT * FROM students WHERE student_id = 11215016;
+------------+----------------------+----------+
|        name|               address|student_id|
+------------+----------------------+----------+
|Kent Yao Jr.|       Hangzhou, China|  11215016|
+------------+----------------------+----------+

Related Statements

INSERT OVERWRITE DIRECTORY statement





















  




LOAD DATA - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







LOAD DATA
Description
LOAD DATA statement loads the data into a Hive serde table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the LOAD DATA statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table.
If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed.
Syntax
LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table_identifier [ partition_spec ]

Parameters


path
Path of the file system. It can be either an absolute or a relative path.


table_identifier
Specifies a table name, which may be optionally qualified with a database name.
Syntax: [ database_name. ] table_name


partition_spec
An optional parameter that specifies a comma separated list of key and value pairs
  for partitions.
Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] )


LOCAL
If specified, it causes the INPATH to be resolved against the local file system, instead of the default file system, which is typically a distributed storage.


OVERWRITE
By default, new data is appended to the table. If OVERWRITE is used, the table is instead overwritten with new data.


Examples
-- Example without partition specification.
-- Assuming the students table has already been created and populated.
SELECT * FROM students;
+---------+----------------------+----------+
|     name|               address|student_id|
+---------+----------------------+----------+
|Amy Smith|123 Park Ave, San Jose|    111111|
+---------+----------------------+----------+

CREATE TABLE test_load (name VARCHAR(64), address VARCHAR(64), student_id INT) USING HIVE;

-- Assuming the students table is in '/user/hive/warehouse/'
LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test_load;

SELECT * FROM test_load;
+---------+----------------------+----------+
|     name|               address|student_id|
+---------+----------------------+----------+
|Amy Smith|123 Park Ave, San Jose|    111111|
+---------+----------------------+----------+

-- Example with partition specification.
CREATE TABLE test_partition (c1 INT, c2 INT, c3 INT) PARTITIONED BY (c2, c3);

INSERT INTO test_partition PARTITION (c2 = 2, c3 = 3) VALUES (1);

INSERT INTO test_partition PARTITION (c2 = 5, c3 = 6) VALUES (4);

INSERT INTO test_partition PARTITION (c2 = 8, c3 = 9) VALUES (7);

SELECT * FROM test_partition;
+---+---+---+
| c1| c2| c3|
+---+---+---+
|  1|  2|  3|
|  4|  5|  6|
|  7|  8|  9|
+---+---+---+

CREATE TABLE test_load_partition (c1 INT, c2 INT, c3 INT) USING HIVE PARTITIONED BY (c2, c3);

-- Assuming the test_partition table is in '/user/hive/warehouse/'
LOAD DATA LOCAL INPATH '/user/hive/warehouse/test_partition/c2=2/c3=3'
    OVERWRITE INTO TABLE test_load_partition PARTITION (c2=2, c3=3);

SELECT * FROM test_load_partition;
+---+---+---+
| c1| c2| c3|
+---+---+---+
|  1|  2|  3|
+---+---+---+





















  




EXPLAIN - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







EXPLAIN
Description
The EXPLAIN statement is used to provide logical/physical plans for an input statement. 
By default, this clause provides information about a physical plan only.
Syntax
EXPLAIN [ EXTENDED | CODEGEN | COST | FORMATTED ] statement

Parameters


EXTENDED
Generates parsed logical plan, analyzed logical plan, optimized logical plan and physical plan.
  Parsed Logical plan is a unresolved plan that extracted from the query.
  Analyzed logical plans transforms which translates unresolvedAttribute and unresolvedRelation into fully typed objects.
  The optimized logical plan transforms through a set of optimization rules, resulting in the physical plan.


CODEGEN
Generates code for the statement, if any and a physical plan.


COST
If plan node statistics are available, generates a logical plan and the statistics.


FORMATTED
Generates two sections: a physical plan outline and node details.


statement
Specifies a SQL statement to be explained.


Examples
-- Default Output
EXPLAIN select k, sum(v) from values (1, 2), (1, 3) t(k, v) group by k;
+----------------------------------------------------+
|                                                plan|
+----------------------------------------------------+
| == Physical Plan ==
 *(2) HashAggregate(keys=[k#33], functions=[sum(cast(v#34 as bigint))])
 +- Exchange hashpartitioning(k#33, 200), true, [id=#59]
    +- *(1) HashAggregate(keys=[k#33], functions=[partial_sum(cast(v#34 as bigint))])
       +- *(1) LocalTableScan [k#33, v#34]
|
+----------------------------------------------------

-- Using Extended
EXPLAIN EXTENDED select k, sum(v) from values (1, 2), (1, 3) t(k, v) group by k;
+----------------------------------------------------+
|                                                plan|
+----------------------------------------------------+
| == Parsed Logical Plan ==
 'Aggregate ['k], ['k, unresolvedalias('sum('v), None)]
 +- 'SubqueryAlias `t`
    +- 'UnresolvedInlineTable [k, v], [List(1, 2), List(1, 3)]
   
 == Analyzed Logical Plan ==
 k: int, sum(v): bigint
 Aggregate [k#47], [k#47, sum(cast(v#48 as bigint)) AS sum(v)#50L]
 +- SubqueryAlias `t`
    +- LocalRelation [k#47, v#48]
   
 == Optimized Logical Plan ==
 Aggregate [k#47], [k#47, sum(cast(v#48 as bigint)) AS sum(v)#50L]
 +- LocalRelation [k#47, v#48]
   
 == Physical Plan ==
 *(2) HashAggregate(keys=[k#47], functions=[sum(cast(v#48 as bigint))], output=[k#47, sum(v)#50L])
+- Exchange hashpartitioning(k#47, 200), true, [id=#79]
   +- *(1) HashAggregate(keys=[k#47], functions=[partial_sum(cast(v#48 as bigint))], output=[k#47, sum#52L])
    +- *(1) LocalTableScan [k#47, v#48]
|
+----------------------------------------------------+

-- Using Formatted
EXPLAIN FORMATTED select k, sum(v) from values (1, 2), (1, 3) t(k, v) group by k;
+----------------------------------------------------+
|                                                plan|
+----------------------------------------------------+
| == Physical Plan ==
 * HashAggregate (4)
 +- Exchange (3)
    +- * HashAggregate (2)
       +- * LocalTableScan (1)
   
   
 (1) LocalTableScan [codegen id : 1]
 Output: [k#19, v#20]
        
 (2) HashAggregate [codegen id : 1]
 Input: [k#19, v#20]
        
 (3) Exchange
 Input: [k#19, sum#24L]
        
 (4) HashAggregate [codegen id : 2]
 Input: [k#19, sum#24L]
|
+----------------------------------------------------+





















  




Aggregate Functions - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Aggregate Functions
Description
Aggregate functions operate on values across rows to perform mathematical calculations such as sum, average, counting, minimum/maximum values, standard deviation, and estimation, as well as some non-mathematical operations.
Syntax
aggregate_function(input1 [, input2, ...]) FILTER (WHERE boolean_expression)

Parameters


aggregate_function
Please refer to the Built-in Aggregation Functions document for a complete list of Spark aggregate functions.


boolean_expression
Specifies any expression that evaluates to a result type boolean. Two or more expressions may be combined together using the logical operators ( AND, OR ).


Examples
Please refer to the Built-in Aggregation Functions document for all the examples of Spark aggregate functions.
Ordered-Set Aggregate Functions
These aggregate Functions use different syntax than the other aggregate functions so that to specify an expression (typically a column name) by which to order the values.
Syntax
{ PERCENTILE_CONT | PERCENTILE_DISC }(percentile) WITHIN GROUP (ORDER BY { order_by_expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ] }) FILTER (WHERE boolean_expression)

Parameters


percentile
The percentile of the value that you want to find. The percentile must be a constant between 0.0 and 1.0.


order_by_expression
The expression (typically a column name) by which to order the values before aggregating them.


boolean_expression
Specifies any expression that evaluates to a result type boolean. Two or more expressions may be combined together using the logical operators ( AND, OR ).


Examples
CREATE OR REPLACE TEMPORARY VIEW basic_pays AS SELECT * FROM VALUES
('Diane Murphy','Accounting',8435),
('Mary Patterson','Accounting',9998),
('Jeff Firrelli','Accounting',8992),
('William Patterson','Accounting',8870),
('Gerard Bondur','Accounting',11472),
('Anthony Bow','Accounting',6627),
('Leslie Jennings','IT',8113),
('Leslie Thompson','IT',5186),
('Julie Firrelli','Sales',9181),
('Steve Patterson','Sales',9441),
('Foon Yue Tseng','Sales',6660),
('George Vanauf','Sales',10563),
('Loui Bondur','SCM',10449),
('Gerard Hernandez','SCM',6949),
('Pamela Castillo','SCM',11303),
('Larry Bott','SCM',11798),
('Barry Jones','SCM',10586)
AS basic_pays(employee_name, department, salary);

SELECT * FROM basic_pays;
+-----------------+----------+------+
|    employee_name|department|salary|
+-----------------+----------+------+
|      Anthony Bow|Accounting|	6627|
|      Barry Jones|       SCM| 10586|
|     Diane Murphy|Accounting|	8435|
|   Foon Yue Tseng|     Sales|	6660|
|    George Vanauf|     Sales| 10563|
|    Gerard Bondur|Accounting| 11472|
| Gerard Hernandez|       SCM|	6949|
|    Jeff Firrelli|Accounting|	8992|
|   Julie Firrelli|     Sales|	9181|
|       Larry Bott|       SCM| 11798|
|  Leslie Jennings|        IT|	8113|
|  Leslie Thompson|        IT|	5186|
|      Loui Bondur|       SCM| 10449|
|   Mary Patterson|Accounting|	9998|
|  Pamela Castillo|       SCM| 11303|
|  Steve Patterson|     Sales|	9441|
|William Patterson|Accounting|	8870|
+-----------------+----------+------+

SELECT
    department,
    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) AS pc1,
    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) FILTER (WHERE employee_name LIKE '%Bo%') AS pc2,
    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) AS pc3,
    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) FILTER (WHERE employee_name LIKE '%Bo%') AS pc4,
    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) AS pd1,
    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) FILTER (WHERE employee_name LIKE '%Bo%') AS pd2,
    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) AS pd3,
    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) FILTER (WHERE employee_name LIKE '%Bo%') AS pd4
FROM basic_pays
GROUP BY department
ORDER BY department;
+----------+-------+--------+-------+--------+-----+-----+-----+-----+
|department|    pc1|     pc2|    pc3|     pc4|  pd1|  pd2|  pd3|  pd4|
+----------+-------+--------+-------+--------+-----+-----+-----+-----+
|Accounting|8543.75| 7838.25| 9746.5|10260.75| 8435| 6627| 9998|11472|
|        IT|5917.75|    NULL|7381.25|    NULL| 5186| NULL| 8113| NULL|
|     Sales|8550.75|    NULL| 9721.5|    NULL| 6660| NULL|10563| NULL|
|       SCM|10449.0|10786.25|11303.0|11460.75|10449|10449|11303|11798|
+----------+-------+--------+-------+--------+-----+-----+-----+-----+

Related Statements

SELECT





















  




CASE Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CASE Clause
Description
CASE clause uses a rule to return a specific result based on the specified condition, similar to if/else statements in other programming languages.
Syntax
CASE [ expression ] { WHEN boolean_expression THEN then_expression } [ ... ]
    [ ELSE else_expression ]
END

Parameters


boolean_expression
Specifies any expression that evaluates to a result type boolean. Two or
  more expressions may be combined together using the logical
  operators ( AND, OR ).


then_expression
Specifies the then expression based on the boolean_expression condition; then_expression and else_expression should all be same type or coercible to a common type.


else_expression
Specifies the default expression; then_expression and else_expression should all be same type or coercible to a common type.


Examples
CREATE TABLE person (id INT, name STRING, age INT);
INSERT INTO person VALUES
    (100, 'John', 30),
    (200, 'Mary', NULL),
    (300, 'Mike', 80),
    (400, 'Dan', 50);

SELECT id, CASE WHEN id > 200 THEN 'bigger' ELSE 'small' END FROM person;
+------+--------------------------------------------------+
|  id  | CASE WHEN (id > 200) THEN bigger ELSE small END  |
+------+--------------------------------------------------+
| 100  | small                                            |
| 200  | small                                            |
| 300  | bigger                                           |
| 400  | bigger                                           |
+------+--------------------------------------------------+

SELECT id, CASE id WHEN 100 then 'bigger' WHEN  id > 300 THEN '300' ELSE 'small' END FROM person;
+------+-----------------------------------------------------------------------------------------------+
|  id  | CASE WHEN (id = 100) THEN bigger WHEN (id = CAST((id > 300) AS INT)) THEN 300 ELSE small END  |
+------+-----------------------------------------------------------------------------------------------+
| 100  | bigger                                                                                        |
| 200  | small                                                                                         |
| 300  | small                                                                                         |
| 400  | small                                                                                         |
+------+-----------------------------------------------------------------------------------------------+

SELECT * FROM person
    WHERE 
        CASE 1 = 1 
            WHEN 100 THEN 'big' 
            WHEN 200 THEN 'bigger'
            WHEN 300 THEN 'biggest' 
            ELSE 'small'
        END = 'small';
+------+-------+-------+
|  id  | name  |  age  |
+------+-------+-------+
| 100  | John  | 30    |
| 200  | Mary  | NULL  |
| 300  | Mike  | 80    |
| 400  | Dan   | 50    |
+------+-------+-------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




CLUSTER BY Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







CLUSTER BY Clause
Description
The CLUSTER BY clause is used to first repartition the data based
on the input expressions and then sort the data within each partition. This is
semantically equivalent to performing a
DISTRIBUTE BY followed by a
SORT BY. This clause only ensures that the
resultant rows are sorted within each partition and does not guarantee a total order of output.
Syntax
CLUSTER BY { expression [ , ... ] }

Parameters


expression
Specifies combination of one or more values, operators and SQL functions that results in a value.


Examples
CREATE TABLE person (name STRING, age INT);
INSERT INTO person VALUES
    ('Zen Hui', 25),
    ('Anil B', 18),
    ('Shone S', 16),
    ('Mike A', 25),
    ('John A', 18),
    ('Jack N', 16);

-- Reduce the number of shuffle partitions to 2 to illustrate the behavior of `CLUSTER BY`.
-- It's easier to see the clustering and sorting behavior with less number of partitions.
SET spark.sql.shuffle.partitions = 2;

-- Select the rows with no ordering. Please note that without any sort directive, the results
-- of the query is not deterministic. It's included here to show the difference in behavior
-- of a query when `CLUSTER BY` is not used vs when it's used. The query below produces rows
-- where age column is not sorted.
SELECT age, name FROM person;
+---+-------+
|age|   name|
+---+-------+
| 16|Shone S|
| 25|Zen Hui|
| 16| Jack N|
| 25| Mike A|
| 18| John A|
| 18| Anil B|
+---+-------+

-- Produces rows clustered by age. Persons with same age are clustered together.
-- In the query below, persons with age 18 and 25 are in first partition and the
-- persons with age 16 are in the second partition. The rows are sorted based
-- on age within each partition.
SELECT age, name FROM person CLUSTER BY age;
+---+-------+
|age|   name|
+---+-------+
| 18| John A|
| 18| Anil B|
| 25|Zen Hui|
| 25| Mike A|
| 16|Shone S|
| 16| Jack N|
+---+-------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




Common Table Expression (CTE) - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Common Table Expression (CTE)
Description
A common table expression (CTE) defines a temporary result set that a user can reference possibly multiple times within the scope of a SQL statement. A CTE is used mainly in a SELECT statement.
Syntax
WITH common_table_expression [ , ... ]

While common_table_expression is defined as
expression_name [ ( column_name [ , ... ] ) ] [ AS ] ( query )

Parameters


expression_name
Specifies a name for the common table expression.


query
A SELECT statement.


Examples
-- CTE with multiple column aliases
WITH t(x, y) AS (SELECT 1, 2)
SELECT * FROM t WHERE x = 1 AND y = 2;
+---+---+
|  x|  y|
+---+---+
|  1|  2|
+---+---+

-- CTE in CTE definition
WITH t AS (
    WITH t2 AS (SELECT 1)
    SELECT * FROM t2
)
SELECT * FROM t;
+---+
|  1|
+---+
|  1|
+---+

-- CTE in subquery
SELECT max(c) FROM (
    WITH t(c) AS (SELECT 1)
    SELECT * FROM t
);
+------+
|max(c)|
+------+
|     1|
+------+

-- CTE in subquery expression
SELECT (
    WITH t AS (SELECT 1)
    SELECT * FROM t
);
+----------------+
|scalarsubquery()|
+----------------+
|               1|
+----------------+

-- CTE in CREATE VIEW statement
CREATE VIEW v AS
    WITH t(a, b, c, d) AS (SELECT 1, 2, 3, 4)
    SELECT * FROM t;
SELECT * FROM v;
+---+---+---+---+
|  a|  b|  c|  d|
+---+---+---+---+
|  1|  2|  3|  4|
+---+---+---+---+

-- If name conflict is detected in nested CTE, then AnalysisException is thrown by default.
-- SET spark.sql.legacy.ctePrecedencePolicy = CORRECTED (which is recommended),
-- inner CTE definitions take precedence over outer definitions.
SET spark.sql.legacy.ctePrecedencePolicy = CORRECTED;
WITH
    t AS (SELECT 1),
    t2 AS (
        WITH t AS (SELECT 2)
        SELECT * FROM t
    )
SELECT * FROM t2;
+---+
|  2|
+---+
|  2|
+---+

Related Statements

SELECT





















  




DISTRIBUTE BY Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







DISTRIBUTE BY Clause
Description
The DISTRIBUTE BY clause is used to repartition the data based
on the input expressions. Unlike the CLUSTER BY
clause, this does not sort the data within each partition.
Syntax
DISTRIBUTE BY { expression [ , ... ] }

Parameters


expression
Specifies combination of one or more values, operators and SQL functions that results in a value.


Examples
CREATE TABLE person (name STRING, age INT);
INSERT INTO person VALUES
    ('Zen Hui', 25),
    ('Anil B', 18),
    ('Shone S', 16),
    ('Mike A', 25),
    ('John A', 18),
    ('Jack N', 16);

-- Reduce the number of shuffle partitions to 2 to illustrate the behavior of `DISTRIBUTE BY`.
-- It's easier to see the clustering and sorting behavior with less number of partitions.
SET spark.sql.shuffle.partitions = 2;

-- Select the rows with no ordering. Please note that without any sort directive, the result
-- of the query is not deterministic. It's included here to just contrast it with the
-- behavior of `DISTRIBUTE BY`. The query below produces rows where age columns are not
-- clustered together.
SELECT age, name FROM person;
+---+-------+
|age|   name|
+---+-------+
| 16|Shone S|
| 25|Zen Hui|
| 16| Jack N|
| 25| Mike A|
| 18| John A|
| 18| Anil B|
+---+-------+

-- Produces rows clustered by age. Persons with same age are clustered together.
-- Unlike `CLUSTER BY` clause, the rows are not sorted within a partition.
SELECT age, name FROM person DISTRIBUTE BY age;
+---+-------+
|age|   name|
+---+-------+
| 25|Zen Hui|
| 25| Mike A|
| 18| John A|
| 18| Anil B|
| 16|Shone S|
| 16| Jack N|
+---+-------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
CLUSTER BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




File - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







File
Description
You can query a file with a specified format directly with SQL.
Syntax
file_format.`file_path`

Parameters


file_format
Specifies a file format for a given file path, could be TEXTFILE, ORC, PARQUET, etc.


file_path
Specifies a file path with a given format.


Examples
-- PARQUET file
SELECT * FROM parquet.`examples/src/main/resources/users.parquet`;
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+

-- ORC file
SELECT * FROM orc.`examples/src/main/resources/users.orc`;
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+

-- JSON file
SELECT * FROM json.`examples/src/main/resources/people.json`;
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

Related Statements

SELECT





















  




GROUP BY Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







GROUP BY Clause
Description
The GROUP BY clause is used to group the rows based on a set of specified grouping expressions and compute aggregations on
the group of rows based on one or more specified aggregate functions. Spark also supports advanced aggregations to do multiple
aggregations for the same input record set via GROUPING SETS, CUBE, ROLLUP clauses.
The grouping expressions and advanced aggregations can be mixed in the GROUP BY clause and nested in a GROUPING SETS clause.
See more details in the Mixed/Nested Grouping Analytics section. When a FILTER clause is attached to
an aggregate function, only the matching rows are passed to that function.
Syntax
GROUP BY group_expression [ , group_expression [ , ... ] ] [ WITH { ROLLUP | CUBE } ]

GROUP BY { group_expression | { ROLLUP | CUBE | GROUPING SETS } (grouping_set [ , ...]) } [ , ... ]

While aggregate functions are defined as
aggregate_name ( [ DISTINCT ] expression [ , ... ] ) [ FILTER ( WHERE boolean_expression ) ]

Parameters


group_expression
Specifies the criteria based on which the rows are grouped together. The grouping of rows is performed based on
  result values of the grouping expressions. A grouping expression may be a column name like GROUP BY a, a column position like
  GROUP BY 0, or an expression like GROUP BY a + b.


grouping_set
A grouping set is specified by zero or more comma-separated expressions in parentheses. When the
  grouping set has only one element, parentheses can be omitted. For example, GROUPING SETS ((a), (b))
  is the same as GROUPING SETS (a, b).
Syntax: { ( [ expression [ , ... ] ] ) | expression }


GROUPING SETS
Groups the rows for each grouping set specified after GROUPING SETS. For example,
  GROUP BY GROUPING SETS ((warehouse), (product)) is semantically equivalent
  to union of results of GROUP BY warehouse and GROUP BY product. This clause
  is a shorthand for a UNION ALL where each leg of the UNION ALL
  operator performs aggregation of each grouping set specified in the GROUPING SETS clause.
  Similarly, GROUP BY GROUPING SETS ((warehouse, product), (product), ()) is semantically
  equivalent to the union of results of GROUP BY warehouse, product, GROUP BY product
  and global aggregate.
Note: For Hive compatibility Spark allows GROUP BY ... GROUPING SETS (...). The GROUP BY
  expressions are usually ignored, but if it contains extra expressions than the GROUPING SETS
  expressions, the extra expressions will be included in the grouping expressions and the value
  is always null. For example, SELECT a, b, c FROM ... GROUP BY a, b, c GROUPING SETS (a, b),
  the output of column c is always null.


ROLLUP
Specifies multiple levels of aggregations in a single statement. This clause is used to compute aggregations
  based on multiple grouping sets. ROLLUP is a shorthand for GROUPING SETS. For example,
  GROUP BY warehouse, product WITH ROLLUP or GROUP BY ROLLUP(warehouse, product) is equivalent to
  GROUP BY GROUPING SETS((warehouse, product), (warehouse), ()).
  GROUP BY ROLLUP(warehouse, product, (warehouse, location)) is equivalent to
  GROUP BY GROUPING SETS((warehouse, product, location), (warehouse, product), (warehouse), ()).
  The N elements of a ROLLUP specification results in N+1 GROUPING SETS.


CUBE
CUBE clause is used to perform aggregations based on combination of grouping columns specified in the
  GROUP BY clause. CUBE is a shorthand for GROUPING SETS. For example,
  GROUP BY warehouse, product WITH CUBE or GROUP BY CUBE(warehouse, product) is equivalent to 
  GROUP BY GROUPING SETS((warehouse, product), (warehouse), (product), ()).
  GROUP BY CUBE(warehouse, product, (warehouse, location)) is equivalent to
  GROUP BY GROUPING SETS((warehouse, product, location), (warehouse, product), (warehouse, location),
   (product, warehouse, location), (warehouse), (product), (warehouse, product), ()).
  The N elements of a CUBE specification results in 2^N GROUPING SETS.


Mixed/Nested Grouping Analytics
A GROUP BY clause can include multiple group_expressions and multiple CUBE|ROLLUP|GROUPING SETSs.
  GROUPING SETS can also have nested CUBE|ROLLUP|GROUPING SETS clauses, e.g.
  GROUPING SETS(ROLLUP(warehouse, location), CUBE(warehouse, location)),
  GROUPING SETS(warehouse, GROUPING SETS(location, GROUPING SETS(ROLLUP(warehouse, location), CUBE(warehouse, location)))).
  CUBE|ROLLUP is just a syntax sugar for GROUPING SETS, please refer to the sections above for
  how to translate CUBE|ROLLUP to GROUPING SETS. group_expression can be treated as a single-group
  GROUPING SETS under this context. For multiple GROUPING SETS in the GROUP BY clause, we generate
  a single GROUPING SETS by doing a cross-product of the original GROUPING SETSs. For nested GROUPING SETS in the GROUPING SETS clause,
  we simply take its grouping sets and strip it. For example,
  GROUP BY warehouse, GROUPING SETS((product), ()), GROUPING SETS((location, size), (location), (size), ())
  and GROUP BY warehouse, ROLLUP(product), CUBE(location, size) is equivalent to 
  GROUP BY GROUPING SETS(
      (warehouse, product, location, size), 
      (warehouse, product, location),
      (warehouse, product, size), 
      (warehouse, product),
      (warehouse, location, size),
      (warehouse, location),
      (warehouse, size),
      (warehouse)).
GROUP BY GROUPING SETS(GROUPING SETS(warehouse), GROUPING SETS((warehouse, product))) is equivalent to 
  GROUP BY GROUPING SETS((warehouse), (warehouse, product)).


aggregate_name
Specifies an aggregate function name (MIN, MAX, COUNT, SUM, AVG, etc.).


DISTINCT
Removes duplicates in input rows before they are passed to aggregate functions.


FILTER
Filters the input rows for which the boolean_expression in the WHERE clause evaluates
  to true are passed to the aggregate function; other rows are discarded.


Examples
CREATE TABLE dealer (id INT, city STRING, car_model STRING, quantity INT);
INSERT INTO dealer VALUES
    (100, 'Fremont', 'Honda Civic', 10),
    (100, 'Fremont', 'Honda Accord', 15),
    (100, 'Fremont', 'Honda CRV', 7),
    (200, 'Dublin', 'Honda Civic', 20),
    (200, 'Dublin', 'Honda Accord', 10),
    (200, 'Dublin', 'Honda CRV', 3),
    (300, 'San Jose', 'Honda Civic', 5),
    (300, 'San Jose', 'Honda Accord', 8);

-- Sum of quantity per dealership. Group by `id`.
SELECT id, sum(quantity) FROM dealer GROUP BY id ORDER BY id;
+---+-------------+
| id|sum(quantity)|
+---+-------------+
|100|           32|
|200|           33|
|300|           13|
+---+-------------+

-- Use column position in GROUP by clause.
SELECT id, sum(quantity) FROM dealer GROUP BY 1 ORDER BY 1;
+---+-------------+
| id|sum(quantity)|
+---+-------------+
|100|           32|
|200|           33|
|300|           13|
+---+-------------+

-- Multiple aggregations.
-- 1. Sum of quantity per dealership.
-- 2. Max quantity per dealership.
SELECT id, sum(quantity) AS sum, max(quantity) AS max FROM dealer GROUP BY id ORDER BY id;
+---+---+---+
| id|sum|max|
+---+---+---+
|100| 32| 15|
|200| 33| 20|
|300| 13|  8|
+---+---+---+

-- Count the number of distinct dealer cities per car_model.
SELECT car_model, count(DISTINCT city) AS count FROM dealer GROUP BY car_model;
+------------+-----+
|   car_model|count|
+------------+-----+
| Honda Civic|    3|
|   Honda CRV|    2|
|Honda Accord|    3|
+------------+-----+

-- Sum of only 'Honda Civic' and 'Honda CRV' quantities per dealership.
SELECT id, sum(quantity) FILTER (
            WHERE car_model IN ('Honda Civic', 'Honda CRV')
        ) AS `sum(quantity)` FROM dealer
    GROUP BY id ORDER BY id;
+---+-------------+
| id|sum(quantity)|
+---+-------------+
|100|           17|
|200|           23|
|300|            5|
+---+-------------+

-- Aggregations using multiple sets of grouping columns in a single statement.
-- Following performs aggregations based on four sets of grouping columns.
-- 1. city, car_model
-- 2. city
-- 3. car_model
-- 4. Empty grouping set. Returns quantities for all city and car models.
SELECT city, car_model, sum(quantity) AS sum FROM dealer
    GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())
    ORDER BY city;
+---------+------------+---+
|     city|   car_model|sum|
+---------+------------+---+
|     null|        null| 78|
|     null| HondaAccord| 33|
|     null|    HondaCRV| 10|
|     null|  HondaCivic| 35|
|   Dublin|        null| 33|
|   Dublin| HondaAccord| 10|
|   Dublin|    HondaCRV|  3|
|   Dublin|  HondaCivic| 20|
|  Fremont|        null| 32|
|  Fremont| HondaAccord| 15|
|  Fremont|    HondaCRV|  7|
|  Fremont|  HondaCivic| 10|
| San Jose|        null| 13|
| San Jose| HondaAccord|  8|
| San Jose|  HondaCivic|  5|
+---------+------------+---+

-- Group by processing with `ROLLUP` clause.
-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), ())
SELECT city, car_model, sum(quantity) AS sum FROM dealer
    GROUP BY city, car_model WITH ROLLUP
    ORDER BY city, car_model;
+---------+------------+---+
|     city|   car_model|sum|
+---------+------------+---+
|     null|        null| 78|
|   Dublin|        null| 33|
|   Dublin| HondaAccord| 10|
|   Dublin|    HondaCRV|  3|
|   Dublin|  HondaCivic| 20|
|  Fremont|        null| 32|
|  Fremont| HondaAccord| 15|
|  Fremont|    HondaCRV|  7|
|  Fremont|  HondaCivic| 10|
| San Jose|        null| 13|
| San Jose| HondaAccord|  8|
| San Jose|  HondaCivic|  5|
+---------+------------+---+

-- Group by processing with `CUBE` clause.
-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())
SELECT city, car_model, sum(quantity) AS sum FROM dealer
    GROUP BY city, car_model WITH CUBE
    ORDER BY city, car_model;
+---------+------------+---+
|     city|   car_model|sum|
+---------+------------+---+
|     null|        null| 78|
|     null| HondaAccord| 33|
|     null|    HondaCRV| 10|
|     null|  HondaCivic| 35|
|   Dublin|        null| 33|
|   Dublin| HondaAccord| 10|
|   Dublin|    HondaCRV|  3|
|   Dublin|  HondaCivic| 20|
|  Fremont|        null| 32|
|  Fremont| HondaAccord| 15|
|  Fremont|    HondaCRV|  7|
|  Fremont|  HondaCivic| 10|
| San Jose|        null| 13|
| San Jose| HondaAccord|  8|
| San Jose|  HondaCivic|  5|
+---------+------------+---+

--Prepare data for ignore nulls example
CREATE TABLE person (id INT, name STRING, age INT);
INSERT INTO person VALUES
    (100, 'Mary', NULL),
    (200, 'John', 30),
    (300, 'Mike', 80),
    (400, 'Dan', 50);

--Select the first row in column age
SELECT FIRST(age) FROM person;
+--------------------+
| first(age, false)  |
+--------------------+
| NULL               |
+--------------------+

--Get the first row in column `age` ignore nulls,last row in column `id` and sum of column `id`.
SELECT FIRST(age IGNORE NULLS), LAST(id), SUM(id) FROM person;
+-------------------+------------------+----------+
| first(age, true)  | last(id, false)  | sum(id)  |
+-------------------+------------------+----------+
| 30                | 400              | 1000     |
+-------------------+------------------+----------+

Related Statements

SELECT Main
WHERE Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




HAVING Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







HAVING Clause
Description
The HAVING clause is used to filter the results produced by
GROUP BY based on the specified condition. It is often used
in conjunction with a GROUP BY
clause.
Syntax
HAVING boolean_expression

Parameters


boolean_expression
Specifies any expression that evaluates to a result type boolean. Two or
  more expressions may be combined together using the logical
  operators ( AND, OR ).
Note
The expressions specified in the HAVING clause can only refer to:

Constants
Expressions that appear in GROUP BY
Aggregate functions



Examples
CREATE TABLE dealer (id INT, city STRING, car_model STRING, quantity INT);
INSERT INTO dealer VALUES
    (100, 'Fremont', 'Honda Civic', 10),
    (100, 'Fremont', 'Honda Accord', 15),
    (100, 'Fremont', 'Honda CRV', 7),
    (200, 'Dublin', 'Honda Civic', 20),
    (200, 'Dublin', 'Honda Accord', 10),
    (200, 'Dublin', 'Honda CRV', 3),
    (300, 'San Jose', 'Honda Civic', 5),
    (300, 'San Jose', 'Honda Accord', 8);

-- `HAVING` clause referring to column in `GROUP BY`.
SELECT city, sum(quantity) AS sum FROM dealer GROUP BY city HAVING city = 'Fremont';
+-------+---+
|   city|sum|
+-------+---+
|Fremont| 32|
+-------+---+

-- `HAVING` clause referring to aggregate function.
SELECT city, sum(quantity) AS sum FROM dealer GROUP BY city HAVING sum(quantity) > 15;
+-------+---+
|   city|sum|
+-------+---+
| Dublin| 33|
|Fremont| 32|
+-------+---+

-- `HAVING` clause referring to aggregate function by its alias.
SELECT city, sum(quantity) AS sum FROM dealer GROUP BY city HAVING sum > 15;
+-------+---+
|   city|sum|
+-------+---+
| Dublin| 33|
|Fremont| 32|
+-------+---+

-- `HAVING` clause referring to a different aggregate function than what is present in
-- `SELECT` list.
SELECT city, sum(quantity) AS sum FROM dealer GROUP BY city HAVING max(quantity) > 15;
+------+---+
|  city|sum|
+------+---+
|Dublin| 33|
+------+---+

-- `HAVING` clause referring to constant expression.
SELECT city, sum(quantity) AS sum FROM dealer GROUP BY city HAVING 1 > 0 ORDER BY city;
+--------+---+
|    city|sum|
+--------+---+
|  Dublin| 33|
| Fremont| 32|
|San Jose| 13|
+--------+---+

-- `HAVING` clause without a `GROUP BY` clause.
SELECT sum(quantity) AS sum FROM dealer HAVING sum(quantity) > 10;
+---+
|sum|
+---+
| 78|
+---+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
ORDER BY Clause
SORT BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




Hints - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Hints
Description
Hints give users a way to suggest how Spark SQL to use specific approaches to generate its execution plan.
Syntax
/*+ hint [ , ... ] */

Partitioning Hints
Partitioning hints allow users to suggest a partitioning strategy that Spark should follow. COALESCE, REPARTITION,
and REPARTITION_BY_RANGE hints are supported and are equivalent to coalesce, repartition, and
repartitionByRange Dataset APIs, respectively. The REBALANCE can only
be used as a hint .These hints give users a way to tune performance and control the number of output files in Spark SQL.
When multiple partitioning hints are specified, multiple nodes are inserted into the logical plan, but the leftmost hint
is picked by the optimizer.
Partitioning Hints Types


COALESCE
The COALESCE hint can be used to reduce the number of partitions to the specified number of partitions. It takes a partition number as a parameter.


REPARTITION
The REPARTITION hint can be used to repartition to the specified number of partitions using the specified partitioning expressions. It takes a partition number, column names, or both as parameters.


REPARTITION_BY_RANGE
The REPARTITION_BY_RANGE hint can be used to repartition to the specified number of partitions using the specified partitioning expressions. It takes column names and an optional partition number as parameters.


REBALANCE
The REBALANCE hint can be used to rebalance the query result output partitions, so that every partition is of a reasonable size (not too small and not too big). It can take column names as parameters, and try its best to partition the query result by these columns. This is a best-effort: if there are skews, Spark will split the skewed partitions, to make these partitions not too big. This hint is useful when you need to write the result of this query to a table, to avoid too small/big files. This hint is ignored if AQE is not enabled.


Examples
SELECT /*+ COALESCE(3) */ * FROM t;

SELECT /*+ REPARTITION(3) */ * FROM t;

SELECT /*+ REPARTITION(c) */ * FROM t;

SELECT /*+ REPARTITION(3, c) */ * FROM t;

SELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t;

SELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t;

SELECT /*+ REBALANCE */ * FROM t;

SELECT /*+ REBALANCE(3) */ * FROM t;

SELECT /*+ REBALANCE(c) */ * FROM t;

SELECT /*+ REBALANCE(3, c) */ * FROM t;

-- multiple partitioning hints
EXPLAIN EXTENDED SELECT /*+ REPARTITION(100), COALESCE(500), REPARTITION_BY_RANGE(3, c) */ * FROM t;
== Parsed Logical Plan ==
'UnresolvedHint REPARTITION, [100]
+- 'UnresolvedHint COALESCE, [500]
   +- 'UnresolvedHint REPARTITION_BY_RANGE, [3, 'c]
      +- 'Project [*]
         +- 'UnresolvedRelation [t]

== Analyzed Logical Plan ==
name: string, c: int
Repartition 100, true
+- Repartition 500, false
   +- RepartitionByExpression [c#30 ASC NULLS FIRST], 3
      +- Project [name#29, c#30]
         +- SubqueryAlias spark_catalog.default.t
            +- Relation[name#29,c#30] parquet

== Optimized Logical Plan ==
Repartition 100, true
+- Relation[name#29,c#30] parquet

== Physical Plan ==
Exchange RoundRobinPartitioning(100), false, [id=#121]
+- *(1) ColumnarToRow
   +- FileScan parquet default.t[name#29,c#30] Batched: true, DataFilters: [], Format: Parquet,
      Location: CatalogFileIndex[file:/spark/spark-warehouse/t], PartitionFilters: [],
      PushedFilters: [], ReadSchema: struct<name:string>

Join Hints
Join hints allow users to suggest the join strategy that Spark should use. Prior to Spark 3.0, only the BROADCAST Join Hint was supported. MERGE, SHUFFLE_HASH and SHUFFLE_REPLICATE_NL Joint Hints support was added in 3.0. When different join strategy hints are specified on both sides of a join, Spark prioritizes hints in the following order: BROADCAST over MERGE over SHUFFLE_HASH over SHUFFLE_REPLICATE_NL. When both sides are specified with the BROADCAST hint or the SHUFFLE_HASH hint, Spark will pick the build side based on the join type and the sizes of the relations. Since a given strategy may not support all join types, Spark is not guaranteed to use the join strategy suggested by the hint.
Join Hints Types


BROADCAST
Suggests that Spark use broadcast join. The join side with the hint will be broadcast regardless of autoBroadcastJoinThreshold. If both sides of the join have the broadcast hints, the one with the smaller size (based on stats) will be broadcast. The aliases for BROADCAST are BROADCASTJOIN and MAPJOIN.


MERGE
Suggests that Spark use shuffle sort merge join. The aliases for MERGE are SHUFFLE_MERGE and MERGEJOIN.


SHUFFLE_HASH
Suggests that Spark use shuffle hash join. If both sides have the shuffle hash hints, Spark chooses the smaller side (based on stats) as the build side.


SHUFFLE_REPLICATE_NL
Suggests that Spark use shuffle-and-replicate nested loop join.


Examples
-- Join Hints for broadcast join
SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
SELECT /*+ BROADCASTJOIN (t1) */ * FROM t1 left JOIN t2 ON t1.key = t2.key;
SELECT /*+ MAPJOIN(t2) */ * FROM t1 right JOIN t2 ON t1.key = t2.key;

-- Join Hints for shuffle sort merge join
SELECT /*+ SHUFFLE_MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
SELECT /*+ MERGEJOIN(t2) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;
SELECT /*+ MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;

-- Join Hints for shuffle hash join
SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;

-- Join Hints for shuffle-and-replicate nested loop join
SELECT /*+ SHUFFLE_REPLICATE_NL(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;

-- When different join strategy hints are specified on both sides of a join, Spark
-- prioritizes the BROADCAST hint over the MERGE hint over the SHUFFLE_HASH hint
-- over the SHUFFLE_REPLICATE_NL hint.
-- Spark will issue Warning in the following example
-- org.apache.spark.sql.catalyst.analysis.HintErrorLogger: Hint (strategy=merge)
-- is overridden by another hint and will not take effect.
SELECT /*+ BROADCAST(t1), MERGE(t1, t2) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key;

Related Statements

JOIN
SELECT





















  




Inline Table - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Inline Table
Description
An inline table is a temporary table created using a VALUES clause.
Syntax
VALUES ( expression [ , ... ] ) [ table_alias ]

Parameters


expression
Specifies a combination of one or more values, operators and SQL functions that results in a value.


table_alias
Specifies a temporary name with an optional column name list.
Syntax: [ AS ] table_name [ ( column_name [ , ... ] ) ]


Examples
-- single row, without a table alias
SELECT * FROM VALUES ("one", 1);
+----+----+
|col1|col2|
+----+----+
| one|   1|
+----+----+

-- three rows with a table alias
SELECT * FROM VALUES ("one", 1), ("two", 2), ("three", null) AS data(a, b);
+-----+----+
|    a|   b|
+-----+----+
|  one|   1|
|  two|   2|
|three|null|
+-----+----+

-- complex types with a table alias
SELECT * FROM VALUES ("one", array(0, 1)), ("two", array(2, 3)) AS data(a, b);
+---+------+
|  a|     b|
+---+------+
|one|[0, 1]|
|two|[2, 3]|
+---+------+

Related Statements

SELECT





















  




JOIN - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







JOIN
Description
A SQL join is used to combine rows from two relations based on join criteria. The following section describes the overall join syntax and the sub-sections cover different types of joins along with examples.
Syntax
relation { [ join_type ] JOIN [ LATERAL ] relation [ join_criteria ] | NATURAL join_type JOIN [ LATERAL ] relation }

Parameters


relation
Specifies the relation to be joined.


join_type
Specifies the join type.
Syntax:
[ INNER ] | CROSS | LEFT [ OUTER ] | [ LEFT ] SEMI | RIGHT [ OUTER ] | FULL [ OUTER ] | [ LEFT ] ANTI


join_criteria
Specifies how the rows from one relation will be combined with the rows of another relation.
Syntax: ON boolean_expression | USING ( column_name [ , ... ] )
boolean_expression
Specifies an expression with a return type of boolean.


Join Types
Inner Join
The inner join is the default join in Spark SQL. It selects rows that have matching values in both relations.
Syntax:
relation [ INNER ] JOIN relation [ join_criteria ]
Left Join
A left join returns all values from the left relation and the matched values from the right relation, or appends NULL if there is no match. It is also referred to as a left outer join.
Syntax:
relation LEFT [ OUTER ] JOIN relation [ join_criteria ]
Right Join
A right join returns all values from the right relation and the matched values from the left relation, or appends NULL if there is no match. It is also referred to as a right outer join.
Syntax:
relation RIGHT [ OUTER ] JOIN relation [ join_criteria ]
Full Join
A full join returns all values from both relations, appending NULL values on the side that does not have a match. It is also referred to as a full outer join.
Syntax:
relation FULL [ OUTER ] JOIN relation [ join_criteria ]
Cross Join
A cross join returns the Cartesian product of two relations.
Syntax:
relation CROSS JOIN relation [ join_criteria ]
Semi Join
A semi join returns values from the left side of the relation that has a match with the right. It is also referred to as a left semi join.
Syntax:
relation [ LEFT ] SEMI JOIN relation [ join_criteria ]
Anti Join
An anti join returns values from the left relation that has no match with the right. It is also referred to as a left anti join.
Syntax:
relation [ LEFT ] ANTI JOIN relation [ join_criteria ]
Examples
-- Use employee and department tables to demonstrate different type of joins.
SELECT * FROM employee;
+---+-----+------+
| id| name|deptno|
+---+-----+------+
|105|Chloe|     5|
|103| Paul|     3|
|101| John|     1|
|102| Lisa|     2|
|104| Evan|     4|
|106|  Amy|     6|
+---+-----+------+

SELECT * FROM department;
+------+-----------+
|deptno|   deptname|
+------+-----------+
|     3|Engineering|
|     2|      Sales|
|     1|  Marketing|
+------+-----------+

-- Use employee and department tables to demonstrate inner join.
SELECT id, name, employee.deptno, deptname
    FROM employee INNER JOIN department ON employee.deptno = department.deptno;
+---+-----+------+-----------|
| id| name|deptno|   deptname|
+---+-----+------+-----------|
|103| Paul|     3|Engineering|
|101| John|     1|  Marketing|
|102| Lisa|     2|      Sales|
+---+-----+------+-----------|

-- Use employee and department tables to demonstrate left join.
SELECT id, name, employee.deptno, deptname
    FROM employee LEFT JOIN department ON employee.deptno = department.deptno;
+---+-----+------+-----------|
| id| name|deptno|   deptname|
+---+-----+------+-----------|
|105|Chloe|     5|       NULL|
|103| Paul|     3|Engineering|
|101| John|     1|  Marketing|
|102| Lisa|     2|      Sales|
|104| Evan|     4|       NULL|
|106|  Amy|     6|       NULL|
+---+-----+------+-----------|

-- Use employee and department tables to demonstrate right join.
SELECT id, name, employee.deptno, deptname
    FROM employee RIGHT JOIN department ON employee.deptno = department.deptno;
+---+-----+------+-----------|
| id| name|deptno|   deptname|
+---+-----+------+-----------|
|103| Paul|     3|Engineering|
|101| John|     1|  Marketing|
|102| Lisa|     2|      Sales|
+---+-----+------+-----------|

-- Use employee and department tables to demonstrate full join.
SELECT id, name, employee.deptno, deptname
    FROM employee FULL JOIN department ON employee.deptno = department.deptno;
+---+-----+------+-----------|
| id| name|deptno|   deptname|
+---+-----+------+-----------|
|101| John|     1|  Marketing|
|106|  Amy|     6|       NULL|
|103| Paul|     3|Engineering|
|105|Chloe|     5|       NULL|
|104| Evan|     4|       NULL|
|102| Lisa|     2|      Sales|
+---+-----+------+-----------|

-- Use employee and department tables to demonstrate cross join.
SELECT id, name, employee.deptno, deptname FROM employee CROSS JOIN department;
+---+-----+------+-----------|
| id| name|deptno|   deptname|
+---+-----+------+-----------|
|105|Chloe|     5|Engineering|
|105|Chloe|     5|  Marketing|
|105|Chloe|     5|      Sales|
|103| Paul|     3|Engineering|
|103| Paul|     3|  Marketing|
|103| Paul|     3|      Sales|
|101| John|     1|Engineering|
|101| John|     1|  Marketing|
|101| John|     1|      Sales|
|102| Lisa|     2|Engineering|
|102| Lisa|     2|  Marketing|
|102| Lisa|     2|      Sales|
|104| Evan|     4|Engineering|
|104| Evan|     4|  Marketing|
|104| Evan|     4|      Sales|
|106|  Amy|     4|Engineering|
|106|  Amy|     4|  Marketing|
|106|  Amy|     4|      Sales|
+---+-----+------+-----------|

-- Use employee and department tables to demonstrate semi join.
SELECT * FROM employee SEMI JOIN department ON employee.deptno = department.deptno;
+---+-----+------+
| id| name|deptno|
+---+-----+------+
|103| Paul|     3|
|101| John|     1|
|102| Lisa|     2|
+---+-----+------+

-- Use employee and department tables to demonstrate anti join.
SELECT * FROM employee ANTI JOIN department ON employee.deptno = department.deptno;
+---+-----+------+
| id| name|deptno|
+---+-----+------+
|105|Chloe|     5|
|104| Evan|     4|
|106|  Amy|     6|
+---+-----+------+

Related Statements

SELECT
Hints
LATERAL Subquery





















  




LATERAL SUBQUERY - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







LATERAL SUBQUERY
Description
LATERAL SUBQUERY is a subquery that is preceded by the keyword LATERAL. It provides a way to reference columns in the preceding FROM clause.
Without the LATERAL keyword, subqueries can only refer to columns in the outer query, but not in the FROM clause. LATERAL SUBQUERY makes the complicated
queries simpler and more efficient.
Syntax
[ LATERAL ] primary_relation [ join_relation ]

Parameters


primary_relation
Specifies the primary relation. It can be one of the following:

Table relation

Aliased query
Syntax: ( query ) [ [ AS ] alias ]


Aliased relation
Syntax: ( relation ) [ [ AS ] alias ]

Table-value function
Inline table



join_relation
Specifies a Join relation.


Examples
CREATE TABLE t1 (c1 INT, c2 INT);
INSERT INTO t1 VALUES (0, 1), (1, 2);

CREATE TABLE t2 (c1 INT, c2 INT);
INSERT INTO t2 VALUES (0, 2), (0, 3);

SELECT * FROM t1,
  LATERAL (SELECT * FROM t2 WHERE t1.c1 = t2.c1);
+--------+-------+--------+-------+
|  t1.c1 | t1.c2 |  t2.c1 | t2.c2 |
+-------+--------+--------+-------+
|    0   |   1   |    0   |   3   |
|    0   |   1   |    0   |   2   |
+-------+--------+--------+-------+

SELECT a, b, c FROM t1,
  LATERAL (SELECT c1 + c2 AS a),
  LATERAL (SELECT c1 - c2 AS b),
  LATERAL (SELECT a * b AS c);
+--------+-------+--------+
|    a   |   b   |    c   |
+-------+--------+--------+
|    3   |  -1   |   -3   |
|    1   |  -1   |   -1   |
+-------+--------+--------+

Related Statements

SELECT Main
JOIN





















  




LATERAL VIEW Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







LATERAL VIEW Clause
Description
The LATERAL VIEW clause is used in conjunction with generator functions such as EXPLODE, which will generate a virtual table containing one or more rows. LATERAL VIEW will apply the rows to each original output row.
Syntax
LATERAL VIEW [ OUTER ] generator_function ( expression [ , ... ] ) [ table_alias ] AS column_alias [ , ... ]

Parameters


OUTER
If OUTER specified, returns null if an input array/map is empty or null.


generator_function
Specifies a generator function (EXPLODE, INLINE, etc.).


table_alias
The alias for generator_function, which is optional.


column_alias
Lists the column aliases of generator_function, which may be used in output rows. We may have multiple aliases if generator_function have multiple output columns.


Examples
CREATE TABLE person (id INT, name STRING, age INT, class INT, address STRING);
INSERT INTO person VALUES
    (100, 'John', 30, 1, 'Street 1'),
    (200, 'Mary', NULL, 1, 'Street 2'),
    (300, 'Mike', 80, 3, 'Street 3'),
    (400, 'Dan', 50, 4, 'Street 4');

SELECT * FROM person
    LATERAL VIEW EXPLODE(ARRAY(30, 60)) tableName AS c_age
    LATERAL VIEW EXPLODE(ARRAY(40, 80)) AS d_age;
+------+-------+-------+--------+-----------+--------+--------+
|  id  | name  |  age  | class  |  address  | c_age  | d_age  |
+------+-------+-------+--------+-----------+--------+--------+
| 100  | John  | 30    | 1      | Street 1  | 30     | 40     |
| 100  | John  | 30    | 1      | Street 1  | 30     | 80     |
| 100  | John  | 30    | 1      | Street 1  | 60     | 40     |
| 100  | John  | 30    | 1      | Street 1  | 60     | 80     |
| 200  | Mary  | NULL  | 1      | Street 2  | 30     | 40     |
| 200  | Mary  | NULL  | 1      | Street 2  | 30     | 80     |
| 200  | Mary  | NULL  | 1      | Street 2  | 60     | 40     |
| 200  | Mary  | NULL  | 1      | Street 2  | 60     | 80     |
| 300  | Mike  | 80    | 3      | Street 3  | 30     | 40     |
| 300  | Mike  | 80    | 3      | Street 3  | 30     | 80     |
| 300  | Mike  | 80    | 3      | Street 3  | 60     | 40     |
| 300  | Mike  | 80    | 3      | Street 3  | 60     | 80     |
| 400  | Dan   | 50    | 4      | Street 4  | 30     | 40     |
| 400  | Dan   | 50    | 4      | Street 4  | 30     | 80     |
| 400  | Dan   | 50    | 4      | Street 4  | 60     | 40     |
| 400  | Dan   | 50    | 4      | Street 4  | 60     | 80     |
+------+-------+-------+--------+-----------+--------+--------+

SELECT c_age, COUNT(1) FROM person
    LATERAL VIEW EXPLODE(ARRAY(30, 60)) AS c_age
    LATERAL VIEW EXPLODE(ARRAY(40, 80)) AS d_age 
GROUP BY c_age;
+--------+-----------+
| c_age  | count(1)  |
+--------+-----------+
| 60     | 8         |
| 30     | 8         |
+--------+-----------+

SELECT * FROM person
    LATERAL VIEW EXPLODE(ARRAY()) tableName AS c_age;
+-----+-------+------+--------+----------+--------+
| id  | name  | age  | class  | address  | c_age  |
+-----+-------+------+--------+----------+--------+
+-----+-------+------+--------+----------+--------+

SELECT * FROM person
    LATERAL VIEW OUTER EXPLODE(ARRAY()) tableName AS c_age;
+------+-------+-------+--------+-----------+--------+
|  id  | name  |  age  | class  |  address  | c_age  |
+------+-------+-------+--------+-----------+--------+
| 100  | John  | 30    | 1      | Street 1  | NULL   |
| 200  | Mary  | NULL  | 1      | Street 2  | NULL   |
| 300  | Mike  | 80    | 3      | Street 3  | NULL   |
| 400  | Dan   | 50    | 4      | Street 4  | NULL   |
+------+-------+-------+--------+-----------+--------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause





















  




LIKE Predicate - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







LIKE Predicate
Description
A LIKE predicate is used to search for a specific pattern. This predicate also supports multiple patterns with quantifiers include ANY, SOME and ALL.
Syntax
[ NOT ] { LIKE search_pattern [ ESCAPE esc_char ] | [ RLIKE | REGEXP ] regex_pattern }

[ NOT ] { LIKE quantifiers ( search_pattern [ , ... ]) }

Parameters


search_pattern
Specifies a string pattern to be searched by the LIKE clause. It can contain special pattern-matching characters:

% matches zero or more characters.
_ matches exactly one character.



esc_char
Specifies the escape character. The default escape character is \.


regex_pattern
Specifies a regular expression search pattern to be searched by the RLIKE or REGEXP clause.


quantifiers
Specifies the predicate quantifiers include ANY, SOME and ALL. ANY or SOME means if one of the patterns matches the input, then return true; ALL means if all the patterns matches the input, then return true.


Examples
CREATE TABLE person (id INT, name STRING, age INT);
INSERT INTO person VALUES
    (100, 'John', 30),
    (200, 'Mary', NULL),
    (300, 'Mike', 80),
    (400, 'Dan',  50),
    (500, 'Evan_w', 16);

SELECT * FROM person WHERE name LIKE 'M%';
+---+----+----+
| id|name| age|
+---+----+----+
|300|Mike|  80|
|200|Mary|null|
+---+----+----+

SELECT * FROM person WHERE name LIKE 'M_ry';
+---+----+----+
| id|name| age|
+---+----+----+
|200|Mary|null|
+---+----+----+

SELECT * FROM person WHERE name NOT LIKE 'M_ry';
+---+------+---+
| id|  name|age|
+---+------+---+
|500|Evan_W| 16|
|300|  Mike| 80|
|100|  John| 30|
|400|   Dan| 50|
+---+------+---+

SELECT * FROM person WHERE name RLIKE 'M+';
+---+----+----+
| id|name| age|
+---+----+----+
|300|Mike|  80|
|200|Mary|null|
+---+----+----+

SELECT * FROM person WHERE name REGEXP 'M+';
+---+----+----+
| id|name| age|
+---+----+----+
|300|Mike|  80|
|200|Mary|null|
+---+----+----+

SELECT * FROM person WHERE name LIKE '%\_%';
+---+------+---+
| id|  name|age|
+---+------+---+
|500|Evan_W| 16|
+---+------+---+

SELECT * FROM person WHERE name LIKE '%$_%' ESCAPE '$';
+---+------+---+
| id|  name|age|
+---+------+---+
|500|Evan_W| 16|
+---+------+---+

SELECT * FROM person WHERE name LIKE ALL ('%an%', '%an');
+---+----+----+
| id|name| age|
+---+----+----+
|400| Dan|  50|
+---+----+----+

SELECT * FROM person WHERE name LIKE ANY ('%an%', '%an');
+---+------+---+
| id|  name|age|
+---+------+---+
|400|   Dan| 50|
|500|Evan_W| 16|
+---+------+---+

SELECT * FROM person WHERE name LIKE SOME ('%an%', '%an');
+---+------+---+
| id|  name|age|
+---+------+---+
|400|   Dan| 50|
|500|Evan_W| 16|
+---+------+---+

SELECT * FROM person WHERE name NOT LIKE ALL ('%an%', '%an');
+---+----+----+
| id|name| age|
+---+----+----+
|100|John|  30|
|200|Mary|null|
|300|Mike|  80|
+---+----+----+

SELECT * FROM person WHERE name NOT LIKE ANY ('%an%', '%an');
+---+------+----+
| id|  name| age|
+---+------+----+
|100|  John|  30|
|200|  Mary|null|
|300|  Mike|  80|
|500|Evan_W|  16|
+---+------+----+

SELECT * FROM person WHERE name NOT LIKE SOME ('%an%', '%an');
+---+------+----+
| id|  name| age|
+---+------+----+
|100|  John|  30|
|200|  Mary|null|
|300|  Mike|  80|
|500|Evan_W|  16|
+---+------+----+

Related Statements

SELECT
WHERE Clause





















  




LIMIT Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







LIMIT Clause
Description
The LIMIT clause is used to constrain the number of rows returned by
the SELECT statement. In general, this clause
is used in conjunction with ORDER BY to
ensure that the results are deterministic.
Syntax
LIMIT { ALL | integer_expression }

Parameters


ALL
If specified, the query returns all the rows. In other words, no limit is applied if this
  option is specified.


integer_expression
Specifies a foldable expression that returns an integer.


Examples
CREATE TABLE person (name STRING, age INT);
INSERT INTO person VALUES
    ('Zen Hui', 25),
    ('Anil B', 18),
    ('Shone S', 16),
    ('Mike A', 25),
    ('John A', 18),
    ('Jack N', 16);

-- Select the first two rows.
SELECT name, age FROM person ORDER BY name LIMIT 2;
+------+---+
|  name|age|
+------+---+
|Anil B| 18|
|Jack N| 16|
+------+---+

-- Specifying ALL option on LIMIT returns all the rows.
SELECT name, age FROM person ORDER BY name LIMIT ALL;
+-------+---+
|   name|age|
+-------+---+
| Anil B| 18|
| Jack N| 16|
| John A| 18|
| Mike A| 25|
|Shone S| 16|
|Zen Hui| 25|
+-------+---+

-- A function expression as an input to LIMIT.
SELECT name, age FROM person ORDER BY name LIMIT length('SPARK');
+-------+---+
|   name|age|
+-------+---+
| Anil B| 18|
| Jack N| 16|
| John A| 18|
| Mike A| 25|
|Shone S| 16|
+-------+---+

-- A non-foldable expression as an input to LIMIT is not allowed.
SELECT name, age FROM person ORDER BY name LIMIT length(name);
org.apache.spark.sql.AnalysisException: The limit expression must evaluate to a constant value ...

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




OFFSET Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







OFFSET Clause
Description
The OFFSET clause is used to specify the number of rows to skip before beginning to return rows
returned by the SELECT statement. In general, this clause
is used in conjunction with ORDER BY to
ensure that the results are deterministic.
Syntax
OFFSET integer_expression

Parameters


integer_expression
Specifies a foldable expression that returns an integer.


Examples
CREATE TABLE person (name STRING, age INT);
INSERT INTO person VALUES
    ('Zen Hui', 25),
    ('Anil B', 18),
    ('Shone S', 16),
    ('Mike A', 25),
    ('John A', 18),
    ('Jack N', 16);

-- Skip the first two rows.
SELECT name, age FROM person ORDER BY name OFFSET 2;
+-------+---+
|   name|age|
+-------+---+
| John A| 18|
| Mike A| 25|
|Shone S| 16|
|Zen Hui| 25|
+-------+---+

-- Skip the first two rows and returns the next three rows.
SELECT name, age FROM person ORDER BY name LIMIT 3 OFFSET 2;
+-------+---+
|   name|age|
+-------+---+
| John A| 18|
| Mike A| 25|
|Shone S| 16|
+-------+---+

-- A function expression as an input to OFFSET.
SELECT name, age FROM person ORDER BY name OFFSET length('SPARK');
+-------+---+
|   name|age|
+-------+---+
|Zen Hui| 25|
+-------+---+

-- A non-foldable expression as an input to OFFSET is not allowed.
SELECT name, age FROM person ORDER BY name OFFSET length(name);
org.apache.spark.sql.AnalysisException: The offset expression must evaluate to a constant value ...

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




ORDER BY Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







ORDER BY Clause
Description
The ORDER BY clause is used to return the result rows in a sorted manner
in the user specified order. Unlike the SORT BY
clause, this clause guarantees a total order in the output.
Syntax
ORDER BY { expression [ sort_direction | nulls_sort_order ] [ , ... ] }

Parameters


ORDER BY
Specifies a comma-separated list of expressions along with optional parameters sort_direction
  and nulls_sort_order which are used to sort the rows.


sort_direction
Optionally specifies whether to sort the rows in ascending or descending
  order. The valid values for the sort direction are ASC for ascending
  and DESC for descending. If sort direction is not explicitly specified, then by default
  rows are sorted ascending.
Syntax: [ ASC | DESC ]


nulls_sort_order
Optionally specifies whether NULL values are returned before/after non-NULL values. If
  null_sort_order is not specified, then NULLs sort first if sort order is
  ASC and NULLS sort last if sort order is DESC.

If NULLS FIRST is specified, then NULL values are returned first
regardless of the sort order.
If NULLS LAST is specified, then NULL values are returned last regardless of
the sort order.

Syntax: [ NULLS { FIRST | LAST } ]


Examples
CREATE TABLE person (id INT, name STRING, age INT);
INSERT INTO person VALUES
    (100, 'John', 30),
    (200, 'Mary', NULL),
    (300, 'Mike', 80),
    (400, 'Jerry', NULL),
    (500, 'Dan',  50);

-- Sort rows by age. By default rows are sorted in ascending manner with NULL FIRST.
SELECT name, age FROM person ORDER BY age;
+-----+----+
| name| age|
+-----+----+
|Jerry|null|
| Mary|null|
| John|  30|
|  Dan|  50|
| Mike|  80|
+-----+----+

-- Sort rows in ascending manner keeping null values to be last.
SELECT name, age FROM person ORDER BY age NULLS LAST;
+-----+----+
| name| age|
+-----+----+
| John|  30|
|  Dan|  50|
| Mike|  80|
| Mary|null|
|Jerry|null|
+-----+----+

-- Sort rows by age in descending manner, which defaults to NULL LAST.
SELECT name, age FROM person ORDER BY age DESC;
+-----+----+
| name| age|
+-----+----+
| Mike|  80|
|  Dan|  50|
| John|  30|
|Jerry|null|
| Mary|null|
+-----+----+

-- Sort rows in ascending manner keeping null values to be first.
SELECT name, age FROM person ORDER BY age DESC NULLS FIRST;
+-----+----+
| name| age|
+-----+----+
|Jerry|null|
| Mary|null|
| Mike|  80|
|  Dan|  50|
| John|  30|
+-----+----+

-- Sort rows based on more than one column with each column having different
-- sort direction.
SELECT * FROM person ORDER BY name ASC, age DESC;
+---+-----+----+
| id| name| age|
+---+-----+----+
|500|  Dan|  50|
|400|Jerry|null|
|100| John|  30|
|200| Mary|null|
|300| Mike|  80|
+---+-----+----+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
SORT BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




PIVOT Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







PIVOT Clause
Description
The PIVOT clause is used for data perspective. We can get the aggregated values based on specific column values, which will be turned to multiple columns used in SELECT clause. The PIVOT clause can be specified after the table name or subquery.
Syntax
PIVOT ( { aggregate_expression [ AS aggregate_expression_alias ] } [ , ... ]
    FOR column_list IN ( expression_list ) )

Parameters


aggregate_expression
Specifies an aggregate expression (SUM(a), COUNT(DISTINCT b), etc.).


aggregate_expression_alias
Specifies an alias for the aggregate expression.


column_list
Contains columns in the FROM clause, which specifies the columns we want to replace with new columns. We can use brackets to surround the columns, such as (c1, c2).


expression_list
Specifies new columns, which are used to match values in column_list as the aggregating condition. We can also add aliases for them.


Examples
CREATE TABLE person (id INT, name STRING, age INT, class INT, address STRING);
INSERT INTO person VALUES
    (100, 'John', 30, 1, 'Street 1'),
    (200, 'Mary', NULL, 1, 'Street 2'),
    (300, 'Mike', 80, 3, 'Street 3'),
    (400, 'Dan', 50, 4, 'Street 4');

SELECT * FROM person
    PIVOT (
        SUM(age) AS a, AVG(class) AS c
        FOR name IN ('John' AS john, 'Mike' AS mike)
    );
+------+-----------+---------+---------+---------+---------+
|  id  |  address  | john_a  | john_c  | mike_a  | mike_c  |
+------+-----------+---------+---------+---------+---------+
| 200  | Street 2  | NULL    | NULL    | NULL    | NULL    |
| 100  | Street 1  | 30      | 1.0     | NULL    | NULL    |
| 300  | Street 3  | NULL    | NULL    | 80      | 3.0     |
| 400  | Street 4  | NULL    | NULL    | NULL    | NULL    |
+------+-----------+---------+---------+---------+---------+

SELECT * FROM person
    PIVOT (
        SUM(age) AS a, AVG(class) AS c
        FOR (name, age) IN (('John', 30) AS c1, ('Mike', 40) AS c2)
    );
+------+-----------+-------+-------+-------+-------+
|  id  |  address  | c1_a  | c1_c  | c2_a  | c2_c  |
+------+-----------+-------+-------+-------+-------+
| 200  | Street 2  | NULL  | NULL  | NULL  | NULL  |
| 100  | Street 1  | 30    | 1.0   | NULL  | NULL  |
| 300  | Street 3  | NULL  | NULL  | NULL  | NULL  |
| 400  | Street 4  | NULL  | NULL  | NULL  | NULL  |
+------+-----------+-------+-------+-------+-------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




Sampling Queries - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Sampling Queries
Description
The TABLESAMPLE statement is used to sample the table. It supports the following sampling methods:

TABLESAMPLE(x ROWS): Sample the table down to the given number of rows.
TABLESAMPLE(x PERCENT): Sample the table down to the given percentage. Note that percentages are defined as a number between 0 and 100.
TABLESAMPLE(BUCKET x OUT OF y): Sample the table down to a x out of y fraction.

Note: TABLESAMPLE returns the approximate number of rows or fraction requested.
Syntax
TABLESAMPLE ({ integer_expression | decimal_expression } PERCENT)
    | TABLESAMPLE ( integer_expression ROWS )
    | TABLESAMPLE ( BUCKET integer_expression OUT OF integer_expression )

Examples
SELECT * FROM test;
+--+----+
|id|name|
+--+----+
| 5|Alex|
| 8|Lucy|
| 2|Mary|
| 4|Fred|
| 1|Lisa|
| 9|Eric|
|10|Adam|
| 6|Mark|
| 7|Lily|
| 3|Evan|
+--+----+

SELECT * FROM test TABLESAMPLE (50 PERCENT);
+--+----+
|id|name|
+--+----+
| 5|Alex|
| 2|Mary|
| 4|Fred|
| 9|Eric|
|10|Adam|
| 3|Evan|
+--+----+

SELECT * FROM test TABLESAMPLE (5 ROWS);
+--+----+
|id|name|
+--+----+
| 5|Alex|
| 8|Lucy|
| 2|Mary|
| 4|Fred|
| 1|Lisa|
+--+----+

SELECT * FROM test TABLESAMPLE (BUCKET 4 OUT OF 10);
+--+----+
|id|name|
+--+----+
| 8|Lucy|
| 2|Mary|
| 9|Eric|
| 6|Mark|
+--+----+

Related Statements

SELECT





















  




Set Operators - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Set Operators
Description
Set operators are used to combine two input relations into a single one. Spark SQL supports three types of set operators:

EXCEPT or MINUS
INTERSECT
UNION

Note that input relations must have the same number of columns and compatible data types for the respective columns.
EXCEPT
EXCEPT and EXCEPT ALL return the rows that are found in one relation but not the other. EXCEPT (alternatively, EXCEPT DISTINCT) takes only distinct rows while EXCEPT ALL does not remove duplicates from the result rows. Note that MINUS is an alias for EXCEPT.
Syntax
[ ( ] relation [ ) ] EXCEPT | MINUS [ ALL | DISTINCT ] [ ( ] relation [ ) ]

Examples
-- Use number1 and number2 tables to demonstrate set operators in this page.
SELECT * FROM number1;
+---+
|  c|
+---+
|  3|
|  1|
|  2|
|  2|
|  3|
|  4|
+---+
  
SELECT * FROM number2;
+---+
|  c|
+---+
|  5|
|  1|
|  2|
|  2|
+---+

SELECT c FROM number1 EXCEPT SELECT c FROM number2;
+---+
|  c|
+---+
|  3|
|  4|
+---+

SELECT c FROM number1 MINUS SELECT c FROM number2;
+---+
|  c|
+---+
|  3|
|  4|
+---+

SELECT c FROM number1 EXCEPT ALL (SELECT c FROM number2);
+---+
|  c|
+---+
|  3|
|  3|
|  4|
+---+

SELECT c FROM number1 MINUS ALL (SELECT c FROM number2);
+---+
|  c|
+---+
|  3|
|  3|
|  4|
+---+

INTERSECT
INTERSECT and INTERSECT ALL return the rows that are found in both relations. INTERSECT (alternatively, INTERSECT DISTINCT) takes only distinct rows while INTERSECT ALL does not remove duplicates from the result rows.
Syntax
[ ( ] relation [ ) ] INTERSECT [ ALL | DISTINCT ] [ ( ] relation [ ) ]

Examples
(SELECT c FROM number1) INTERSECT (SELECT c FROM number2);
+---+
|  c|
+---+
|  1|
|  2|
+---+

(SELECT c FROM number1) INTERSECT DISTINCT (SELECT c FROM number2);
+---+
|  c|
+---+
|  1|
|  2|
+---+

(SELECT c FROM number1) INTERSECT ALL (SELECT c FROM number2);
+---+
|  c|
+---+
|  1|
|  2|
|  2|
+---+

UNION
UNION and UNION ALL return the rows that are found in either relation. UNION (alternatively, UNION DISTINCT) takes only distinct rows while UNION ALL does not remove duplicates from the result rows.
Syntax
[ ( ] relation [ ) ] UNION [ ALL | DISTINCT ] [ ( ] relation [ ) ]

Examples
(SELECT c FROM number1) UNION (SELECT c FROM number2);
+---+
|  c|
+---+
|  1|
|  3|
|  5|
|  4|
|  2|
+---+

(SELECT c FROM number1) UNION DISTINCT (SELECT c FROM number2);
+---+
|  c|
+---+
|  1|
|  3|
|  5|
|  4|
|  2|
+---+

SELECT c FROM number1 UNION ALL (SELECT c FROM number2);
+---+
|  c|
+---+
|  3|
|  1|
|  2|
|  2|
|  3|
|  4|
|  5|
|  1|
|  2|
|  2|
+---+

Related Statements

SELECT Statement





















  




SORT BY Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SORT BY Clause
Description
The SORT BY clause is used to return the result rows sorted
within each partition in the user specified order. When there is more than one partition
SORT BY may return result that is partially ordered. This is different
than ORDER BY clause which guarantees a
total order of the output.
Syntax
SORT BY { expression [ sort_direction | nulls_sort_order ] [ , ... ] }

Parameters


SORT BY
Specifies a comma-separated list of expressions along with optional parameters sort_direction
  and nulls_sort_order which are used to sort the rows within each partition.


sort_direction
Optionally specifies whether to sort the rows in ascending or descending
  order. The valid values for the sort direction are ASC for ascending
  and DESC for descending. If sort direction is not explicitly specified, then by default
  rows are sorted ascending.
Syntax: [ ASC | DESC ]


nulls_sort_order
Optionally specifies whether NULL values are returned before/after non-NULL values. If
  null_sort_order is not specified, then NULLs sort first if sort order is
  ASC and NULLS sort last if sort order is DESC.

If NULLS FIRST is specified, then NULL values are returned first
regardless of the sort order.
If NULLS LAST is specified, then NULL values are returned last regardless of
the sort order.

Syntax: [ NULLS { FIRST | LAST } ]


Examples
CREATE TABLE person (zip_code INT, name STRING, age INT);
INSERT INTO person VALUES
    (94588, 'Zen Hui', 50),
    (94588, 'Dan Li', 18),
    (94588, 'Anil K', 27),
    (94588, 'John V', NULL),
    (94511, 'David K', 42),
    (94511, 'Aryan B.', 18),
    (94511, 'Lalit B.', NULL);

-- Use `REPARTITION` hint to partition the data by `zip_code` to
-- examine the `SORT BY` behavior. This is used in rest of the
-- examples.

-- Sort rows by `name` within each partition in ascending manner
SELECT /*+ REPARTITION(zip_code) */ name, age, zip_code FROM person SORT BY name;
+--------+----+--------+
|    name| age|zip_code|
+--------+----+--------+
|  Anil K|  27|   94588|
|  Dan Li|  18|   94588|
|  John V|null|   94588|
| Zen Hui|  50|   94588|
|Aryan B.|  18|   94511|
| David K|  42|   94511|
|Lalit B.|null|   94511|
+--------+----+--------+

-- Sort rows within each partition using column position.
SELECT /*+ REPARTITION(zip_code) */ name, age, zip_code FROM person SORT BY 1;
+--------+----+--------+
|    name| age|zip_code|
+--------+----+--------+
|  Anil K|  27|   94588|
|  Dan Li|  18|   94588|
|  John V|null|   94588|
| Zen Hui|  50|   94588|
|Aryan B.|  18|   94511|
| David K|  42|   94511|
|Lalit B.|null|   94511|
+--------+----+--------+

-- Sort rows within partition in ascending manner keeping null values to be last.
SELECT /*+ REPARTITION(zip_code) */ age, name, zip_code FROM person SORT BY age NULLS LAST;
+----+--------+--------+
| age|    name|zip_code|
+----+--------+--------+
|  18|  Dan Li|   94588|
|  27|  Anil K|   94588|
|  50| Zen Hui|   94588|
|null|  John V|   94588|
|  18|Aryan B.|   94511|
|  42| David K|   94511|
|null|Lalit B.|   94511|
+----+--------+--------+

-- Sort rows by age within each partition in descending manner, which defaults to NULL LAST.
SELECT /*+ REPARTITION(zip_code) */ age, name, zip_code FROM person SORT BY age DESC;
+----+--------+--------+
| age|    name|zip_code|
+----+--------+--------+
|  50| Zen Hui|   94588|
|  27|  Anil K|   94588|
|  18|  Dan Li|   94588|
|null|  John V|   94588|
|  42| David K|   94511|
|  18|Aryan B.|   94511|
|null|Lalit B.|   94511|
+----+--------+--------+

-- Sort rows by age within each partition in descending manner keeping null values to be first.
SELECT /*+ REPARTITION(zip_code) */ age, name, zip_code FROM person SORT BY age DESC NULLS FIRST;
+----+--------+--------+
| age|    name|zip_code|
+----+--------+--------+
|null|  John V|   94588|
|  50| Zen Hui|   94588|
|  27|  Anil K|   94588|
|  18|  Dan Li|   94588|
|null|Lalit B.|   94511|
|  42| David K|   94511|
|  18|Aryan B.|   94511|
+----+--------+--------+

-- Sort rows within each partition based on more than one column with each column having
-- different sort direction.
SELECT /*+ REPARTITION(zip_code) */ name, age, zip_code FROM person
    SORT BY name ASC, age DESC;
+--------+----+--------+
|    name| age|zip_code|
+--------+----+--------+
|  Anil K|  27|   94588|
|  Dan Li|  18|   94588|
|  John V|null|   94588|
| Zen Hui|  50|   94588|
|Aryan B.|  18|   94511|
| David K|  42|   94511|
|Lalit B.|null|   94511|
+--------+----+--------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




TRANSFORM - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







TRANSFORM
Description
The TRANSFORM clause is used to specify a Hive-style transform query specification 
to transform the inputs by running a user-specified command or script.
Spark’s script transform supports two modes:

Hive support disabled: Spark script transform can run with spark.sql.catalogImplementation=in-memory
  or without SparkSession.builder.enableHiveSupport(). In this case, now Spark only uses the script transform with
  ROW FORMAT DELIMITED and treats all values passed to the script as strings.
Hive support enabled: When Spark is run with spark.sql.catalogImplementation=hive or Spark SQL is started
  with SparkSession.builder.enableHiveSupport(), Spark can use the script transform with both Hive SerDe and 
  ROW FORMAT DELIMITED.

Syntax
SELECT TRANSFORM ( expression [ , ... ] )
    [ ROW FORMAT row_format ]
    [ RECORDWRITER record_writer_class ]
    USING command_or_script [ AS ( [ col_name [ col_type ] ] [ , ... ] ) ]
    [ ROW FORMAT row_format ]
    [ RECORDREADER record_reader_class ]

Parameters


expression
Specifies a combination of one or more values, operators and SQL functions that results in a value.


row_format
Specifies the row format for input and output. See HIVE FORMAT for more syntax details.


RECORDWRITER
Specifies a fully-qualified class name of a custom RecordWriter. The default value is org.apache.hadoop.hive.ql.exec.TextRecordWriter.


RECORDREADER
Specifies a fully-qualified class name of a custom RecordReader. The default value is org.apache.hadoop.hive.ql.exec.TextRecordReader.


command_or_script
Specifies a command or a path to script to process data.


ROW FORMAT DELIMITED BEHAVIOR
When Spark uses ROW FORMAT DELIMITED format:

Spark uses the character \u0001 as the default field delimiter and this delimiter can be overridden by FIELDS TERMINATED BY.
Spark uses the character \n as the default line delimiter and this delimiter can be overridden by LINES TERMINATED BY.
Spark uses a string \N as the default NULL value in order to differentiate NULL values 
 from the literal string NULL. This delimiter can be overridden by NULL DEFINED AS.
Spark casts all columns to STRING and combines columns by tabs before feeding to the user script.
 For complex types such as ARRAY/MAP/STRUCT, Spark uses to_json casts it to an input JSON string and uses 
 from_json to convert the result output JSON string to ARRAY/MAP/STRUCT data.
COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY are delimiters to split complex data such as 
 ARRAY/MAP/STRUCT, Spark uses to_json and from_json to handle complex data types with JSON format. So 
 COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY won’t work in default row format.
The standard output of the user script is treated as tab-separated STRING columns. Any cell containing only a string \N
 is re-interpreted as a literal NULL value, and then the resulting STRING column will be cast to the data types specified in col_type.
If the actual number of output columns is less than the number of specified output columns,
  additional output columns will be filled with NULL. For example:
      output tabs: 1, 2
  output columns: A: INT, B INT, C: INT
  result: 
    +---+---+------+
    |  a|  b|     c|
    +---+---+------+
    |  1|  2|  NULL|
    +---+---+------+
 

If the actual number of output columns is more than the number of specified output columns, 
 the output columns only select the corresponding columns, and the remaining part will be discarded.
 For example, if the output has three tabs and there are only two output columns:
      output tabs: 1, 2, 3
  output columns: A: INT, B INT
  result: 
    +---+---+
    |  a|  b|
    +---+---+
    |  1|  2|
    +---+---+
 

If there is no AS clause after USING my_script, the output schema is key: STRING, value: STRING.
 The key column contains all the characters before the first tab and the value column contains the remaining characters after the first tab.
 If there are no tabs, Spark returns the NULL value. For example:
       output tabs: 1, 2, 3
   output columns: 
   result: 
     +-----+-------+
     |  key|  value|
     +-----+-------+
     |    1|      2|
     +-----+-------+
   
   output tabs: 1, 2
   output columns: 
   result: 
     +-----+-------+
     |  key|  value|
     +-----+-------+
     |    1|   NULL|
     +-----+-------+
 


Hive SerDe behavior
When Hive support is enabled and Hive SerDe mode is used:

Spark uses the Hive SerDe org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe by default, so columns are cast
 to STRING and combined by tabs before feeding to the user script.
All literal NULL values are converted to a string \N in order to differentiate literal NULL values from the literal string NULL.
The standard output of the user script is treated as tab-separated STRING columns, any cell containing only a string \N is re-interpreted
 as a NULL value, and then the resulting STRING column will be cast to the data type specified in col_type.
If the actual number of output columns is less than the number of specified output columns,
  additional output columns will be filled with NULL.
If the actual number of output columns is more than the number of specified output columns,
 the output columns only select the corresponding columns, and the remaining part will be discarded.
If there is no AS clause after USING my_script, the output schema is key: STRING, value: STRING.
 The key column contains all the characters before the first tab and the value column contains the remaining characters after the first tab.
 If there is no tab, Spark returns the NULL value.
These defaults can be overridden with ROW FORMAT SERDE or ROW FORMAT DELIMITED.

Examples
CREATE TABLE person (zip_code INT, name STRING, age INT);
INSERT INTO person VALUES
    (94588, 'Zen Hui', 50),
    (94588, 'Dan Li', 18),
    (94588, 'Anil K', 27),
    (94588, 'John V', NULL),
    (94511, 'David K', 42),
    (94511, 'Aryan B.', 18),
    (94511, 'Lalit B.', NULL);

-- With specified output without data type
SELECT TRANSFORM(zip_code, name, age)
   USING 'cat' AS (a, b, c)
FROM person
WHERE zip_code > 94511;
+-------+---------+-----+
|    a  |        b|    c|
+-------+---------+-----+
|  94588|   Anil K|   27|
|  94588|   John V| NULL|
|  94588|  Zen Hui|   50|
|  94588|   Dan Li|   18|
+-------+---------+-----+

-- With specified output with data type
SELECT TRANSFORM(zip_code, name, age)
   USING 'cat' AS (a STRING, b STRING, c STRING)
FROM person
WHERE zip_code > 94511;
+-------+---------+-----+
|    a  |        b|    c|
+-------+---------+-----+
|  94588|   Anil K|   27|
|  94588|   John V| NULL|
|  94588|  Zen Hui|   50|
|  94588|   Dan Li|   18|
+-------+---------+-----+

-- Using ROW FORMAT DELIMITED
SELECT TRANSFORM(name, age)
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ','
    LINES TERMINATED BY '\n'
    NULL DEFINED AS 'NULL'
    USING 'cat' AS (name_age string)
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '@'
    LINES TERMINATED BY '\n'
    NULL DEFINED AS 'NULL'
FROM person;
+---------------+
|       name_age|
+---------------+
|      Anil K,27|
|    John V,null|
|     ryan B.,18|
|     David K,42|
|     Zen Hui,50|
|      Dan Li,18|
|  Lalit B.,null|
+---------------+

-- Using Hive Serde
SELECT TRANSFORM(zip_code, name, age)
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
    WITH SERDEPROPERTIES (
      'field.delim' = '\t'
    )
    USING 'cat' AS (a STRING, b STRING, c STRING)
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
    WITH SERDEPROPERTIES (
      'field.delim' = '\t'
    )
FROM person
WHERE zip_code > 94511;
+-------+---------+-----+
|    a  |        b|    c|
+-------+---------+-----+
|  94588|   Anil K|   27|
|  94588|   John V| NULL|
|  94588|  Zen Hui|   50|
|  94588|   Dan Li|   18|
+-------+---------+-----+

-- Schema-less mode
SELECT TRANSFORM(zip_code, name, age)
    USING 'cat'
FROM person
WHERE zip_code > 94500;
+-------+---------------------+
|    key|                value|
+-------+---------------------+
|  94588|	  Anil K    27|
|  94588|	  John V    \N|
|  94511|	Aryan B.    18|
|  94511|	 David K    42|
|  94588|	 Zen Hui    50|
|  94588|	  Dan Li    18|
|  94511|	Lalit B.    \N|
+-------+---------------------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




Table-valued Functions (TVF) - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Table-valued Functions (TVF)
Description
A table-valued function (TVF) is a function that returns a relation or a set of rows. There are two types of TVFs in Spark SQL:

a TVF that can be specified in a FROM clause, e.g. range;
a TVF that can be specified in SELECT/LATERAL VIEW clauses, e.g. explode.

Supported Table-valued Functions
TVFs that can be specified in a FROM clause:



Function
Argument Type(s)
Description




range ( end )
Long
Creates a table with a single LongType column named id,  containing rows in a range from 0 to end (exclusive) with step value 1.


range ( start, end )
Long, Long
Creates a table with a single LongType column named id,  containing rows in a range from start to end (exclusive) with step value 1.


range ( start, end, step )
Long, Long, Long
Creates a table with a single LongType column named id,  containing rows in a range from start to end (exclusive) with step value.


range ( start, end, step, numPartitions )
Long, Long, Long, Int
Creates a table with a single LongType column named id,  containing rows in a range from start to end (exclusive) with step value, with partition number numPartitions specified.



TVFs that can be specified in SELECT/LATERAL VIEW clauses:



Function
Argument Type(s)
Description




explode ( expr )
Array/Map
Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Unless specified otherwise, uses the default column name col for elements of the array or key and value for the elements of the map.


explode_outer  ( expr )
Array/Map
Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Unless specified otherwise, uses the default column name col for elements of the array or key and value for the elements of the map.


inline ( expr )
Expression
Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.


inline_outer  ( expr )
Expression
Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.


posexplode  ( expr )
Array/Map
Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Unless specified otherwise, uses the column name pos for position, col for elements of the array or key and value for elements of the map.


posexplode_outer ( expr )
Array/Map
Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Unless specified otherwise, uses the column name pos for position, col for elements of the array or key and value for elements of the map.


stack ( n, expr1, …, exprk )
Seq[Expression]
Separates expr1, …, exprk into n rows. Uses column names col0, col1, etc. by default unless specified otherwise.


json_tuple  ( jsonStr, p1, p2, …, pn )
Seq[Expression]
Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.


parse_url  ( url, partToExtract[, key] )
Seq[Expression]
Extracts a part from a URL.



Examples
-- range call with end
SELECT * FROM range(6 + cos(3));
+---+
| id|
+---+
|  0|
|  1|
|  2|
|  3|
|  4|
+---+

-- range call with start and end
SELECT * FROM range(5, 10);
+---+
| id|
+---+
|  5|
|  6|
|  7|
|  8|
|  9|
+---+

-- range call with numPartitions
SELECT * FROM range(0, 10, 2, 200);
+---+
| id|
+---+
|  0|
|  2|
|  4|
|  6|
|  8|
+---+

-- range call with a table alias
SELECT * FROM range(5, 8) AS test;
+---+
| id|
+---+
|  5|
|  6|
|  7|
+---+

SELECT explode(array(10, 20));
+---+
|col|
+---+
| 10|
| 20|
+---+

SELECT inline(array(struct(1, 'a'), struct(2, 'b')));
+----+----+
|col1|col2|
+----+----+
|   1|   a|
|   2|   b|
+----+----+

SELECT posexplode(array(10,20));
+---+---+
|pos|col|
+---+---+
|  0| 10|
|  1| 20|
+---+---+

SELECT stack(2, 1, 2, 3);
+----+----+
|col0|col1|
+----+----+
|   1|   2|
|   3|null|
+----+----+

SELECT json_tuple('{"a":1, "b":2}', 'a', 'b');
+---+---+
| c0| c1|
+---+---+
|  1|  2|
+---+---+

SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST');
+-----------------------------------------------------+
|parse_url(http://spark.apache.org/path?query=1, HOST)|
+-----------------------------------------------------+
|                                     spark.apache.org|
+-----------------------------------------------------+

-- Use explode in a LATERAL VIEW clause
CREATE TABLE test (c1 INT);
INSERT INTO test VALUES (1);
INSERT INTO test VALUES (2);
SELECT * FROM test LATERAL VIEW explode (ARRAY(3,4)) AS c2;
+--+--+
|c1|c2|
+--+--+
| 1| 3|
| 1| 4|
| 2| 3|
| 2| 4|
+--+--+

Related Statements

SELECT
LATERAL VIEW Clause





















  




UNPIVOT Clause - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







UNPIVOT Clause
Description
The UNPIVOT clause transforms multiple columns into multiple rows used in SELECT clause.
The UNPIVOT clause can be specified after the table name or subquery.
Syntax
UNPIVOT [ { INCLUDE | EXCLUDE } NULLS ] (
    { single_value_column_unpivot | multi_value_column_unpivot }
) [[AS] alias]

single_value_column_unpivot:
    values_column
    FOR name_column
    IN (unpivot_column [[AS] alias] [, ...])

multi_value_column_unpivot:
    (values_column [, ...])
    FOR name_column
    IN ((unpivot_column [, ...]) [[AS] alias] [, ...])

Parameters


unpivot_column
Contains columns in the FROM clause, which specifies the columns we want to unpivot.


name_column
The name for the column that holds the names of the unpivoted columns.


values_column
The name for the column that holds the values of the unpivoted columns.


Examples
CREATE TABLE sales_quarterly (year INT, q1 INT, q2 INT, q3 INT, q4 INT);
INSERT INTO sales_quarterly VALUES
    (2020, null, 1000, 2000, 2500),
    (2021, 2250, 3200, 4200, 5900),
    (2022, 4200, 3100, null, null);

-- column names are used as unpivot columns
SELECT * FROM sales_quarterly
    UNPIVOT (
        sales FOR quarter IN (q1, q2, q3, q4)
    );
+------+---------+-------+
| year | quarter | sales |
+------+---------+-------+
| 2020 | q2      | 1000  |
| 2020 | q3      | 2000  |
| 2020 | q4      | 2500  |
| 2021 | q1      | 2250  |
| 2021 | q2      | 3200  |
| 2021 | q3      | 4200  |
| 2021 | q4      | 5900  |
| 2022 | q1      | 4200  |
| 2022 | q2      | 3100  |
+------+---------+-------+

-- NULL values are excluded by default, they can be included
-- unpivot columns can be alias
-- unpivot result can be referenced via its alias
SELECT up.* FROM sales_quarterly
    UNPIVOT INCLUDE NULLS (
        sales FOR quarter IN (q1 AS Q1, q2 AS Q2, q3 AS Q3, q4 AS Q4)
    ) AS up;
+------+---------+-------+
| year | quarter | sales |
+------+---------+-------+
| 2020 | Q1      | NULL  |
| 2020 | Q2      | 1000  |
| 2020 | Q3      | 2000  |
| 2020 | Q4      | 2500  |
| 2021 | Q1      | 2250  |
| 2021 | Q2      | 3200  |
| 2021 | Q3      | 4200  |
| 2021 | Q4      | 5900  |
| 2022 | Q1      | 4200  |
| 2022 | Q2      | 3100  |
| 2022 | Q3      | NULL  |
| 2022 | Q4      | NULL  |
+------+---------+-------+

-- multiple value columns can be unpivoted per row
SELECT * FROM sales_quarterly
    UNPIVOT EXCLUDE NULLS (
        (first_quarter, second_quarter)
        FOR half_of_the_year IN (
            (q1, q2) AS H1,
            (q3, q4) AS H2
        )
    );
+------+------------------+---------------+----------------+
|  id  | half_of_the_year | first_quarter | second_quarter |
+------+------------------+---------------+----------------+
| 2020 | H1               | NULL          | 1000           |
| 2020 | H2               | 2000          | 2500           |
| 2021 | H1               | 2250          | 3200           |
| 2021 | H2               | 4200          | 5900           |
| 2022 | H1               | 4200          | 3100           |
+------+------------------+---------------+----------------+

Related Statements

SELECT Main
WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
LATERAL VIEW Clause





















  




SELECT - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







WHERE clause
Description
The WHERE clause is used to limit the results of the FROM
clause of a query or a subquery based on the specified condition.
Syntax
WHERE boolean_expression

Parameters


boolean_expression
Specifies any expression that evaluates to a result type boolean. Two or
  more expressions may be combined together using the logical
  operators ( AND, OR ).


Examples
CREATE TABLE person (id INT, name STRING, age INT);
INSERT INTO person VALUES
    (100, 'John', 30),
    (200, 'Mary', NULL),
    (300, 'Mike', 80),
    (400, 'Dan',  50);

-- Comparison operator in `WHERE` clause.
SELECT * FROM person WHERE id > 200 ORDER BY id;
+---+----+---+
| id|name|age|
+---+----+---+
|300|Mike| 80|
|400| Dan| 50|
+---+----+---+

-- Comparison and logical operators in `WHERE` clause.
SELECT * FROM person WHERE id = 200 OR id = 300 ORDER BY id;
+---+----+----+
| id|name| age|
+---+----+----+
|200|Mary|null|
|300|Mike|  80|
+---+----+----+

-- IS NULL expression in `WHERE` clause.
SELECT * FROM person WHERE id > 300 OR age IS NULL ORDER BY id;
+---+----+----+
| id|name| age|
+---+----+----+
|200|Mary|null|
|400| Dan|  50|
+---+----+----+

-- Function expression in `WHERE` clause.
SELECT * FROM person WHERE length(name) > 3 ORDER BY id;
+---+----+----+
| id|name| age|
+---+----+----+
|100|John|  30|
|200|Mary|null|
|300|Mike|  80|
+---+----+----+

-- `BETWEEN` expression in `WHERE` clause.
SELECT * FROM person WHERE id BETWEEN 200 AND 300 ORDER BY id;
+---+----+----+
| id|name| age|
+---+----+----+
|200|Mary|null|
|300|Mike|  80|
+---+----+----+

-- Scalar Subquery in `WHERE` clause.
SELECT * FROM person WHERE age > (SELECT avg(age) FROM person);
+---+----+---+
| id|name|age|
+---+----+---+
|300|Mike| 80|
+---+----+---+

-- Correlated Subquery in `WHERE` clause.
SELECT * FROM person AS parent
    WHERE EXISTS (
        SELECT 1 FROM person AS child
        WHERE parent.id = child.id AND child.age IS NULL
    );
+---+----+----+
|id |name|age |
+---+----+----+
|200|Mary|null|
+---+----+----+

Related Statements

SELECT Main
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause





















  




Window Functions - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







Window Functions
Description
Window functions operate on a group of rows, referred to as a window, and calculate a return value for each row based on the group of rows. Window functions are useful for processing tasks such as calculating a moving average, computing a cumulative statistic, or accessing the value of rows given the relative position of the current row.
Syntax
window_function [ nulls_option ] OVER
( [  { PARTITION | DISTRIBUTE } BY partition_col_name = partition_col_val ( [ , ... ] ) ]
  { ORDER | SORT } BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ]
  [ window_frame ] )

Parameters


window_function


Ranking Functions
Syntax: RANK | DENSE_RANK | PERCENT_RANK | NTILE | ROW_NUMBER


Analytic Functions
Syntax: CUME_DIST | LAG | LEAD | NTH_VALUE | FIRST_VALUE | LAST_VALUE


Aggregate Functions
Syntax: MAX | MIN | COUNT | SUM | AVG | ...
Please refer to the Built-in Aggregation Functions document for a complete list of Spark aggregate functions.




nulls_option
Specifies whether or not to skip null values when evaluating the window function. RESPECT NULLS means not skipping null values, while IGNORE NULLS means skipping. If not specified, the default is RESPECT NULLS.
Syntax:
{ IGNORE | RESPECT } NULLS
Note: Only LAG | LEAD | NTH_VALUE | FIRST_VALUE | LAST_VALUE can be used with IGNORE NULLS.


window_frame
Specifies which row to start the window on and where to end it.
Syntax:
{ RANGE | ROWS } { frame_start | BETWEEN frame_start AND frame_end }


frame_start and frame_end have the following syntax:
Syntax:
UNBOUNDED PRECEDING | offset PRECEDING | CURRENT ROW | offset FOLLOWING | UNBOUNDED FOLLOWING
offset: specifies the offset from the position of the current row.


Note: If frame_end is omitted it defaults to CURRENT ROW.


Examples
CREATE TABLE employees (name STRING, dept STRING, salary INT, age INT);

INSERT INTO employees VALUES ("Lisa", "Sales", 10000, 35);
INSERT INTO employees VALUES ("Evan", "Sales", 32000, 38);
INSERT INTO employees VALUES ("Fred", "Engineering", 21000, 28);
INSERT INTO employees VALUES ("Alex", "Sales", 30000, 33);
INSERT INTO employees VALUES ("Tom", "Engineering", 23000, 33);
INSERT INTO employees VALUES ("Jane", "Marketing", 29000, 28);
INSERT INTO employees VALUES ("Jeff", "Marketing", 35000, 38);
INSERT INTO employees VALUES ("Paul", "Engineering", 29000, 23);
INSERT INTO employees VALUES ("Chloe", "Engineering", 23000, 25);

SELECT * FROM employees;
+-----+-----------+------+-----+
| name|       dept|salary|  age|
+-----+-----------+------+-----+
|Chloe|Engineering| 23000|   25|
| Fred|Engineering| 21000|   28|
| Paul|Engineering| 29000|   23|
|Helen|  Marketing| 29000|   40|
|  Tom|Engineering| 23000|   33|
| Jane|  Marketing| 29000|   28|
| Jeff|  Marketing| 35000|   38|
| Evan|      Sales| 32000|   38|
| Lisa|      Sales| 10000|   35|
| Alex|      Sales| 30000|   33|
+-----+-----------+------+-----+

SELECT name, dept, salary, RANK() OVER (PARTITION BY dept ORDER BY salary) AS rank FROM employees;
+-----+-----------+------+----+
| name|       dept|salary|rank|
+-----+-----------+------+----+
| Lisa|      Sales| 10000|   1|
| Alex|      Sales| 30000|   2|
| Evan|      Sales| 32000|   3|
| Fred|Engineering| 21000|   1|
|  Tom|Engineering| 23000|   2|
|Chloe|Engineering| 23000|   2|
| Paul|Engineering| 29000|   4|
|Helen|  Marketing| 29000|   1|
| Jane|  Marketing| 29000|   1|
| Jeff|  Marketing| 35000|   3|
+-----+-----------+------+----+

SELECT name, dept, salary, DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary ROWS BETWEEN
    UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank FROM employees;
+-----+-----------+------+----------+
| name|       dept|salary|dense_rank|
+-----+-----------+------+----------+
| Lisa|      Sales| 10000|         1|
| Alex|      Sales| 30000|         2|
| Evan|      Sales| 32000|         3|
| Fred|Engineering| 21000|         1|
|  Tom|Engineering| 23000|         2|
|Chloe|Engineering| 23000|         2|
| Paul|Engineering| 29000|         3|
|Helen|  Marketing| 29000|         1|
| Jane|  Marketing| 29000|         1|
| Jeff|  Marketing| 35000|         2|
+-----+-----------+------+----------+

SELECT name, dept, age, CUME_DIST() OVER (PARTITION BY dept ORDER BY age
    RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cume_dist FROM employees;
+-----+-----------+------+------------------+
| name|       dept|age   |         cume_dist|
+-----+-----------+------+------------------+
| Alex|      Sales|    33|0.3333333333333333|
| Lisa|      Sales|    35|0.6666666666666666|
| Evan|      Sales|    38|               1.0|
| Paul|Engineering|    23|              0.25|
|Chloe|Engineering|    25|              0.75|
| Fred|Engineering|    28|              0.25|
|  Tom|Engineering|    33|               1.0|
| Jane|  Marketing|    28|0.3333333333333333|
| Jeff|  Marketing|    38|0.6666666666666666|
|Helen|  Marketing|    40|               1.0|
+-----+-----------+------+------------------+

SELECT name, dept, salary, MIN(salary) OVER (PARTITION BY dept ORDER BY salary) AS min
    FROM employees;
+-----+-----------+------+-----+
| name|       dept|salary|  min|
+-----+-----------+------+-----+
| Lisa|      Sales| 10000|10000|
| Alex|      Sales| 30000|10000|
| Evan|      Sales| 32000|10000|
|Helen|  Marketing| 29000|29000|
| Jane|  Marketing| 29000|29000|
| Jeff|  Marketing| 35000|29000|
| Fred|Engineering| 21000|21000|
|  Tom|Engineering| 23000|21000|
|Chloe|Engineering| 23000|21000|
| Paul|Engineering| 29000|21000|
+-----+-----------+------+-----+

SELECT name, salary,
    LAG(salary) OVER (PARTITION BY dept ORDER BY salary) AS lag,
    LEAD(salary, 1, 0) OVER (PARTITION BY dept ORDER BY salary) AS lead
    FROM employees;
+-----+-----------+------+-----+-----+
| name|       dept|salary|  lag| lead|
+-----+-----------+------+-----+-----+
| Lisa|      Sales| 10000|NULL |30000|
| Alex|      Sales| 30000|10000|32000|
| Evan|      Sales| 32000|30000|    0|
| Fred|Engineering| 21000| NULL|23000|
|Chloe|Engineering| 23000|21000|23000|
|  Tom|Engineering| 23000|23000|29000|
| Paul|Engineering| 29000|23000|    0|
|Helen|  Marketing| 29000| NULL|29000|
| Jane|  Marketing| 29000|29000|35000|
| Jeff|  Marketing| 35000|29000|    0|
+-----+-----------+------+-----+-----+

SELECT id, v,
    LEAD(v, 0) IGNORE NULLS OVER w lead,
    LAG(v, 0) IGNORE NULLS OVER w lag,
    NTH_VALUE(v, 2) IGNORE NULLS OVER w nth_value,
    FIRST_VALUE(v) IGNORE NULLS OVER w first_value,
    LAST_VALUE(v) IGNORE NULLS OVER w last_value
    FROM test_ignore_null
    WINDOW w AS (ORDER BY id)
    ORDER BY id;
+--+----+----+----+---------+-----------+----------+
|id|   v|lead| lag|nth_value|first_value|last_value|
+--+----+----+----+---------+-----------+----------+
| 0|NULL|NULL|NULL|     NULL|       NULL|      NULL|
| 1|   x|   x|   x|     NULL|          x|         x|
| 2|NULL|NULL|NULL|     NULL|          x|         x|
| 3|NULL|NULL|NULL|     NULL|          x|         x|
| 4|   y|   y|   y|        y|          x|         y|
| 5|NULL|NULL|NULL|        y|          x|         y|
| 6|   z|   z|   z|        y|          x|         z|
| 7|   v|   v|   v|        y|          x|         v|
| 8|NULL|NULL|NULL|        y|          x|         v|
+--+----+----+----+---------+-----------+----------+

Related Statements

SELECT





















  




SELECT - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SELECT
Description
Spark supports a SELECT statement and conforms to the ANSI SQL standard. Queries are
used to retrieve result sets from one or more tables. The following section
describes the overall query syntax and the sub-sections cover different constructs
of a query along with examples.
Syntax
[ WITH with_query [ , ... ] ]
select_statement [ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select_statement, ... ]
    [ ORDER BY { expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ] } ]
    [ SORT BY { expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ] } ]
    [ CLUSTER BY { expression [ , ... ] } ]
    [ DISTRIBUTE BY { expression [, ... ] } ]
    [ WINDOW { named_window [ , WINDOW named_window, ... ] } ]
    [ LIMIT { ALL | expression } ]

While select_statement is defined as
SELECT [ hints , ... ] [ ALL | DISTINCT ] { [ [ named_expression | regex_column_names ] [ , ... ] | TRANSFORM (...) ] }
    FROM { from_item [ , ... ] }
    [ PIVOT clause ]
    [ UNPIVOT clause ]
    [ LATERAL VIEW clause ] [ ... ] 
    [ WHERE boolean_expression ]
    [ GROUP BY expression [ , ... ] ]
    [ HAVING boolean_expression ]

Parameters


with_query
Specifies the common table expressions (CTEs) before the main query block.
  These table expressions are allowed to be referenced later in the FROM clause. This is useful to abstract
  out repeated subquery blocks in the FROM clause and improves readability of the query.


hints
Hints can be specified to help spark optimizer make better planning decisions. Currently spark supports hints
  that influence selection of join strategies and repartitioning of the data.


ALL
Select all matching rows from the relation and is enabled by default.


DISTINCT
Select all matching rows from the relation after removing duplicates in results.


named_expression
An expression with an assigned name. In general, it denotes a column expression.
Syntax: expression [[AS] alias]


from_item
Specifies a source of input for the query. It can be one of the following:

Table relation
Join relation
Pivot relation
Unpivot relation
Table-value function
Inline table
[ LATERAL ] ( Subquery )
File



PIVOT
The PIVOT clause is used for data perspective; We can get the aggregated values based on specific column value.


UNPIVOT
The UNPIVOT clause transforms columns into rows. It is the reverse of PIVOT, except for aggregation of values.


LATERAL VIEW
The LATERAL VIEW clause is used in conjunction with generator functions such as EXPLODE, which will generate a virtual table containing one or more rows. LATERAL VIEW will apply the rows to each original output row.


WHERE
Filters the result of the FROM clause based on the supplied predicates.


GROUP BY
Specifies the expressions that are used to group the rows. This is used in conjunction with aggregate functions
   (MIN, MAX, COUNT, SUM, AVG, etc.) to group rows based on the grouping expressions and aggregate values in each group.
   When a FILTER clause is attached to an aggregate function, only the matching rows are passed to that function.


HAVING
Specifies the predicates by which the rows produced by GROUP BY are filtered. The HAVING clause is used to
   filter rows after the grouping is performed. If HAVING is specified without GROUP BY, it indicates a GROUP BY
   without grouping expressions (global aggregate).


ORDER BY
Specifies an ordering of the rows of the complete result set of the query. The output rows are ordered
   across the partitions. This parameter is mutually exclusive with SORT BY,
   CLUSTER BY and DISTRIBUTE BY and can not be specified together.


SORT BY
Specifies an ordering by which the rows are ordered within each partition. This parameter is mutually
   exclusive with ORDER BY and CLUSTER BY and can not be specified together.


CLUSTER BY
Specifies a set of expressions that is used to repartition and sort the rows. Using this clause has
   the same effect of using DISTRIBUTE BY and SORT BY together.


DISTRIBUTE BY
Specifies a set of expressions by which the result rows are repartitioned. This parameter is mutually
   exclusive with ORDER BY and CLUSTER BY and can not be specified together.


LIMIT
Specifies the maximum number of rows that can be returned by a statement or subquery. This clause
   is mostly used in the conjunction with ORDER BY to produce a deterministic result.


boolean_expression
Specifies any expression that evaluates to a result type boolean. Two or
   more expressions may be combined together using the logical
   operators ( AND, OR ).


expression
Specifies a combination of one or more values, operators, and SQL functions that evaluates to a value.


named_window
Specifies aliases for one or more source window specifications. The source window specifications can
   be referenced in the widow definitions in the query.


regex_column_names
When spark.sql.parser.quotedRegexColumnNames is true, quoted identifiers (using backticks) in SELECT
   statement are interpreted as regular expressions and SELECT statement can take regex-based column specification.
   For example, below SQL will only take column c:
   SELECT `(a|b)?+.+` FROM (
     SELECT 1 as a, 2 as b, 3 as c
   )
 


TRANSFORM
Specifies a hive-style transform query specification to transform the input by forking and running user-specified command or script.


Related Statements

WHERE Clause
GROUP BY Clause
HAVING Clause
ORDER BY Clause
SORT BY Clause
CLUSTER BY Clause
DISTRIBUTE BY Clause
LIMIT Clause
OFFSET Clause
Common Table Expression
Hints
Inline Table
File
JOIN
LIKE Predicate
Set Operators
TABLESAMPLE
Table-valued Function
Window Function
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause
TRANSFORM Clause
LATERAL Subquery





















  




SQL Syntax - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SQL Syntax
Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements.
DDL Statements
Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements:

ALTER DATABASE
ALTER TABLE
ALTER VIEW
CREATE DATABASE
CREATE FUNCTION
CREATE TABLE
CREATE VIEW
DROP DATABASE
DROP FUNCTION
DROP TABLE
DROP VIEW
REPAIR TABLE
TRUNCATE TABLE
USE DATABASE

DML Statements
Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements:

INSERT TABLE
INSERT OVERWRITE DIRECTORY
LOAD

Data Retrieval Statements
Spark supports SELECT statement that is used to retrieve rows
from one or more tables according to the specified clauses. The full syntax
and brief description of supported clauses are explained in
SELECT section. The SQL statements related
to SELECT are also included in this section. Spark also provides the
ability to generate logical and physical plan for a given query using
EXPLAIN statement.

SELECT Statement

Common Table Expression
CLUSTER BY Clause
DISTRIBUTE BY Clause
GROUP BY Clause
HAVING Clause
Hints
Inline Table
File
JOIN
LIKE Predicate
LIMIT Clause
OFFSET Clause
ORDER BY Clause
Set Operators
SORT BY Clause
TABLESAMPLE
Table-valued Function
WHERE Clause
Aggregate Function
Window Function
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause
LATERAL SUBQUERY
TRANSFORM Clause


EXPLAIN

Auxiliary Statements

ADD FILE
ADD JAR
ANALYZE TABLE
CACHE TABLE
CLEAR CACHE
DESCRIBE DATABASE
DESCRIBE FUNCTION
DESCRIBE QUERY
DESCRIBE TABLE
LIST FILE
LIST JAR
REFRESH
REFRESH TABLE
REFRESH FUNCTION
RESET
SET
SHOW COLUMNS
SHOW CREATE TABLE
SHOW DATABASES
SHOW FUNCTIONS
SHOW PARTITIONS
SHOW TABLE EXTENDED
SHOW TABLES
SHOW TBLPROPERTIES
SHOW VIEWS
UNCACHE TABLE





















  




SQL Syntax - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SQL Syntax
Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements.
DDL Statements
Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements:

ALTER DATABASE
ALTER TABLE
ALTER VIEW
CREATE DATABASE
CREATE FUNCTION
CREATE TABLE
CREATE VIEW
DROP DATABASE
DROP FUNCTION
DROP TABLE
DROP VIEW
REPAIR TABLE
TRUNCATE TABLE
USE DATABASE

DML Statements
Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements:

INSERT TABLE
INSERT OVERWRITE DIRECTORY
LOAD

Data Retrieval Statements
Spark supports SELECT statement that is used to retrieve rows
from one or more tables according to the specified clauses. The full syntax
and brief description of supported clauses are explained in
SELECT section. The SQL statements related
to SELECT are also included in this section. Spark also provides the
ability to generate logical and physical plan for a given query using
EXPLAIN statement.

SELECT Statement

Common Table Expression
CLUSTER BY Clause
DISTRIBUTE BY Clause
GROUP BY Clause
HAVING Clause
Hints
Inline Table
File
JOIN
LIKE Predicate
LIMIT Clause
OFFSET Clause
ORDER BY Clause
Set Operators
SORT BY Clause
TABLESAMPLE
Table-valued Function
WHERE Clause
Aggregate Function
Window Function
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause
LATERAL SUBQUERY
TRANSFORM Clause


EXPLAIN

Auxiliary Statements

ADD FILE
ADD JAR
ANALYZE TABLE
CACHE TABLE
CLEAR CACHE
DESCRIBE DATABASE
DESCRIBE FUNCTION
DESCRIBE QUERY
DESCRIBE TABLE
LIST FILE
LIST JAR
REFRESH
REFRESH TABLE
REFRESH FUNCTION
RESET
SET
SHOW COLUMNS
SHOW CREATE TABLE
SHOW DATABASES
SHOW FUNCTIONS
SHOW PARTITIONS
SHOW TABLE EXTENDED
SHOW TABLES
SHOW TBLPROPERTIES
SHOW VIEWS
UNCACHE TABLE





















  




SQL Syntax - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SQL Syntax
Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements.
DDL Statements
Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements:

ALTER DATABASE
ALTER TABLE
ALTER VIEW
CREATE DATABASE
CREATE FUNCTION
CREATE TABLE
CREATE VIEW
DROP DATABASE
DROP FUNCTION
DROP TABLE
DROP VIEW
REPAIR TABLE
TRUNCATE TABLE
USE DATABASE

DML Statements
Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements:

INSERT TABLE
INSERT OVERWRITE DIRECTORY
LOAD

Data Retrieval Statements
Spark supports SELECT statement that is used to retrieve rows
from one or more tables according to the specified clauses. The full syntax
and brief description of supported clauses are explained in
SELECT section. The SQL statements related
to SELECT are also included in this section. Spark also provides the
ability to generate logical and physical plan for a given query using
EXPLAIN statement.

SELECT Statement

Common Table Expression
CLUSTER BY Clause
DISTRIBUTE BY Clause
GROUP BY Clause
HAVING Clause
Hints
Inline Table
File
JOIN
LIKE Predicate
LIMIT Clause
OFFSET Clause
ORDER BY Clause
Set Operators
SORT BY Clause
TABLESAMPLE
Table-valued Function
WHERE Clause
Aggregate Function
Window Function
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause
LATERAL SUBQUERY
TRANSFORM Clause


EXPLAIN

Auxiliary Statements

ADD FILE
ADD JAR
ANALYZE TABLE
CACHE TABLE
CLEAR CACHE
DESCRIBE DATABASE
DESCRIBE FUNCTION
DESCRIBE QUERY
DESCRIBE TABLE
LIST FILE
LIST JAR
REFRESH
REFRESH TABLE
REFRESH FUNCTION
RESET
SET
SHOW COLUMNS
SHOW CREATE TABLE
SHOW DATABASES
SHOW FUNCTIONS
SHOW PARTITIONS
SHOW TABLE EXTENDED
SHOW TABLES
SHOW TBLPROPERTIES
SHOW VIEWS
UNCACHE TABLE





















  




SQL Syntax - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SQL Syntax
Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements.
DDL Statements
Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements:

ALTER DATABASE
ALTER TABLE
ALTER VIEW
CREATE DATABASE
CREATE FUNCTION
CREATE TABLE
CREATE VIEW
DROP DATABASE
DROP FUNCTION
DROP TABLE
DROP VIEW
REPAIR TABLE
TRUNCATE TABLE
USE DATABASE

DML Statements
Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements:

INSERT TABLE
INSERT OVERWRITE DIRECTORY
LOAD

Data Retrieval Statements
Spark supports SELECT statement that is used to retrieve rows
from one or more tables according to the specified clauses. The full syntax
and brief description of supported clauses are explained in
SELECT section. The SQL statements related
to SELECT are also included in this section. Spark also provides the
ability to generate logical and physical plan for a given query using
EXPLAIN statement.

SELECT Statement

Common Table Expression
CLUSTER BY Clause
DISTRIBUTE BY Clause
GROUP BY Clause
HAVING Clause
Hints
Inline Table
File
JOIN
LIKE Predicate
LIMIT Clause
OFFSET Clause
ORDER BY Clause
Set Operators
SORT BY Clause
TABLESAMPLE
Table-valued Function
WHERE Clause
Aggregate Function
Window Function
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause
LATERAL SUBQUERY
TRANSFORM Clause


EXPLAIN

Auxiliary Statements

ADD FILE
ADD JAR
ANALYZE TABLE
CACHE TABLE
CLEAR CACHE
DESCRIBE DATABASE
DESCRIBE FUNCTION
DESCRIBE QUERY
DESCRIBE TABLE
LIST FILE
LIST JAR
REFRESH
REFRESH TABLE
REFRESH FUNCTION
RESET
SET
SHOW COLUMNS
SHOW CREATE TABLE
SHOW DATABASES
SHOW FUNCTIONS
SHOW PARTITIONS
SHOW TABLE EXTENDED
SHOW TABLES
SHOW TBLPROPERTIES
SHOW VIEWS
UNCACHE TABLE





















  




SQL Syntax - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Data Definition Statements
            
        



            
                Data Manipulation Statements
            
        



            
                Data Retrieval(Queries)
            
        



            
                Auxiliary Statements
            
        





            
                Error Conditions
            
        







SQL Syntax
Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements.
DDL Statements
Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements:

ALTER DATABASE
ALTER TABLE
ALTER VIEW
CREATE DATABASE
CREATE FUNCTION
CREATE TABLE
CREATE VIEW
DROP DATABASE
DROP FUNCTION
DROP TABLE
DROP VIEW
REPAIR TABLE
TRUNCATE TABLE
USE DATABASE

DML Statements
Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements:

INSERT TABLE
INSERT OVERWRITE DIRECTORY
LOAD

Data Retrieval Statements
Spark supports SELECT statement that is used to retrieve rows
from one or more tables according to the specified clauses. The full syntax
and brief description of supported clauses are explained in
SELECT section. The SQL statements related
to SELECT are also included in this section. Spark also provides the
ability to generate logical and physical plan for a given query using
EXPLAIN statement.

SELECT Statement

Common Table Expression
CLUSTER BY Clause
DISTRIBUTE BY Clause
GROUP BY Clause
HAVING Clause
Hints
Inline Table
File
JOIN
LIKE Predicate
LIMIT Clause
OFFSET Clause
ORDER BY Clause
Set Operators
SORT BY Clause
TABLESAMPLE
Table-valued Function
WHERE Clause
Aggregate Function
Window Function
CASE Clause
PIVOT Clause
UNPIVOT Clause
LATERAL VIEW Clause
LATERAL SUBQUERY
TRANSFORM Clause


EXPLAIN

Auxiliary Statements

ADD FILE
ADD JAR
ANALYZE TABLE
CACHE TABLE
CLEAR CACHE
DESCRIBE DATABASE
DESCRIBE FUNCTION
DESCRIBE QUERY
DESCRIBE TABLE
LIST FILE
LIST JAR
REFRESH
REFRESH TABLE
REFRESH FUNCTION
RESET
SET
SHOW COLUMNS
SHOW CREATE TABLE
SHOW DATABASES
SHOW FUNCTIONS
SHOW PARTITIONS
SHOW TABLE EXTENDED
SHOW TABLES
SHOW TBLPROPERTIES
SHOW VIEWS
UNCACHE TABLE





















  




SQL Reference - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects












Spark SQL Guide



            
                Getting Started
            
        



            
                Data Sources
            
        



            
                Performance Tuning
            
        



            
                Distributed SQL Engine
            
        



            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        



            
                Migration Guide
            
        



            
                SQL Reference
            
        




            
                ANSI Compliance
            
        



            
                Data Types
            
        



            
                Datetime Pattern
            
        



            
                Number Pattern
            
        



            
                Functions
            
        



            
                Identifiers
            
        



            
                IDENTIFIER clause
            
        



            
                Literals
            
        



            
                Null Semantics
            
        



            
                SQL Syntax
            
        




            
                Error Conditions
            
        







SQL Reference
Spark SQL is Apache Spark’s module for working with structured data. This guide is a reference for Structured Query Language (SQL) and includes syntax, semantics, keywords, and examples for common SQL usage. It contains information for the following topics:

ANSI Compliance
Data Types
Datetime Pattern
Number Pattern
Functions

Built-in Functions
Scalar User-Defined Functions (UDFs)
User-Defined Aggregate Functions (UDAFs)
Integration with Hive UDFs/UDAFs/UDTFs


Identifiers
IDENTIFIER clause
Literals
Null Semantics
SQL Syntax

DDL Statements
DML Statements
Data Retrieval Statements
Auxiliary Statements























  




Spark Streaming - Spark 3.5.5 Documentation



















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Spark Streaming Programming Guide

Note
Overview
A Quick Example
Basic Concepts 
Linking
Initializing StreamingContext
Discretized Streams (DStreams)
Input DStreams and Receivers
Transformations on DStreams
Output Operations on DStreams
DataFrame and SQL Operations
MLlib Operations
Caching / Persistence
Checkpointing
Accumulators, Broadcast Variables, and Checkpoints
Deploying Applications
Monitoring Applications


Performance Tuning 
Reducing the Batch Processing Times
Setting the Right Batch Interval
Memory Tuning


Fault-tolerance Semantics
Where to Go from Here

Note
Spark Streaming is the previous generation of Spark’s streaming engine. There are no longer
updates to Spark Streaming and it’s a legacy project. There is a newer and easier to use
streaming engine in Spark called Structured Streaming. You should use Spark Structured Streaming
for your streaming applications and pipelines. See
Structured Streaming Programming Guide.
Overview
Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput,
fault-tolerant stream processing of live data streams. Data can be ingested from many sources
like Kafka, Kinesis, or TCP sockets, and can be processed using complex
algorithms expressed with high-level functions like map, reduce, join and window.
Finally, processed data can be pushed out to filesystems, databases,
and live dashboards. In fact, you can apply Spark’s
machine learning and
graph processing algorithms on data streams.



Internally, it works as follows. Spark Streaming receives live input data streams and divides
the data into batches, which are then processed by the Spark engine to generate the final
stream of results in batches.



Spark Streaming provides a high-level abstraction called discretized stream or DStream,
which represents a continuous stream of data. DStreams can be created either from input data
streams from sources such as Kafka, and Kinesis, or by applying high-level
operations on other DStreams. Internally, a DStream is represented as a sequence of
RDDs.
This guide shows you how to start writing Spark Streaming programs with DStreams. You can
write Spark Streaming programs in Scala, Java or Python (introduced in Spark 1.2),
all of which are presented in this guide.
You will find tabs throughout this guide that let you choose between code snippets of
different languages.
Note: There are a few APIs that are either different or not available in Python. Throughout this guide, you will find the tag Python API highlighting these differences.

A Quick Example
Before we go into the details of how to write your own Spark Streaming program,
let’s take a quick look at what a simple Spark Streaming program looks like. Let’s say we want to
count the number of words in text data received from a data server listening on a TCP
socket. All you need to
do is as follows.


First, we import StreamingContext, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second.
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Create a local StreamingContext with two working thread and batch interval of 1 second
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)
Using this context, we can create a DStream that represents streaming data from a TCP
source, specified as hostname (e.g. localhost) and port (e.g. 9999).
# Create a DStream that will connect to hostname:port, like localhost:9999
lines = ssc.socketTextStream("localhost", 9999)
This lines DStream represents the stream of data that will be received from the data
server. Each record in this DStream is a line of text. Next, we want to split the lines by
space into words.
# Split each line into words
words = lines.flatMap(lambda line: line.split(" "))
flatMap is a one-to-many DStream operation that creates a new DStream by
generating multiple new records from each record in the source DStream. In this case,
each line will be split into multiple words and the stream of words is represented as the
words DStream.  Next, we want to count these words.
# Count each word in each batch
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.pprint()
The words DStream is further mapped (one-to-one transformation) to a DStream of (word,
1) pairs, which is then reduced to get the frequency of words in each batch of data.
Finally, wordCounts.pprint() will print a few of the counts generated every second.
Note that when these lines are executed, Spark Streaming only sets up the computation it
will perform when it is started, and no real processing has started yet. To start the processing
after all the transformations have been setup, we finally call
ssc.start()             # Start the computation
ssc.awaitTermination()  # Wait for the computation to terminate
The complete code can be found in the Spark Streaming example
NetworkWordCount.



First, we import the names of the Spark Streaming classes and some implicit
conversions from StreamingContext into our environment in order to add useful methods to
other classes we need (like DStream). StreamingContext is the
main entry point for all streaming functionality. We create a local StreamingContext with two execution threads,  and a batch interval of 1 second.
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3

// Create a local StreamingContext with two working thread and batch interval of 1 second.
// The master requires 2 cores to prevent a starvation scenario.

val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
val ssc = new StreamingContext(conf, Seconds(1))
Using this context, we can create a DStream that represents streaming data from a TCP
source, specified as hostname (e.g. localhost) and port (e.g. 9999).
// Create a DStream that will connect to hostname:port, like localhost:9999
val lines = ssc.socketTextStream("localhost", 9999)
This lines DStream represents the stream of data that will be received from the data
server. Each record in this DStream is a line of text. Next, we want to split the lines by
space characters into words.
// Split each line into words
val words = lines.flatMap(_.split(" "))
flatMap is a one-to-many DStream operation that creates a new DStream by
generating multiple new records from each record in the source DStream. In this case,
each line will be split into multiple words and the stream of words is represented as the
words DStream.  Next, we want to count these words.
import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3
// Count each word in each batch
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

// Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.print()
The words DStream is further mapped (one-to-one transformation) to a DStream of (word,
1) pairs, which is then reduced to get the frequency of words in each batch of data.
Finally, wordCounts.print() will print a few of the counts generated every second.
Note that when these lines are executed, Spark Streaming only sets up the computation it
will perform when it is started, and no real processing has started yet. To start the processing
after all the transformations have been setup, we finally call
ssc.start()             // Start the computation
ssc.awaitTermination()  // Wait for the computation to terminate
The complete code can be found in the Spark Streaming example
NetworkWordCount.



First, we create a
JavaStreamingContext object,
which is the main entry point for all streaming
functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second.
import org.apache.spark.*;
import org.apache.spark.api.java.function.*;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;
import scala.Tuple2;

// Create a local StreamingContext with two working thread and batch interval of 1 second
SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount");
JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(1));
Using this context, we can create a DStream that represents streaming data from a TCP
source, specified as hostname (e.g. localhost) and port (e.g. 9999).
// Create a DStream that will connect to hostname:port, like localhost:9999
JavaReceiverInputDStream<String> lines = jssc.socketTextStream("localhost", 9999);
This lines DStream represents the stream of data that will be received from the data
server. Each record in this stream is a line of text. Then, we want to split the lines by
space into words.
// Split each line into words
JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(x.split(" ")).iterator());
flatMap is a DStream operation that creates a new DStream by
generating multiple new records from each record in the source DStream. In this case,
each line will be split into multiple words and the stream of words is represented as the
words DStream. Note that we defined the transformation using a
FlatMapFunction object.
As we will discover along the way, there are a number of such convenience classes in the Java API
that help defines DStream transformations.
Next, we want to count these words.
// Count each word in each batch
JavaPairDStream<String, Integer> pairs = words.mapToPair(s -> new Tuple2<>(s, 1));
JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey((i1, i2) -> i1 + i2);

// Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.print();
The words DStream is further mapped (one-to-one transformation) to a DStream of (word,
1) pairs, using a PairFunction
object. Then, it is reduced to get the frequency of words in each batch of data,
using a Function2 object.
Finally, wordCounts.print() will print a few of the counts generated every second.
Note that when these lines are executed, Spark Streaming only sets up the computation it
will perform after it is started, and no real processing has started yet. To start the processing
after all the transformations have been setup, we finally call start method.
jssc.start();              // Start the computation
jssc.awaitTermination();   // Wait for the computation to terminate
The complete code can be found in the Spark Streaming example
JavaNetworkWordCount.



If you have already downloaded and built Spark,
you can run this example as follows. You will first need to run Netcat
(a small utility found in most Unix-like systems) as a data server by using
$ nc -lk 9999
Then, in a different terminal, you can start the example by using


$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999


$ ./bin/run-example streaming.NetworkWordCount localhost 9999


$ ./bin/run-example streaming.JavaNetworkWordCount localhost 9999


Then, any lines typed in the terminal running the netcat server will be counted and printed on
screen every second. It will look something like the following.


# TERMINAL 1:
# Running Netcat

$ nc -lk 9999

hello world



...





# TERMINAL 2: RUNNING network_wordcount.py

$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999
...
-------------------------------------------
Time: 2014-10-14 15:25:21
-------------------------------------------
(hello,1)
(world,1)
...


# TERMINAL 2: RUNNING NetworkWordCount

$ ./bin/run-example streaming.NetworkWordCount localhost 9999
...
-------------------------------------------
Time: 1357008430000 ms
-------------------------------------------
(hello,1)
(world,1)
...


# TERMINAL 2: RUNNING JavaNetworkWordCount

$ ./bin/run-example streaming.JavaNetworkWordCount localhost 9999
...
-------------------------------------------
Time: 1357008430000 ms
-------------------------------------------
(hello,1)
(world,1)
...






Basic Concepts
Next, we move beyond the simple example and elaborate on the basics of Spark Streaming.
Linking
Similar to Spark, Spark Streaming is available through Maven Central. To write your own Spark Streaming program, you will have to add the following dependency to your SBT or Maven project.


<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming_2.12</artifactId>
    <version>3.5.5</version>
    <scope>provided</scope>
</dependency>
 


libraryDependencies += "org.apache.spark" % "spark-streaming_2.12" % "3.5.5" % "provided"
 


For ingesting data from sources like Kafka and Kinesis that are not present in the Spark
Streaming core
 API, you will have to add the corresponding
artifact spark-streaming-xyz_2.12 to the dependencies. For example,
some of the common ones are as follows.

SourceArtifact
 Kafka  spark-streaming-kafka-0-10_2.12 
 Kinesisspark-streaming-kinesis-asl_2.12 [Amazon Software License] 


For an up-to-date list, please refer to the
Maven repository
for the full list of supported sources and artifacts.

Initializing StreamingContext
To initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality.


A StreamingContext object can be created from a SparkContext object.
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext(master, appName)
ssc = StreamingContext(sc, 1)
The appName parameter is a name for your application to show on the cluster UI.
master is a Spark, Mesos or YARN cluster URL,
or a special “local[*]” string to run in local mode. In practice, when running on a cluster,
you will not want to hardcode master in the program,
but rather launch the application with spark-submit and
receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming
in-process (detects the number of cores in the local system).
The batch interval must be set based on the latency requirements of your application
and available cluster resources. See the Performance Tuning
section for more details.


A StreamingContext object can be created from a SparkConf object.
import org.apache.spark._
import org.apache.spark.streaming._

val conf = new SparkConf().setAppName(appName).setMaster(master)
val ssc = new StreamingContext(conf, Seconds(1))
The appName parameter is a name for your application to show on the cluster UI.
master is a Spark, Mesos, Kubernetes or YARN cluster URL,
or a special “local[*]” string to run in local mode. In practice, when running on a cluster,
you will not want to hardcode master in the program,
but rather launch the application with spark-submit and
receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming
in-process (detects the number of cores in the local system). Note that this internally creates a SparkContext (starting point of all Spark functionality) which can be accessed as ssc.sparkContext.
The batch interval must be set based on the latency requirements of your application
and available cluster resources. See the Performance Tuning
section for more details.
A StreamingContext object can also be created from an existing SparkContext object.
import org.apache.spark.streaming._

val sc = ...                // existing SparkContext
val ssc = new StreamingContext(sc, Seconds(1))


A JavaStreamingContext object can be created from a SparkConf object.
import org.apache.spark.*;
import org.apache.spark.streaming.api.java.*;

SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
JavaStreamingContext ssc = new JavaStreamingContext(conf, new Duration(1000));
The appName parameter is a name for your application to show on the cluster UI.
master is a Spark, Mesos or YARN cluster URL,
or a special “local[*]” string to run in local mode. In practice, when running on a cluster,
you will not want to hardcode master in the program,
but rather launch the application with spark-submit and
receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming
in-process. Note that this internally creates a JavaSparkContext (starting point of all Spark functionality) which can be accessed as ssc.sparkContext.
The batch interval must be set based on the latency requirements of your application
and available cluster resources. See the Performance Tuning
section for more details.
A JavaStreamingContext object can also be created from an existing JavaSparkContext.
import org.apache.spark.streaming.api.java.*;

JavaSparkContext sc = ...   //existing JavaSparkContext
JavaStreamingContext ssc = new JavaStreamingContext(sc, Durations.seconds(1));


After a context is defined, you have to do the following.

Define the input sources by creating input DStreams.
Define the streaming computations by applying transformation and output operations to DStreams.
Start receiving data and processing it using streamingContext.start().
Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().
The processing can be manually stopped using streamingContext.stop().

Points to remember:

Once a context has been started, no new streaming computations can be set up or added to it.
Once a context has been stopped, it cannot be restarted.
Only one StreamingContext can be active in a JVM at the same time.
stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.
A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.


Discretized Streams (DStreams)
Discretized Stream or DStream is the basic abstraction provided by Spark Streaming.
It represents a continuous stream of data, either the input data stream received from source,
or the processed data stream generated by transforming the input stream. Internally,
a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable,
distributed dataset (see Spark Programming Guide for more details). Each RDD in a DStream contains data from a certain interval,
as shown in the following figure.



Any operation applied on a DStream translates to operations on the underlying RDDs. For example,
in the earlier example of converting a stream of lines to words,
the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the
 words DStream. This is shown in the following figure.



These underlying RDD transformations are computed by the Spark engine. The DStream operations
hide most of these details and provide the developer with a higher-level API for convenience.
These operations are discussed in detail in later sections.

Input DStreams and Receivers
Input DStreams are DStreams representing the stream of input data received from streaming
sources. In the quick example, lines was an input DStream as it represented
the stream of data received from the netcat server. Every input DStream
(except file stream, discussed later in this section) is associated with a Receiver
(Scala doc,
Java doc) object which receives the
data from a source and stores it in Spark’s memory for processing.
Spark Streaming provides two categories of built-in streaming sources.

Basic sources: Sources directly available in the StreamingContext API.
Examples: file systems, and socket connections.
Advanced sources: Sources like Kafka, Kinesis, etc. are available through
extra utility classes. These require linking against extra dependencies as discussed in the
linking section.

We are going to discuss some of the sources present in each category later in this section.
Note that, if you want to receive multiple streams of data in parallel in your streaming
application, you can create multiple input DStreams (discussed
further in the Performance Tuning section). This will
create multiple receivers which will simultaneously receive multiple data streams. But note that a
Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the
Spark Streaming application. Therefore, it is important to remember that a Spark Streaming application
needs to be allocated enough cores (or threads, if running locally) to process the received data,
as well as to run the receiver(s).
Points to remember


When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL.
Either of these means that only one thread will be used for running tasks locally. If you are using
an input DStream based on a receiver (e.g. sockets, Kafka, etc.), then the single thread will
be used to run the receiver, leaving no thread for processing the received data. Hence, when
running locally, always use “local[n]” as the master URL, where n > number of receivers to run
(see Spark Properties for information on how to set
the master).


Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming
application must be more than the number of receivers. Otherwise the system will receive data, but
not be able to process it.


Basic Sources
We have already taken a look at the ssc.socketTextStream(...) in the quick example
which creates a DStream from text
data received over a TCP socket connection. Besides sockets, the StreamingContext API provides
methods for creating DStreams from files as input sources.
File Streams
For reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as
via StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass].
File streams do not require running a receiver so there is no need to allocate any cores for receiving file data.
For simple text files, the easiest method is StreamingContext.textFileStream(dataDirectory).


fileStream is not available in the Python API; only textFileStream is available.
streamingContext.textFileStream(dataDirectory)


streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory)
For text files
streamingContext.textFileStream(dataDirectory)


streamingContext.fileStream<KeyClass, ValueClass, InputFormatClass>(dataDirectory);
For text files
streamingContext.textFileStream(dataDirectory);


How Directories are Monitored
Spark Streaming will monitor the directory dataDirectory and process any files created in that directory.

A simple directory can be monitored, such as "hdfs://namenode:8040/logs/".
All files directly under such a path will be processed as they are discovered.
A POSIX glob pattern can be supplied, such as
"hdfs://namenode:8040/logs/2017/*".
Here, the DStream will consist of all files in the directories
matching the pattern.
That is: it is a pattern of directories, not of files in directories.
All files must be in the same data format.
A file is considered part of a time period based on its modification time,
not its creation time.
Once processed, changes to a file within the current window will not cause the file to be reread.
That is: updates are ignored.
The more files under a directory, the longer it will take to
scan for changes — even if no files have been modified.
If a wildcard is used to identify directories, such as "hdfs://namenode:8040/logs/2016-*",
renaming an entire directory to match the path will add the directory to the list of
monitored directories. Only the files in the directory whose modification time is
within the current window will be included in the stream.
Calling FileSystem.setTimes()
to fix the timestamp is a way to have the file picked up in a later window, even if its contents have not changed.

Using Object Stores as a source of data
“Full” Filesystems such as HDFS tend to set the modification time on their files as soon
as the output stream is created.
When a file is opened, even before data has been completely written,
it may be included in the DStream - after which updates to the file within the same window
will be ignored. That is: changes may be missed, and data omitted from the stream.
To guarantee that changes are picked up in a window, write the file
to an unmonitored directory, then, immediately after the output stream is closed,
rename it into the destination directory.
Provided the renamed file appears in the scanned destination directory during the window
of its creation, the new data will be picked up.
In contrast, Object Stores such as Amazon S3 and Azure Storage usually have slow rename operations, as the
data is actually copied.
Furthermore, a renamed object may have the time of the rename() operation as its modification time, so
may not be considered part of the window which the original create time implied they were.
Careful testing is needed against the target object store to verify that the timestamp behavior
of the store is consistent with that expected by Spark Streaming. It may be
that writing directly into a destination directory is the appropriate strategy for
streaming data via the chosen object store.
For more details on this topic, consult the Hadoop Filesystem Specification.
Streams based on Custom Receivers
DStreams can be created with data streams received through custom receivers. See the Custom Receiver
  Guide for more details.
Queue of RDDs as a Stream
For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using streamingContext.queueStream(queueOfRDDs). Each RDD pushed into the queue will be treated as a batch of data in the DStream, and processed like a stream.
For more details on streams from sockets and files, see the API documentations of the relevant functions in
StreamingContext for
Scala, JavaStreamingContext
for Java, and StreamingContext for Python.
Advanced Sources
Python API As of Spark 3.5.5,
out of these sources, Kafka and Kinesis are available in the Python API.
This category of sources requires interfacing with external non-Spark libraries, some of them with
complex dependencies (e.g., Kafka). Hence, to minimize issues related to version conflicts
of dependencies, the functionality to create DStreams from these sources has been moved to separate
libraries that can be linked to explicitly when necessary.
Note that these advanced sources are not available in the Spark shell, hence applications based on
these advanced sources cannot be tested in the shell. If you really want to use them in the Spark
shell you will have to download the corresponding Maven artifact’s JAR along with its dependencies
and add it to the classpath.
Some of these advanced sources are as follows.


Kafka: Spark Streaming 3.5.5 is compatible with Kafka broker versions 0.10 or higher. See the Kafka Integration Guide for more details.


Kinesis: Spark Streaming 3.5.5 is compatible with Kinesis Client Library 1.2.1. See the Kinesis Integration Guide for more details.


Custom Sources
Python API This is not yet supported in Python.
Input DStreams can also be created out of custom data sources. All you have to do is implement a
user-defined receiver (see next section to understand what that is) that can receive data from
the custom sources and push it into Spark. See the Custom Receiver
Guide for details.
Receiver Reliability
There can be two kinds of data sources based on their reliability. Sources
(like Kafka) allow the transferred data to be acknowledged. If the system receiving
data from these reliable sources acknowledges the received data correctly, it can be ensured
that no data will be lost due to any kind of failure. This leads to two kinds of receivers:

Reliable Receiver - A reliable receiver correctly sends acknowledgment to a reliable
  source when the data has been received and stored in Spark with replication.
Unreliable Receiver - An unreliable receiver does not send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment.

The details of how to write a reliable receiver are discussed in the
Custom Receiver Guide.

Transformations on DStreams
Similar to that of RDDs, transformations allow the data from the input DStream to be modified.
DStreams support many of the transformations available on normal Spark RDD’s.
Some of the common ones are as follows.

TransformationMeaning

 map(func) 
 Return a new DStream by passing each element of the source DStream through a
  function func. 


 flatMap(func) 
 Similar to map, but each input item can be mapped to 0 or more output items. 


 filter(func) 
 Return a new DStream by selecting only the records of the source DStream on which
  func returns true. 


 repartition(numPartitions) 
 Changes the level of parallelism in this DStream by creating more or fewer partitions. 


 union(otherStream) 
 Return a new DStream that contains the union of the elements in the source DStream and
  otherDStream. 


 count() 
 Return a new DStream of single-element RDDs by counting the number of elements in each RDD
   of the source DStream. 


 reduce(func) 
 Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the
  source DStream using a function func (which takes two arguments and returns one).
  The function should be associative and commutative so that it can be computed in parallel. 


 countByValue() 
 When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs
  where the value of each key is its frequency in each RDD of the source DStream.  


 reduceByKey(func, [numTasks]) 
 When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the
  values for each key are aggregated using the given reduce function. Note: By default,
  this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number
  is determined by the config property spark.default.parallelism) to do the grouping.
  You can pass an optional numTasks argument to set a different number of tasks.


 join(otherStream, [numTasks]) 
 When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W))
  pairs with all pairs of elements for each key. 


 cogroup(otherStream, [numTasks]) 
 When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of
  (K, Seq[V], Seq[W]) tuples.


 transform(func) 
 Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream.
  This can be used to do arbitrary RDD operations on the DStream. 


 updateStateByKey(func) 
 Return a new "state" DStream where the state for each key is updated by applying the
  given function on the previous state of the key and the new values for the key. This can be
  used to maintain arbitrary state data for each key.



A few of these transformations are worth discussing in more detail.
UpdateStateByKey Operation
The updateStateByKey operation allows you to maintain arbitrary state while continuously updating
it with new information. To use this, you will have to do two steps.

Define the state - The state can be an arbitrary data type.
Define the state update function - Specify with a function how to update the state using the
previous state and the new values from an input stream.

In every batch, Spark will apply the state  update function for all existing keys, regardless of whether they have new data in a batch or not. If the update function returns None then the key-value pair will be eliminated.
Let’s illustrate this with an example. Say you want to maintain a running count of each word
seen in a text data stream. Here, the running count is the state and it is an integer. We
define the update function as:


def updateFunction(newValues, runningCount):
    if runningCount is None:
        runningCount = 0
    return sum(newValues, runningCount)  # add the new values with the previous running count to get the new count
This is applied on a DStream containing words (say, the pairs DStream containing (word,
1) pairs in the earlier example).
runningCounts = pairs.updateStateByKey(updateFunction)
The update function will be called for each word, with newValues having a sequence of 1’s (from
the (word, 1) pairs) and the runningCount having the previous count. For the complete
Python code, take a look at the example
stateful_network_wordcount.py.


def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {
    val newCount = ...  // add the new values with the previous running count to get the new count
    Some(newCount)
}
This is applied on a DStream containing words (say, the pairs DStream containing (word,
1) pairs in the earlier example).
val runningCounts = pairs.updateStateByKey[Int](updateFunction _)
The update function will be called for each word, with newValues having a sequence of 1’s (from
the (word, 1) pairs) and the runningCount having the previous count.


Function2<List<Integer>, Optional<Integer>, Optional<Integer>> updateFunction =
  (values, state) -> {
    Integer newSum = ...  // add the new values with the previous running count to get the new count
    return Optional.of(newSum);
  };
This is applied on a DStream containing words (say, the pairs DStream containing (word,
1) pairs in the quick example).
JavaPairDStream<String, Integer> runningCounts = pairs.updateStateByKey(updateFunction);
The update function will be called for each word, with newValues having a sequence of 1’s (from
the (word, 1) pairs) and the runningCount having the previous count. For the complete
Java code, take a look at the example
JavaStatefulNetworkWordCount.java.


Note that using updateStateByKey requires the checkpoint directory to be configured, which is
discussed in detail in the checkpointing section.
Transform Operation
The transform operation (along with its variations like transformWith) allows
arbitrary RDD-to-RDD functions to be applied on a DStream. It can be used to apply any RDD
operation that is not exposed in the DStream API.
For example, the functionality of joining every batch in a data stream
with another dataset is not directly exposed in the DStream API. However,
you can easily use transform to do this. This enables very powerful possibilities. For example,
one can do real-time data cleaning by joining the input data stream with precomputed
spam information (maybe generated with Spark as well) and then filtering based on it.


spamInfoRDD = sc.pickleFile(...)  # RDD containing spam information

# join data stream with spam information to do data cleaning
cleanedDStream = wordCounts.transform(lambda rdd: rdd.join(spamInfoRDD).filter(...))


val spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) // RDD containing spam information

val cleanedDStream = wordCounts.transform { rdd =>
  rdd.join(spamInfoRDD).filter(...) // join data stream with spam information to do data cleaning
  ...
}


import org.apache.spark.streaming.api.java.*;
// RDD containing spam information
JavaPairRDD<String, Double> spamInfoRDD = jssc.sparkContext().newAPIHadoopRDD(...);

JavaPairDStream<String, Integer> cleanedDStream = wordCounts.transform(rdd -> {
  rdd.join(spamInfoRDD).filter(...); // join data stream with spam information to do data cleaning
  ...
});


Note that the supplied function gets called in every batch interval. This allows you to do
time-varying RDD operations, that is, RDD operations, number of partitions, broadcast variables,
etc. can be changed between batches.
Window Operations
Spark Streaming also provides windowed computations, which allow you to apply
transformations over a sliding window of data. The following figure illustrates this sliding
window.



As shown in the figure, every time the window slides over a source DStream,
the source RDDs that fall within the window are combined and operated upon to produce the
RDDs of the windowed DStream. In this specific case, the operation is applied over the last 3 time
units of data, and slides by 2 time units. This shows that any window operation needs to
specify two parameters.

window length - The duration of the window (3 in the figure).
sliding interval - The interval at which the window operation is performed (2 in
 the figure).

These two parameters must be multiples of the batch interval of the source DStream (1 in the
figure).
Let’s illustrate the window operations with an example. Say, you want to extend the
earlier example by generating word counts over the last 30 seconds of data,
every 10 seconds. To do this, we have to apply the reduceByKey operation on the pairs DStream of
(word, 1) pairs over the last 30 seconds of data. This is done using the
operation reduceByKeyAndWindow.


# Reduce last 30 seconds of data, every 10 seconds
windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)


// Reduce last 30 seconds of data, every 10 seconds
val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) => (a + b), Seconds(30), Seconds(10))


// Reduce last 30 seconds of data, every 10 seconds
JavaPairDStream<String, Integer> windowedWordCounts = pairs.reduceByKeyAndWindow((i1, i2) -> i1 + i2, Durations.seconds(30), Durations.seconds(10));


Some of the common window operations are as follows. All of these operations take the
said two parameters - windowLength and slideInterval.

TransformationMeaning

 window(windowLength, slideInterval) 
 Return a new DStream which is computed based on windowed batches of the source DStream.
  


 countByWindow(windowLength, slideInterval) 
 Return a sliding window count of elements in the stream.
  


 reduceByWindow(func, windowLength, slideInterval) 
 Return a new single-element stream, created by aggregating elements in the stream over a
  sliding interval using func. The function should be associative and commutative so that it can be computed
  correctly in parallel.
  


 reduceByKeyAndWindow(func, windowLength, slideInterval,
  [numTasks]) 
 When called on a DStream of (K, V) pairs, returns a new DStream of (K, V)
  pairs where the values for each key are aggregated using the given reduce function func
  over batches in a sliding window. Note: By default, this uses Spark's default number of
  parallel tasks (2 for local mode, and in cluster mode the number is determined by the config
  property spark.default.parallelism) to do the grouping. You can pass an optional
  numTasks argument to set a different number of tasks.
  


 reduceByKeyAndWindow(func, invFunc, windowLength,
  slideInterval, [numTasks]) 

A more efficient version of the above reduceByKeyAndWindow() where the reduce
  value of each window is calculated incrementally using the reduce values of the previous window.
  This is done by reducing the new data that enters the sliding window, and “inverse reducing” the
  old data that leaves the window. An example would be that of “adding” and “subtracting” counts
  of keys as the window slides. However, it is applicable only to “invertible reduce functions”,
  that is, those reduce functions which have a corresponding “inverse reduce” function (taken as
  parameter invFunc). Like in reduceByKeyAndWindow, the number of reduce tasks
  is configurable through an optional argument. Note that checkpointing must be
  enabled for using this operation.



 countByValueAndWindow(windowLength,
  slideInterval, [numTasks]) 
 When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the
  value of each key is its frequency within a sliding window. Like in
  reduceByKeyAndWindow, the number of reduce tasks is configurable through an
  optional argument.




Join Operations
Finally, it’s worth highlighting how easily you can perform different kinds of joins in Spark Streaming.
Stream-stream joins
Streams can be very easily joined with other streams.


stream1 = ...
stream2 = ...
joinedStream = stream1.join(stream2)


val stream1: DStream[String, String] = ...
val stream2: DStream[String, String] = ...
val joinedStream = stream1.join(stream2)


JavaPairDStream<String, String> stream1 = ...
JavaPairDStream<String, String> stream2 = ...
JavaPairDStream<String, Tuple2<String, String>> joinedStream = stream1.join(stream2);


Here, in each batch interval, the RDD generated by stream1 will be joined with the RDD generated by stream2. You can also do leftOuterJoin, rightOuterJoin, fullOuterJoin. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well.


windowedStream1 = stream1.window(20)
windowedStream2 = stream2.window(60)
joinedStream = windowedStream1.join(windowedStream2)


val windowedStream1 = stream1.window(Seconds(20))
val windowedStream2 = stream2.window(Minutes(1))
val joinedStream = windowedStream1.join(windowedStream2)


JavaPairDStream<String, String> windowedStream1 = stream1.window(Durations.seconds(20));
JavaPairDStream<String, String> windowedStream2 = stream2.window(Durations.minutes(1));
JavaPairDStream<String, Tuple2<String, String>> joinedStream = windowedStream1.join(windowedStream2);


Stream-dataset joins
This has already been shown earlier while explain DStream.transform operation. Here is yet another example of joining a windowed stream with a dataset.


dataset = ... # some RDD
windowedStream = stream.window(20)
joinedStream = windowedStream.transform(lambda rdd: rdd.join(dataset))


val dataset: RDD[String, String] = ...
val windowedStream = stream.window(Seconds(20))...
val joinedStream = windowedStream.transform { rdd => rdd.join(dataset) }


JavaPairRDD<String, String> dataset = ...
JavaPairDStream<String, String> windowedStream = stream.window(Durations.seconds(20));
JavaPairDStream<String, String> joinedStream = windowedStream.transform(rdd -> rdd.join(dataset));


In fact, you can also dynamically change the dataset you want to join against. The function provided to transform is evaluated every batch interval and therefore will use the current dataset that dataset reference points to.
The complete list of DStream transformations is available in the API documentation. For the Scala API,
see DStream
and PairDStreamFunctions.
For the Java API, see JavaDStream
and JavaPairDStream.
For the Python API, see DStream.

Output Operations on DStreams
Output operations allow DStream’s data to be pushed out to external systems like a database or a file system.
Since the output operations actually allow the transformed data to be consumed by external systems,
they trigger the actual execution of all the DStream transformations (similar to actions for RDDs).
Currently, the following output operations are defined:

Output OperationMeaning

 print()
 Prints the first ten elements of every batch of data in a DStream on the driver node running
  the streaming application. This is useful for development and debugging.
  
Python API This is called
  pprint() in the Python API.
  


 saveAsTextFiles(prefix, [suffix]) 
 Save this DStream's contents as text files. The file name at each batch interval is
  generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]". 


 saveAsObjectFiles(prefix, [suffix]) 
 Save this DStream's contents as SequenceFiles of serialized Java objects. The file
  name at each batch interval is generated based on prefix and
  suffix: "prefix-TIME_IN_MS[.suffix]".
  
Python API This is not available in
  the Python API.
  


 saveAsHadoopFiles(prefix, [suffix]) 
 Save this DStream's contents as Hadoop files. The file name at each batch interval is
  generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]".
  
Python API This is not available in
  the Python API.
  


 foreachRDD(func) 
 The most generic output operator that applies a function, func, to each RDD generated from
  the stream. This function should push the data in each RDD to an external system, such as saving the RDD to
  files, or writing it over the network to a database. Note that the function func is executed
  in the driver process running the streaming application, and will usually have RDD actions in it
  that will force the computation of the streaming RDDs.



Design Patterns for using foreachRDD
dstream.foreachRDD is a powerful primitive that allows data to be sent out to external systems.
However, it is important to understand how to use this primitive correctly and efficiently.
Some of the common mistakes to avoid are as follows.
Often writing data to external systems requires creating a connection object
(e.g. TCP connection to a remote server) and using it to send data to a remote system.
For this purpose, a developer may inadvertently try creating a connection object at
the Spark driver, and then try to use it in a Spark worker to save records in the RDDs.
For example (in Scala),


def sendRecord(rdd):
    connection = createNewConnection()  # executed at the driver
    rdd.foreach(lambda record: connection.send(record))
    connection.close()

dstream.foreachRDD(sendRecord)


dstream.foreachRDD { rdd =>
  val connection = createNewConnection()  // executed at the driver
  rdd.foreach { record =>
    connection.send(record) // executed at the worker
  }
}


dstream.foreachRDD(rdd -> {
  Connection connection = createNewConnection(); // executed at the driver
  rdd.foreach(record -> {
    connection.send(record); // executed at the worker
  });
});


This is incorrect as this requires the connection object to be serialized and sent from the
driver to the worker. Such connection objects are rarely transferable across machines. This
error may manifest as serialization errors (connection object not serializable), initialization
errors (connection object needs to be initialized at the workers), etc. The correct solution is
to create the connection object at the worker.
However, this can lead to another common mistake - creating a new connection for every record.
For example,


def sendRecord(record):
    connection = createNewConnection()
    connection.send(record)
    connection.close()

dstream.foreachRDD(lambda rdd: rdd.foreach(sendRecord))


dstream.foreachRDD { rdd =>
  rdd.foreach { record =>
    val connection = createNewConnection()
    connection.send(record)
    connection.close()
  }
}


dstream.foreachRDD(rdd -> {
  rdd.foreach(record -> {
    Connection connection = createNewConnection();
    connection.send(record);
    connection.close();
  });
});


Typically, creating a connection object has time and resource overheads. Therefore, creating and
destroying a connection object for each record can incur unnecessarily high overheads and can
significantly reduce the overall throughput of the system. A better solution is to use
rdd.foreachPartition - create a single connection object and send all the records in a RDD
partition using that connection.


def sendPartition(iter):
    connection = createNewConnection()
    for record in iter:
        connection.send(record)
    connection.close()

dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))


dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    val connection = createNewConnection()
    partitionOfRecords.foreach(record => connection.send(record))
    connection.close()
  }
}


dstream.foreachRDD(rdd -> {
  rdd.foreachPartition(partitionOfRecords -> {
    Connection connection = createNewConnection();
    while (partitionOfRecords.hasNext()) {
      connection.send(partitionOfRecords.next());
    }
    connection.close();
  });
});


This amortizes the connection creation overheads over many records.
Finally, this can be further optimized by reusing connection objects across multiple RDDs/batches.
One can maintain a static pool of connection objects than can be reused as
RDDs of multiple batches are pushed to the external system, thus further reducing the overheads.


def sendPartition(iter):
    # ConnectionPool is a static, lazily initialized pool of connections
    connection = ConnectionPool.getConnection()
    for record in iter:
        connection.send(record)
    # return to the pool for future reuse
    ConnectionPool.returnConnection(connection)

dstream.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))


dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    // ConnectionPool is a static, lazily initialized pool of connections
    val connection = ConnectionPool.getConnection()
    partitionOfRecords.foreach(record => connection.send(record))
    ConnectionPool.returnConnection(connection)  // return to the pool for future reuse
  }
}


dstream.foreachRDD(rdd -> {
  rdd.foreachPartition(partitionOfRecords -> {
    // ConnectionPool is a static, lazily initialized pool of connections
    Connection connection = ConnectionPool.getConnection();
    while (partitionOfRecords.hasNext()) {
      connection.send(partitionOfRecords.next());
    }
    ConnectionPool.returnConnection(connection); // return to the pool for future reuse
  });
});


Note that the connections in the pool should be lazily created on demand and timed out if not used for a while. This achieves the most efficient sending of data to external systems.
Other points to remember:


DStreams are executed lazily by the output operations, just like RDDs are lazily executed by RDD actions. Specifically, RDD actions inside the DStream output operations force the processing of the received data. Hence, if your application does not have any output operation, or has output operations like dstream.foreachRDD() without any RDD action inside them, then nothing will get executed. The system will simply receive the data and discard it.


By default, output operations are executed one-at-a-time. And they are executed in the order they are defined in the application.



DataFrame and SQL Operations
You can easily use DataFrames and SQL operations on streaming data. You have to create a SparkSession using the SparkContext that the StreamingContext is using. Furthermore, this has to be done such that it can be restarted on driver failures. This is done by creating a lazily instantiated singleton instance of SparkSession. This is shown in the following example. It modifies the earlier word count example to generate word counts using DataFrames and SQL. Each RDD is converted to a DataFrame, registered as a temporary table and then queried using SQL.


# Lazily instantiated global instance of SparkSession
def getSparkSessionInstance(sparkConf):
    if ("sparkSessionSingletonInstance" not in globals()):
        globals()["sparkSessionSingletonInstance"] = SparkSession \
            .builder \
            .config(conf=sparkConf) \
            .getOrCreate()
    return globals()["sparkSessionSingletonInstance"]

...

# DataFrame operations inside your streaming program

words = ... # DStream of strings

def process(time, rdd):
    print("========= %s =========" % str(time))
    try:
        # Get the singleton instance of SparkSession
        spark = getSparkSessionInstance(rdd.context.getConf())

        # Convert RDD[String] to RDD[Row] to DataFrame
        rowRdd = rdd.map(lambda w: Row(word=w))
        wordsDataFrame = spark.createDataFrame(rowRdd)

        # Creates a temporary view using the DataFrame
        wordsDataFrame.createOrReplaceTempView("words")

        # Do word count on table using SQL and print it
        wordCountsDataFrame = spark.sql("select word, count(*) as total from words group by word")
        wordCountsDataFrame.show()
    except:
        pass

words.foreachRDD(process)
See the full source code.


/** DataFrame operations inside your streaming program */

val words: DStream[String] = ...

words.foreachRDD { rdd =>

  // Get the singleton instance of SparkSession
  val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()
  import spark.implicits._

  // Convert RDD[String] to DataFrame
  val wordsDataFrame = rdd.toDF("word")

  // Create a temporary view
  wordsDataFrame.createOrReplaceTempView("words")

  // Do word count on DataFrame using SQL and print it
  val wordCountsDataFrame =
    spark.sql("select word, count(*) as total from words group by word")
  wordCountsDataFrame.show()
}
See the full source code.


/** Java Bean class for converting RDD to DataFrame */
public class JavaRow implements java.io.Serializable {
  private String word;

  public String getWord() {
    return word;
  }

  public void setWord(String word) {
    this.word = word;
  }
}

...

/** DataFrame operations inside your streaming program */

JavaDStream<String> words = ...

words.foreachRDD((rdd, time) -> {
  // Get the singleton instance of SparkSession
  SparkSession spark = SparkSession.builder().config(rdd.sparkContext().getConf()).getOrCreate();

  // Convert RDD[String] to RDD[case class] to DataFrame
  JavaRDD<JavaRow> rowRDD = rdd.map(word -> {
    JavaRow record = new JavaRow();
    record.setWord(word);
    return record;
  });
  DataFrame wordsDataFrame = spark.createDataFrame(rowRDD, JavaRow.class);

  // Creates a temporary view using the DataFrame
  wordsDataFrame.createOrReplaceTempView("words");

  // Do word count on table using SQL and print it
  DataFrame wordCountsDataFrame =
    spark.sql("select word, count(*) as total from words group by word");
  wordCountsDataFrame.show();
});
See the full source code.


You can also run SQL queries on tables defined on streaming data from a different thread (that is, asynchronous to the running StreamingContext). Just make sure that you set the StreamingContext to remember a sufficient amount of streaming data such that the query can run. Otherwise the StreamingContext, which is unaware of any of the asynchronous SQL queries, will delete off old streaming data before the query can complete. For example, if you want to query the last batch, but your query can take 5 minutes to run, then call streamingContext.remember(Minutes(5)) (in Scala, or equivalent in other languages).
See the DataFrames and SQL guide to learn more about DataFrames.

MLlib Operations
You can also easily use machine learning algorithms provided by MLlib. First of all, there are streaming machine learning algorithms (e.g. Streaming Linear Regression, Streaming KMeans, etc.) which can simultaneously learn from the streaming data as well as apply the model on the streaming data. Beyond these, for a much larger class of machine learning algorithms, you can learn a learning model offline (i.e. using historical data) and then apply the model online on streaming data. See the MLlib guide for more details.

Caching / Persistence
Similar to RDDs, DStreams also allow developers to persist the stream’s data in memory. That is,
using the persist() method on a DStream will automatically persist every RDD of that DStream in
memory. This is useful if the data in the DStream will be computed multiple times (e.g., multiple
operations on the same data). For window-based operations like reduceByWindow and
reduceByKeyAndWindow and state-based operations like updateStateByKey, this is implicitly true.
Hence, DStreams generated by window-based operations are automatically persisted in memory, without
the developer calling persist().
For input streams that receive data over the network (such as, Kafka, sockets, etc.), the
default persistence level is set to replicate the data to two nodes for fault-tolerance.
Note that, unlike RDDs, the default persistence level of DStreams keeps the data serialized in
memory. This is further discussed in the Performance Tuning section. More
information on different persistence levels can be found in the Spark Programming Guide.

Checkpointing
A streaming application must operate 24/7 and hence must be resilient to failures unrelated
to the application logic (e.g., system failures, JVM crashes, etc.). For this to be possible,
Spark Streaming needs to checkpoint enough information to a fault-
tolerant storage system such that it can recover from failures. There are two types of data
that are checkpointed.

Metadata checkpointing - Saving of the information defining the streaming computation to
fault-tolerant storage like HDFS. This is used to recover from failure of the node running the
driver of the streaming application (discussed in detail later). Metadata includes:
    
Configuration - The configuration that was used to create the streaming application.
DStream operations - The set of DStream operations that define the streaming application.
Incomplete batches - Batches whose jobs are queued but have not completed yet.


Data checkpointing - Saving of the generated RDDs to reliable storage. This is necessary
in some stateful transformations that combine data across multiple batches. In such
transformations, the generated RDDs depend on RDDs of previous batches, which causes the length
of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery
 time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically
checkpointed to reliable storage (e.g. HDFS) to cut off the dependency chains.

To summarize, metadata checkpointing is primarily needed for recovery from driver failures,
whereas data or RDD checkpointing is necessary even for basic functioning if stateful
transformations are used.
When to enable Checkpointing
Checkpointing must be enabled for applications with any of the following requirements:

Usage of stateful transformations - If either updateStateByKey or reduceByKeyAndWindow (with
inverse function) is used in the application, then the checkpoint directory must be provided to
allow for periodic RDD checkpointing.
Recovering from failures of the driver running the application - Metadata checkpoints are used
 to recover with progress information.

Note that simple streaming applications without the aforementioned stateful transformations can be
run without enabling checkpointing. The recovery from driver failures will also be partial in
that case (some received but unprocessed data may be lost). This is often acceptable and many run
Spark Streaming applications in this way. Support for non-Hadoop environments is expected
to improve in the future.
How to configure Checkpointing
Checkpointing can be enabled by setting a directory in a fault-tolerant,
reliable file system (e.g., HDFS, S3, etc.) to which the checkpoint information will be saved.
This is done by using streamingContext.checkpoint(checkpointDirectory). This will allow you to
use the aforementioned stateful transformations. Additionally,
if you want to make the application recover from driver failures, you should rewrite your
streaming application to have the following behavior.

When the program is being started for the first time, it will create a new StreamingContext,
set up all the streams and then call start().
When the program is being restarted after failure, it will re-create a StreamingContext
from the checkpoint data in the checkpoint directory.



This behavior is made simple by using StreamingContext.getOrCreate. This is used as follows.
# Function to create and setup a new StreamingContext
def functionToCreateContext():
    sc = SparkContext(...)  # new context
    ssc = StreamingContext(...)
    lines = ssc.socketTextStream(...)  # create DStreams
    ...
    ssc.checkpoint(checkpointDirectory)  # set checkpoint directory
    return ssc

# Get StreamingContext from checkpoint data or create a new one
context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)

# Do additional setup on context that needs to be done,
# irrespective of whether it is being started or restarted
context. ...

# Start the context
context.start()
context.awaitTermination()
If the checkpointDirectory exists, then the context will be recreated from the checkpoint data.
If the directory does not exist (i.e., running for the first time),
then the function functionToCreateContext will be called to create a new
context and set up the DStreams. See the Python example
recoverable_network_wordcount.py.
This example appends the word counts of network data into a file.
You can also explicitly create a StreamingContext from the checkpoint data and start the
 computation by using StreamingContext.getOrCreate(checkpointDirectory, None).


This behavior is made simple by using StreamingContext.getOrCreate. This is used as follows.
// Function to create and setup a new StreamingContext
def functionToCreateContext(): StreamingContext = {
  val ssc = new StreamingContext(...)   // new context
  val lines = ssc.socketTextStream(...) // create DStreams
  ...
  ssc.checkpoint(checkpointDirectory)   // set checkpoint directory
  ssc
}

// Get StreamingContext from checkpoint data or create a new one
val context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)

// Do additional setup on context that needs to be done,
// irrespective of whether it is being started or restarted
context. ...

// Start the context
context.start()
context.awaitTermination()
If the checkpointDirectory exists, then the context will be recreated from the checkpoint data.
If the directory does not exist (i.e., running for the first time),
then the function functionToCreateContext will be called to create a new
context and set up the DStreams. See the Scala example
RecoverableNetworkWordCount.
This example appends the word counts of network data into a file.


This behavior is made simple by using JavaStreamingContext.getOrCreate. This is used as follows.
// Create a factory object that can create and setup a new JavaStreamingContext
JavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() {
  @Override public JavaStreamingContext create() {
    JavaStreamingContext jssc = new JavaStreamingContext(...);  // new context
    JavaDStream<String> lines = jssc.socketTextStream(...);     // create DStreams
    ...
    jssc.checkpoint(checkpointDirectory);                       // set checkpoint directory
    return jssc;
  }
};

// Get JavaStreamingContext from checkpoint data or create a new one
JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);

// Do additional setup on context that needs to be done,
// irrespective of whether it is being started or restarted
context. ...

// Start the context
context.start();
context.awaitTermination();
If the checkpointDirectory exists, then the context will be recreated from the checkpoint data.
If the directory does not exist (i.e., running for the first time),
then the function contextFactory will be called to create a new
context and set up the DStreams. See the Java example
JavaRecoverableNetworkWordCount.
This example appends the word counts of network data into a file.


In addition to using getOrCreate one also needs to ensure that the driver process gets
restarted automatically on failure. This can only be done by the deployment infrastructure that is
used to run the application. This is further discussed in the
Deployment section.
Note that checkpointing of RDDs incurs the cost of saving to reliable storage.
This may cause an increase in the processing time of those batches where RDDs get checkpointed.
Hence, the interval of
checkpointing needs to be set carefully. At small batch sizes (say 1 second), checkpointing every
batch may significantly reduce operation throughput. Conversely, checkpointing too infrequently
causes the lineage and task sizes to grow, which may have detrimental effects. For stateful
transformations that require RDD checkpointing, the default interval is a multiple of the
batch interval that is at least 10 seconds. It can be set by using
dstream.checkpoint(checkpointInterval). Typically, a checkpoint interval of 5 - 10 sliding intervals of a DStream is a good setting to try.

Accumulators, Broadcast Variables, and Checkpoints
Accumulators and Broadcast variables
cannot be recovered from checkpoint in Spark Streaming. If you enable checkpointing and use
Accumulators or Broadcast variables
as well, you’ll have to create lazily instantiated singleton instances for
Accumulators and Broadcast variables
so that they can be re-instantiated after the driver restarts on failure.
This is shown in the following example.


def getWordExcludeList(sparkContext):
    if ("wordExcludeList" not in globals()):
        globals()["wordExcludeList"] = sparkContext.broadcast(["a", "b", "c"])
    return globals()["wordExcludeList"]

def getDroppedWordsCounter(sparkContext):
    if ("droppedWordsCounter" not in globals()):
        globals()["droppedWordsCounter"] = sparkContext.accumulator(0)
    return globals()["droppedWordsCounter"]

def echo(time, rdd):
    # Get or register the excludeList Broadcast
    excludeList = getWordExcludeList(rdd.context)
    # Get or register the droppedWordsCounter Accumulator
    droppedWordsCounter = getDroppedWordsCounter(rdd.context)

    # Use excludeList to drop words and use droppedWordsCounter to count them
    def filterFunc(wordCount):
        if wordCount[0] in excludeList.value:
            droppedWordsCounter.add(wordCount[1])
            False
        else:
            True

    counts = "Counts at time %s %s" % (time, rdd.filter(filterFunc).collect())

wordCounts.foreachRDD(echo)
See the full source code.


object WordExcludeList {

  @volatile private var instance: Broadcast[Seq[String]] = null

  def getInstance(sc: SparkContext): Broadcast[Seq[String]] = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          val wordExcludeList = Seq("a", "b", "c")
          instance = sc.broadcast(wordExcludeList)
        }
      }
    }
    instance
  }
}

object DroppedWordsCounter {

  @volatile private var instance: LongAccumulator = null

  def getInstance(sc: SparkContext): LongAccumulator = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          instance = sc.longAccumulator("DroppedWordsCounter")
        }
      }
    }
    instance
  }
}

wordCounts.foreachRDD { (rdd: RDD[(String, Int)], time: Time) =>
  // Get or register the excludeList Broadcast
  val excludeList = WordExcludeList.getInstance(rdd.sparkContext)
  // Get or register the droppedWordsCounter Accumulator
  val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext)
  // Use excludeList to drop words and use droppedWordsCounter to count them
  val counts = rdd.filter { case (word, count) =>
    if (excludeList.value.contains(word)) {
      droppedWordsCounter.add(count)
      false
    } else {
      true
    }
  }.collect().mkString("[", ", ", "]")
  val output = "Counts at time " + time + " " + counts
})
See the full source code.


class JavaWordExcludeList {

  private static volatile Broadcast<List<String>> instance = null;

  public static Broadcast<List<String>> getInstance(JavaSparkContext jsc) {
    if (instance == null) {
      synchronized (JavaWordExcludeList.class) {
        if (instance == null) {
          List<String> wordExcludeList = Arrays.asList("a", "b", "c");
          instance = jsc.broadcast(wordExcludeList);
        }
      }
    }
    return instance;
  }
}

class JavaDroppedWordsCounter {

  private static volatile LongAccumulator instance = null;

  public static LongAccumulator getInstance(JavaSparkContext jsc) {
    if (instance == null) {
      synchronized (JavaDroppedWordsCounter.class) {
        if (instance == null) {
          instance = jsc.sc().longAccumulator("DroppedWordsCounter");
        }
      }
    }
    return instance;
  }
}

wordCounts.foreachRDD((rdd, time) -> {
  // Get or register the excludeList Broadcast
  Broadcast<List<String>> excludeList = JavaWordExcludeList.getInstance(new JavaSparkContext(rdd.context()));
  // Get or register the droppedWordsCounter Accumulator
  LongAccumulator droppedWordsCounter = JavaDroppedWordsCounter.getInstance(new JavaSparkContext(rdd.context()));
  // Use excludeList to drop words and use droppedWordsCounter to count them
  String counts = rdd.filter(wordCount -> {
    if (excludeList.value().contains(wordCount._1())) {
      droppedWordsCounter.add(wordCount._2());
      return false;
    } else {
      return true;
    }
  }).collect().toString();
  String output = "Counts at time " + time + " " + counts;
}
See the full source code.



Deploying Applications
This section discusses the steps to deploy a Spark Streaming application.
Requirements
To run Spark Streaming applications, you need to have the following.


Cluster with a cluster manager - This is the general requirement of any Spark application,
and discussed in detail in the deployment guide.


Package the application JAR - You have to compile your streaming application into a JAR.
If you are using spark-submit to start the
application, then you will not need to provide Spark and Spark Streaming in the JAR. However,
if your application uses advanced sources (e.g. Kafka),
then you will have to package the extra artifact they link to, along with their dependencies,
in the JAR that is used to deploy the application. For example, an application using KafkaUtils
will have to include spark-streaming-kafka-0-10_2.12 and all its
transitive dependencies in the application JAR.


Configuring sufficient memory for the executors - Since the received data must be stored in
memory, the executors must be configured with sufficient memory to hold the received data. Note
that if you are doing 10 minute window operations, the system has to keep at least last 10 minutes
of data in memory. So the memory requirements for the application depends on the operations
used in it.


Configuring checkpointing - If the stream application requires it, then a directory in the
Hadoop API compatible fault-tolerant storage (e.g. HDFS, S3, etc.) must be configured as the
checkpoint directory and the streaming application written in a way that checkpoint
information can be used for failure recovery. See the checkpointing section
for more details.

Configuring automatic restart of the application driver - To automatically recover from a
driver failure, the deployment infrastructure that is
used to run the streaming application must monitor the driver process and relaunch the driver
if it fails. Different cluster managers
have different tools to achieve this.
    
Spark Standalone - A Spark application driver can be submitted to run within the Spark
Standalone cluster (see
cluster deploy mode), that is, the
application driver itself runs on one of the worker nodes. Furthermore, the
Standalone cluster manager can be instructed to supervise the driver,
and relaunch it if the driver fails either due to non-zero exit code,
or due to failure of the node running the driver. See cluster mode and supervise in the
Spark Standalone guide for more details.
YARN - Yarn supports a similar mechanism for automatically restarting an application.
Please refer to YARN documentation for more details.
Mesos - Marathon has been used to achieve this
with Mesos.



Configuring write-ahead logs - Since Spark 1.2,
we have introduced write-ahead logs for achieving strong
fault-tolerance guarantees. If enabled,  all the data received from a receiver gets written into
a write-ahead log in the configuration checkpoint directory. This prevents data loss on driver
recovery, thus ensuring zero data loss (discussed in detail in the
Fault-tolerance Semantics section). This can be enabled by setting
the configuration parameter
spark.streaming.receiver.writeAheadLog.enable to true. However, these stronger semantics may
come at the cost of the receiving throughput of individual receivers. This can be corrected by
running more receivers in parallel
to increase aggregate throughput. Additionally, it is recommended that the replication of the
received data within Spark be disabled when the write-ahead log is enabled as the log is already
stored in a replicated storage system. This can be done by setting the storage level for the
input stream to StorageLevel.MEMORY_AND_DISK_SER. While using S3 (or any file system that
does not support flushing) for write-ahead logs, please remember to enable
spark.streaming.driver.writeAheadLog.closeFileAfterWrite and
spark.streaming.receiver.writeAheadLog.closeFileAfterWrite. See
Spark Streaming Configuration for more details.
Note that Spark will not encrypt data written to the write-ahead log when I/O encryption is
enabled. If encryption of the write-ahead log data is desired, it should be stored in a file
system that supports encryption natively.

Setting the max receiving rate - If the cluster resources are not large enough for the streaming
application to process data as fast as it is being received, the receivers can be rate limited
by setting a maximum rate limit in terms of records / sec.
See the configuration parameters
spark.streaming.receiver.maxRate for receivers and spark.streaming.kafka.maxRatePerPartition
for Direct Kafka approach. In Spark 1.5, we have introduced a feature called backpressure that
eliminates the need to set this rate limit, as Spark Streaming automatically figures out the
rate limits and dynamically adjusts them if the processing conditions change. This backpressure
can be enabled by setting the configuration parameter
spark.streaming.backpressure.enabled to true.

Upgrading Application Code
If a running Spark Streaming application needs to be upgraded with new
application code, then there are two possible mechanisms.


The upgraded Spark Streaming application is started and run in parallel to the existing application.
Once the new one (receiving the same data as the old one) has been warmed up and is ready
for prime time, the old one can be brought down. Note that this can be done for data sources that support
sending the data to two destinations (i.e., the earlier and upgraded applications).


The existing application is shutdown gracefully (see
StreamingContext.stop(...)
or JavaStreamingContext.stop(...)
for graceful shutdown options) which ensure data that has been received is completely
processed before shutdown. Then the
upgraded application can be started, which will start processing from the same point where the earlier
application left off. Note that this can be done only with input sources that support source-side buffering
(like Kafka) as data needs to be buffered while the previous application was down and
the upgraded application is not yet up. And restarting from earlier checkpoint
information of pre-upgrade code cannot be done. The checkpoint information essentially
contains serialized Scala/Java/Python objects and trying to deserialize objects with new,
modified classes may lead to errors. In this case, either start the upgraded app with a different
checkpoint directory, or delete the previous checkpoint directory.



Monitoring Applications
Beyond Spark’s monitoring capabilities, there are additional capabilities
specific to Spark Streaming. When a StreamingContext is used, the
Spark web UI shows
an additional Streaming tab which shows statistics about running receivers (whether
receivers are active, number of records received, receiver error, etc.)
and completed batches (batch processing times, queueing delays, etc.). This can be used to
monitor the progress of the streaming application.
The following two metrics in web UI are particularly important:

Processing Time - The time to process each batch of data.
Scheduling Delay - the time a batch waits in a queue for the processing of previous batches
to finish.

If the batch processing time is consistently more than the batch interval and/or the queueing
delay keeps increasing, then it indicates that the system is
not able to process the batches as fast they are being generated and is falling behind.
In that case, consider
reducing the batch processing time.
The progress of a Spark Streaming program can also be monitored using the
StreamingListener interface,
which allows you to get receiver status and processing times. Note that this is a developer API
and it is likely to be improved upon (i.e., more information reported) in the future.


Performance Tuning
Getting the best performance out of a Spark Streaming application on a cluster requires a bit of
tuning. This section explains a number of the parameters and configurations that can be tuned to
improve the performance of your application. At a high level, you need to consider two things:


Reducing the processing time of each batch of data by efficiently using cluster resources.


Setting the right batch size such that the batches of data can be processed as fast as they
  	are received (that is, data processing keeps up with the data ingestion).


Reducing the Batch Processing Times
There are a number of optimizations that can be done in Spark to minimize the processing time of
each batch. These have been discussed in detail in the Tuning Guide. This section
highlights some of the most important ones.
Level of Parallelism in Data Receiving
Receiving data over the network (like Kafka, socket, etc.) requires the data to be deserialized
and stored in Spark. If the data receiving becomes a bottleneck in the system, then consider
parallelizing the data receiving. Note that each input DStream
creates a single receiver (running on a worker machine) that receives a single stream of data.
Receiving multiple data streams can therefore be achieved by creating multiple input DStreams
and configuring them to receive different partitions of the data stream from the source(s).
For example, a single Kafka input DStream receiving two topics of data can be split into two
Kafka input streams, each receiving only one topic. This would run two receivers,
allowing data to be received in parallel, thus increasing overall throughput. These multiple
DStreams can be unioned together to create a single DStream. Then the transformations that were
being applied on a single input DStream can be applied on the unified stream. This is done as follows.


numStreams = 5
kafkaStreams = [KafkaUtils.createStream(...) for _ in range (numStreams)]
unifiedStream = streamingContext.union(*kafkaStreams)
unifiedStream.pprint()


val numStreams = 5
val kafkaStreams = (1 to numStreams).map { i => KafkaUtils.createStream(...) }
val unifiedStream = streamingContext.union(kafkaStreams)
unifiedStream.print()


int numStreams = 5;
List<JavaPairDStream<String, String>> kafkaStreams = new ArrayList<>(numStreams);
for (int i = 0; i < numStreams; i++) {
  kafkaStreams.add(KafkaUtils.createStream(...));
}
JavaPairDStream<String, String> unifiedStream = streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));
unifiedStream.print();


Another parameter that should be considered is the receiver’s block interval,
which is determined by the configuration parameter
spark.streaming.blockInterval. For most receivers, the received data is coalesced together into
blocks of data before storing inside Spark’s memory. The number of blocks in each batch
determines the number of tasks that will be used to process
the received data in a map-like transformation. The number of tasks per receiver per batch will be
approximately (batch interval / block interval). For example, a block interval of 200 ms will
create 10 tasks per 2 second batches. If the number of tasks is too low (that is, less than the number
of cores per machine), then it will be inefficient as all available cores will not be used to
process the data. To increase the number of tasks for a given batch interval, reduce the
block interval. However, the recommended minimum value of block interval is about 50 ms,
below which the task launching overheads may be a problem.
An alternative to receiving data with multiple input streams / receivers is to explicitly repartition
the input data stream (using inputStream.repartition(<number of partitions>)).
This distributes the received batches of data across the specified number of machines in the cluster
before further processing.
For direct stream, please refer to Spark Streaming + Kafka Integration Guide
Level of Parallelism in Data Processing
Cluster resources can be under-utilized if the number of parallel tasks used in any stage of the
computation is not high enough. For example, for distributed reduce operations like reduceByKey
and reduceByKeyAndWindow, the default number of parallel tasks is controlled by
the spark.default.parallelism configuration property. You
can pass the level of parallelism as an argument (see
PairDStreamFunctions
documentation), or set the spark.default.parallelism
configuration property to change the default.
Data Serialization
The overheads of data serialization can be reduced by tuning the serialization formats. In the case of streaming, there are two types of data that are being serialized.


Input data: By default, the input data received through Receivers is stored in the executors’ memory with StorageLevel.MEMORY_AND_DISK_SER_2. That is, the data is serialized into bytes to reduce GC overheads, and replicated for tolerating executor failures. Also, the data is kept first in memory, and spilled over to disk only if the memory is insufficient to hold all of the input data necessary for the streaming computation. This serialization obviously has overheads – the receiver must deserialize the received data and re-serialize it using Spark’s serialization format.


Persisted RDDs generated by Streaming Operations: RDDs generated by streaming computations may be persisted in memory. For example, window operations persist data in memory as they would be processed multiple times. However, unlike the Spark Core default of StorageLevel.MEMORY_ONLY, persisted RDDs generated by streaming computations are persisted with StorageLevel.MEMORY_ONLY_SER (i.e. serialized) by default to minimize GC overheads.


In both cases, using Kryo serialization can reduce both CPU and memory overheads. See the Spark Tuning Guide for more details. For Kryo, consider registering custom classes, and disabling object reference tracking (see Kryo-related configurations in the Configuration Guide).
In specific cases where the amount of data that needs to be retained for the streaming application is not large, it may be feasible to persist data (both types) as deserialized objects without incurring excessive GC overheads. For example, if you are using batch intervals of a few seconds and no window operations, then you can try disabling serialization in persisted data by explicitly setting the storage level accordingly. This would reduce the CPU overheads due to serialization, potentially improving performance without too much GC overheads.
Task Launching Overheads
If the number of tasks launched per second is high (say, 50 or more per second), then the overhead
of sending out tasks to the executors may be significant and will make it hard to achieve sub-second
latencies. The overhead can be reduced by the following changes:

Execution mode: Running Spark in Standalone mode or coarse-grained Mesos mode leads to
better task launch times than the fine-grained Mesos mode. Please refer to the
Running on Mesos guide for more details.

These changes may reduce batch processing time by 100s of milliseconds,
thus allowing sub-second batch size to be viable.

Setting the Right Batch Interval
For a Spark Streaming application running on a cluster to be stable, the system should be able to
process data as fast as it is being received. In other words, batches of data should be processed
as fast as they are being generated. Whether this is true for an application can be found by
monitoring the processing times in the streaming web UI, where the batch
processing time should be less than the batch interval.
Depending on the nature of the streaming
computation, the batch interval used may have significant impact on the data rates that can be
sustained by the application on a fixed set of cluster resources. For example, let us
consider the earlier WordCountNetwork example. For a particular data rate, the system may be able
to keep up with reporting word counts every 2 seconds (i.e., batch interval of 2 seconds), but not
every 500 milliseconds. So the batch interval needs to be set such that the expected data rate in
production can be sustained.
A good approach to figure out the right batch size for your application is to test it with a
conservative batch interval (say, 5-10 seconds) and a low data rate. To verify whether the system
is able to keep up with the data rate, you can check the value of the end-to-end delay experienced
by each processed batch (either look for “Total delay” in Spark driver log4j logs, or use the
StreamingListener
interface).
If the delay is maintained to be comparable to the batch size, then the system is stable. Otherwise,
if the delay is continuously increasing, it means that the system is unable to keep up and it is
therefore unstable. Once you have an idea of a stable configuration, you can try increasing the
data rate and/or reducing the batch size. Note that a momentary increase in the delay due to
temporary data rate increases may be fine as long as the delay reduces back to a low value
(i.e., less than batch size).

Memory Tuning
Tuning the memory usage and GC behavior of Spark applications has been discussed in great detail
in the Tuning Guide. It is strongly recommended that you read that. In this section, we discuss a few tuning parameters specifically in the context of Spark Streaming applications.
The amount of cluster memory required by a Spark Streaming application depends heavily on the type of transformations used. For example, if you want to use a window operation on the last 10 minutes of data, then your cluster should have sufficient memory to hold 10 minutes worth of data in memory. Or if you want to use updateStateByKey with a large number of keys, then the necessary memory  will be high. On the contrary, if you want to do a simple map-filter-store operation, then the necessary memory will be low.
In general, since the data received through receivers is stored with StorageLevel.MEMORY_AND_DISK_SER_2, the data that does not fit in memory will spill over to the disk. This may reduce the performance of the streaming application, and hence it is advised to provide sufficient memory as required by your streaming application. Its best to try and see the memory usage on a small scale and estimate accordingly.
Another aspect of memory tuning is garbage collection. For a streaming application that requires low latency, it is undesirable to have large pauses caused by JVM Garbage Collection.
There are a few parameters that can help you tune the memory usage and GC overheads:


Persistence Level of DStreams: As mentioned earlier in the Data Serialization section, the input data and RDDs are by default persisted as serialized bytes. This reduces both the memory usage and GC overheads, compared to deserialized persistence. Enabling Kryo serialization further reduces serialized sizes and memory usage. Further reduction in memory usage can be achieved with compression (see the Spark configuration spark.rdd.compress), at the cost of CPU time.


Clearing old data: By default, all input data and persisted RDDs generated by DStream transformations are automatically cleared. Spark Streaming decides when to clear the data based on the transformations that are used. For example, if you are using a window operation of 10 minutes, then Spark Streaming will keep around the last 10 minutes of data, and actively throw away older data.
Data can be retained for a longer duration (e.g. interactively querying older data) by setting streamingContext.remember.


CMS Garbage Collector: Use of the concurrent mark-and-sweep GC is strongly recommended for keeping GC-related pauses consistently low. Even though concurrent GC is known to reduce the
overall processing throughput of the system, its use is still recommended to achieve more
consistent batch processing times. Make sure you set the CMS GC on both the driver (using --driver-java-options in spark-submit) and the executors (using Spark configuration spark.executor.extraJavaOptions).


Other tips: To further reduce GC overheads, here are some more tips to try.

Persist RDDs using the OFF_HEAP storage level. See more detail in the Spark Programming Guide.
Use more executors with smaller heap sizes. This will reduce the GC pressure within each JVM heap.




Important points to remember:


A DStream is associated with a single receiver. For attaining read parallelism multiple receivers i.e. multiple DStreams need to be created. A receiver is run within an executor. It occupies one core. Ensure that there are enough cores for processing after receiver slots are booked i.e. spark.cores.max should take the receiver slots into account. The receivers are allocated to executors in a round robin fashion.


When data is received from a stream source, the receiver creates blocks of data.  A new block of data is generated every blockInterval milliseconds. N blocks of data are created during the batchInterval where N = batchInterval/blockInterval. These blocks are distributed by the BlockManager of the current executor to the block managers of other executors. After that, the Network Input Tracker running on the driver is informed about the block locations for further processing.


An RDD is created on the driver for the blocks created during the batchInterval. The blocks generated during the batchInterval are partitions of the RDD. Each partition is a task in spark. blockInterval== batchinterval would mean that a single partition is created and probably it is processed locally.


The map tasks on the blocks are processed in the executors (one that received the block, and another where the block was replicated) that has the blocks irrespective of block interval, unless non-local scheduling kicks in.
Having a bigger blockinterval means bigger blocks. A high value of spark.locality.wait increases the chance of processing a block on the local node. A balance needs to be found out between these two parameters to ensure that the bigger blocks are processed locally.


Instead of relying on batchInterval and blockInterval, you can define the number of partitions by calling inputDstream.repartition(n). This reshuffles the data in RDD randomly to create n number of partitions. Yes, for greater parallelism. Though comes at the cost of a shuffle. An RDD’s processing is scheduled by the driver’s jobscheduler as a job. At a given point of time only one job is active. So, if one job is executing the other jobs are queued.


If you have two dstreams there will be two RDDs formed and there will be two jobs created which will be scheduled one after the another. To avoid this, you can union two dstreams. This will ensure that a single unionRDD is formed for the two RDDs of the dstreams. This unionRDD is then considered as a single job. However, the partitioning of the RDDs is not impacted.


If the batch processing time is more than batchinterval then obviously the receiver’s memory will start filling up and will end up in throwing exceptions (most probably BlockNotFoundException). Currently, there is  no way to pause the receiver. Using SparkConf configuration spark.streaming.receiver.maxRate, rate of receiver can be limited.




Fault-tolerance Semantics
In this section, we will discuss the behavior of Spark Streaming applications in the event
of failures.
Background
To understand the semantics provided by Spark Streaming, let us remember the basic fault-tolerance semantics of Spark’s RDDs.

An RDD is an immutable, deterministically re-computable, distributed dataset. Each RDD
remembers the lineage of deterministic operations that were used on a fault-tolerant input
dataset to create it.
If any partition of an RDD is lost due to a worker node failure, then that partition can be
re-computed from the original fault-tolerant dataset using the lineage of operations.
Assuming that all of the RDD transformations are deterministic, the data in the final transformed
RDD will always be the same irrespective of failures in the Spark cluster.

Spark operates on data in fault-tolerant file systems like HDFS or S3. Hence,
all of the RDDs generated from the fault-tolerant data are also fault-tolerant. However, this is not
the case for Spark Streaming as the data in most cases is received over the network (except when
fileStream is used). To achieve the same fault-tolerance properties for all of the generated RDDs,
the received data is replicated among multiple Spark executors in worker nodes in the cluster
(default replication factor is 2). This leads to two kinds of data in the
system that need to be recovered in the event of failures:

Data received and replicated - This data survives failure of a single worker node as a copy
  of it exists on one of the other nodes.
Data received but buffered for replication - Since this is not replicated,
the only way to recover this data is to get it again from the source.

Furthermore, there are two kinds of failures that we should be concerned about:

Failure of a Worker Node - Any of the worker nodes running executors can fail,
and all in-memory data on those nodes will be lost. If any receivers were running on failed
nodes, then their buffered data will be lost.
Failure of the Driver Node - If the driver node running the Spark Streaming application
fails, then obviously the SparkContext is lost, and all executors with their in-memory
data are lost.

With this basic knowledge, let us understand the fault-tolerance semantics of Spark Streaming.
Definitions
The semantics of streaming systems are often captured in terms of how many times each record can be processed by the system. There are three types of guarantees that a system can provide under all possible operating conditions (despite failures, etc.)

At most once: Each record will be either processed once or not processed at all.
At least once: Each record will be processed one or more times. This is stronger than at-most once as it ensures that no data will be lost. But there may be duplicates.
Exactly once: Each record will be processed exactly once - no data will be lost and no data will be processed multiple times. This is obviously the strongest guarantee of the three.

Basic Semantics
In any stream processing system, broadly speaking, there are three steps in processing the data.


Receiving the data: The data is received from sources using Receivers or otherwise.


Transforming the data: The received data is transformed using DStream and RDD transformations.


Pushing out the data: The final transformed data is pushed out to external systems like file systems, databases, dashboards, etc.


If a streaming application has to achieve end-to-end exactly-once guarantees, then each step has to provide an exactly-once guarantee. That is, each record must be received exactly once, transformed exactly once, and pushed to downstream systems exactly once. Let’s understand the semantics of these steps in the context of Spark Streaming.


Receiving the data: Different input sources provide different guarantees. This is discussed in detail in the next subsection.


Transforming the data: All data that has been received will be processed exactly once, thanks to the guarantees that RDDs provide. Even if there are failures, as long as the received input data is accessible, the final transformed RDDs will always have the same contents.


Pushing out the data: Output operations by default ensure at-least once semantics because it depends on the type of output operation (idempotent, or not) and the semantics of the downstream system (supports transactions or not). But users can implement their own transaction mechanisms to achieve exactly-once semantics. This is discussed in more details later in the section.


Semantics of Received Data
Different input sources provide different guarantees, ranging from at-least once to exactly once. Read for more details.
With Files
If all of the input data is already present in a fault-tolerant file system like
HDFS, Spark Streaming can always recover from any failure and process all of the data. This gives
exactly-once semantics, meaning all of the data will be processed exactly once no matter what fails.
With Receiver-based Sources
For input sources based on receivers, the fault-tolerance semantics depend on both the failure
scenario and the type of receiver.
As we discussed earlier, there are two types of receivers:

Reliable Receiver - These receivers acknowledge reliable sources only after ensuring that
  the received data has been replicated. If such a receiver fails, the source will not receive
  acknowledgment for the buffered (unreplicated) data. Therefore, if the receiver is
  restarted, the source will resend the data, and no data will be lost due to the failure.
Unreliable Receiver - Such receivers do not send acknowledgment and therefore can lose
  data when they fail due to worker or driver failures.

Depending on what type of receivers are used we achieve the following semantics.
If a worker node fails, then there is no data loss with reliable receivers. With unreliable
receivers, data received but not replicated can get lost. If the driver node fails,
then besides these losses, all of the past data that was received and replicated in memory will be
lost. This will affect the results of the stateful transformations.
To avoid this loss of past received data, Spark 1.2 introduced write
ahead logs which save the received data to fault-tolerant storage. With the write-ahead logs
enabled and reliable receivers, there is zero data loss. In terms of semantics, it provides an at-least once guarantee.
The following table summarizes the semantics under failures:



Deployment Scenario
Worker Failure
Driver Failure




Spark 1.1 or earlier, OR
Spark 1.2 or later without write-ahead logs


      Buffered data lost with unreliable receivers
      Zero data loss with reliable receivers
      At-least once semantics
    

      Buffered data lost with unreliable receivers
      Past data lost with all receivers
      Undefined semantics
    


Spark 1.2 or later with write-ahead logs

        Zero data loss with reliable receivers
        At-least once semantics
    

        Zero data loss with reliable receivers and files
        At-least once semantics
    







With Kafka Direct API
In Spark 1.3, we have introduced a new Kafka Direct API, which can ensure that all the Kafka data is received by Spark Streaming exactly once. Along with this, if you implement exactly-once output operation, you can achieve end-to-end exactly-once guarantees. This approach is further discussed in the Kafka Integration Guide.
Semantics of output operations
Output operations (like foreachRDD) have at-least once semantics, that is,
the transformed data may get written to an external entity more than once in
the event of a worker failure. While this is acceptable for saving to file systems using the
saveAs***Files operations (as the file will simply get overwritten with the same data),
additional effort may be necessary to achieve exactly-once semantics. There are two approaches.


Idempotent updates: Multiple attempts always write the same data. For example, saveAs***Files always writes the same data to the generated files.


Transactional updates: All updates are made transactionally so that updates are made exactly once atomically. One way to do this would be the following.

Use the batch time (available in foreachRDD) and the partition index of the RDD to create an identifier. This identifier uniquely identifies a blob data in the streaming application.

Update external system with this blob transactionally (that is, exactly once, atomically) using the identifier. That is, if the identifier is not already committed, commit the partition data and the identifier atomically. Else, if this was already committed, skip the update.
dstream.foreachRDD { (rdd, time) =>
  rdd.foreachPartition { partitionIterator =>
    val partitionId = TaskContext.get.partitionId()
    val uniqueId = generateUniqueId(time.milliseconds, partitionId)
    // use this uniqueId to transactionally commit the data in partitionIterator
  }
}
 






Where to Go from Here

Additional guides
    
Kafka Integration Guide
Kinesis Integration Guide
Custom Receiver Guide


Third-party DStream data sources can be found in Third Party Projects
API documentation
    
Scala docs
        
StreamingContext and
DStream
KafkaUtils,
KinesisUtils,


Java docs
        
JavaStreamingContext,
JavaDStream and
JavaPairDStream
KafkaUtils,
KinesisUtils


Python docs
        
StreamingContext and DStream




More examples in Scala
and Java
and Python
Paper and video describing Spark Streaming.





















  




Structured Streaming Programming Guide - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Structured Streaming Programming Guide

Overview
Quick Example
Programming Model 
Basic Concepts
Handling Event-time and Late Data
Fault Tolerance Semantics


API using Datasets and DataFrames 
Creating streaming DataFrames and streaming Datasets 
Input Sources
Schema inference and partition of streaming DataFrames/Datasets


Operations on streaming DataFrames/Datasets 
Basic Operations - Selection, Projection, Aggregation
Window Operations on Event Time 
Handling Late Data and Watermarking
Types of time windows
Representation of the time for time window


Join Operations 
Stream-static Joins
Stream-stream Joins 
Inner Joins with optional Watermarking
Outer Joins with Watermarking
Semi Joins with Watermarking
Support matrix for joins in streaming queries




Streaming Deduplication
Policy for handling multiple watermarks
Arbitrary Stateful Operations
Unsupported Operations
State Store 
HDFS state store provider
RocksDB state store implementation 
RocksDB State Store Memory Management
RocksDB State Store Changelog Checkpointing
Performance-aspect considerations


State Store and task locality




Starting Streaming Queries 
Output Modes
Output Sinks 
Using Foreach and ForeachBatch 
ForeachBatch
Foreach




Streaming Table APIs
Triggers


Managing Streaming Queries
Monitoring Streaming Queries 
Reading Metrics Interactively
Reporting Metrics programmatically using Asynchronous APIs
Reporting Metrics using Dropwizard


Recovering from Failures with Checkpointing
Recovery Semantics after Changes in a Streaming Query


Asynchronous Progress Tracking 
What is it?
How does it work?
How to use it?
Limitations
Switching the setting off


Continuous Processing
Additional Information
Migration Guide

Overview
Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.
Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.
In this guide, we are going to walk you through the programming model and the APIs. We are going to explain the concepts mostly using the default micro-batch processing model, and then later discuss Continuous Processing model. First, let’s start with a simple example of a Structured Streaming query - a streaming word count.
Quick Example
Let’s say you want to maintain a running word count of text data received from a data server listening on a TCP socket. Let’s see how you can express this using Structured Streaming. You can see the full code in
Scala/Java/Python/R.
And if you download Spark, you can directly run the example. In any case, let’s walk through the example step-by-step and understand how it works. First, we have to import the necessary classes and create a local SparkSession, the starting point of all functionalities related to Spark.


from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.functions import split

spark = SparkSession \
    .builder \
    .appName("StructuredNetworkWordCount") \
    .getOrCreate()


import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder
  .appName("StructuredNetworkWordCount")
  .getOrCreate()

import spark.implicits._


import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.streaming.StreamingQuery;

import java.util.Arrays;
import java.util.Iterator;

SparkSession spark = SparkSession
  .builder()
  .appName("JavaStructuredNetworkWordCount")
  .getOrCreate();


sparkR.session(appName = "StructuredNetworkWordCount")


Next, let’s create a streaming DataFrame that represents text data received from a server listening on localhost:9999, and transform the DataFrame to calculate word counts.


# Create DataFrame representing the stream of input lines from connection to localhost:9999
lines = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# Split the lines into words
words = lines.select(
   explode(
       split(lines.value, " ")
   ).alias("word")
)

# Generate running word count
wordCounts = words.groupBy("word").count()
This lines DataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have used two built-in SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we use the function alias to name the new column as “word”. Finally, we have defined the wordCounts DataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represents the running word counts of the stream.


// Create DataFrame representing the stream of input lines from connection to localhost:9999
val lines = spark.readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load()

// Split the lines into words
val words = lines.as[String].flatMap(_.split(" "))

// Generate running word count
val wordCounts = words.groupBy("value").count()
This lines DataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have converted the DataFrame to a  Dataset of String using .as[String], so that we can apply the flatMap operation to split each line into multiple words. The resultant words Dataset contains all the words. Finally, we have defined the wordCounts DataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represents the running word counts of the stream.


// Create DataFrame representing the stream of input lines from connection to localhost:9999
Dataset<Row> lines = spark
  .readStream()
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load();

// Split the lines into words
Dataset<String> words = lines
  .as(Encoders.STRING())
  .flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());

// Generate running word count
Dataset<Row> wordCounts = words.groupBy("value").count();
This lines DataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have converted the DataFrame to a  Dataset of String using .as(Encoders.STRING()), so that we can apply the flatMap operation to split each line into multiple words. The resultant words Dataset contains all the words. Finally, we have defined the wordCounts DataFrame by grouping by the unique values in the Dataset and counting them. Note that this is a streaming DataFrame which represents the running word counts of the stream.


# Create DataFrame representing the stream of input lines from connection to localhost:9999
lines <- read.stream("socket", host = "localhost", port = 9999)

# Split the lines into words
words <- selectExpr(lines, "explode(split(value, ' ')) as word")

# Generate running word count
wordCounts <- count(group_by(words, "word"))
This lines SparkDataFrame represents an unbounded table containing the streaming text data. This table contains one column of strings named “value”, and each line in the streaming text data becomes a row in the table. Note, that this is not currently receiving any data as we are just setting up the transformation, and have not yet started it. Next, we have a SQL expression with two SQL functions - split and explode, to split each line into multiple rows with a word each. In addition, we name the new column as “word”. Finally, we have defined the wordCounts SparkDataFrame by grouping by the unique values in the SparkDataFrame and counting them. Note that this is a streaming SparkDataFrame which represents the running word counts of the stream.


We have now set up the query on the streaming data. All that is left is to actually start receiving data and computing the counts. To do this, we set it up to print the complete set of counts (specified by outputMode("complete")) to the console every time they are updated. And then start the streaming computation using start().


 # Start running the query that prints the running counts to the console
query = wordCounts \
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()


// Start running the query that prints the running counts to the console
val query = wordCounts.writeStream
  .outputMode("complete")
  .format("console")
  .start()

query.awaitTermination()


// Start running the query that prints the running counts to the console
StreamingQuery query = wordCounts.writeStream()
  .outputMode("complete")
  .format("console")
  .start();

query.awaitTermination();


# Start running the query that prints the running counts to the console
query <- write.stream(wordCounts, "console", outputMode = "complete")

awaitTermination(query)


After this code is executed, the streaming computation will have started in the background. The query object is a handle to that active streaming query, and we have decided to wait for the termination of the query using awaitTermination() to prevent the process from exiting while the query is active.
To actually execute this example code, you can either compile the code in your own
Spark application, or simply
run the example once you have downloaded Spark. We are showing the latter. You will first need to run Netcat (a small utility found in most Unix-like systems) as a data server by using
$ nc -lk 9999

Then, in a different terminal, you can start the example by using


$ ./bin/spark-submit examples/src/main/python/sql/streaming/structured_network_wordcount.py localhost 9999


$ ./bin/run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost 9999


$ ./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999


$ ./bin/spark-submit examples/src/main/r/streaming/structured_network_wordcount.R localhost 9999


Then, any lines typed in the terminal running the netcat server will be counted and printed on screen every second. It will look something like the following.


# TERMINAL 1:
# Running Netcat

$ nc -lk 9999
apache spark
apache hadoop



















...





# TERMINAL 2: RUNNING structured_network_wordcount.py

$ ./bin/spark-submit examples/src/main/python/sql/streaming/structured_network_wordcount.py localhost 9999

-------------------------------------------
Batch: 0
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    1|
| spark|    1|
+------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    2|
| spark|    1|
|hadoop|    1|
+------+-----+
...


# TERMINAL 2: RUNNING StructuredNetworkWordCount

$ ./bin/run-example org.apache.spark.examples.sql.streaming.StructuredNetworkWordCount localhost 9999

-------------------------------------------
Batch: 0
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    1|
| spark|    1|
+------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    2|
| spark|    1|
|hadoop|    1|
+------+-----+
...


# TERMINAL 2: RUNNING JavaStructuredNetworkWordCount

$ ./bin/run-example org.apache.spark.examples.sql.streaming.JavaStructuredNetworkWordCount localhost 9999

-------------------------------------------
Batch: 0
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    1|
| spark|    1|
+------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    2|
| spark|    1|
|hadoop|    1|
+------+-----+
...


# TERMINAL 2: RUNNING structured_network_wordcount.R

$ ./bin/spark-submit examples/src/main/r/streaming/structured_network_wordcount.R localhost 9999

-------------------------------------------
Batch: 0
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    1|
| spark|    1|
+------+-----+

-------------------------------------------
Batch: 1
-------------------------------------------
+------+-----+
| value|count|
+------+-----+
|apache|    2|
| spark|    1|
|hadoop|    1|
+------+-----+
...




Programming Model
The key idea in Structured Streaming is to treat a live data stream as a
table that is being continuously appended. This leads to a new stream
processing model that is very similar to a batch processing model. You will
express your streaming computation as standard batch-like query as on a static
table, and Spark runs it as an incremental query on the unbounded input
table. Let’s understand this model in more detail.
Basic Concepts
Consider the input data stream as the “Input Table”. Every data item that is
arriving on the stream is like a new row being appended to the Input Table.

A query on the input will generate the “Result Table”. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.

The “Output” is defined as what gets written out to the external storage. The output can be defined in a different mode:


Complete Mode - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.


Append Mode - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.


Update Mode - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode.


Note that each mode is applicable on certain types of queries. This is discussed in detail later.
To illustrate the use of this model, let’s understand the model in context of
the Quick Example above. The first lines DataFrame is the input table, and
the final wordCounts DataFrame is the result table. Note that the query on
streaming lines DataFrame to generate wordCounts is exactly the same as
it would be a static DataFrame. However, when this query is started, Spark
will continuously check for new data from the socket connection. If there is
new data, Spark will run an “incremental” query that combines the previous
running counts with the new data to compute updated counts, as shown below.

Note that Structured Streaming does not materialize the entire table. It reads the latest
available data from the streaming data source, processes it incrementally to update the result,
and then discards the source data. It only keeps around the minimal intermediate state data as
required to update the result (e.g. intermediate counts in the earlier example).
This model is significantly different from many other stream processing
engines. Many streaming systems require the user to maintain running
aggregations themselves, thus having to reason about fault-tolerance, and
data consistency (at-least-once, or at-most-once, or exactly-once). In this
model, Spark is responsible for updating the Result Table when there is new
data, thus relieving the users from reasoning about it. As an example, let’s
see how this model handles event-time based processing and late arriving data.
Handling Event-time and Late Data
Event-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time. For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them. This event-time is very naturally expressed in this model – each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column – each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.
Furthermore, this model naturally handles data that has arrived later than
expected based on its event-time. Since Spark is updating the Result Table,
it has full control over updating old aggregates when there is late data,
as well as cleaning up old aggregates to limit the size of intermediate
state data. Since Spark 2.1, we have support for watermarking which
allows the user to specify the threshold of late data, and allows the engine
to accordingly clean up old state. These are explained later in more
detail in the Window Operations section.
Fault Tolerance Semantics
Delivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming. To achieve that, we have designed the Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing. Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers)
to track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure end-to-end exactly-once semantics under any failure.
API using Datasets and DataFrames
Since Spark 2.0, DataFrames and Datasets can represent static, bounded data, as well as streaming, unbounded data. Similar to static Datasets/DataFrames, you can use the common entry point SparkSession
(Scala/Java/Python/R docs)
to create streaming DataFrames/Datasets from streaming sources, and apply the same operations on them as static DataFrames/Datasets. If you are not familiar with Datasets/DataFrames, you are strongly advised to familiarize yourself with them using the
DataFrame/Dataset Programming Guide.
Creating streaming DataFrames and streaming Datasets
Streaming DataFrames can be created through the DataStreamReader interface
(Scala/Java/Python docs)
returned by SparkSession.readStream(). In R, with the read.stream() method. Similar to the read interface for creating static DataFrame, you can specify the details of the source – data format, schema, options, etc.
Input Sources
There are a few built-in sources.

File source - Reads files written in a directory as a stream of data. Files will be processed in the order of file modification time. If latestFirst is set, order will be reversed. Supported file formats are text, CSV, JSON, ORC, Parquet. See the docs of the DataStreamReader interface for a more up-to-date list, and supported options for each file format. Note that the files must be atomically placed in the given directory, which in most file systems, can be achieved by file move operations.

Kafka source - Reads data from Kafka. It’s compatible with Kafka broker versions 0.10.0 or higher. See the Kafka Integration Guide for more details.


Socket source (for testing) - Reads UTF8 text data from a socket connection. The listening server socket is at the driver. Note that this should be used only for testing as this does not provide end-to-end fault-tolerance guarantees.


Rate source (for testing) - Generates data at the specified number of rows per second, each output row contains a timestamp and value. Where timestamp is a Timestamp type containing the time of message dispatch, and value is of Long type containing the message count, starting from 0 as the first row. This source is intended for testing and benchmarking.

Rate Per Micro-Batch source (for testing) - Generates data at the specified number of rows per micro-batch, each output row contains a timestamp and value. Where timestamp is a Timestamp type containing the time of message dispatch, and value is of Long type containing the message count, starting from 0 as the first row. Unlike rate data source, this data source provides a consistent set of input rows per micro-batch regardless of query execution (configuration of trigger, query being lagging, etc.), say, batch 0 will produce 0~999 and batch 1 will produce 1000~1999, and so on. Same applies to the generated time. This source is intended for testing and benchmarking.

Some sources are not fault-tolerant because they do not guarantee that data can be replayed using
checkpointed offsets after a failure. See the earlier section on
fault-tolerance semantics.
Here are the details of all the sources in Spark.



Source
Options
Fault-tolerant
Notes



File source

path: path to the input directory, and common to all file formats.
        
maxFilesPerTrigger: maximum number of new files to be considered in every trigger (default: no max)
        
latestFirst: whether to process the latest new files first, useful when there is a large backlog of files (default: false)
        
fileNameOnly: whether to check new files based on only the filename instead of on the full path (default: false). With this set to `true`, the following files would be considered as the same file, because their filenames, "dataset.txt", are the same:
        
        "file:///dataset.txt"
        "s3://a/dataset.txt"
        "s3n://a/b/dataset.txt"
        "s3a://a/b/c/dataset.txt"
        
maxFileAge: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If latestFirst is set to `true` and maxFilesPerTrigger is set, then this parameter will be ignored, because old files that are valid, and should be processed, may be ignored. The max age is specified with respect to the timestamp of the latest file, and not the timestamp of the current system.(default: 1 week)
        
cleanSource: option to clean up completed files after processing.
        Available options are "archive", "delete", "off". If the option is not provided, the default value is "off".
        When "archive" is provided, additional option sourceArchiveDir must be provided as well. The value of "sourceArchiveDir" must not match with source pattern in depth (the number of directories from the root directory), where the depth is minimum of depth on both paths. This will ensure archived files are never included as new source files.
        For example, suppose you provide '/hello?/spark/*' as source pattern, '/hello1/spark/archive/dir' cannot be used as the value of "sourceArchiveDir", as '/hello?/spark/*' and '/hello1/spark/archive' will be matched. '/hello1/spark' cannot be also used as the value of "sourceArchiveDir", as '/hello?/spark' and '/hello1/spark' will be matched. '/archived/here' would be OK as it doesn't match.
        Spark will move source files respecting their own path. For example, if the path of source file is /a/b/dataset.txt and the path of archive directory is /archived/here, file will be moved to /archived/here/a/b/dataset.txt.
        NOTE: Both archiving (via moving) or deleting completed files will introduce overhead (slow down, even if it's happening in separate thread) in each micro-batch, so you need to understand the cost for each operation in your file system before enabling this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation.
        Number of threads used in completed file cleaner can be configured with spark.sql.streaming.fileSource.cleaner.numThreads (default: 1).
        NOTE 2: The source path should not be used from multiple sources or queries when enabling this option. Similarly, you must ensure the source path doesn't match to any files in output directory of file stream sink.
        NOTE 3: Both delete and move actions are best effort. Failing to delete or move files will not fail the streaming query. Spark may not clean up some source files in some circumstances - e.g. the application doesn't shut down gracefully, too many files are queued to clean up.
        
        For file-format-specific options, see the related methods in DataStreamReader
        (Scala/Java/Python/R).
        E.g. for "parquet" format options see DataStreamReader.parquet().
        
        In addition, there are session configurations that affect certain file-formats. See the SQL Programming Guide for more details. E.g., for "parquet", see Parquet configuration section.
        
Yes
Supports glob paths, but does not support multiple comma-separated paths/globs.


Socket Source

host: host to connect to, must be specified
port: port to connect to, must be specified
    
No



Rate Source

rowsPerSecond (e.g. 100, default: 1): How many rows should be generated per second.
rampUpTime (e.g. 5s, default: 0s): How long to ramp up before the generating speed becomes rowsPerSecond. Using finer granularities than seconds will be truncated to integer seconds. 
numPartitions (e.g. 10, default: Spark's default parallelism): The partition number for the generated rows. 

        The source will try its best to reach rowsPerSecond, but the query may be resource constrained, and numPartitions can be tweaked to help reach the desired speed.
    
Yes



Rate Per Micro-Batch Source (format: rate-micro-batch)

rowsPerBatch (e.g. 100): How many rows should be generated per micro-batch.
numPartitions (e.g. 10, default: Spark's default parallelism): The partition number for the generated rows. 
startTimestamp (e.g. 1000, default: 0): starting value of generated time. 
advanceMillisPerBatch (e.g. 1000, default: 1000): the amount of time being advanced in generated time on each micro-batch. 

Yes



Kafka Source

        See the Kafka Integration Guide.
    
Yes









Here are some examples.


spark = SparkSession. ...

# Read text from socket
socketDF = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

socketDF.isStreaming()    # Returns True for DataFrames that have streaming sources

socketDF.printSchema()

# Read all the csv files written atomically in a directory
userSchema = StructType().add("name", "string").add("age", "integer")
csvDF = spark \
    .readStream \
    .option("sep", ";") \
    .schema(userSchema) \
    .csv("/path/to/directory")  # Equivalent to format("csv").load("/path/to/directory")


val spark: SparkSession = ...

// Read text from socket
val socketDF = spark
  .readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load()

socketDF.isStreaming    // Returns True for DataFrames that have streaming sources

socketDF.printSchema

// Read all the csv files written atomically in a directory
val userSchema = new StructType().add("name", "string").add("age", "integer")
val csvDF = spark
  .readStream
  .option("sep", ";")
  .schema(userSchema)      // Specify schema of the csv files
  .csv("/path/to/directory")    // Equivalent to format("csv").load("/path/to/directory")


SparkSession spark = ...

// Read text from socket
Dataset<Row> socketDF = spark
  .readStream()
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load();

socketDF.isStreaming();    // Returns True for DataFrames that have streaming sources

socketDF.printSchema();

// Read all the csv files written atomically in a directory
StructType userSchema = new StructType().add("name", "string").add("age", "integer");
Dataset<Row> csvDF = spark
  .readStream()
  .option("sep", ";")
  .schema(userSchema)      // Specify schema of the csv files
  .csv("/path/to/directory");    // Equivalent to format("csv").load("/path/to/directory")


sparkR.session(...)

# Read text from socket
socketDF <- read.stream("socket", host = hostname, port = port)

isStreaming(socketDF)    # Returns TRUE for SparkDataFrames that have streaming sources

printSchema(socketDF)

# Read all the csv files written atomically in a directory
schema <- structType(structField("name", "string"),
                     structField("age", "integer"))
csvDF <- read.stream("csv", path = "/path/to/directory", schema = schema, sep = ";")


These examples generate streaming DataFrames that are untyped, meaning that the schema of the DataFrame is not checked at compile time, only checked at runtime when the query is submitted. Some operations like map, flatMap, etc. need the type to be known at compile time. To do those, you can convert these untyped streaming DataFrames to typed streaming Datasets using the same methods as static DataFrame. See the SQL Programming Guide for more details. Additionally, more details on the supported streaming sources are discussed later in the document.
Since Spark 3.1, you can also create streaming DataFrames from tables with DataStreamReader.table(). See Streaming Table APIs for more details.
Schema inference and partition of streaming DataFrames/Datasets
By default, Structured Streaming from file based sources requires you to specify the schema, rather than rely on Spark to infer it automatically. This restriction ensures a consistent schema will be used for the streaming query, even in the case of failures. For ad-hoc use cases, you can reenable schema inference by setting spark.sql.streaming.schemaInference to true.
Partition discovery does occur when subdirectories that are named /key=value/ are present and listing will automatically recurse into these directories. If these columns appear in the user-provided schema, they will be filled in by Spark based on the path of the file being read. The directories that make up the partitioning scheme must be present when the query starts and must remain static. For example, it is okay to add /data/year=2016/ when /data/year=2015/ was present, but it is invalid to change the partitioning column (i.e. by creating the directory /data/date=2016-04-17/).
Operations on streaming DataFrames/Datasets
You can apply all kinds of operations on streaming DataFrames/Datasets – ranging from untyped, SQL-like operations (e.g. select, where, groupBy), to typed RDD-like operations (e.g. map, filter, flatMap). See the SQL programming guide for more details. Let’s take a look at a few example operations that you can use.
Basic Operations - Selection, Projection, Aggregation
Most of the common operations on DataFrame/Dataset are supported for streaming. The few operations that are not supported are discussed later in this section.


df = ...  # streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }

# Select the devices which have signal more than 10
df.select("device").where("signal > 10")

# Running count of the number of updates for each device type
df.groupBy("deviceType").count()


case class DeviceData(device: String, deviceType: String, signal: Double, time: DateTime)

val df: DataFrame = ... // streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }
val ds: Dataset[DeviceData] = df.as[DeviceData]    // streaming Dataset with IOT device data

// Select the devices which have signal more than 10
df.select("device").where("signal > 10")      // using untyped APIs
ds.filter(_.signal > 10).map(_.device)         // using typed APIs

// Running count of the number of updates for each device type
df.groupBy("deviceType").count()                          // using untyped API

// Running average signal for each device type
import org.apache.spark.sql.expressions.scalalang.typed
ds.groupByKey(_.deviceType).agg(typed.avg(_.signal))    // using typed API


import org.apache.spark.api.java.function.*;
import org.apache.spark.sql.*;
import org.apache.spark.sql.expressions.javalang.typed;
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;

public class DeviceData {
  private String device;
  private String deviceType;
  private Double signal;
  private java.sql.Date time;
  ...
  // Getter and setter methods for each field
}

Dataset<Row> df = ...;    // streaming DataFrame with IOT device data with schema { device: string, type: string, signal: double, time: DateType }
Dataset<DeviceData> ds = df.as(ExpressionEncoder.javaBean(DeviceData.class)); // streaming Dataset with IOT device data

// Select the devices which have signal more than 10
df.select("device").where("signal > 10"); // using untyped APIs
ds.filter((FilterFunction<DeviceData>) value -> value.getSignal() > 10)
  .map((MapFunction<DeviceData, String>) value -> value.getDevice(), Encoders.STRING());

// Running count of the number of updates for each device type
df.groupBy("deviceType").count(); // using untyped API

// Running average signal for each device type
ds.groupByKey((MapFunction<DeviceData, String>) value -> value.getDeviceType(), Encoders.STRING())
  .agg(typed.avg((MapFunction<DeviceData, Double>) value -> value.getSignal()));


df <- ...  # streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }

# Select the devices which have signal more than 10
select(where(df, "signal > 10"), "device")

# Running count of the number of updates for each device type
count(groupBy(df, "deviceType"))


You can also register a streaming DataFrame/Dataset as a temporary view and then apply SQL commands on it.


df.createOrReplaceTempView("updates")
spark.sql("select count(*) from updates")  # returns another streaming DF


df.createOrReplaceTempView("updates")
spark.sql("select count(*) from updates")  // returns another streaming DF


df.createOrReplaceTempView("updates");
spark.sql("select count(*) from updates");  // returns another streaming DF


createOrReplaceTempView(df, "updates")
sql("select count(*) from updates")


Note, you can identify whether a DataFrame/Dataset has streaming data or not by using df.isStreaming.


df.isStreaming()


df.isStreaming


df.isStreaming()


isStreaming(df)


You may want to check the query plan of the query, as Spark could inject stateful operations during interpret of SQL statement against streaming dataset. Once stateful operations are injected in the query plan, you may need to check your query with considerations in stateful operations. (e.g. output mode, watermark, state store size maintenance, etc.)
Window Operations on Event Time
Aggregations over a sliding event-time window are straightforward with Structured Streaming and are very similar to grouped aggregations. In a grouped aggregation, aggregate values (e.g. counts) are maintained for each unique value in the user-specified grouping column. In case of window-based aggregations, aggregate values are maintained for each window the event-time of a row falls into. Let’s understand this with an illustration.
Imagine our quick example is modified and the stream now contains lines along with the time when the line was generated. Instead of running word counts, we want to count words within 10 minute windows, updating every 5 minutes. That is, word counts in words received between 10 minute windows 12:00 - 12:10, 12:05 - 12:15, 12:10 - 12:20, etc. Note that 12:00 - 12:10 means data that arrived after 12:00 but before 12:10. Now, consider a word that was received at 12:07. This word should increment the counts corresponding to two windows 12:00 - 12:10 and 12:05 - 12:15. So the counts will be indexed by both, the grouping key (i.e. the word) and the window (can be calculated from the event-time).
The result tables would look something like the following.

Since this windowing is similar to grouping, in code, you can use groupBy() and window() operations to express windowed aggregations. You can see the full code for the below examples in
Scala/Java/Python.


words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }

# Group the data by window and word and compute the count of each group
windowedCounts = words.groupBy(
    window(words.timestamp, "10 minutes", "5 minutes"),
    words.word
).count()


import spark.implicits._

val words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
val windowedCounts = words.groupBy(
  window($"timestamp", "10 minutes", "5 minutes"),
  $"word"
).count()


Dataset<Row> words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
Dataset<Row> windowedCounts = words.groupBy(
  functions.window(words.col("timestamp"), "10 minutes", "5 minutes"),
  words.col("word")
).count();


words <- ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }

# Group the data by window and word and compute the count of each group
windowedCounts <- count(
                    groupBy(
                      words,
                      window(words$timestamp, "10 minutes", "5 minutes"),
                      words$word))


Handling Late Data and Watermarking
Now consider what happens if one of the events arrives late to the application.
For example, say, a word generated at 12:04 (i.e. event time) could be received by
the application at 12:11. The application should use the time 12:04 instead of 12:11
to update the older counts for the window 12:00 - 12:10. This occurs
naturally in our window-based grouping – Structured Streaming can maintain the intermediate state
for partial aggregates for a long period of time such that late data can update aggregates of
old windows correctly, as illustrated below.

However, to run this query for days, it’s necessary for the system to bound the amount of
intermediate in-memory state it accumulates. This means the system needs to know when an old
aggregate can be dropped from the in-memory state because the application is not going to receive
late data for that aggregate any more. To enable this, in Spark 2.1, we have introduced
watermarking, which lets the engine automatically track the current event time in the data
and attempt to clean up old state accordingly. You can define the watermark of a query by
specifying the event time column and the threshold on how late the data is expected to be in terms of
event time. For a specific window ending at time T, the engine will maintain state and allow late
data to update the state until (max event time seen by the engine - late threshold > T).
In other words, late data within the threshold will be aggregated,
but data later than the threshold will start getting dropped
(see later
in the section for the exact guarantees). Let’s understand this with an example. We can
easily define watermarking on the previous example using withWatermark() as shown below.


words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }

# Group the data by window and word and compute the count of each group
windowedCounts = words \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        window(words.timestamp, "10 minutes", "5 minutes"),
        words.word) \
    .count()


import spark.implicits._

val words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
val windowedCounts = words
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        window($"timestamp", "10 minutes", "5 minutes"),
        $"word")
    .count()


Dataset<Row> words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
Dataset<Row> windowedCounts = words
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        window(col("timestamp"), "10 minutes", "5 minutes"),
        col("word"))
    .count();


words <- ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }

# Group the data by window and word and compute the count of each group

words <- withWatermark(words, "timestamp", "10 minutes")
windowedCounts <- count(
                    groupBy(
                      words,
                      window(words$timestamp, "10 minutes", "5 minutes"),
                      words$word))


In this example, we are defining the watermark of the query on the value of the column “timestamp”,
and also defining “10 minutes” as the threshold of how late is the data allowed to be. If this query
is run in Update output mode (discussed later in Output Modes section),
the engine will keep updating counts of a window in the Result Table until the window is older
than the watermark, which lags behind the current event time in column “timestamp” by 10 minutes.
Here is an illustration.

As shown in the illustration, the maximum event time tracked by the engine is the
blue dashed line, and the watermark set as (max event time - '10 mins')
at the beginning of every trigger is the red line. For example, when the engine observes the data
(12:14, dog), it sets the watermark for the next trigger as 12:04.
This watermark lets the engine maintain intermediate state for additional 10 minutes to allow late
data to be counted. For example, the data (12:09, cat) is out of order and late, and it falls in
windows 12:00 - 12:10 and 12:05 - 12:15. Since, it is still ahead of the watermark 12:04 in
the trigger, the engine still maintains the intermediate counts as state and correctly updates the
counts of the related windows. However, when the watermark is updated to 12:11, the intermediate
state for window (12:00 - 12:10) is cleared, and all subsequent data (e.g. (12:04, donkey))
is considered “too late” and therefore ignored. Note that after every trigger,
the updated counts (i.e. purple rows) are written to sink as the trigger output, as dictated by
the Update mode.
Some sinks (e.g. files) may not supported fine-grained updates that Update Mode requires. To work
with them, we have also support Append Mode, where only the final counts are written to sink.
This is illustrated below.
Note that using withWatermark on a non-streaming Dataset is no-op. As the watermark should not affect
any batch query in any way, we will ignore it directly.

Similar to the Update Mode earlier, the engine maintains intermediate counts for each window.
However, the partial counts are not updated to the Result Table and not written to sink. The engine
waits for “10 mins” for late date to be counted,
then drops intermediate state of a window < watermark, and appends the final
counts to the Result Table/sink. For example, the final counts of window 12:00 - 12:10 is
appended to the Result Table only after the watermark is updated to 12:11.
Types of time windows
Spark supports three types of time windows: tumbling (fixed), sliding and session.

Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. An input
can only be bound to a single window.
Sliding windows are similar to the tumbling windows from the point of being “fixed-sized”, but windows
can overlap if the duration of slide is smaller than the duration of window, and in this case an input
can be bound to the multiple windows.
Tumbling and sliding window use window function, which has been described on above examples.
Session windows have different characteristic compared to the previous two types. Session window has a dynamic size
of the window length, depending on the inputs. A session window starts with an input, and expands itself
if following input has been received within gap duration. For static gap duration, a session window closes when
there’s no input received within gap duration after receiving the latest input.
Session window uses session_window function. The usage of the function is similar to the window function.


events = ...  # streaming DataFrame of schema { timestamp: Timestamp, userId: String }

# Group the data by session window and userId, and compute the count of each group
sessionizedCounts = events \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        session_window(events.timestamp, "5 minutes"),
        events.userId) \
    .count()


import spark.implicits._

val events = ... // streaming DataFrame of schema { timestamp: Timestamp, userId: String }

// Group the data by session window and userId, and compute the count of each group
val sessionizedCounts = events
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        session_window($"timestamp", "5 minutes"),
        $"userId")
    .count()


Dataset<Row> events = ... // streaming DataFrame of schema { timestamp: Timestamp, userId: String }

// Group the data by session window and userId, and compute the count of each group
Dataset<Row> sessionizedCounts = events
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        session_window(col("timestamp"), "5 minutes"),
        col("userId"))
    .count();


Instead of static value, we can also provide an expression to specify gap duration dynamically
based on the input row. Note that the rows with negative or zero gap duration will be filtered
out from the aggregation.
With dynamic gap duration, the closing of a session window does not depend on the latest input
anymore. A session window’s range is the union of all events’ ranges which are determined by
event start time and evaluated gap duration during the query execution.


from pyspark.sql import functions as sf

events = ...  # streaming DataFrame of schema { timestamp: Timestamp, userId: String }

session_window = session_window(events.timestamp, \
    sf.when(events.userId == "user1", "5 seconds") \
    .when(events.userId == "user2", "20 seconds").otherwise("5 minutes"))

# Group the data by session window and userId, and compute the count of each group
sessionizedCounts = events \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        session_window,
        events.userId) \
    .count()


import spark.implicits._

val events = ... // streaming DataFrame of schema { timestamp: Timestamp, userId: String }

val sessionWindow = session_window($"timestamp", when($"userId" === "user1", "5 seconds")
  .when($"userId" === "user2", "20 seconds")
  .otherwise("5 minutes"))

// Group the data by session window and userId, and compute the count of each group
val sessionizedCounts = events
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        Column(sessionWindow),
        $"userId")
    .count()


Dataset<Row> events = ... // streaming DataFrame of schema { timestamp: Timestamp, userId: String }

SessionWindow sessionWindow = session_window(col("timestamp"), when(col("userId").equalTo("user1"), "5 seconds")
  .when(col("userId").equalTo("user2"), "20 seconds")
  .otherwise("5 minutes"))

// Group the data by session window and userId, and compute the count of each group
Dataset<Row> sessionizedCounts = events
    .withWatermark("timestamp", "10 minutes")
    .groupBy(
        new Column(sessionWindow),
        col("userId"))
    .count();


Note that there are some restrictions when you use session window in streaming query, like below:

“Update mode” as output mode is not supported.
There should be at least one column in addition to session_window in grouping key.

For batch query, global window (only having session_window in grouping key) is supported.
By default, Spark does not perform partial aggregation for session window aggregation, since it requires additional
sort in local partitions before grouping. It works better for the case there are only few number of input rows in
same group key for each local partition, but for the case there are numerous input rows having same group key in
local partition, doing partial aggregation can still increase the performance significantly despite additional sort.
You can enable spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition to indicate Spark to perform partial aggregation.
Representation of the time for time window
In some use cases, it is necessary to extract the representation of the time for time window, to apply operations requiring timestamp to the time windowed data.
One example is chained time window aggregations, where users want to define another time window against the time window. Say, someone wants to aggregate 5 minutes time windows as 1 hour tumble time window.
There are two ways to achieve this, like below:

Use window_time SQL function with time window column as parameter
Use window SQL function with time window column as parameter

window_time function will produce a timestamp which represents the time for time window.
User can pass the result to the parameter of window function (or anywhere requiring timestamp) to perform operation(s) with time window which requires timestamp.


words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }

# Group the data by window and word and compute the count of each group
windowedCounts = words.groupBy(
    window(words.timestamp, "10 minutes", "5 minutes"),
    words.word
).count()

# Group the windowed data by another window and word and compute the count of each group
anotherWindowedCounts = windowedCounts.groupBy(
    window(window_time(windowedCounts.window), "1 hour"),
    windowedCounts.word
).count()


import spark.implicits._

val words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
val windowedCounts = words.groupBy(
  window($"timestamp", "10 minutes", "5 minutes"),
  $"word"
).count()

// Group the windowed data by another window and word and compute the count of each group
val anotherWindowedCounts = windowedCounts.groupBy(
  window(window_time($"window"), "1 hour"),
  $"word"
).count()


Dataset<Row> words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
Dataset<Row> windowedCounts = words.groupBy(
  functions.window(words.col("timestamp"), "10 minutes", "5 minutes"),
  words.col("word")
).count();

// Group the windowed data by another window and word and compute the count of each group
Dataset<Row> anotherWindowedCounts = windowedCounts.groupBy(
  functions.window(functions.window_time("window"), "1 hour"),
  windowedCounts.col("word")
).count();


window function does not only take timestamp column, but also take the time window column. This is specifically useful for cases where users want to apply chained time window aggregations.


words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }

# Group the data by window and word and compute the count of each group
windowedCounts = words.groupBy(
    window(words.timestamp, "10 minutes", "5 minutes"),
    words.word
).count()

# Group the windowed data by another window and word and compute the count of each group
anotherWindowedCounts = windowedCounts.groupBy(
    window(windowedCounts.window, "1 hour"),
    windowedCounts.word
).count()


import spark.implicits._

val words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
val windowedCounts = words.groupBy(
  window($"timestamp", "10 minutes", "5 minutes"),
  $"word"
).count()

// Group the windowed data by another window and word and compute the count of each group
val anotherWindowedCounts = windowedCounts.groupBy(
  window($"window", "1 hour"),
  $"word"
).count()


Dataset<Row> words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }

// Group the data by window and word and compute the count of each group
Dataset<Row> windowedCounts = words.groupBy(
  functions.window(words.col("timestamp"), "10 minutes", "5 minutes"),
  words.col("word")
).count();

// Group the windowed data by another window and word and compute the count of each group
Dataset<Row> anotherWindowedCounts = windowedCounts.groupBy(
  functions.window("window", "1 hour"),
  windowedCounts.col("word")
).count();


Conditions for watermarking to clean aggregation state
It is important to note that the following conditions must be satisfied for the watermarking to
clean the state in aggregation queries (as of Spark 2.1.1, subject to change in the future).


Output mode must be Append or Update. Complete mode requires all aggregate data to be preserved,
and hence cannot use watermarking to drop intermediate state. See the Output Modes
section for detailed explanation of the semantics of each output mode.


The aggregation must have either the event-time column, or a window on the event-time column.


withWatermark must be called on the
same column as the timestamp column used in the aggregate. For example,
df.withWatermark("time", "1 min").groupBy("time2").count() is invalid
in Append output mode, as watermark is defined on a different column
from the aggregation column.


withWatermark must be called before the aggregation for the watermark details to be used.
For example, df.groupBy("time").count().withWatermark("time", "1 min") is invalid in Append
output mode.


Semantic Guarantees of Aggregation with Watermarking


A watermark delay (set with withWatermark) of “2 hours” guarantees that the engine will never
drop any data that is less than 2 hours delayed. In other words, any data less than 2 hours behind
(in terms of event-time) the latest data processed till then is guaranteed to be aggregated.


However, the guarantee is strict only in one direction. Data delayed by more than 2 hours is
not guaranteed to be dropped; it may or may not get aggregated. More delayed is the data, less
likely is the engine going to process it.


Join Operations
Structured Streaming supports joining a streaming Dataset/DataFrame with a static Dataset/DataFrame
as well as another streaming Dataset/DataFrame. The result of the streaming join is generated
incrementally, similar to the results of streaming aggregations in the previous section. In this
section we will explore what type of joins (i.e. inner, outer, semi, etc.) are supported in the above
cases. Note that in all the supported join types, the result of the join with a streaming
Dataset/DataFrame will be the exactly the same as if it was with a static Dataset/DataFrame
containing the same data in the stream.
Stream-static Joins
Since the introduction in Spark 2.0, Structured Streaming has supported joins (inner join and some
type of outer joins) between a streaming and a static DataFrame/Dataset. Here is a simple example.


staticDf = spark.read. ...
streamingDf = spark.readStream. ...
streamingDf.join(staticDf, "type")  # inner equi-join with a static DF
streamingDf.join(staticDf, "type", "left_outer")  # left outer join with a static DF


val staticDf = spark.read. ...
val streamingDf = spark.readStream. ...

streamingDf.join(staticDf, "type")          // inner equi-join with a static DF
streamingDf.join(staticDf, "type", "left_outer")  // left outer join with a static DF


Dataset<Row> staticDf = spark.read(). ...;
Dataset<Row> streamingDf = spark.readStream(). ...;
streamingDf.join(staticDf, "type");         // inner equi-join with a static DF
streamingDf.join(staticDf, "type", "left_outer");  // left outer join with a static DF


staticDf <- read.df(...)
streamingDf <- read.stream(...)
joined <- merge(streamingDf, staticDf, sort = FALSE)  # inner equi-join with a static DF
joined <- join(
            streamingDf,
            staticDf,
            streamingDf$value == staticDf$value,
            "left_outer")  # left outer join with a static DF


Note that stream-static joins are not stateful, so no state management is necessary.
However, a few types of stream-static outer joins are not yet supported.
These are listed at the end of this Join section.
Stream-stream Joins
In Spark 2.3, we have added support for stream-stream joins, that is, you can join two streaming
Datasets/DataFrames. The challenge of generating join results between two data streams is that,
at any point of time, the view of the dataset is incomplete for both sides of the join making
it much harder to find matches between inputs. Any row received from one input stream can match
with any future, yet-to-be-received row from the other input stream. Hence, for both the input
streams, we buffer past input as streaming state, so that we can match every future input with
past input and accordingly generate joined results. Furthermore, similar to streaming aggregations,
we automatically handle late, out-of-order data and can limit the state using watermarks.
Let’s discuss the different types of supported stream-stream joins and how to use them.
Inner Joins with optional Watermarking
Inner joins on any kind of columns along with any kind of join conditions are supported.
However, as the stream runs, the size of streaming state will keep growing indefinitely as
all past input must be saved as any new input can match with any input from the past.
To avoid unbounded state, you have to define additional join conditions such that indefinitely
old inputs cannot match with future inputs and therefore can be cleared from the state.
In other words, you will have to do the following additional steps in the join.


Define watermark delays on both inputs such that the engine knows how delayed the input can be
(similar to streaming aggregations)


Define a constraint on event-time across the two inputs such that the engine can figure out when
old rows of one input is not going to be required (i.e. will not satisfy the time constraint) for
matches with the other input. This constraint can be defined in one of the two ways.


Time range join conditions (e.g. ...JOIN ON leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR),


Join on event-time windows (e.g. ...JOIN ON leftTimeWindow = rightTimeWindow).




Let’s understand this with an example.
Let’s say we want to join a stream of advertisement impressions (when an ad was shown) with
another stream of user clicks on advertisements to correlate when impressions led to
monetizable clicks. To allow the state cleanup in this stream-stream join, you will have to
specify the watermarking delays and the time constraints as follows.


Watermark delays: Say, the impressions and the corresponding clicks can be late/out-of-order
in event-time by at most 2 and 3 hours, respectively.


Event-time range condition: Say, a click can occur within a time range of 0 seconds to 1 hour
after the corresponding impression.


The code would look like this.


from pyspark.sql.functions import expr

impressions = spark.readStream. ...
clicks = spark.readStream. ...

# Apply watermarks on event-time columns
impressionsWithWatermark = impressions.withWatermark("impressionTime", "2 hours")
clicksWithWatermark = clicks.withWatermark("clickTime", "3 hours")

# Join with event-time constraints
impressionsWithWatermark.join(
  clicksWithWatermark,
  expr("""
    clickAdId = impressionAdId AND
    clickTime >= impressionTime AND
    clickTime <= impressionTime + interval 1 hour
    """)
)


import org.apache.spark.sql.functions.expr

val impressions = spark.readStream. ...
val clicks = spark.readStream. ...

// Apply watermarks on event-time columns
val impressionsWithWatermark = impressions.withWatermark("impressionTime", "2 hours")
val clicksWithWatermark = clicks.withWatermark("clickTime", "3 hours")

// Join with event-time constraints
impressionsWithWatermark.join(
  clicksWithWatermark,
  expr("""
    clickAdId = impressionAdId AND
    clickTime >= impressionTime AND
    clickTime <= impressionTime + interval 1 hour
    """)
)


import static org.apache.spark.sql.functions.expr

Dataset<Row> impressions = spark.readStream(). ...
Dataset<Row> clicks = spark.readStream(). ...

// Apply watermarks on event-time columns
Dataset<Row> impressionsWithWatermark = impressions.withWatermark("impressionTime", "2 hours");
Dataset<Row> clicksWithWatermark = clicks.withWatermark("clickTime", "3 hours");

// Join with event-time constraints
impressionsWithWatermark.join(
  clicksWithWatermark,
  expr(
    "clickAdId = impressionAdId AND " +
    "clickTime >= impressionTime AND " +
    "clickTime <= impressionTime + interval 1 hour ")
);


impressions <- read.stream(...)
clicks <- read.stream(...)

# Apply watermarks on event-time columns
impressionsWithWatermark <- withWatermark(impressions, "impressionTime", "2 hours")
clicksWithWatermark <- withWatermark(clicks, "clickTime", "3 hours")

# Join with event-time constraints
joined <- join(
  impressionsWithWatermark,
  clicksWithWatermark,
  expr(
    paste(
      "clickAdId = impressionAdId AND",
      "clickTime >= impressionTime AND",
      "clickTime <= impressionTime + interval 1 hour"
)))


Semantic Guarantees of Stream-stream Inner Joins with Watermarking
This is similar to the guarantees provided by watermarking on aggregations.
A watermark delay of “2 hours” guarantees that the engine will never drop any data that is less than
 2 hours delayed. But data delayed by more than 2 hours may or may not get processed.
Outer Joins with Watermarking
While the watermark + event-time constraints is optional for inner joins, for outer joins
they must be specified. This is because for generating the NULL results in outer join, the
engine must know when an input row is not going to match with anything in future. Hence, the
watermark + event-time constraints must be specified for generating correct results. Therefore,
a query with outer-join will look quite like the ad-monetization example earlier, except that
there will be an additional parameter specifying it to be an outer-join.


impressionsWithWatermark.join(
  clicksWithWatermark,
  expr("""
    clickAdId = impressionAdId AND
    clickTime >= impressionTime AND
    clickTime <= impressionTime + interval 1 hour
    """),
  "leftOuter"                 # can be "inner", "leftOuter", "rightOuter", "fullOuter", "leftSemi"
)


impressionsWithWatermark.join(
  clicksWithWatermark,
  expr("""
    clickAdId = impressionAdId AND
    clickTime >= impressionTime AND
    clickTime <= impressionTime + interval 1 hour
    """),
  joinType = "leftOuter"      // can be "inner", "leftOuter", "rightOuter", "fullOuter", "leftSemi"
 )


impressionsWithWatermark.join(
  clicksWithWatermark,
  expr(
    "clickAdId = impressionAdId AND " +
    "clickTime >= impressionTime AND " +
    "clickTime <= impressionTime + interval 1 hour "),
  "leftOuter"                 // can be "inner", "leftOuter", "rightOuter", "fullOuter", "leftSemi"
);


joined <- join(
  impressionsWithWatermark,
  clicksWithWatermark,
  expr(
    paste(
      "clickAdId = impressionAdId AND",
      "clickTime >= impressionTime AND",
      "clickTime <= impressionTime + interval 1 hour"),
  "left_outer"                 # can be "inner", "left_outer", "right_outer", "full_outer", "left_semi"
))


Semantic Guarantees of Stream-stream Outer Joins with Watermarking
Outer joins have the same guarantees as inner joins
regarding watermark delays and whether data will be dropped or not.
Caveats
There are a few important characteristics to note regarding how the outer results are generated.


The outer NULL results will be generated with a delay that depends on the specified watermark
delay and the time range condition. This is because the engine has to wait for that long to ensure
there were no matches and there will be no more matches in future.


In the current implementation in the micro-batch engine, watermarks are advanced at the end of a
micro-batch, and the next micro-batch uses the updated watermark to clean up state and output
outer results. Since we trigger a micro-batch only when there is new data to be processed, the
generation of the outer result may get delayed if there no new data being received in the stream.
In short, if any of the two input streams being joined does not receive data for a while, the
outer (both cases, left or right) output may get delayed.


Semi Joins with Watermarking
A semi join returns values from the left side of the relation that has a match with the right.
It is also referred to as a left semi join. Similar to outer joins, watermark + event-time
constraints must be specified for semi join. This is to evict unmatched input rows on left side,
the engine must know when an input row on left side is not going to match with anything on right
side in future.
Semantic Guarantees of Stream-stream Semi Joins with Watermarking
Semi joins have the same guarantees as inner joins
regarding watermark delays and whether data will be dropped or not.
Support matrix for joins in streaming queries



Left Input
Right Input
Join Type




Static
Static
All types

        Supported, since its not on streaming data even though it
        can be present in a streaming query
      


Stream
Static
Inner
Supported, not stateful


Left Outer
Supported, not stateful


Right Outer
Not supported


Full Outer
Not supported


Left Semi
Supported, not stateful


Static
Stream
Inner
Supported, not stateful


Left Outer
Not supported


Right Outer
Supported, not stateful


Full Outer
Not supported


Left Semi
Not supported


Stream
Stream
Inner

      Supported, optionally specify watermark on both sides +
      time constraints for state cleanup
    


Left Outer

      Conditionally supported, must specify watermark on right + time constraints for correct
      results, optionally specify watermark on left for all state cleanup
    


Right Outer

      Conditionally supported, must specify watermark on left + time constraints for correct
      results, optionally specify watermark on right for all state cleanup
    


Full Outer

      Conditionally supported, must specify watermark on one side + time constraints for correct
      results, optionally specify watermark on the other side for all state cleanup
    


Left Semi

      Conditionally supported, must specify watermark on right + time constraints for correct
      results, optionally specify watermark on left for all state cleanup
    








Additional details on supported joins:


Joins can be cascaded, that is, you can do df1.join(df2, ...).join(df3, ...).join(df4, ....).


As of Spark 2.4, you can use joins only when the query is in Append output mode. Other output modes are not yet supported.


You cannot use mapGroupsWithState and flatMapGroupsWithState before and after joins.


In append output mode, you can construct a query having non-map-like operations e.g. aggregation, deduplication, stream-stream join before/after join.
For example, here’s an example of time window aggregation in both streams followed by stream-stream join with event time window:


val clicksWindow = clicksWithWatermark
  .groupBy(window("clickTime", "1 hour"))
  .count()

val impressionsWindow = impressionsWithWatermark
  .groupBy(window("impressionTime", "1 hour"))
  .count()

clicksWindow.join(impressionsWindow, "window", "inner")


Dataset<Row> clicksWindow = clicksWithWatermark
  .groupBy(functions.window(clicksWithWatermark.col("clickTime"), "1 hour"))
  .count();

Dataset<Row> impressionsWindow = impressionsWithWatermark
  .groupBy(functions.window(impressionsWithWatermark.col("impressionTime"), "1 hour"))
  .count();

clicksWindow.join(impressionsWindow, "window", "inner");


clicksWindow = clicksWithWatermark.groupBy(
  clicksWithWatermark.clickAdId,
  window(clicksWithWatermark.clickTime, "1 hour")
).count()

impressionsWindow = impressionsWithWatermark.groupBy(
  impressionsWithWatermark.impressionAdId,
  window(impressionsWithWatermark.impressionTime, "1 hour")
).count()

clicksWindow.join(impressionsWindow, "window", "inner")


Here’s another example of stream-stream join with time range join condition followed by time window aggregation:


val joined = impressionsWithWatermark.join(
  clicksWithWatermark,
  expr("""
    clickAdId = impressionAdId AND
    clickTime >= impressionTime AND
    clickTime <= impressionTime + interval 1 hour
  """),
  joinType = "leftOuter"      // can be "inner", "leftOuter", "rightOuter", "fullOuter", "leftSemi"
)

joined
  .groupBy($"clickAdId", window($"clickTime", "1 hour"))
  .count()


Dataset<Row> joined = impressionsWithWatermark.join(
  clicksWithWatermark,
  expr(
    "clickAdId = impressionAdId AND " +
    "clickTime >= impressionTime AND " +
    "clickTime <= impressionTime + interval 1 hour "),
  "leftOuter"                 // can be "inner", "leftOuter", "rightOuter", "fullOuter", "leftSemi"
);

joined
  .groupBy(joined.col("clickAdId"), functions.window(joined.col("clickTime"), "1 hour"))
  .count();


joined = impressionsWithWatermark.join(
  clicksWithWatermark,
  expr("""
    clickAdId = impressionAdId AND
    clickTime >= impressionTime AND
    clickTime <= impressionTime + interval 1 hour
    """),
  "leftOuter"                 # can be "inner", "leftOuter", "rightOuter", "fullOuter", "leftSemi"
)

joined.groupBy(
  joined.clickAdId,
  window(joined.clickTime, "1 hour")
).count()


Streaming Deduplication
You can deduplicate records in data streams using a unique identifier in the events. This is exactly same as deduplication on static using a unique identifier column. The query will store the necessary amount of data from previous records such that it can filter duplicate records. Similar to aggregations, you can use deduplication with or without watermarking.


With watermark - If there is an upper bound on how late a duplicate record may arrive, then you can define a watermark on an event time column and deduplicate using both the guid and the event time columns. The query will use the watermark to remove old state data from past records that are not expected to get any duplicates any more. This bounds the amount of the state the query has to maintain.


Without watermark - Since there are no bounds on when a duplicate record may arrive, the query stores the data from all the past records as state.




streamingDf = spark.readStream. ...

# Without watermark using guid column
streamingDf.dropDuplicates("guid")

# With watermark using guid and eventTime columns
streamingDf \
  .withWatermark("eventTime", "10 seconds") \
  .dropDuplicates("guid", "eventTime")


val streamingDf = spark.readStream. ...  // columns: guid, eventTime, ...

// Without watermark using guid column
streamingDf.dropDuplicates("guid")

// With watermark using guid and eventTime columns
streamingDf
  .withWatermark("eventTime", "10 seconds")
  .dropDuplicates("guid", "eventTime")


Dataset<Row> streamingDf = spark.readStream(). ...;  // columns: guid, eventTime, ...

// Without watermark using guid column
streamingDf.dropDuplicates("guid");

// With watermark using guid and eventTime columns
streamingDf
  .withWatermark("eventTime", "10 seconds")
  .dropDuplicates("guid", "eventTime");


streamingDf <- read.stream(...)

# Without watermark using guid column
streamingDf <- dropDuplicates(streamingDf, "guid")

# With watermark using guid and eventTime columns
streamingDf <- withWatermark(streamingDf, "eventTime", "10 seconds")
streamingDf <- dropDuplicates(streamingDf, "guid", "eventTime")


Specifically for streaming, you can deduplicate records in data streams using a unique identifier in the events, within the time range of watermark.
For example, if you set the delay threshold of watermark as “1 hour”, duplicated events which occurred within 1 hour can be correctly deduplicated.
(For more details, please refer to the API doc of dropDuplicatesWithinWatermark.)
This can be used to deal with use case where event time column cannot be a part of unique identifier, mostly due to the case
where event times are somehow different for the same records. (E.g. non-idempotent writer where issuing event time happens at write)
Users are encouraged to set the delay threshold of watermark longer than max timestamp differences among duplicated events.
This feature requires watermark with delay threshold to be set in streaming DataFrame/Dataset.


streamingDf = spark.readStream. ...

# deduplicate using guid column with watermark based on eventTime column
streamingDf \
  .withWatermark("eventTime", "10 hours") \
  .dropDuplicatesWithinWatermark("guid")


val streamingDf = spark.readStream. ...  // columns: guid, eventTime, ...

// deduplicate using guid column with watermark based on eventTime column
streamingDf
  .withWatermark("eventTime", "10 hours")
  .dropDuplicatesWithinWatermark("guid")


Dataset<Row> streamingDf = spark.readStream(). ...;  // columns: guid, eventTime, ...

// deduplicate using guid column with watermark based on eventTime column
streamingDf
  .withWatermark("eventTime", "10 hours")
  .dropDuplicatesWithinWatermark("guid");


Policy for handling multiple watermarks
A streaming query can have multiple input streams that are unioned or joined together.
Each of the input streams can have a different threshold of late data that needs to
be tolerated for stateful operations. You specify these thresholds using
withWatermarks("eventTime", delay) on each of the input streams. For example, consider
a query with stream-stream joins between inputStream1 and inputStream2.


inputStream1.withWatermark("eventTime1", "1 hour")
  .join(
    inputStream2.withWatermark("eventTime2", "2 hours"),
    joinCondition)


While executing the query, Structured Streaming individually tracks the maximum
event time seen in each input stream, calculates watermarks based on the corresponding delay,
and chooses a single global watermark with them to be used for stateful operations. By default,
the minimum is chosen as the global watermark because it ensures that no data is
accidentally dropped as too late if one of the streams falls behind the others
(for example, one of the streams stops receiving data due to upstream failures). In other words,
the global watermark will safely move at the pace of the slowest stream and the query output will
be delayed accordingly.
However, in some cases, you may want to get faster results even if it means dropping data from the
slowest stream. Since Spark 2.4, you can set the multiple watermark policy to choose
the maximum value as the global watermark by setting the SQL configuration
spark.sql.streaming.multipleWatermarkPolicy to max (default is min).
This lets the global watermark move at the pace of the fastest stream.
However, as a side effect, data from the slower streams will be aggressively dropped. Hence, use
this configuration judiciously.
Arbitrary Stateful Operations
Many usecases require more advanced stateful operations than aggregations. For example, in many usecases, you have to track sessions from data streams of events. For doing such sessionization, you will have to save arbitrary types of data as state, and perform arbitrary operations on the state using the data stream events in every trigger. Since Spark 2.2, this can be done using the operation mapGroupsWithState and the more powerful operation flatMapGroupsWithState. Both operations allow you to apply user-defined code on grouped Datasets to update user-defined state. For more concrete details, take a look at the API documentation (Scala/Java) and the examples (Scala/Java).
Though Spark cannot check and force it, the state function should be implemented with respect to the semantics of the output mode. For example, in Update mode Spark doesn’t expect that the state function will emit rows which are older than current watermark plus allowed late record delay, whereas in Append mode the state function can emit these rows.
Unsupported Operations
There are a few DataFrame/Dataset operations that are not supported with streaming DataFrames/Datasets.
Some of them are as follows.


Limit and take the first N rows are not supported on streaming Datasets.


Distinct operations on streaming Datasets are not supported.


Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.


Few types of outer joins on streaming Datasets are not supported. See the
support matrix in the Join Operations section
for more details.


Chaining multiple stateful operations on streaming Datasets is not supported with Update and Complete mode.

In addition, mapGroupsWithState/flatMapGroupsWithState operation followed by other stateful operation is not supported in Append mode.
A known workaround is to split your streaming query into multiple queries having a single stateful operation per each query,
and ensure end-to-end exactly once per query. Ensuring end-to-end exactly once for the last query is optional.



In addition, there are some Dataset methods that will not work on streaming Datasets. They are actions that will immediately run queries and return results, which does not make sense on a streaming Dataset. Rather, those functionalities can be done by explicitly starting a streaming query (see the next section regarding that).


count() - Cannot return a single count from a streaming Dataset. Instead, use ds.groupBy().count() which returns a streaming Dataset containing a running count.


foreach() - Instead use ds.writeStream.foreach(...) (see next section).


show() - Instead use the console sink (see next section).


If you try any of these operations, you will see an AnalysisException like “operation XYZ is not supported with streaming DataFrames/Datasets”.
While some of them may be supported in future releases of Spark,
there are others which are fundamentally hard to implement on streaming data efficiently.
For example, sorting on the input stream is not supported, as it requires keeping
track of all the data received in the stream. This is therefore fundamentally hard to execute
efficiently.
State Store
State store is a versioned key-value store which provides both read and write operations. In
Structured Streaming, we use the state store provider to handle the stateful operations across
batches. There are two built-in state store provider implementations. End users can also implement
their own state store provider by extending StateStoreProvider interface.
HDFS state store provider
The HDFS backend state store provider is the default implementation of [[StateStoreProvider]] and
[[StateStore]] in which all the data is stored in memory map in the first stage, and then backed
by files in an HDFS-compatible file system. All updates to the store have to be done in sets
transactionally, and each set of updates increments the store’s version. These versions can be
used to re-execute the updates (by retries in RDD operations) on the correct version of the store,
and regenerate the store version.
RocksDB state store implementation
As of Spark 3.2, we add a new built-in state store implementation, RocksDB state store provider.
If you have stateful operations in your streaming query (for example, streaming aggregation,
streaming dropDuplicates, stream-stream joins, mapGroupsWithState, or flatMapGroupsWithState)
and you want to maintain millions of keys in the state, then you may face issues related to large
JVM garbage collection (GC) pauses causing high variations in the micro-batch processing times.
This occurs because, by the implementation of HDFSBackedStateStore, the state data is maintained
in the JVM memory of the executors and large number of state objects puts memory pressure on the
JVM causing high GC pauses.
In such cases, you can choose to use a more optimized state management solution based on
RocksDB. Rather than keeping the state in the JVM memory, this solution
uses RocksDB to efficiently manage the state in the native memory and the local disk. Furthermore,
any changes to this state are automatically saved by Structured Streaming to the checkpoint
location you have provided, thus providing full fault-tolerance guarantees (the same as default
state management).
To enable the new build-in state store implementation, set spark.sql.streaming.stateStore.providerClass
to org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider.
Here are the configs regarding to RocksDB instance of the state store provider:



Config Name
Description
Default Value



spark.sql.streaming.stateStore.rocksdb.compactOnCommit
Whether we perform a range compaction of RocksDB instance for commit operation
False


spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled
Whether to upload changelog instead of snapshot during RocksDB StateStore commit
False


spark.sql.streaming.stateStore.rocksdb.blockSizeKB
Approximate size in KB of user data packed per block for a RocksDB BlockBasedTable, which is a RocksDB's default SST file format.
4


spark.sql.streaming.stateStore.rocksdb.blockCacheSizeMB
The size capacity in MB for a cache of blocks.
8


spark.sql.streaming.stateStore.rocksdb.lockAcquireTimeoutMs
The waiting time in millisecond for acquiring lock in the load operation for RocksDB instance.
60000


spark.sql.streaming.stateStore.rocksdb.maxOpenFiles
The number of open files that can be used by the RocksDB instance. Value of -1 means that files opened are always kept open. If the open file limit is reached, RocksDB will evict entries from the open file cache and close those file descriptors and remove the entries from the cache.
-1


spark.sql.streaming.stateStore.rocksdb.resetStatsOnLoad
Whether we resets all ticker and histogram stats for RocksDB on load.
True


spark.sql.streaming.stateStore.rocksdb.trackTotalNumberOfRows
Whether we track the total number of rows in state store. Please refer the details in Performance-aspect considerations.
True


spark.sql.streaming.stateStore.rocksdb.writeBufferSizeMB
The maximum size of MemTable in RocksDB. Value of -1 means that RocksDB internal default values will be used
-1


spark.sql.streaming.stateStore.rocksdb.maxWriteBufferNumber
The maximum number of MemTables in RocksDB, both active and immutable. Value of -1 means that RocksDB internal default values will be used
-1


spark.sql.streaming.stateStore.rocksdb.boundedMemoryUsage
Whether total memory usage for RocksDB state store instances on a single node is bounded.
false


spark.sql.streaming.stateStore.rocksdb.maxMemoryUsageMB
Total memory limit in MB for RocksDB state store instances on a single node.
500


spark.sql.streaming.stateStore.rocksdb.writeBufferCacheRatio
Total memory to be occupied by write buffers as a fraction of memory allocated across all RocksDB instances on a single node using maxMemoryUsageMB.
0.5


spark.sql.streaming.stateStore.rocksdb.highPriorityPoolRatio
Total memory to be occupied by blocks in high priority pool as a fraction of memory allocated across all RocksDB instances on a single node using maxMemoryUsageMB.
0.1


RocksDB State Store Memory Management
RocksDB allocates memory for different objects such as memtables, block cache and filter/index blocks. If left unbounded, RocksDB memory usage across multiple instances could grow indefinitely and potentially cause OOM (out-of-memory) issues.
RocksDB provides a way to limit the memory usage for all DB instances running on a single node by using the write buffer manager functionality.
If you want to cap RocksDB memory usage in your Spark Structured Streaming deployment, this feature can be enabled by setting the spark.sql.streaming.stateStore.rocksdb.boundedMemoryUsage config to true.
You can also determine the max allowed memory for RocksDB instances by setting the spark.sql.streaming.stateStore.rocksdb.maxMemoryUsageMB value to a static number or as a fraction of the physical memory available on the node.
Limits for individual RocksDB instances can also be configured by setting spark.sql.streaming.stateStore.rocksdb.writeBufferSizeMB and spark.sql.streaming.stateStore.rocksdb.maxWriteBufferNumber to the required values. By default, RocksDB internal defaults are used for these settings.
RocksDB State Store Changelog Checkpointing
In newer version of Spark, changelog checkpointing is introduced for RocksDB state store. The traditional checkpointing mechanism for RocksDB State Store is incremental snapshot checkpointing, where the manifest files and newly generated RocksDB SST files of RocksDB instances are uploaded to a durable storage.
Instead of uploading data files of RocksDB instances, changelog checkpointing uploads changes made to the state since the last checkpoint for durability.
Snapshots are persisted periodically in the background for predictable failure recovery and changelog trimming.
Changelog checkpointing avoids cost of capturing and uploading snapshots of RocksDB instances and significantly reduce streaming query latency.
Changelog checkpointing is disabled by default. You can enable RocksDB State Store changelog checkpointing by setting spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled config to true.
Changelog checkpointing is designed to be backward compatible with traditional checkpointing mechanism.
RocksDB state store provider offers seamless support for transitioning between two checkpointing mechanisms in both directions. This allows you to leverage the performance benefits of changelog checkpointing without discarding the old state checkpoint.
In a version of spark that supports changelog checkpointing, you can migrate streaming queries from older versions of Spark to changelog checkpointing by enabling changelog checkpointing in the spark session.
Vice versa, you can disable changelog checkpointing safely in newer version of Spark, then any query that already run with changelog checkpointing will switch back to traditional checkpointing.
You would need to restart you streaming queries for change in checkpointing mechanism to be applied, but you won’t observe any performance degrade in the process.
Performance-aspect considerations

You may want to disable the track of total number of rows to aim the better performance on RocksDB state store.

Tracking the number of rows brings additional lookup on write operations - you’re encouraged to try turning off the config on tuning RocksDB state store, especially the values of metrics for state operator are big - numRowsUpdated, numRowsRemoved.
You can change the config during restarting the query, which enables you to change the trade-off decision on “observability vs performance”.
If the config is disabled, the number of rows in state (numTotalStateRows) will be reported as 0.
State Store and task locality
The stateful operations store states for events in state stores of executors. State stores occupy resources such as memory and disk space to store the states.
So it is more efficient to keep a state store provider running in the same executor across different streaming batches.
Changing the location of a state store provider requires the extra overhead of loading checkpointed states. The overhead of loading state from checkpoint depends
on the external storage and the size of the state, which tends to hurt the latency of micro-batch run. For some use cases such as processing very large state data,
loading new state store providers from checkpointed states can be very time-consuming and inefficient.
The stateful operations in Structured Streaming queries rely on the preferred location feature of Spark’s RDD to run the state store provider on the same executor.
If in the next batch the corresponding state store provider is scheduled on this executor again, it could reuse the previous states and save the time of loading checkpointed states.
However, generally the preferred location is not a hard requirement and it is still possible that Spark schedules tasks to the executors other than the preferred ones.
In this case, Spark will load state store providers from checkpointed states on new executors. The state store providers run in the previous batch will not be unloaded immediately.
Spark runs a maintenance task which checks and unloads the state store providers that are inactive on the executors.
By changing the Spark configurations related to task scheduling, for example spark.locality.wait, users can configure Spark how long to wait to launch a data-local task.
For stateful operations in Structured Streaming, it can be used to let state store providers running on the same executors across batches.
Specifically for built-in HDFS state store provider, users can check the state store metrics such as loadedMapCacheHitCount and loadedMapCacheMissCount. Ideally,
it is best if cache missing count is minimized that means Spark won’t waste too much time on loading checkpointed state.
User can increase Spark locality waiting configurations to avoid loading state store providers in different executors across batches.
Starting Streaming Queries
Once you have defined the final result DataFrame/Dataset, all that is left is for you to start the streaming computation. To do that, you have to use the DataStreamWriter
(Scala/Java/Python docs)
returned through Dataset.writeStream(). You will have to specify one or more of the following in this interface.


Details of the output sink: Data format, location, etc.


Output mode: Specify what gets written to the output sink.


Query name: Optionally, specify a unique name of the query for identification.


Trigger interval: Optionally, specify the trigger interval. If it is not specified, the system will check for availability of new data as soon as the previous processing has been completed. If a trigger time is missed because the previous processing has not been completed, then the system will trigger processing immediately.


Checkpoint location: For some output sinks where the end-to-end fault-tolerance can be guaranteed, specify the location where the system will write all the checkpoint information. This should be a directory in an HDFS-compatible fault-tolerant file system. The semantics of checkpointing is discussed in more detail in the next section.


Output Modes
There are a few types of output modes.


Append mode (default) - This is the default mode, where only the
new rows added to the Result Table since the last trigger will be
outputted to the sink. This is supported for only those queries where
rows added to the Result Table is never going to change. Hence, this mode
guarantees that each row will be output only once (assuming
fault-tolerant sink). For example, queries with only select,
where, map, flatMap, filter, join, etc. will support Append mode.


Complete mode - The whole Result Table will be outputted to the sink after every trigger.
 This is supported for aggregation queries.


Update mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were
updated since the last trigger will be outputted to the sink.
More information to be added in future releases.


Different types of streaming queries support different output modes.
Here is the compatibility matrix.



Query Type

Supported Output Modes
Notes



Queries with aggregation
Aggregation on event-time with watermark
Append, Update, Complete

        Append mode uses watermark to drop old aggregation state. But the output of a
        windowed aggregation is delayed the late threshold specified in withWatermark() as by
        the modes semantics, rows can be added to the Result Table only once after they are
        finalized (i.e. after watermark is crossed). See the
        Late Data section for more details.
        
        Update mode uses watermark to drop old aggregation state.
        
        Complete mode does not drop old aggregation state since by definition this mode
        preserves all data in the Result Table.
    


Other aggregations
Complete, Update

        Since no watermark is defined (only defined in other category),
        old aggregation state is not dropped.
        
        Append mode is not supported as aggregates can update thus violating the semantics of
        this mode.
    


Queries with mapGroupsWithState
Update

      Aggregations not allowed in a query with mapGroupsWithState.
    


Queries with flatMapGroupsWithState
Append operation mode
Append

      Aggregations are allowed after flatMapGroupsWithState.
    


Update operation mode
Update

      Aggregations not allowed in a query with flatMapGroupsWithState.
    


Queries with joins
Append

        Update and Complete mode not supported yet. See the
        support matrix in the Join Operations section
         for more details on what types of joins are supported.
      


Other queries
Append, Update

      Complete mode not supported as it is infeasible to keep all unaggregated data in the Result Table.
    








Output Sinks
There are a few types of built-in output sinks.

File sink - Stores the output to a directory.

writeStream
    .format("parquet")        // can be "orc", "json", "csv", etc.
    .option("path", "path/to/destination/dir")
    .start()

Kafka sink - Stores the output to one or more topics in Kafka.

writeStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
    .option("topic", "updates")
    .start()

Foreach sink - Runs arbitrary computation on the records in the output. See later in the section for more details.

writeStream
    .foreach(...)
    .start()

Console sink (for debugging) - Prints the output to the console/stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.

writeStream
    .format("console")
    .start()

Memory sink (for debugging) - The output is stored in memory as an in-memory table.
Both, Append and Complete output modes, are supported. This should be used for debugging purposes
on low data volumes as the entire output is collected and stored in the driver’s memory.
Hence, use it with caution.

writeStream
    .format("memory")
    .queryName("tableName")
    .start()
Some sinks are not fault-tolerant because they do not guarantee persistence of the output and are
meant for debugging purposes only. See the earlier section on
fault-tolerance semantics.
Here are the details of all the sinks in Spark.



Sink
Supported Output Modes
Options
Fault-tolerant
Notes



File Sink
Append

path: path to the output directory, must be specified.
retention: time to live (TTL) for output files. Output files which batches were
        committed older than TTL will be eventually excluded in metadata log. This means reader queries which read
        the sink's output directory may not process them. You can provide the value as string format of the time. (like "12h", "7d", etc.)
        By default it's disabled.
        
        For file-format-specific options, see the related methods in DataFrameWriter
        (Scala/Java/Python/R).
        E.g. for "parquet" format options see DataFrameWriter.parquet()

Yes (exactly-once)
Supports writes to partitioned tables. Partitioning by time may be useful.


Kafka Sink
Append, Update, Complete
See the Kafka Integration Guide
Yes (at-least-once)
More details in the Kafka Integration Guide


Foreach Sink
Append, Update, Complete
None
Yes (at-least-once)
More details in the next section


ForeachBatch Sink
Append, Update, Complete
None
Depends on the implementation
More details in the next section


Console Sink
Append, Update, Complete

numRows: Number of rows to print every trigger (default: 20)
        
truncate: Whether to truncate the output if too long (default: true)
    
No



Memory Sink
Append, Complete
None
No. But in Complete Mode, restarted query will recreate the full table.
Table name is the query name.









Note that you have to call start() to actually start the execution of the query. This returns a StreamingQuery object which is a handle to the continuously running execution. You can use this object to manage the query, which we will discuss in the next subsection. For now, let’s understand all this with a few examples.


# ========== DF with no aggregations ==========
noAggDF = deviceDataDf.select("device").where("signal > 10")

# Print new data to console
noAggDF \
    .writeStream \
    .format("console") \
    .start()

# Write new data to Parquet files
noAggDF \
    .writeStream \
    .format("parquet") \
    .option("checkpointLocation", "path/to/checkpoint/dir") \
    .option("path", "path/to/destination/dir") \
    .start()

# ========== DF with aggregation ==========
aggDF = df.groupBy("device").count()

# Print updated aggregations to console
aggDF \
    .writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

# Have all the aggregates in an in-memory table. The query name will be the table name
aggDF \
    .writeStream \
    .queryName("aggregates") \
    .outputMode("complete") \
    .format("memory") \
    .start()

spark.sql("select * from aggregates").show()   # interactively query in-memory table


// ========== DF with no aggregations ==========
val noAggDF = deviceDataDf.select("device").where("signal > 10")

// Print new data to console
noAggDF
  .writeStream
  .format("console")
  .start()

// Write new data to Parquet files
noAggDF
  .writeStream
  .format("parquet")
  .option("checkpointLocation", "path/to/checkpoint/dir")
  .option("path", "path/to/destination/dir")
  .start()

// ========== DF with aggregation ==========
val aggDF = df.groupBy("device").count()

// Print updated aggregations to console
aggDF
  .writeStream
  .outputMode("complete")
  .format("console")
  .start()

// Have all the aggregates in an in-memory table
aggDF
  .writeStream
  .queryName("aggregates")    // this query name will be the table name
  .outputMode("complete")
  .format("memory")
  .start()

spark.sql("select * from aggregates").show()   // interactively query in-memory table


// ========== DF with no aggregations ==========
Dataset<Row> noAggDF = deviceDataDf.select("device").where("signal > 10");

// Print new data to console
noAggDF
  .writeStream()
  .format("console")
  .start();

// Write new data to Parquet files
noAggDF
  .writeStream()
  .format("parquet")
  .option("checkpointLocation", "path/to/checkpoint/dir")
  .option("path", "path/to/destination/dir")
  .start();

// ========== DF with aggregation ==========
Dataset<Row> aggDF = df.groupBy("device").count();

// Print updated aggregations to console
aggDF
  .writeStream()
  .outputMode("complete")
  .format("console")
  .start();

// Have all the aggregates in an in-memory table
aggDF
  .writeStream()
  .queryName("aggregates")    // this query name will be the table name
  .outputMode("complete")
  .format("memory")
  .start();

spark.sql("select * from aggregates").show();   // interactively query in-memory table


# ========== DF with no aggregations ==========
noAggDF <- select(where(deviceDataDf, "signal > 10"), "device")

# Print new data to console
write.stream(noAggDF, "console")

# Write new data to Parquet files
write.stream(noAggDF,
             "parquet",
             path = "path/to/destination/dir",
             checkpointLocation = "path/to/checkpoint/dir")

# ========== DF with aggregation ==========
aggDF <- count(groupBy(df, "device"))

# Print updated aggregations to console
write.stream(aggDF, "console", outputMode = "complete")

# Have all the aggregates in an in memory table. The query name will be the table name
write.stream(aggDF, "memory", queryName = "aggregates", outputMode = "complete")

# Interactively query in-memory table
head(sql("select * from aggregates"))


Using Foreach and ForeachBatch
The foreach and foreachBatch operations allow you to apply arbitrary operations and writing
logic on the output of a streaming query. They have slightly different use cases - while foreach
allows custom write logic on every row, foreachBatch allows arbitrary operations
and custom logic on the output of each micro-batch. Let’s understand their usages in more detail.
ForeachBatch
foreachBatch(...) allows you to specify a function that is executed on
the output data of every micro-batch of a streaming query. Since Spark 2.4, this is supported in Scala, Java and Python.
It takes two parameters: a DataFrame or Dataset that has the output data of a micro-batch and the unique ID of the micro-batch.


def foreach_batch_function(df, epoch_id):
    # Transform and write batchDF
    pass

streamingDF.writeStream.foreachBatch(foreach_batch_function).start()


streamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>
  // Transform and write batchDF
}.start()


streamingDatasetOfString.writeStream().foreachBatch(
  new VoidFunction2<Dataset<String>, Long>() {
    public void call(Dataset<String> dataset, Long batchId) {
      // Transform and write batchDF
    }
  }
).start();


R is not yet supported.


With foreachBatch, you can do the following.

Reuse existing batch data sources - For many storage systems, there may not be a streaming sink available yet,
but there may already exist a data writer for batch queries. Using foreachBatch, you can use the batch
data writers on the output of each micro-batch.
Write to multiple locations - If you want to write the output of a streaming query to multiple locations,
then you can simply write the output DataFrame/Dataset multiple times. However, each attempt to write can
cause the output data to be recomputed (including possible re-reading of the input data). To avoid recomputations,
you should cache the output DataFrame/Dataset, write it to multiple locations, and then uncache it. Here is an outline.



streamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>
  batchDF.persist()
  batchDF.write.format(...).save(...)  // location 1
  batchDF.write.format(...).save(...)  // location 2
  batchDF.unpersist()
}



Apply additional DataFrame operations - Many DataFrame and Dataset operations are not supported
in streaming DataFrames because Spark does not support generating incremental plans in those cases.
Using foreachBatch, you can apply some of these operations on each micro-batch output. However, you will have to reason about the end-to-end semantics of doing that operation yourself.

Note:

By default, foreachBatch provides only at-least-once write guarantees. However, you can use the
batchId provided to the function as way to deduplicate the output and get an exactly-once guarantee.
foreachBatch does not work with the continuous processing mode as it fundamentally relies on the
micro-batch execution of a streaming query. If you write data in the continuous mode, use foreach instead.

Foreach
If foreachBatch is not an option (for example, corresponding batch data writer does not exist, or
continuous processing mode), then you can express your custom writer logic using foreach.
Specifically, you can express the data writing logic by dividing it into three methods: open, process, and close.
Since Spark 2.4, foreach is available in Scala, Java and Python.


In Python, you can invoke foreach in two ways: in a function or in an object.
The function offers a simple way to express your processing logic but does not allow you to
deduplicate generated data when failures cause reprocessing of some input data.
For that situation you must specify the processing logic in an object.

First, the function takes a row as input.

def process_row(row):
    # Write row to storage
    pass

query = streamingDF.writeStream.foreach(process_row).start()

Second, the object has a process method and optional open and close methods:

class ForeachWriter:
    def open(self, partition_id, epoch_id):
        # Open connection. This method is optional in Python.
        pass

    def process(self, row):
        # Write row to connection. This method is NOT optional in Python.
        pass

    def close(self, error):
        # Close the connection. This method in optional in Python.
        pass

query = streamingDF.writeStream.foreach(ForeachWriter()).start()


In Scala, you have to extend the class ForeachWriter (docs).
streamingDatasetOfString.writeStream.foreach(
  new ForeachWriter[String] {

    def open(partitionId: Long, version: Long): Boolean = {
      // Open connection
    }

    def process(record: String): Unit = {
      // Write string to connection
    }

    def close(errorOrNull: Throwable): Unit = {
      // Close the connection
    }
  }
).start()


In Java, you have to extend the class ForeachWriter (docs).
streamingDatasetOfString.writeStream().foreach(
  new ForeachWriter<String>() {

    @Override public boolean open(long partitionId, long version) {
      // Open connection
    }

    @Override public void process(String record) {
      // Write string to connection
    }

    @Override public void close(Throwable errorOrNull) {
      // Close the connection
    }
  }
).start();


R is not yet supported.


Execution semantics
When the streaming query is started, Spark calls the function or the object’s methods in the following way:


A single copy of this object is responsible for all the data generated by a single task in a query.
In other words, one instance is responsible for processing one partition of the data generated in a distributed manner.


This object must be serializable, because each task will get a fresh serialized-deserialized copy
of the provided object. Hence, it is strongly recommended that any initialization for writing data
(for example. opening a connection or starting a transaction) is done after the open() method has
been called, which signifies that the task is ready to generate data.


The lifecycle of the methods are as follows:


For each partition with partition_id:


For each batch/epoch of streaming data with epoch_id:


Method open(partitionId, epochId) is called.


If open(…) returns true, for each row in the partition and batch/epoch, method process(row) is called.


Method close(error) is called with error (if any) seen while processing rows.








The close() method (if it exists) is called if an open() method exists and returns successfully (irrespective of the return value), except if the JVM or Python process crashes in the middle.


Note: Spark does not guarantee same output for (partitionId, epochId), so deduplication
cannot be achieved with (partitionId, epochId). e.g. source provides different number of
partitions for some reasons, Spark optimization changes number of partitions, etc.
See SPARK-28650 for more details.
If you need deduplication on output, try out foreachBatch instead.


Streaming Table APIs
Since Spark 3.1, you can also use DataStreamReader.table() to read tables as streaming DataFrames and use DataStreamWriter.toTable() to write streaming DataFrames as tables:


spark = ...  # spark session

# Create a streaming DataFrame
df = spark.readStream \
    .format("rate") \
    .option("rowsPerSecond", 10) \
    .load()

# Write the streaming DataFrame to a table
df.writeStream \
    .option("checkpointLocation", "path/to/checkpoint/dir") \
    .toTable("myTable")

# Check the table result
spark.read.table("myTable").show()

# Transform the source dataset and write to a new table
spark.readStream \
    .table("myTable") \
    .select("value") \
    .writeStream \
    .option("checkpointLocation", "path/to/checkpoint/dir") \
    .format("parquet") \
    .toTable("newTable")

# Check the new table result
spark.read.table("newTable").show()


val spark: SparkSession = ...

// Create a streaming DataFrame
val df = spark.readStream
  .format("rate")
  .option("rowsPerSecond", 10)
  .load()

// Write the streaming DataFrame to a table
df.writeStream
  .option("checkpointLocation", "path/to/checkpoint/dir")
  .toTable("myTable")

// Check the table result
spark.read.table("myTable").show()

// Transform the source dataset and write to a new table
spark.readStream
  .table("myTable")
  .select("value")
  .writeStream
  .option("checkpointLocation", "path/to/checkpoint/dir")
  .format("parquet")
  .toTable("newTable")

// Check the new table result
spark.read.table("newTable").show()


SparkSession spark = ...

// Create a streaming DataFrame
Dataset<Row> df = spark.readStream()
  .format("rate")
  .option("rowsPerSecond", 10)
  .load();

// Write the streaming DataFrame to a table
df.writeStream()
  .option("checkpointLocation", "path/to/checkpoint/dir")
  .toTable("myTable");

// Check the table result
spark.read().table("myTable").show();

// Transform the source dataset and write to a new table
spark.readStream()
  .table("myTable")
  .select("value")
  .writeStream()
  .option("checkpointLocation", "path/to/checkpoint/dir")
  .format("parquet")
  .toTable("newTable");

// Check the new table result
spark.read().table("newTable").show();


Not available in R.


For more details, please check the docs for DataStreamReader (Scala/Java/Python docs) and DataStreamWriter (Scala/Java/Python docs).
Triggers
The trigger settings of a streaming query define the timing of streaming data processing, whether
the query is going to be executed as micro-batch query with a fixed batch interval or as a continuous processing query.
Here are the different kinds of triggers that are supported.



Trigger Type
Description



unspecified (default)

        If no trigger setting is explicitly specified, then by default, the query will be
        executed in micro-batch mode, where micro-batches will be generated as soon as
        the previous micro-batch has completed processing.
    


Fixed interval micro-batches

        The query will be executed with micro-batches mode, where micro-batches will be kicked off
        at the user-specified intervals.
        
If the previous micro-batch completes within the interval, then the engine will wait until
          the interval is over before kicking off the next micro-batch.
If the previous micro-batch takes longer than the interval to complete (i.e. if an
          interval boundary is missed), then the next micro-batch will start as soon as the
          previous one completes (i.e., it will not wait for the next interval boundary).
If no new data is available, then no micro-batch will be kicked off.




One-time micro-batch(deprecated)

        The query will execute only one micro-batch to process all the available data and then
        stop on its own. This is useful in scenarios you want to periodically spin up a cluster,
        process everything that is available since the last period, and then shutdown the
        cluster. In some case, this may lead to significant cost savings.
        Note that this trigger is deprecated and users are encouraged to migrate to Available-now micro-batch,
        as it provides the better guarantee of processing, fine-grained scale of batches, and better gradual processing
        of watermark advancement including no-data batch.
    


Available-now micro-batch

        Similar to queries one-time micro-batch trigger, the query will process all the available data and then
        stop on its own. The difference is that, it will process the data in (possibly) multiple micro-batches
        based on the source options (e.g. maxFilesPerTrigger for file source), which will result
        in better query scalability.
        
This trigger provides a strong guarantee of processing: regardless of how many batches were
                left over in previous run, it ensures all available data at the time of execution gets
                processed before termination. All uncommitted batches will be processed first.
Watermark gets advanced per each batch, and no-data batch gets executed before termination
                if the last batch advances the watermark. This helps to maintain smaller and predictable
                state size and smaller latency on the output of stateful operators.




Continuous with fixed checkpoint interval(experimental)

        The query will be executed in the new low-latency, continuous processing mode. Read more
        about this in the Continuous Processing section below.
    


Here are a few code examples.


# Default trigger (runs micro-batch as soon as it can)
df.writeStream \
  .format("console") \
  .start()

# ProcessingTime trigger with two-seconds micro-batch interval
df.writeStream \
  .format("console") \
  .trigger(processingTime='2 seconds') \
  .start()

# One-time trigger (Deprecated, encouraged to use Available-now trigger)
df.writeStream \
  .format("console") \
  .trigger(once=True) \
  .start()

# Available-now trigger
df.writeStream \
  .format("console") \
  .trigger(availableNow=True) \
  .start()

# Continuous trigger with one-second checkpointing interval
df.writeStream
  .format("console")
  .trigger(continuous='1 second')
  .start()


import org.apache.spark.sql.streaming.Trigger

// Default trigger (runs micro-batch as soon as it can)
df.writeStream
  .format("console")
  .start()

// ProcessingTime trigger with two-seconds micro-batch interval
df.writeStream
  .format("console")
  .trigger(Trigger.ProcessingTime("2 seconds"))
  .start()

// One-time trigger (Deprecated, encouraged to use Available-now trigger)
df.writeStream
  .format("console")
  .trigger(Trigger.Once())
  .start()

// Available-now trigger
df.writeStream
  .format("console")
  .trigger(Trigger.AvailableNow())
  .start()

// Continuous trigger with one-second checkpointing interval
df.writeStream
  .format("console")
  .trigger(Trigger.Continuous("1 second"))
  .start()


import org.apache.spark.sql.streaming.Trigger

// Default trigger (runs micro-batch as soon as it can)
df.writeStream
  .format("console")
  .start();

// ProcessingTime trigger with two-seconds micro-batch interval
df.writeStream
  .format("console")
  .trigger(Trigger.ProcessingTime("2 seconds"))
  .start();

// One-time trigger (Deprecated, encouraged to use Available-now trigger)
df.writeStream
  .format("console")
  .trigger(Trigger.Once())
  .start();

// Available-now trigger
df.writeStream
  .format("console")
  .trigger(Trigger.AvailableNow())
  .start();

// Continuous trigger with one-second checkpointing interval
df.writeStream
  .format("console")
  .trigger(Trigger.Continuous("1 second"))
  .start();


# Default trigger (runs micro-batch as soon as it can)
write.stream(df, "console")

# ProcessingTime trigger with two-seconds micro-batch interval
write.stream(df, "console", trigger.processingTime = "2 seconds")

# One-time trigger
write.stream(df, "console", trigger.once = TRUE)

# Continuous trigger is not yet supported


Managing Streaming Queries
The StreamingQuery object created when a query is started can be used to monitor and manage the query.


query = df.writeStream.format("console").start()   # get the query object

query.id()          # get the unique identifier of the running query that persists across restarts from checkpoint data

query.runId()       # get the unique id of this run of the query, which will be generated at every start/restart

query.name()        # get the name of the auto-generated or user-specified name

query.explain()   # print detailed explanations of the query

query.stop()      # stop the query

query.awaitTermination()   # block until query is terminated, with stop() or with error

query.exception()       # the exception if the query has been terminated with error

query.recentProgress  # a list of the most recent progress updates for this query

query.lastProgress    # the most recent progress update of this streaming query


val query = df.writeStream.format("console").start()   // get the query object

query.id          // get the unique identifier of the running query that persists across restarts from checkpoint data

query.runId       // get the unique id of this run of the query, which will be generated at every start/restart

query.name        // get the name of the auto-generated or user-specified name

query.explain()   // print detailed explanations of the query

query.stop()      // stop the query

query.awaitTermination()   // block until query is terminated, with stop() or with error

query.exception       // the exception if the query has been terminated with error

query.recentProgress  // an array of the most recent progress updates for this query

query.lastProgress    // the most recent progress update of this streaming query


StreamingQuery query = df.writeStream().format("console").start();   // get the query object

query.id();          // get the unique identifier of the running query that persists across restarts from checkpoint data

query.runId();       // get the unique id of this run of the query, which will be generated at every start/restart

query.name();        // get the name of the auto-generated or user-specified name

query.explain();   // print detailed explanations of the query

query.stop();      // stop the query

query.awaitTermination();   // block until query is terminated, with stop() or with error

query.exception();       // the exception if the query has been terminated with error

query.recentProgress();  // an array of the most recent progress updates for this query

query.lastProgress();    // the most recent progress update of this streaming query


query <- write.stream(df, "console")  # get the query object

queryName(query)          # get the name of the auto-generated or user-specified name

explain(query)            # print detailed explanations of the query

stopQuery(query)          # stop the query

awaitTermination(query)   # block until query is terminated, with stop() or with error

lastProgress(query)       # the most recent progress update of this streaming query


You can start any number of queries in a single SparkSession. They will all be running concurrently sharing the cluster resources. You can use sparkSession.streams() to get the StreamingQueryManager
(Scala/Java/Python docs)
that can be used to manage the currently active queries.


spark = ...  # spark session

spark.streams.active  # get the list of currently active streaming queries

spark.streams.get(id)  # get a query object by its unique id

spark.streams.awaitAnyTermination()  # block until any one of them terminates


val spark: SparkSession = ...

spark.streams.active    // get the list of currently active streaming queries

spark.streams.get(id)   // get a query object by its unique id

spark.streams.awaitAnyTermination()   // block until any one of them terminates


SparkSession spark = ...

spark.streams().active();    // get the list of currently active streaming queries

spark.streams().get(id);   // get a query object by its unique id

spark.streams().awaitAnyTermination();   // block until any one of them terminates


Not available in R.


Monitoring Streaming Queries
There are multiple ways to monitor active streaming queries. You can either push metrics to external systems using Spark’s Dropwizard Metrics support, or access them programmatically.
Reading Metrics Interactively
You can directly get the current status and metrics of an active query using
streamingQuery.lastProgress() and streamingQuery.status().
lastProgress() returns a StreamingQueryProgress object
in Scala
and Java
and a dictionary with the same fields in Python. It has all the information about
the progress made in the last trigger of the stream - what data was processed,
what were the processing rates, latencies, etc. There is also
streamingQuery.recentProgress which returns an array of last few progresses.
In addition, streamingQuery.status() returns a StreamingQueryStatus object
in Scala
and Java
and a dictionary with the same fields in Python. It gives information about
what the query is immediately doing - is a trigger active, is data being processed, etc.
Here are a few examples.


query = ...  # a StreamingQuery
print(query.lastProgress)

'''
Will print something like the following.

{u'stateOperators': [], u'eventTime': {u'watermark': u'2016-12-14T18:45:24.873Z'}, u'name': u'MyQuery', u'timestamp': u'2016-12-14T18:45:24.873Z', u'processedRowsPerSecond': 200.0, u'inputRowsPerSecond': 120.0, u'numInputRows': 10, u'sources': [{u'description': u'KafkaSource[Subscribe[topic-0]]', u'endOffset': {u'topic-0': {u'1': 134, u'0': 534, u'3': 21, u'2': 0, u'4': 115}}, u'processedRowsPerSecond': 200.0, u'inputRowsPerSecond': 120.0, u'numInputRows': 10, u'startOffset': {u'topic-0': {u'1': 1, u'0': 1, u'3': 1, u'2': 0, u'4': 1}}}], u'durationMs': {u'getOffset': 2, u'triggerExecution': 3}, u'runId': u'88e2ff94-ede0-45a8-b687-6316fbef529a', u'id': u'ce011fdc-8762-4dcb-84eb-a77333e28109', u'sink': {u'description': u'MemorySink'}}
'''

print(query.status)
'''
Will print something like the following.

{u'message': u'Waiting for data to arrive', u'isTriggerActive': False, u'isDataAvailable': False}
'''


val query: StreamingQuery = ...

println(query.lastProgress)

/* Will print something like the following.

{
  "id" : "ce011fdc-8762-4dcb-84eb-a77333e28109",
  "runId" : "88e2ff94-ede0-45a8-b687-6316fbef529a",
  "name" : "MyQuery",
  "timestamp" : "2016-12-14T18:45:24.873Z",
  "numInputRows" : 10,
  "inputRowsPerSecond" : 120.0,
  "processedRowsPerSecond" : 200.0,
  "durationMs" : {
    "triggerExecution" : 3,
    "getOffset" : 2
  },
  "eventTime" : {
    "watermark" : "2016-12-14T18:45:24.873Z"
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaSource[Subscribe[topic-0]]",
    "startOffset" : {
      "topic-0" : {
        "2" : 0,
        "4" : 1,
        "1" : 1,
        "3" : 1,
        "0" : 1
      }
    },
    "endOffset" : {
      "topic-0" : {
        "2" : 0,
        "4" : 115,
        "1" : 134,
        "3" : 21,
        "0" : 534
      }
    },
    "numInputRows" : 10,
    "inputRowsPerSecond" : 120.0,
    "processedRowsPerSecond" : 200.0
  } ],
  "sink" : {
    "description" : "MemorySink"
  }
}
*/


println(query.status)

/*  Will print something like the following.
{
  "message" : "Waiting for data to arrive",
  "isDataAvailable" : false,
  "isTriggerActive" : false
}
*/


StreamingQuery query = ...

System.out.println(query.lastProgress());
/* Will print something like the following.

{
  "id" : "ce011fdc-8762-4dcb-84eb-a77333e28109",
  "runId" : "88e2ff94-ede0-45a8-b687-6316fbef529a",
  "name" : "MyQuery",
  "timestamp" : "2016-12-14T18:45:24.873Z",
  "numInputRows" : 10,
  "inputRowsPerSecond" : 120.0,
  "processedRowsPerSecond" : 200.0,
  "durationMs" : {
    "triggerExecution" : 3,
    "getOffset" : 2
  },
  "eventTime" : {
    "watermark" : "2016-12-14T18:45:24.873Z"
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaSource[Subscribe[topic-0]]",
    "startOffset" : {
      "topic-0" : {
        "2" : 0,
        "4" : 1,
        "1" : 1,
        "3" : 1,
        "0" : 1
      }
    },
    "endOffset" : {
      "topic-0" : {
        "2" : 0,
        "4" : 115,
        "1" : 134,
        "3" : 21,
        "0" : 534
      }
    },
    "numInputRows" : 10,
    "inputRowsPerSecond" : 120.0,
    "processedRowsPerSecond" : 200.0
  } ],
  "sink" : {
    "description" : "MemorySink"
  }
}
*/


System.out.println(query.status());
/*  Will print something like the following.
{
  "message" : "Waiting for data to arrive",
  "isDataAvailable" : false,
  "isTriggerActive" : false
}
*/


query <- ...  # a StreamingQuery
lastProgress(query)

'''
Will print something like the following.

{
  "id" : "8c57e1ec-94b5-4c99-b100-f694162df0b9",
  "runId" : "ae505c5a-a64e-4896-8c28-c7cbaf926f16",
  "name" : null,
  "timestamp" : "2017-04-26T08:27:28.835Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 0,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 4,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "TextSocketSource[host: localhost, port: 9999]",
    "startOffset" : 1,
    "endOffset" : 1,
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@76b37531"
  }
}
'''

status(query)
'''
Will print something like the following.

{
  "message" : "Waiting for data to arrive",
  "isDataAvailable" : false,
  "isTriggerActive" : false
}
'''


Reporting Metrics programmatically using Asynchronous APIs
You can also asynchronously monitor all queries associated with a
SparkSession by attaching a StreamingQueryListener
(Scala/Java/Python docs).
Once you attach your custom StreamingQueryListener object with
sparkSession.streams.addListener(), you will get callbacks when a query is started and
stopped and when there is progress made in an active query. Here is an example,


spark = ...

class Listener(StreamingQueryListener):
    def onQueryStarted(self, event):
        print("Query started: " + queryStarted.id)

    def onQueryProgress(self, event):
        print("Query made progress: " + queryProgress.progress)

    def onQueryTerminated(self, event):
    	print("Query terminated: " + queryTerminated.id)


spark.streams.addListener(Listener())


val spark: SparkSession = ...

spark.streams.addListener(new StreamingQueryListener() {
    override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {
        println("Query started: " + queryStarted.id)
    }
    override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {
        println("Query terminated: " + queryTerminated.id)
    }
    override def onQueryProgress(queryProgress: QueryProgressEvent): Unit = {
        println("Query made progress: " + queryProgress.progress)
    }
})


SparkSession spark = ...

spark.streams().addListener(new StreamingQueryListener() {
    @Override
    public void onQueryStarted(QueryStartedEvent queryStarted) {
        System.out.println("Query started: " + queryStarted.id());
    }
    @Override
    public void onQueryTerminated(QueryTerminatedEvent queryTerminated) {
        System.out.println("Query terminated: " + queryTerminated.id());
    }
    @Override
    public void onQueryProgress(QueryProgressEvent queryProgress) {
        System.out.println("Query made progress: " + queryProgress.progress());
    }
});


Not available in R.


Reporting Metrics using Dropwizard
Spark supports reporting metrics using the Dropwizard Library. To enable metrics of Structured Streaming queries to be reported as well, you have to explicitly enable the configuration spark.sql.streaming.metricsEnabled in the SparkSession.


spark.conf.set("spark.sql.streaming.metricsEnabled", "true")
# or
spark.sql("SET spark.sql.streaming.metricsEnabled=true")


spark.conf.set("spark.sql.streaming.metricsEnabled", "true")
// or
spark.sql("SET spark.sql.streaming.metricsEnabled=true")


spark.conf().set("spark.sql.streaming.metricsEnabled", "true");
// or
spark.sql("SET spark.sql.streaming.metricsEnabled=true");


sql("SET spark.sql.streaming.metricsEnabled=true")


All queries started in the SparkSession after this configuration has been enabled will report metrics through Dropwizard to whatever sinks have been configured (e.g. Ganglia, Graphite, JMX, etc.).
Recovering from Failures with Checkpointing
In case of a failure or intentional shutdown, you can recover the previous progress and state of a previous query, and continue where it left off. This is done using checkpointing and write-ahead logs. You can configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the quick example) to the checkpoint location. This checkpoint location has to be a path in an HDFS compatible file system, and can be set as an option in the DataStreamWriter when starting a query.


aggDF \
    .writeStream \
    .outputMode("complete") \
    .option("checkpointLocation", "path/to/HDFS/dir") \
    .format("memory") \
    .start()


aggDF
  .writeStream
  .outputMode("complete")
  .option("checkpointLocation", "path/to/HDFS/dir")
  .format("memory")
  .start()


aggDF
  .writeStream()
  .outputMode("complete")
  .option("checkpointLocation", "path/to/HDFS/dir")
  .format("memory")
  .start();


write.stream(aggDF, "memory", outputMode = "complete", checkpointLocation = "path/to/HDFS/dir")


Recovery Semantics after Changes in a Streaming Query
There are limitations on what changes in a streaming query are allowed between restarts from the
same checkpoint location. Here are a few kinds of changes that are either not allowed, or
the effect of the change is not well-defined. For all of them:


The term allowed means you can do the specified change but whether the semantics of its effect
is well-defined depends on the query and the change.


The term not allowed means you should not do the specified change as the restarted query is likely
to fail with unpredictable errors. sdf represents a streaming DataFrame/Dataset
generated with sparkSession.readStream.


Types of changes


Changes in the number or type (i.e. different source) of input sources: This is not allowed.


Changes in the parameters of input sources: Whether this is allowed and whether the semantics
of the change are well-defined depends on the source and the query. Here are a few examples.


Addition/deletion/modification of rate limits is allowed: spark.readStream.format("kafka").option("subscribe", "topic") to spark.readStream.format("kafka").option("subscribe", "topic").option("maxOffsetsPerTrigger", ...)


Changes to subscribed topics/files are generally not allowed as the results are unpredictable: spark.readStream.format("kafka").option("subscribe", "topic") to spark.readStream.format("kafka").option("subscribe", "newTopic")




Changes in the type of output sink: Changes between a few specific combinations of sinks
are allowed. This needs to be verified on a case-by-case basis. Here are a few examples.


File sink to Kafka sink is allowed. Kafka will see only the new data.


Kafka sink to file sink is not allowed.


Kafka sink changed to foreach, or vice versa is allowed.




Changes in the parameters of output sink: Whether this is allowed and whether the semantics of
the change are well-defined depends on the sink and the query. Here are a few examples.


Changes to output directory of a file sink are not allowed: sdf.writeStream.format("parquet").option("path", "/somePath") to sdf.writeStream.format("parquet").option("path", "/anotherPath")


Changes to output topic are allowed: sdf.writeStream.format("kafka").option("topic", "someTopic") to sdf.writeStream.format("kafka").option("topic", "anotherTopic")


Changes to the user-defined foreach sink (that is, the ForeachWriter code) are allowed, but the semantics of the change depends on the code.




Changes in projection / filter / map-like operations: Some cases are allowed. For example:


Addition / deletion of filters is allowed: sdf.selectExpr("a") to sdf.where(...).selectExpr("a").filter(...).


Changes in projections with same output schema are allowed: sdf.selectExpr("stringColumn AS json").writeStream to sdf.selectExpr("anotherStringColumn AS json").writeStream


Changes in projections with different output schema are conditionally allowed: sdf.selectExpr("a").writeStream to sdf.selectExpr("b").writeStream is allowed only if the output sink allows the schema change from "a" to "b".




Changes in stateful operations: Some operations in streaming queries need to maintain
state data in order to continuously update the result. Structured Streaming automatically checkpoints
the state data to fault-tolerant storage (for example, HDFS, AWS S3, Azure Blob storage) and restores it after restart.
However, this assumes that the schema of the state data remains same across restarts. This means that
any changes (that is, additions, deletions, or schema modifications) to the stateful operations of a streaming query are not allowed between restarts.
Here is the list of stateful operations whose schema should not be changed between restarts in order to ensure state recovery:


Streaming aggregation: For example, sdf.groupBy("a").agg(...). Any change in number or type of grouping keys or aggregates is not allowed.


Streaming deduplication: For example, sdf.dropDuplicates("a"). Any change in number or type of deduplicating columns is not allowed.


Stream-stream join: For example, sdf1.join(sdf2, ...) (i.e. both inputs are generated with sparkSession.readStream). Changes
in the schema or equi-joining columns are not allowed. Changes in join type (outer or inner) are not allowed. Other changes in the join condition are ill-defined.


Arbitrary stateful operation: For example, sdf.groupByKey(...).mapGroupsWithState(...) or sdf.groupByKey(...).flatMapGroupsWithState(...).
Any change to the schema of the user-defined state and the type of timeout is not allowed.
Any change within the user-defined state-mapping function are allowed, but the semantic effect of the change depends on the user-defined logic.
If you really want to support state schema changes, then you can explicitly encode/decode your complex state data
structures into bytes using an encoding/decoding scheme that supports schema migration. For example,
if you save your state as Avro-encoded bytes, then you are free to change the Avro-state-schema between query
restarts as the binary state will always be restored successfully.




Asynchronous Progress Tracking
What is it?
Asynchronous progress tracking allows streaming queries to checkpoint progress asynchronously and in parallel to the actual data processing within a micro-batch, reducing latency associated with maintaining the offset log and commit log.

How does it work?
Structured Streaming relies on persisting and managing offsets as progress indicators for query processing. Offset management operation directly impacts processing latency, because no data processing can occur until these operations are complete. Asynchronous progress tracking enables streaming queries to checkpoint progress without being impacted by these offset management operations.
How to use it?
The code snippet below provides an example of how to use this feature:
val stream = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
      .option("subscribe", "in")
      .load()
val query = stream.writeStream
     .format("kafka")
	.option("topic", "out")
     .option("checkpointLocation", "/tmp/checkpoint")
	.option("asyncProgressTrackingEnabled", "true")
     .start()

The table below describes the configurations for this feature and default values associated with them.



Option
Value
Default
Description




asyncProgressTrackingEnabled
true/false
false
enable or disable asynchronous progress tracking


asyncProgressTrackingCheckpointIntervalMs
millisecond
1000
the interval in which we commit offsets and completion commits



Limitations
The initial version of the feature has the following limitations:

Asynchronous progress tracking is only supported in stateless queries using Kafka Sink
Exactly once end-to-end processing will not be supported with this asynchronous progress tracking because offset ranges for batch can be changed in case of failure. Though many sinks, such as Kafka sink, do not support writing exactly once anyways.

Switching the setting off
Turning the async progress tracking off may cause the following exception to be thrown
java.lang.IllegalStateException: batch x doesn't exist

Also the following error message may be printed in the driver logs:
The offset log for batch x doesn't exist, which is required to restart the query from the latest batch x from the offset log. Please ensure there are two subsequent offset logs available for the latest batch via manually deleting the offset file(s). Please also ensure the latest batch for commit log is equal or one batch earlier than the latest batch for offset log.

This is caused by the fact that when async progress tracking is enabled, the framework will not checkpoint progress for every batch as would be done if async progress tracking is not used. To solve this problem simply re-enable “asyncProgressTrackingEnabled” and set “asyncProgressTrackingCheckpointIntervalMs” to 0 and run the streaming query until at least two micro-batches have been processed. Async progress tracking can be now safely disabled and restarting query should proceed normally.
Continuous Processing
[Experimental]
Continuous processing is a new, experimental streaming execution mode introduced in Spark 2.3 that enables low (~1 ms) end-to-end latency with at-least-once fault-tolerance guarantees. Compare this with the default micro-batch processing engine which can achieve exactly-once guarantees but achieve latencies of ~100ms at best. For some types of queries (discussed below), you can choose which mode to execute them in without modifying the application logic (i.e. without changing the DataFrame/Dataset operations).
To run a supported query in continuous processing mode, all you need to do is specify a continuous trigger with the desired checkpoint interval as a parameter. For example,


spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
  .option("subscribe", "topic1") \
  .load() \
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
  .writeStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
  .option("topic", "topic1") \
  .trigger(continuous="1 second") \     # only change in query
  .start()


import org.apache.spark.sql.streaming.Trigger

spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .load()
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("topic", "topic1")
  .trigger(Trigger.Continuous("1 second"))  // only change in query
  .start()


import org.apache.spark.sql.streaming.Trigger;

spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .load()
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("topic", "topic1")
  .trigger(Trigger.Continuous("1 second"))  // only change in query
  .start();


A checkpoint interval of 1 second means that the continuous processing engine will record the progress of the query every second. The resulting checkpoints are in a format compatible with the micro-batch engine, hence any query can be restarted with any trigger. For example, a supported query started with the micro-batch mode can be restarted in continuous mode, and vice versa. Note that any time you switch to continuous mode, you will get at-least-once fault-tolerance guarantees.
Supported Queries
As of Spark 2.4, only the following type of queries are supported in the continuous processing mode.

Operations: Only map-like Dataset/DataFrame operations are supported in continuous mode, that is, only projections (select, map, flatMap, mapPartitions, etc.) and selections (where, filter, etc.).
    
All SQL functions are supported except aggregation functions (since aggregations are not yet supported), current_timestamp() and current_date() (deterministic computations using time is challenging).


Sources:
    
Kafka source: All options are supported.
Rate source: Good for testing. Only options that are supported in the continuous mode are numPartitions and rowsPerSecond.


Sinks:
    
Kafka sink: All options are supported.
Memory sink: Good for debugging.
Console sink: Good for debugging. All options are supported. Note that the console will print every checkpoint interval that you have specified in the continuous trigger.



See Input Sources and Output Sinks sections for more details on them. While the console sink is good for testing, the end-to-end low-latency processing can be best observed with Kafka as the source and sink, as this allows the engine to process the data and make the results available in the output topic within milliseconds of the input data being available in the input topic.
Caveats

Continuous processing engine launches multiple long-running tasks that continuously read data from sources, process it and continuously write to sinks. The number of tasks required by the query depends on how many partitions the query can read from the sources in parallel. Therefore, before starting a continuous processing query, you must ensure there are enough cores in the cluster to all the tasks in parallel. For example, if you are reading from a Kafka topic that has 10 partitions, then the cluster must have at least 10 cores for the query to make progress.
Stopping a continuous processing stream may produce spurious task termination warnings. These can be safely ignored.
There are currently no automatic retries of failed tasks. Any failure will lead to the query being stopped and it needs to be manually restarted from the checkpoint.

Additional Information
Notes

Several configurations are not modifiable after the query has run. To change them, discard the checkpoint and start a new query. These configurations include:
    
spark.sql.shuffle.partitions

This is due to the physical partitioning of state: state is partitioned via applying hash function to key, hence the number of partitions for state should be unchanged.
If you want to run fewer tasks for stateful operations, coalesce would help with avoiding unnecessary repartitioning.
            
After coalesce, the number of (reduced) tasks will be kept unless another shuffle happens.




spark.sql.streaming.stateStore.providerClass: To read the previous state of the query properly, the class of state store provider should be unchanged.
spark.sql.streaming.multipleWatermarkPolicy: Modification of this would lead inconsistent watermark value when query contains multiple watermarks, hence the policy should be unchanged.



Further Reading

See and run the
Scala/Java/Python/R
examples.
    
Instructions on how to run Spark examples


Read about integrating with Kafka in the Structured Streaming Kafka Integration Guide
Read more details about using DataFrames/Datasets in the Spark SQL Programming Guide
Third-party Blog Posts
    
Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1 (Databricks Blog)
Real-Time End-to-End Integration with Apache Kafka in Apache Spark’s Structured Streaming (Databricks Blog)
Event-time Aggregation and Watermarking in Apache Spark’s Structured Streaming (Databricks Blog)



Talks

Spark Summit Europe 2017
    
Easy, Scalable, Fault-tolerant Stream Processing with Structured Streaming in Apache Spark -
Part 1 slides/video, Part 2 slides/video
Deep Dive into Stateful Stream Processing in Structured Streaming - slides/video


Spark Summit 2016
    
A Deep Dive into Structured Streaming - slides/video



Migration Guide
The migration guide is now archived on this page.




















  




Submitting Applications - Spark 3.5.5 Documentation


















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Submitting Applications
The spark-submit script in Spark’s bin directory is used to launch applications on a cluster.
It can use all of Spark’s supported cluster managers
through a uniform interface so you don’t have to configure your application especially for each one.
Bundling Your Application’s Dependencies
If your code depends on other projects, you will need to package them alongside
your application in order to distribute the code to a Spark cluster. To do this,
create an assembly jar (or “uber” jar) containing your code and its dependencies. Both
sbt and
Maven
have assembly plugins. When creating assembly jars, list Spark and Hadoop
as provided dependencies; these need not be bundled since they are provided by
the cluster manager at runtime. Once you have an assembled jar you can call the bin/spark-submit
script as shown here while passing your jar.
For Python, you can use the --py-files argument of spark-submit to add .py, .zip or .egg
files to be distributed with your application. If you depend on multiple Python files we recommend
packaging them into a .zip or .egg. For third-party Python dependencies,
see Python Package Management.
Launching Applications with spark-submit
Once a user application is bundled, it can be launched using the bin/spark-submit script.
This script takes care of setting up the classpath with Spark and its
dependencies, and can support different cluster managers and deploy modes that Spark supports:
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
Some of the commonly used options are:

--class: The entry point for your application (e.g. org.apache.spark.examples.SparkPi)
--master: The master URL for the cluster (e.g. spark://23.195.26.187:7077)
--deploy-mode: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client)  † 
--conf: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. --conf <key>=<value> --conf <key2>=<value2>)
application-jar: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs:// path or a file:// path that is present on all nodes.
application-arguments: Arguments passed to the main method of your main class, if any

† A common deployment strategy is to submit your application from a gateway machine
that is
physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).
In this setup, client mode is appropriate. In client mode, the driver is launched directly
within the spark-submit process which acts as a client to the cluster. The input and
output of the application is attached to the console. Thus, this mode is especially suitable
for applications that involve the REPL (e.g. Spark shell).
Alternatively, if your application is submitted from a machine far from the worker machines (e.g.
locally on your laptop), it is common to use cluster mode to minimize network latency between
the drivers and the executors. Currently, the standalone mode does not support cluster mode for Python
applications.
For Python applications, simply pass a .py file in the place of <application-jar>,
and add Python .zip, .egg or .py files to the search path with --py-files.
There are a few options available that are specific to the
cluster manager that is being used.
For example, with a Spark standalone cluster with cluster deploy mode,
you can also specify --supervise to make sure that the driver is automatically restarted if it
fails with a non-zero exit code. To enumerate all such options available to spark-submit,
run it with --help. Here are a few examples of common options:
# Run application locally on 8 cores
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8] \
  /path/to/examples.jar \
  100

# Run on a Spark standalone cluster in client deploy mode
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a Spark standalone cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a YARN cluster in cluster deploy mode
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 20G \
  --num-executors 50 \
  /path/to/examples.jar \
  1000

# Run a Python application on a Spark standalone cluster
./bin/spark-submit \
  --master spark://207.184.161.138:7077 \
  examples/src/main/python/pi.py \
  1000

# Run on a Mesos cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master mesos://207.184.161.138:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000

# Run on a Kubernetes cluster in cluster deploy mode
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master k8s://xx.yy.zz.ww:443 \
  --deploy-mode cluster \
  --executor-memory 20G \
  --num-executors 50 \
  http://path/to/examples.jar \
  1000
Master URLs
The master URL passed to Spark can be in one of the following formats:

Master URLMeaning
 local  Run Spark locally with one worker thread (i.e. no parallelism at all). 
 local[K]  Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). 
 local[K,F]  Run Spark locally with K worker threads and F maxFailures (see spark.task.maxFailures for an explanation of this variable). 
 local[*]  Run Spark locally with as many worker threads as logical cores on your machine.
 local[*,F]  Run Spark locally with as many worker threads as logical cores on your machine and F maxFailures.
 local-cluster[N,C,M]  Local-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per worker and M MiB of memory per worker.
 spark://HOST:PORT  Connect to the given Spark standalone
        cluster master. The port must be whichever one your master is configured to use, which is 7077 by default.

 spark://HOST1:PORT1,HOST2:PORT2  Connect to the given Spark standalone
        cluster with standby masters with Zookeeper. The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default.

 mesos://HOST:PORT  Connect to the given Mesos cluster.
        The port must be whichever one your is configured to use, which is 5050 by default.
        Or, for a Mesos cluster using ZooKeeper, use mesos://zk://....
        To submit with --deploy-mode cluster, the HOST:PORT should be configured to connect to the MesosClusterDispatcher.

 yarn  Connect to a  YARN  cluster in
        client or cluster mode depending on the value of --deploy-mode.
        The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.

 k8s://HOST:PORT  Connect to a Kubernetes cluster in
        client or cluster mode depending on the value of --deploy-mode.
        The HOST and PORT refer to the Kubernetes API Server.
        It connects using TLS by default. In order to force it to use an unsecured connection, you can use
        k8s://http://HOST:PORT.


Loading Configuration from a File
The spark-submit script can load default Spark configuration values from a
properties file and pass them on to your application. By default, it will read options
from conf/spark-defaults.conf in the Spark directory. For more detail, see the section on
loading default configurations.
Loading default Spark configurations this way can obviate the need for certain flags to
spark-submit. For instance, if the spark.master property is set, you can safely omit the
--master flag from spark-submit. In general, configuration values explicitly set on a
SparkConf take the highest precedence, then flags passed to spark-submit, then values in the
defaults file.
If you are ever unclear where configuration options are coming from, you can print out fine-grained
debugging information by running spark-submit with the --verbose option.
Advanced Dependency Management
When using spark-submit, the application jar along with any jars included with the --jars option
will be automatically transferred to the cluster. URLs supplied after --jars must be separated by commas. That list is included in the driver and executor classpaths. Directory expansion does not work with --jars.
Spark uses the following URL scheme to allow different strategies for disseminating jars:

file: - Absolute paths and file:/ URIs are served by the driver’s HTTP file server, and
every executor pulls the file from the driver HTTP server.
hdfs:, http:, https:, ftp: - these pull down files and JARs from the URI as expected
local: - a URI starting with local:/ is expected to exist as a local file on each worker node.  This
means that no network IO will be incurred, and works well for large files/JARs that are pushed to each worker,
or shared via NFS, GlusterFS, etc.

Note that JARs and files are copied to the working directory for each SparkContext on the executor nodes.
This can use up a significant amount of space over time and will need to be cleaned up. With YARN, cleanup
is handled automatically, and with Spark standalone, automatic cleanup can be configured with the
spark.worker.cleanup.appDataTtl property.
Users may also include any other dependencies by supplying a comma-delimited list of Maven coordinates
with --packages. All transitive dependencies will be handled when using this command. Additional
repositories (or resolvers in SBT) can be added in a comma-delimited fashion with the flag --repositories.
(Note that credentials for password-protected repositories can be supplied in some cases in the repository URI,
such as in https://user:password@host/.... Be careful when supplying credentials this way.)
These commands can be used with pyspark, spark-shell, and spark-submit to include Spark Packages.
For Python, the equivalent --py-files option can be used to distribute .egg, .zip and .py libraries
to executors.
More Information
Once you have deployed your application, the cluster mode overview describes
the components involved in distributed execution, and how to monitor and debug applications.




















  




Tuning - Spark 3.5.5 Documentation



















3.5.5






Overview

Programming Guides

Quick Start
RDDs, Accumulators, Broadcasts Vars
SQL, DataFrames, and Datasets
Structured Streaming
Spark Streaming (DStreams)
MLlib (Machine Learning)
GraphX (Graph Processing)
SparkR (R on Spark)
PySpark (Python on Spark)



API Docs

Scala
Java
Python
R
SQL, Built-in Functions



Deploying

Overview
Submitting Applications

Spark Standalone
Mesos
YARN
Kubernetes



More

Configuration
Monitoring
Tuning Guide
Job Scheduling
Security
Hardware Provisioning
Migration Guide

Building Spark
Contributing to Spark
Third Party Projects











Tuning Spark

Data Serialization
Memory Tuning 
Memory Management Overview
Determining Memory Consumption
Tuning Data Structures
Serialized RDD Storage
Garbage Collection Tuning


Other Considerations 
Level of Parallelism
Parallel Listing on Input Paths
Memory Usage of Reduce Tasks
Broadcasting Large Variables
Data Locality


Summary

Because of the in-memory nature of most Spark computations, Spark programs can be bottlenecked
by any resource in the cluster: CPU, network bandwidth, or memory.
Most often, if the data fits in memory, the bottleneck is network bandwidth, but sometimes, you
also need to do some tuning, such as
storing RDDs in serialized form, to
decrease memory usage.
This guide will cover two main topics: data serialization, which is crucial for good network
performance and can also reduce memory use, and memory tuning. We also sketch several smaller topics.
Data Serialization
Serialization plays an important role in the performance of any distributed application.
Formats that are slow to serialize objects into, or consume a large number of
bytes, will greatly slow down the computation.
Often, this will be the first thing you should tune to optimize a Spark application.
Spark aims to strike a balance between convenience (allowing you to work with any Java type
in your operations) and performance. It provides two serialization libraries:

Java serialization:
By default, Spark serializes objects using Java’s ObjectOutputStream framework, and can work
with any class you create that implements
java.io.Serializable.
You can also control the performance of your serialization more closely by extending
java.io.Externalizable.
Java serialization is flexible but often quite slow, and leads to large
serialized formats for many classes.
Kryo serialization: Spark can also use
the Kryo library (version 4) to serialize objects more quickly. Kryo is significantly
faster and more compact than Java serialization (often as much as 10x), but does not support all
Serializable types and requires you to register the classes you’ll use in the program in advance
for best performance.

You can switch to using Kryo by initializing your job with a SparkConf
and calling conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").
This setting configures the serializer used for not only shuffling data between worker
nodes but also when serializing RDDs to disk.  The only reason Kryo is not the default is because of the custom
registration requirement, but we recommend trying it in any network-intensive application.
Since Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type.
Spark automatically includes Kryo serializers for the many commonly-used core Scala classes covered
in the AllScalaRegistrar from the Twitter chill library.
To register your own custom classes with Kryo, use the registerKryoClasses method.
val conf = new SparkConf().setMaster(...).setAppName(...)
conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))
val sc = new SparkContext(conf)
The Kryo documentation describes more advanced
registration options, such as adding custom serialization code.
If your objects are large, you may also need to increase the spark.kryoserializer.buffer
config. This value needs to be large enough
to hold the largest object you will serialize.
Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store
the full class name with each object, which is wasteful.
Memory Tuning
There are three considerations in tuning memory usage: the amount of memory used by your objects
(you may want your entire dataset to fit in memory), the cost of accessing those objects, and the
overhead of garbage collection (if you have high turnover in terms of objects).
By default, Java objects are fast to access, but can easily consume a factor of 2-5x more space
than the “raw” data inside their fields. This is due to several reasons:

Each distinct Java object has an “object header”, which is about 16 bytes and contains information
such as a pointer to its class. For an object with very little data in it (say one Int field), this
can be bigger than the data.
Java Strings have about 40 bytes of overhead over the raw string data (since they store it in an
array of Chars and keep extra data such as the length), and store each character
as two bytes due to String’s internal usage of UTF-16 encoding. Thus a 10-character string can
easily consume 60 bytes.
Common collection classes, such as HashMap and LinkedList, use linked data structures, where
there is a “wrapper” object for each entry (e.g. Map.Entry). This object not only has a header,
but also pointers (typically 8 bytes each) to the next object in the list.
Collections of primitive types often store them as “boxed” objects such as java.lang.Integer.

This section will start with an overview of memory management in Spark, then discuss specific
strategies the user can take to make more efficient use of memory in his/her application. In
particular, we will describe how to determine the memory usage of your objects, and how to
improve it – either by changing your data structures, or by storing data in a serialized
format. We will then cover tuning Spark’s cache size and the Java garbage collector.
Memory Management Overview
Memory usage in Spark largely falls under one of two categories: execution and storage.
Execution memory refers to that used for computation in shuffles, joins, sorts and aggregations,
while storage memory refers to that used for caching and propagating internal data across the
cluster. In Spark, execution and storage share a unified region (M). When no execution memory is
used, storage can acquire all the available memory and vice versa. Execution may evict storage
if necessary, but only until total storage memory usage falls under a certain threshold (R).
In other words, R describes a subregion within M where cached blocks are never evicted.
Storage may not evict execution due to complexities in implementation.
This design ensures several desirable properties. First, applications that do not use caching
can use the entire space for execution, obviating unnecessary disk spills. Second, applications
that do use caching can reserve a minimum storage space (R) where their data blocks are immune
to being evicted. Lastly, this approach provides reasonable out-of-the-box performance for a
variety of workloads without requiring user expertise of how memory is divided internally.
Although there are two relevant configurations, the typical user should not need to adjust them
as the default values are applicable to most workloads:

spark.memory.fraction expresses the size of M as a fraction of the (JVM heap space - 300MiB)
(default 0.6). The rest of the space (40%) is reserved for user data structures, internal
metadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually
large records.
spark.memory.storageFraction expresses the size of R as a fraction of M (default 0.5).
R is the storage space within M where cached blocks immune to being evicted by execution.

The value of spark.memory.fraction should be set in order to fit this amount of heap space
comfortably within the JVM’s old or “tenured” generation. See the discussion of advanced GC
tuning below for details.
Determining Memory Consumption
The best way to size the amount of memory consumption a dataset will require is to create an RDD, put it
into cache, and look at the “Storage” page in the web UI. The page will tell you how much memory the RDD
is occupying.
To estimate the memory consumption of a particular object, use SizeEstimator’s estimate method.
This is useful for experimenting with different data layouts to trim memory usage, as well as
determining the amount of space a broadcast variable will occupy on each executor heap.
Tuning Data Structures
The first way to reduce memory consumption is to avoid the Java features that add overhead, such as
pointer-based data structures and wrapper objects. There are several ways to do this:

Design your data structures to prefer arrays of objects, and primitive types, instead of the
standard Java or Scala collection classes (e.g. HashMap). The fastutil
library provides convenient collection classes for primitive types that are compatible with the
Java standard library.
Avoid nested structures with a lot of small objects and pointers when possible.
Consider using numeric IDs or enumeration objects instead of strings for keys.
If you have less than 32 GiB of RAM, set the JVM flag -XX:+UseCompressedOops to make pointers be
four bytes instead of eight. You can add these options in
spark-env.sh.

Serialized RDD Storage
When your objects are still too large to efficiently store despite this tuning, a much simpler way
to reduce memory usage is to store them in serialized form, using the serialized StorageLevels in
the RDD persistence API, such as MEMORY_ONLY_SER.
Spark will then store each RDD partition as one large byte array.
The only downside of storing data in serialized form is slower access times, due to having to
deserialize each object on the fly.
We highly recommend using Kryo if you want to cache data in serialized form, as
it leads to much smaller sizes than Java serialization (and certainly than raw Java objects).
Garbage Collection Tuning
JVM garbage collection can be a problem when you have large “churn” in terms of the RDDs
stored by your program. (It is usually not a problem in programs that just read an RDD once
and then run many operations on it.) When Java needs to evict old objects to make room for new ones, it will
need to trace through all your Java objects and find the unused ones. The main point to remember here is
that the cost of garbage collection is proportional to the number of Java objects, so using data
structures with fewer objects (e.g. an array of Ints instead of a LinkedList) greatly lowers
this cost. An even better method is to persist objects in serialized form, as described above: now
there will be only one object (a byte array) per RDD partition. Before trying other
techniques, the first thing to try if GC is a problem is to use serialized caching.
GC can also be a problem due to interference between your tasks’ working memory (the
amount of space needed to run the task) and the RDDs cached on your nodes. We will discuss how to control
the space allocated to the RDD cache to mitigate this.
Measuring the Impact of GC
The first step in GC tuning is to collect statistics on how frequently garbage collection occurs and the amount of
time spent GC. This can be done by adding -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps to the Java options.  (See the configuration guide for info on passing Java options to Spark jobs.)  Next time your Spark job is run, you will see messages printed in the worker’s logs
each time a garbage collection occurs. Note these logs will be on your cluster’s worker nodes (in the stdout files in
their work directories), not on your driver program.
Advanced GC Tuning
To further tune garbage collection, we first need to understand some basic information about memory management in the JVM:


Java Heap space is divided in to two regions Young and Old. The Young generation is meant to hold short-lived objects
while the Old generation is intended for objects with longer lifetimes.


The Young generation is further divided into three regions [Eden, Survivor1, Survivor2].


A simplified description of the garbage collection procedure: When Eden is full, a minor GC is run on Eden and objects
that are alive from Eden and Survivor1 are copied to Survivor2. The Survivor regions are swapped. If an object is old
enough or Survivor2 is full, it is moved to Old. Finally, when Old is close to full, a full GC is invoked.


The goal of GC tuning in Spark is to ensure that only long-lived RDDs are stored in the Old generation and that
the Young generation is sufficiently sized to store short-lived objects. This will help avoid full GCs to collect
temporary objects created during task execution. Some steps which may be useful are:


Check if there are too many garbage collections by collecting GC stats. If a full GC is invoked multiple times
before a task completes, it means that there isn’t enough memory available for executing tasks.


If there are too many minor collections but not many major GCs, allocating more memory for Eden would help. You
can set the size of the Eden to be an over-estimate of how much memory each task will need. If the size of Eden
is determined to be E, then you can set the size of the Young generation using the option -Xmn=4/3*E. (The scaling
up by 4/3 is to account for space used by survivor regions as well.)


In the GC stats that are printed, if the OldGen is close to being full, reduce the amount of
memory used for caching by lowering spark.memory.fraction; it is better to cache fewer
objects than to slow down task execution. Alternatively, consider decreasing the size of
the Young generation. This means lowering -Xmn if you’ve set it as above. If not, try changing the 
value of the JVM’s NewRatio parameter. Many JVMs default this to 2, meaning that the Old generation 
occupies 2/3 of the heap. It should be large enough such that this fraction exceeds spark.memory.fraction.


Try the G1GC garbage collector with -XX:+UseG1GC. It can improve performance in some situations where
garbage collection is a bottleneck. Note that with large executor heap sizes, it may be important to
increase the G1 region size 
with -XX:G1HeapRegionSize.


As an example, if your task is reading data from HDFS, the amount of memory used by the task can be estimated using
the size of the data block read from HDFS. Note that the size of a decompressed block is often 2 or 3 times the
size of the block. So if we wish to have 3 or 4 tasks’ worth of working space, and the HDFS block size is 128 MiB,
we can estimate the size of Eden to be 4*3*128MiB.


Monitor how the frequency and time taken by garbage collection changes with the new settings.


Our experience suggests that the effect of GC tuning depends on your application and the amount of memory available.
There are many more tuning options described online,
but at a high level, managing how frequently full GC takes place can help in reducing the overhead.
GC tuning flags for executors can be specified by setting spark.executor.defaultJavaOptions or spark.executor.extraJavaOptions in
a job’s configuration.
Other Considerations
Level of Parallelism
Clusters will not be fully utilized unless you set the level of parallelism for each operation high
enough. Spark automatically sets the number of “map” tasks to run on each file according to its size
(though you can control it through optional parameters to SparkContext.textFile, etc), and for
distributed “reduce” operations, such as groupByKey and reduceByKey, it uses the largest
parent RDD’s number of partitions. You can pass the level of parallelism as a second argument
(see the spark.PairRDDFunctions documentation),
or set the config property spark.default.parallelism to change the default.
In general, we recommend 2-3 tasks per CPU core in your cluster.
Parallel Listing on Input Paths
Sometimes you may also need to increase directory listing parallelism when job input has large number of directories,
otherwise the process could take a very long time, especially when against object store like S3.
If your job works on RDD with Hadoop input formats (e.g., via SparkContext.sequenceFile), the parallelism is
controlled via spark.hadoop.mapreduce.input.fileinputformat.list-status.num-threads (currently default is 1).
For Spark SQL with file-based data sources, you can tune spark.sql.sources.parallelPartitionDiscovery.threshold and
spark.sql.sources.parallelPartitionDiscovery.parallelism to improve listing parallelism. Please
refer to Spark SQL performance tuning guide for more details.
Memory Usage of Reduce Tasks
Sometimes, you will get an OutOfMemoryError not because your RDDs don’t fit in memory, but because the
working set of one of your tasks, such as one of the reduce tasks in groupByKey, was too large.
Spark’s shuffle operations (sortByKey, groupByKey, reduceByKey, join, etc) build a hash table
within each task to perform the grouping, which can often be large. The simplest fix here is to
increase the level of parallelism, so that each task’s input set is smaller. Spark can efficiently
support tasks as short as 200 ms, because it reuses one executor JVM across many tasks and it has
a low task launching cost, so you can safely increase the level of parallelism to more than the
number of cores in your clusters.
Broadcasting Large Variables
Using the broadcast functionality
available in SparkContext can greatly reduce the size of each serialized task, and the cost
of launching a job over a cluster. If your tasks use any large object from the driver program
inside of them (e.g. a static lookup table), consider turning it into a broadcast variable.
Spark prints the serialized size of each task on the master, so you can look at that to
decide whether your tasks are too large; in general, tasks larger than about 20 KiB are probably
worth optimizing.
Data Locality
Data locality can have a major impact on the performance of Spark jobs.  If data and the code that
operates on it are together, then computation tends to be fast.  But if code and data are separated,
one must move to the other.  Typically, it is faster to ship serialized code from place to place than
a chunk of data because code size is much smaller than data.  Spark builds its scheduling around
this general principle of data locality.
Data locality is how close data is to the code processing it.  There are several levels of
locality based on the data’s current location.  In order from closest to farthest:

PROCESS_LOCAL data is in the same JVM as the running code.  This is the best locality
possible.
NODE_LOCAL data is on the same node.  Examples might be in HDFS on the same node, or in
another executor on the same node.  This is a little slower than PROCESS_LOCAL because the data
has to travel between processes.
NO_PREF data is accessed equally quickly from anywhere and has no locality preference.
RACK_LOCAL data is on the same rack of servers.  Data is on a different server on the same rack
so needs to be sent over the network, typically through a single switch.
ANY data is elsewhere on the network and not in the same rack.

Spark prefers to schedule all tasks at the best locality level, but this is not always possible.  In
situations where there is no unprocessed data on any idle executor, Spark switches to lower locality
levels. There are two options: a) wait until a busy CPU frees up to start a task on data on the same
server, or b) immediately start a new task in a farther away place that requires moving data there.
What Spark typically does is wait a bit in the hopes that a busy CPU frees up.  Once that timeout
expires, it starts moving the data from far away to the free CPU.  The wait timeout for fallback
between each level can be configured individually or all together in one parameter; see the
spark.locality parameters on the configuration page for details.
You should increase these settings if your tasks are long and see poor locality, but the default
usually works well.
Summary
This has been a short guide to point out the main concerns you should know about when tuning a
Spark application – most importantly, data serialization and memory tuning. For most programs,
switching to Kryo serialization and persisting data in serialized form will solve most common
performance issues. Feel free to ask on the
Spark mailing list about other tuning best practices.






















     Third-Party Projects | Apache Spark
    
  























Download



          Libraries
        

SQL and DataFrames
Spark Connect
Spark Streaming
pandas on Spark
MLlib (machine learning)
GraphX (graph)



Third-Party Projects




          Documentation
        

Latest Release
Older Versions and Other Resources
Frequently Asked Questions



Examples



          Community
        

Mailing Lists & Resources
Contributing to Spark
Improvement Proposals (SPIP)

Issue Tracker

Powered By
Project Committers
Project History




          Developers
        

Useful Developer Tools
Versioning Policy
Release Process
Security




          GitHub
        

spark
spark-connect-go
spark-connect-swift
spark-docker
spark-kubernetes-operator
spark-website






          Apache Software Foundation
        

Apache Homepage
License
Sponsorship
Thanks
Security
Event








This page tracks external software projects that supplement Apache Spark and add to its ecosystem.
Popular libraries with PySpark integrations

great-expectations - Always know what to expect from your data
Apache Airflow - A platform to programmatically author, schedule, and monitor workflows
xgboost - Scalable, portable and distributed gradient boosting
shap - A game theoretic approach to explain the output of any machine learning model
python-deequ - Measures data quality in large datasets
datahub - Metadata platform for the modern data stack
dbt-spark - Enables dbt to work with Apache Spark
Hamilton - Enables one to declaratively describe PySpark transformations that helps keep code testable, modular, and logically visualizable.
ScaleDP - An Open-Source Library for Processing Documents using AI/ML in Apache Spark.

Connectors

spark-redshift - Performant Redshift data source for Apache Spark
spark-sql-connector - Apache Spark Connector for SQL Server and Azure SQL
azure-cosmos-spark - Apache Spark Connector for Azure Cosmos DB
azure-event-hubs-spark - Enables continuous data processing with Apache Spark and Azure Event Hubs
azure-kusto-spark - Apache Spark connector for Azure Kusto
mongo-spark - The MongoDB Spark connector
couchbase-spark-connector - The Official Couchbase Spark connector
spark-cassandra-connector - DataStax connector for Apache Spark to Apache Cassandra
elasticsearch-hadoop - Elasticsearch real-time search and analytics natively integrated with Spark
neo4j-spark-connector - Neo4j Connector for Apache Spark
starrocks-connector-for-apache-spark - StarRocks Apache Spark connector
tispark - TiSpark is built for running Apache Spark on top of TiDB/TiKV
spark-pdf - PDF Datasource for Apache Spark

Open table formats

Delta Lake - Storage layer that provides ACID transactions and scalable metadata handling for Apache Spark workloads
Hudi: Upserts, Deletes And Incremental Processing on Big Data
Iceberg - Open table format for analytic datasets

Infrastructure projects

Kyuubi - Apache Kyuubi is a distributed and multi-tenant gateway to provide serverless SQL on data warehouses and lakehouses
REST Job Server for Apache Spark - REST interface for managing and submitting Spark jobs on the same cluster.
Apache Mesos - Cluster management system that supports 
running Spark
Alluxio (née Tachyon) - Memory speed virtual distributed 
storage system that supports running Spark
FiloDB - a Spark integrated analytical/columnar 
database, with in-memory option capable of sub-second concurrent queries
Zeppelin - Multi-purpose notebook which supports 20+ language backends, including Apache Spark
Kubeflow Spark Operator - Kubernetes operator for managing the lifecycle of Apache Spark applications on Kubernetes.
IBM Spectrum Conductor - Cluster management software that integrates with Spark and modern computing frameworks.
MLflow - Open source platform to manage the machine learning lifecycle, including deploying models from diverse machine learning libraries on Apache Spark.
Apache DataFu - A collection of utils and user-defined-functions for working with large scale data in Apache Spark, as well as making Scala-Python interoperability easier.

Applications using Spark

Apache Mahout - Previously on Hadoop MapReduce, 
Mahout has switched to using Spark as the backend
ADAM - A framework and CLI for loading, 
transforming, and analyzing genomic data using Apache Spark
TransmogrifAI - AutoML library for building modular, reusable, strongly typed machine learning workflows on Spark with minimal hand tuning
Natural Language Processing for Apache Spark - A library to provide simple, performant, and accurate NLP annotations for machine learning pipelines
Rumble for Apache Spark - A JSONiq engine to query, with a functional language, large, nested, and heterogeneous JSON datasets that do not fit in dataframes.
Lightning Catalog - A data catalog for running ad-hoc queries, wrangling data by federating enterprise data assets, and building a unified semantic layer with data quality checks.

Performance, monitoring, and debugging tools for Spark

Data Mechanics Delight - Delight is a free, hosted, cross-platform Spark UI alternative backed by an open-source Spark agent. It features new metrics and visualizations to simplify Spark monitoring and performance tuning.
DataFlint - DataFlint is A Spark UI replacement installed via an open-source library, which updates in real-time and alerts on performance issues

Additional language bindings
C# / .NET

Mobius: C# and F# language binding and extensions to Apache Spark

Clojure

Geni - A Clojure dataframe library that runs on Apache Spark with a focus on optimizing the REPL experience.

Julia

Spark.jl

Kotlin

Kotlin for Apache Spark

Adding new projects
To add a project, open a pull request against the spark-website  repository. Add an entry to  this markdown file,  then run jekyll build to generate the HTML too. Include both in your pull request. See the README in this repo for more information.
Note that all project and product names should follow trademark guidelines.



Latest News

Spark 3.5.5 released
(Feb 27, 2025)
Spark 3.5.4 released
(Dec 20, 2024)
Spark 3.4.4 released
(Oct 27, 2024)
Preview release of Spark 4.0
(Sep 26, 2024)

Archive








          Download Spark
        

          Built-in Libraries:
        

SQL and DataFrames
Spark Streaming
MLlib (machine learning)
GraphX (graph)

Third-Party Projects





    Apache Spark, Spark, Apache, the Apache feather logo, and the Apache Spark project logo are either registered
    trademarks or trademarks of The Apache Software Foundation in the United States and other countries.
    See guidance on use of Apache Spark trademarks.
    All other marks mentioned may be trademarks or registered trademarks of their respective owners.
    Copyright © 2018 The Apache Software Foundation, Licensed under the
    Apache License, Version 2.0.
  







